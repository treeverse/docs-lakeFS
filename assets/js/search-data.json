{"0": {
    "doc": "Page not found",
    "title": "Page not found",
    "content": " ",
    "url": "/404.html",
    "relUrl": "/404.html"
  },"1": {
    "doc": "Page not found",
    "title": "We can’t seem to find the page you’re looking for.",
    "content": "Error code: 404 . Back to Docs . ",
    "url": "/404.html#we-cant-seem-to-find-the-page-youre-looking-for",
    "relUrl": "/404.html#we-cant-seem-to-find-the-page-youre-looking-for"
  },"2": {
    "doc": "Add Data",
    "title": "Add Data",
    "content": "In this section we will copy a file into lakeFS. ",
    "url": "/quickstart/add_data.html",
    "relUrl": "/quickstart/add_data.html"
  },"3": {
    "doc": "Add Data",
    "title": "Configuring the AWS CLI",
    "content": "Since lakeFS exposes an S3-compatible API, we can use the AWS CLI to operate on it. | If you don’t have the AWS CLI installed, follow the instructions here. | Configure a new connection profile using the lakeFS credentials we generated earlier: . aws configure --profile local # fill in the lakeFS credentials generated earlier: # AWS Access Key ID [None]: AKIAJVHTOKZWGCD2QQYQ # AWS Secret Access Key [None]: **************************************** # Default region name [None]: # Default output format [None]: . | Let’s test to see that it works. We’ll do that by calling s3 ls which should list our repositories for us: . aws --endpoint-url=http://localhost:8000 --profile local s3 ls # output: # 2021-06-15 13:43:03 example-repo . Note the usage of the --endpoint-url flag, which tells the AWS CLI to connect to lakeFS instead of AWS S3. | Great, now let’s copy some files. We’ll write to the main branch. This is done by prefixing our path with the name of the branch we’d like to read/write from: . aws --endpoint-url=http://localhost:8000 --profile local s3 cp ./foo.txt s3://example-repo/main/ # output: # upload: ./foo.txt to s3://example-repo/main/foo.txt . | Back in the lakeFS UI, we are able to see our file in the Uncommitted Changes tab: . | . Next steps . It’s time to commit your changes using the lakeFS CLI. ",
    "url": "/quickstart/add_data.html#configuring-the-aws-cli",
    "relUrl": "/quickstart/add_data.html#configuring-the-aws-cli"
  },"4": {
    "doc": "Airbyte",
    "title": "Table of contents",
    "content": ". | Using lakeFS with Airbyte | Use-cases | S3 Connector . | Configuring lakeFS using the connector | . | . ",
    "url": "/integrations/airbyte.html#table-of-contents",
    "relUrl": "/integrations/airbyte.html#table-of-contents"
  },"5": {
    "doc": "Airbyte",
    "title": "Using lakeFS with Airbyte",
    "content": "The integration between the two open-source projects brings resilience and manageability when using Airbyte connectors to sync data to your S3 buckets by leveraging lakeFS branches and atomic commits and merges. ",
    "url": "/integrations/airbyte.html#using-lakefs-with-airbyte",
    "relUrl": "/integrations/airbyte.html#using-lakefs-with-airbyte"
  },"6": {
    "doc": "Airbyte",
    "title": "Use-cases",
    "content": "You can leverage lakeFS consistency guarantees and CI/CD capabilities when ingesting data to S3 using lakeFS: . | Consolidate many data sources to a single branch and expose them to the consumers simultaneously when merging to the main branch. | Test incoming data for breaking schema changes, using lakeFS hooks. | Prevent consumers from reading partial data from connectors which failed half-way through sync. | Experiment with ingested data on a branch before exposing it. | . ",
    "url": "/integrations/airbyte.html#use-cases",
    "relUrl": "/integrations/airbyte.html#use-cases"
  },"7": {
    "doc": "Airbyte",
    "title": "S3 Connector",
    "content": "lakeFS exposes an S3 Gateway that enables applications to communicate with lakeFS in the same way they would with Amazon S3. You can use Airbyte’s S3 Destination for uploading the data to lakeFS. Configuring lakeFS using the connector . Set the following parameters when creating a new Destination of type S3: . | Name | Value | Example | . | Endpoint | The lakeFS S3 gateway URL | https://cute-axolotol.lakefs-demo.io | . | S3 Bucket Name | The lakeFS repository where the data will be written | example-repo | . | S3 Bucket Path | The branch and the path where the data will be written | main/data/from/airbyte Where main is the branch name, and data/from/airbyte is the path under the branch. | . | S3 Bucket Region | Not applicable to lakeFS, use us-east-1 | us-east-1 | . | S3 Key ID | The lakeFS access key id used to authenticate to lakeFS. | AKIAlakefs12345EXAMPLE | . | S3 Access Key | The lakeFS secret access key used to authenticate to lakeFS. | abc/lakefs/1234567bPxRfiCYEXAMPLEKEY | . Note S3 Destination connector supports custom S3 endpoints starting with Airbyte’s version v0.26.0-alpha released on Jun 17th 2021 . The UI configuration will look like: . ",
    "url": "/integrations/airbyte.html#s3-connector",
    "relUrl": "/integrations/airbyte.html#s3-connector"
  },"8": {
    "doc": "Airbyte",
    "title": "Airbyte",
    "content": "Airbyte is an open-source platform to sync data from applications, APIs &amp; databases to warehouses, lakes and other destinations. Use Airbyte’s connectors to get your data pipelines to consolidate many input sources. ",
    "url": "/integrations/airbyte.html",
    "relUrl": "/integrations/airbyte.html"
  },"9": {
    "doc": "Airflow",
    "title": "Using lakeFS with Airflow",
    "content": "Apache Airflow is a platform to programmatically author, schedule and monitor workflows. There are several steps needed to run Airflow with lakeFS. ",
    "url": "/integrations/airflow.html#using-lakefs-with-airflow",
    "relUrl": "/integrations/airflow.html#using-lakefs-with-airflow"
  },"10": {
    "doc": "Airflow",
    "title": "Create a lakeFS connection on Airflow",
    "content": "To access the lakeFS server and authenticate with it, create a new Airflow Connection of type HTTP and add it to your DAG. You can do that using the Airflow UI or the CLI. Here’s an example Airflow command that does just that: . airflow connections add conn_lakefs --conn-type=HTTP --conn-host=http://&lt;LAKEFS_ENDPOINT&gt; \\ --conn-extra='{\"access_key_id\":\"&lt;LAKEFS_ACCESS_KEY_ID&gt;\",\"secret_access_key\":\"&lt;LAKEFS_SECRET_ACCESS_KEY&gt;\"}' . ",
    "url": "/integrations/airflow.html#create-a-lakefs-connection-on-airflow",
    "relUrl": "/integrations/airflow.html#create-a-lakefs-connection-on-airflow"
  },"11": {
    "doc": "Airflow",
    "title": "Install the lakeFS Airflow package",
    "content": "You can use pip to install the package . pip install airflow-provider-lakefs . ",
    "url": "/integrations/airflow.html#install-the-lakefs-airflow-package",
    "relUrl": "/integrations/airflow.html#install-the-lakefs-airflow-package"
  },"12": {
    "doc": "Airflow",
    "title": "Use the package",
    "content": "Operators . The package exposes several operations to interact with a lakeFS server: . | CreateBranchOperator creates a new lakeFS branch from the source branch (main by default). task_create_branch = CreateBranchOperator( task_id='create_branch', repo='example-repo', branch='example-branch', source_branch='main' ) . | CommitOperator commits uncommitted changes to a branch. task_commit = CommitOperator( task_id='commit', repo='example-repo', branch='example-branch', msg='committing to lakeFS using airflow!', metadata={'committed_from\": \"airflow-operator'} ) . | MergeOperator merges 2 lakeFS branches. task_merge = MergeOperator( task_id='merge_branches', source_ref='example-branch', destination_branch='main', msg='merging job outputs', metadata={'committer': 'airflow-operator'} ) . | . Sensors . Sensors are also available that allow synchronizing a running DAG with external operations: . | CommitSensor waits until a commit has been applied to the branch . task_sense_commit = CommitSensor( repo='example-repo', branch='example-branch', task_id='sense_commit' ) . | FileSensor waits until a given file is present on a branch. task_sense_file = FileSensor( task_id='sense_file', repo='example-repo', branch='example-branch', path=\"file/to/sense\" ) . | . Example . This example DAG in the airflow-provider-lakeFS repository shows how to use all of these. Performing other operations . Sometimes an operator might not yet be supported by airflow-provider-lakeFS. You can directly access lakeFS by using: . | SimpleHttpOperator to send API requests to lakeFS. | BashOperator with lakeCTL commands. For example, deleting a branch using BashOperator: commit_extract = BashOperator( task_id='delete_branch', bash_command='lakectl branch delete lakefs://example-repo/example-branch', dag=dag, ) . | . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. ",
    "url": "/integrations/airflow.html#use-the-package",
    "relUrl": "/integrations/airflow.html#use-the-package"
  },"13": {
    "doc": "Airflow",
    "title": "Airflow",
    "content": " ",
    "url": "/integrations/airflow.html",
    "relUrl": "/integrations/airflow.html"
  },"14": {
    "doc": "API Reference",
    "title": "API Reference",
    "content": "| ",
    "url": "/reference/api.html",
    "relUrl": "/reference/api.html"
  },"15": {
    "doc": "Architecture",
    "title": "Architecture Overview",
    "content": " ",
    "url": "/understand/architecture.html#architecture-overview",
    "relUrl": "/understand/architecture.html#architecture-overview"
  },"16": {
    "doc": "Architecture",
    "title": "Table of contents",
    "content": ". | Overview | Ways to deploy lakeFS . | Load Balancing | . | lakeFS Components . | S3 Gateway | OpenAPI Server | Storage Adapter | Graveler | Authentication &amp; Authorization Service | Hooks Engine | UI | . | Applications | lakeFS Clients . | OpenAPI Generated SDKs | lakectl | Spark Metadata Client | lakeFS Hadoop FileSystem | . | . ",
    "url": "/understand/architecture.html#table-of-contents",
    "relUrl": "/understand/architecture.html#table-of-contents"
  },"17": {
    "doc": "Architecture",
    "title": "Overview",
    "content": "lakeFS is distributed as a single binary encapsulating several logical services: . The server itself is stateless, meaning you can easily add more instances to handle bigger load. lakeFS stores data in an underlying object store (GCS, ABS, S3 or any S3-compatible stores like MinIO or Ceph), with some of its metadata stored in PostgreSQL (see Versioning internals). ",
    "url": "/understand/architecture.html#overview",
    "relUrl": "/understand/architecture.html#overview"
  },"18": {
    "doc": "Architecture",
    "title": "Ways to deploy lakeFS",
    "content": "lakeFS releases includes binaries for common operating systems, a containerized option or an Helm chart. Check out our guides for running lakeFS on K8S, ECS, Google Compute Engine and more. Load Balancing . Accessing lakeFS is done using HTTP. lakeFS exposes a frontend UI, an OpenAPI server, as well as an S3-compatible service (see S3 Gateway below). lakeFS uses a single port that serves all 3 endpoints, so for most use-cases a single load-balancer pointing to lakeFS server(s) would do. ",
    "url": "/understand/architecture.html#ways-to-deploy-lakefs",
    "relUrl": "/understand/architecture.html#ways-to-deploy-lakefs"
  },"19": {
    "doc": "Architecture",
    "title": "lakeFS Components",
    "content": "S3 Gateway . S3 Gateway implements lakeFS’s compatibility with S3. It implements a compatible subset of the S3 API to ensure most data systems can use lakeFS as a drop-in replacement for S3. See the S3 API Reference section for information on supported API operations. OpenAPI Server . The Swagger (OpenAPI) server exposes the full set of lakeFS operations (see Reference). This includes basic CRUD operations against repositories and objects, as well as versioning related operations such as branching, merging, committing and reverting changes to data. Storage Adapter . The Storage Adapter is an abstraction layer for communicating with any underlying object store. Its implementations allow compatibility with many types of underlying storage such as S3, GCS, Azure Blob Storage, or non-production usages such as the local storage adapter. See the roadmap for information on future plans for storage compatibility. Graveler . Graveler handles lakeFS versioning by translating lakeFS addresses to the actual stored objects. To learn about the data model used to store lakeFS metadata, see the data model section. Authentication &amp; Authorization Service . The Auth service handles creation, management and validation of user credentials and RBAC policies. The credential scheme, along with the request signing logic are compatible with AWS IAM (both SIGv2 and SIGv4). Currently, the auth service manages its own database of users and credentials and does not use IAM in any way. Hooks Engine . The hooks engine enables CI/CD for data by triggering user defined Actions that will run during commit/merge. UI . The UI layer is a simple browser-based client that uses the OpenAPI server. It allows management, exploration and data access to repositories, branches, commits and objects in the system. ",
    "url": "/understand/architecture.html#lakefs-components",
    "relUrl": "/understand/architecture.html#lakefs-components"
  },"20": {
    "doc": "Architecture",
    "title": "Applications",
    "content": "As a rule of thumb, lakeFS supports any s3-compatible application. This means that many common data applications work with lakeFS out of the box. Check out our integrations to learn more. ",
    "url": "/understand/architecture.html#applications",
    "relUrl": "/understand/architecture.html#applications"
  },"21": {
    "doc": "Architecture",
    "title": "lakeFS Clients",
    "content": "Some data applications benefit from deeper integrations with lakeFS to support different use-cases or enhanced functionality which is provided by lakeFS clients. OpenAPI Generated SDKs . OpenAPI specification can be used to generate lakeFS clients for many programming languages. For example, the Python lakefs-client or the Java client are published with every new lakeFS release. lakectl . lakectl is a CLI tool that enables lakeFS operations using the lakeFS API from your preferred terminal. Spark Metadata Client . The lakeFS Spark Metadata Client makes it easy to perform operations related to lakeFS metadata, at scale. Examples include garbage collection or exporting data from lakeFS. lakeFS Hadoop FileSystem . Thanks to the S3 Gateway, it is possible to interact with lakeFS using Hadoop’s S3AFIleSystem, but due to limitations of the S3 API, doing so requires reading and writing data objects through the lakeFS server. Using lakeFSFileSystem increases Spark ETL jobs performance by executing the metadata operations on the lakeFS server, and all data operations directly through the same underlying object store that lakeFS uses. ",
    "url": "/understand/architecture.html#lakefs-clients",
    "relUrl": "/understand/architecture.html#lakefs-clients"
  },"22": {
    "doc": "Architecture",
    "title": "Architecture",
    "content": " ",
    "url": "/understand/architecture.html",
    "relUrl": "/understand/architecture.html"
  },"23": {
    "doc": "Amazon Athena",
    "title": "Using lakeFS with Amazon Athena",
    "content": "Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Amazon Athena works directly above S3 and can’t access lakeFS. Tables created using Athena aren’t readable by lakeFS. However, tables stored in lakeFS (that were created with glue/hive) can be queried by Athena. In order to support querying data from lakeFS with Amazon Athena, we will use create-symlink, one of the metastore commands in lakectl. create-symlink receives a source table, destination table and the location of the table and does two actions: . | Creates partitioned directories with symlink files in the underlying S3 bucket. | Creates a table in Glue catalog with symlink format type and location pointing to the created symlinks. | . Note .lakectl.yaml file should be configured with the proper hive/glue credentials. For more information . create-symlink receives a table in glue or hive pointing to lakeFS and creates a copy of the table in glue. The table data will use the SymlinkTextInputFormat, which will point to the lakeFS repository storage namespace. Without copying data, you will be able to query your data with Athena. However, the symlinks table will only show the data that existed during the copy. If the table changed in lakeFS, you need to run create-symlink again for your changed to be reflected in Athena. Example: . Let’s assume that some time ago, we created a hive table my_table that is stored in lakeFS repo example under branch main, using the command: . CREATE EXTERNAL TABLE `my_table`( `id` bigint, `key` string ) PARTITIONED BY (YEAR INT, MONTH INT) LOCATION 's3://example/main/my_table'; WITH (format = 'PARQUET', external_location 's3a://example/main/my_table' ); . The repository example has the s3 storage space s3://my-bucket/my-repo-prefix/. After inserting some data to it, object structure under lakefs://example/main/my_table looks like: . Now we would like to query that table with Athena. We need to use the create-symlink command as follows: . lakectl metastore create-symlink \\ --repo example \\ --branch main \\ --path my_table \\ --from-client-type hive \\ --from-schema default \\ --from-table my_table \\ --to-schema default \\ --to-table my_table . The command will have 2 notable outputs: . | For each partition the command will create a symlink file: | . ➜ aws s3 ls s3://my-bucket/my-repo-prefix/my_table/ --recursive 2021-11-23 17:46:29 0 my-repo-prefix/my_table/symlinks/example/main/my_table/year=2021/month=11/symlink.txt 2021-11-23 17:46:29 60 my-repo-prefix/my_table/symlinks/example/main/my_table/year=2021/month=12/symlink.txt 2021-11-23 17:46:30 60 my-repo-prefix/my_table/symlinks/example/main/my_table/year=2022/month=1/symlink.txt . An example content of a symlink file, where each line represents a single object of the specific partition: . s3://my-bucket/my-repo-prefix/5bdc62da516944b49889770d98274227 s3://my-bucket/my-repo-prefix/64262fbf3d6347a79ead641d2b2baee6 s3://my-bucket/my-repo-prefix/64486c8de6484de69f12d7d26804c93e s3://my-bucket/my-repo-prefix/b0165d5c5b13473d8a0f460eece9eb26 . | A glue table pointing to the symlink directories structure: | . aws glue get-table --name my_table --database-name default { \"Table\": { \"Name\": \"my_table\", \"DatabaseName\": \"default\", \"Owner\": \"anonymous\", \"CreateTime\": \"2021-11-23T17:46:30+02:00\", \"UpdateTime\": \"2021-11-23T17:46:30+02:00\", \"LastAccessTime\": \"1970-01-01T02:00:00+02:00\", \"Retention\": 0, \"StorageDescriptor\": { \"Columns\": [ { \"Name\": \"id\", \"Type\": \"bigint\", \"Comment\": \"\" }, { \"Name\": \"key\", \"Type\": \"string\", \"Comment\": \"\" } ], \"Location\": \"s3://my-bucket/my-repo-prefix/symlinks/example/main/my_table\", \"InputFormat\": \"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat\", \"OutputFormat\": \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\", \"Compressed\": false, \"NumberOfBuckets\": -1, \"SerdeInfo\": { \"Name\": \"default\", \"SerializationLibrary\": \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\", \"Parameters\": { \"serialization.format\": \"1\" } }, \"StoredAsSubDirectories\": false }, \"PartitionKeys\": [ { \"Name\": \"year\", \"Type\": \"int\", \"Comment\": \"\" }, { \"Name\": \"month\", \"Type\": \"int\", \"Comment\": \"\" } ], \"ViewOriginalText\": \"\", \"ViewExpandedText\": \"\", \"TableType\": \"EXTERNAL_TABLE\", \"Parameters\": { \"EXTERNAL\": \"TRUE\", \"bucketing_version\": \"2\", \"transient_lastDdlTime\": \"1637681750\" }, \"CreatedBy\": \"arn:aws:iam::************:user/********\", \"IsRegisteredWithLakeFormation\": false, \"CatalogId\": \"*********\" } } . You can now safely use Athena to query my_table. ",
    "url": "/integrations/athena.html#using-lakefs-with-amazon-athena",
    "relUrl": "/integrations/athena.html#using-lakefs-with-amazon-athena"
  },"24": {
    "doc": "Amazon Athena",
    "title": "Amazon Athena",
    "content": " ",
    "url": "/integrations/athena.html",
    "relUrl": "/integrations/athena.html"
  },"25": {
    "doc": "Authentication",
    "title": "Authentication",
    "content": " ",
    "url": "/reference/authentication.html",
    "relUrl": "/reference/authentication.html"
  },"26": {
    "doc": "Authentication",
    "title": "Table of contents",
    "content": ". | Authentication . | Authentication . | User Authentication . | Built-in database | LDAP server | . | API Server Authentication | S3 Gateway Authentication | . | . | . ",
    "url": "/reference/authentication.html#table-of-contents",
    "relUrl": "/reference/authentication.html#table-of-contents"
  },"27": {
    "doc": "Authentication",
    "title": "Authentication",
    "content": "User Authentication . lakeFS authenticates users from a built-in authentication database, or optionally from a configured LDAP server. Built-in database . The built-in authentication database is always present and active. Use the Web UI at Administration / Users to create users. Users have an access key AKIA... and an associated secret access key. These credentials are valid to log into the Web UI, or to authenticate programmatic requests to the API Server or the S3 Gateway. LDAP server . Configure lakeFS to authenticate users on an LDAP server. Once configured, users can additionally log into lakeFS using their credentials LDAP. These users may then generate an access key and a secret access key on the Web UI at Administration / My Credentials. lakeFS generates an internal user once logged in via the LDAP server. Adding this internal user to a group allows assigning them a different policy. Configure the LDAP server using these fields in auth.ldap: . | server_endpoint: the ldaps: (or ldap:) URL of the LDAP server. | bind_dn, bind_password: Credentials for lakeFS to use to query the LDAP server for users. These must identify a user with Basic Authentication, and are used to convert a user ID attribute to a full user DN. | default_user_group: A group to add users the first time they log in using LDAP. Typically “Viewers” or “Developers”. Once logged in, LDAP users may be added as normal to any other group. | username_attribute: Attribute on LDAP user to identify user when logging in. Typically “uid” or “cn”. | user_base_dn: DN of root of DAP tree containing users, e.g. ou=Users,dc=treeverse,dc=io. | user_filter: An additional filter for users allowed to login, e.g. (objectClass=person). | . LDAP users log in using the following flow: . | Bind the lakeFS control connection to server_endpoint using bind_dn, bind_password. | Receive an LDAP user-ID (e.g. “joebloggs”) and a password entered on the Web UI login page. | Attempt to log in as internally-defined users; fail. | Search the LDAP server using the control connection for the user: out of all users under user_base_dn that satisfy user_filter, there must be a single user whose username_attribute was specified by the user. Get their DN. In our example this might be uid=joebloggs,ou=Users,dc=treeverse,dc=io (this entry must have objectClass: person because of user_filter). | Attempt to bind the received DN on the LDAP server using the password. | On success, the user is authenticated! | Create a new internal user with that DN if needed. When creating a user add them to the internal group named default_user_group. | . API Server Authentication . Authenticating against the API server is done using a key-pair, passed via Basic Access Authentication. All HTTP requests must carry an Authorization header with the following structure: . Authorization: Basic &lt;base64 encoded access_key_id:secret_access_key&gt; . For example, assuming my access_key_id is my_access_key_id and my secret_access_key is my_secret_access_key, we’d send the following header with every request: . Authorization: Basic bXlfYWNjZXNzX2tleV9pZDpteV9hY2Nlc3Nfc2VjcmV0X2tleQ== . S3 Gateway Authentication . To provide API compatibility with Amazon S3, authentication with the S3 Gateway supports both SIGv2 and SIGv4. Clients such as the AWS SDK that implement these authentication methods should work without modification. See this example for authenticating with the AWS CLI. ",
    "url": "/reference/authentication.html",
    "relUrl": "/reference/authentication.html"
  },"28": {
    "doc": "Authorization",
    "title": "Authorization",
    "content": " ",
    "url": "/reference/authorization.html",
    "relUrl": "/reference/authorization.html"
  },"29": {
    "doc": "Authorization",
    "title": "Table of contents",
    "content": ". | Authorization . | Authorization Model | Authorization process | Policy Precedence | Resource naming - ARNs | Actions and Permissions | Preconfigured Policies . | FSFullAccess | FSReadAll | FSReadWriteAll | AuthFullAccess | AuthManageOwnCredentials | RepoManagementFullAccess | RepoManagementReadAll | ExportSetConfiguration | . | Additional Policies . | Read/write access for a specific repository | . | Preconfigured Groups . | Admins | SuperUsers | Developers | Viewers | . | . | . ",
    "url": "/reference/authorization.html#table-of-contents",
    "relUrl": "/reference/authorization.html#table-of-contents"
  },"30": {
    "doc": "Authorization",
    "title": "Authorization",
    "content": "Authorization Model . Access to resources is managed very much like AWS IAM. There are 4 basic components to the system: . Users - Representing entities that access and use the system. A user is given one or more Access Credentials for authentication. Actions - Representing a logical action within the system - reading a file, creating a repository, etc. Resources - A unique identifier representing a specific resource in the system - a repository, an object, a user, etc. Policies - Representing a set of Actions, a Resource and an effect: whether or not these actions are allowed or denied for the given resource(s). Groups - A named collection of users. Users can belong to multiple groups. Controlling access is done by attaching Policies, either directly to Users, or to Groups they belong to. Authorization process . Every action in the system, be it an API request, UI interaction, S3 Gateway call or CLI command, requires a set of actions to be allowed for one or more resources. When a user makes a request to perform that action, the following process takes place: . | Authentication - The credentials passed in the request are evaluated, and the user’s identity is extracted. | Action permission resolution - lakeFS would then calculate the set of allowed actions and resources that this request requires. | Effective policy resolution - the user’s policies (either attached directly or through group memberships) are calculated | Policy/Permission evaluation - lakeFS will compare the given user policies with the request actions and determine whether or not the request is allowed to continue | . Policy Precedence . Each policy attached to a user or a group has an Effect - either allow or deny. During evaluation of a request, deny would take precedence over any other allow policy. This helps us compose policies together. For example, we could attach a very permissive policy to a user and use deny rules to then selectively restrict what that user can do. Resource naming - ARNs . lakeFS uses ARN identifier - very similar in structure to those used by AWS. The resource segment of the ARN supports wildcards: use * to match 0 or more characters, or ? to match exactly one character. Additionally, the current user’s ID is interpolated in runtime into the ARN using the ${user} placeholder. Here are a few examples of valid ARNs within lakeFS: . arn:lakefs:auth:::user/jane.doe arn:lakefs:auth:::user/* arn:lakefs:fs:::repository/myrepo/* arn:lakefs:fs:::repository/myrepo/object/foo/bar/baz arn:lakefs:fs:::repository/myrepo/object/* arn:lakefs:fs:::repository/* arn:lakefs:fs:::* . this allows us to create fine-grained policies affecting only a specific subset of resources. See below for a full reference of ARNs and actions . Actions and Permissions . For the full list of actions and their required permissions see the following table: . | Action name | required action | Resource | API endpoint | S3 gateway operation | . | List Repositories | fs:ListRepositories | * | GET /repositories | ListBuckets | . | Get Repository | fs:ReadRepository | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId} | HeadBucket | . | Get Commit | fs:ReadCommit | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/commits/{commitId} | - | . | Create Commit | fs:CreateCommit | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | POST /repositories/{repositoryId}/branches/{branchId}/commits | - | . | Get Commit log | fs:ReadBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | GET /repositories/{repositoryId}/branches/{branchId}/commits | - | . | Create Repository | fs:CreateRepository | arn:lakefs:fs:::repository/{repositoryId} | POST /repositories | - | . | Namespace Attach to Repository | fs:AttachStorageNamespace | arn:lakefs:fs:::namespace/{storageNamespace} | POST /repositories | - | . | Delete Repository | fs:DeleteRepository | arn:lakefs:fs:::repository/{repositoryId} | DELETE /repositories/{repositoryId} | - | . | List Branches | fs:ListBranches | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/branches | ListObjects/ListObjectsV2 (with delimiter = / and empty prefix) | . | Get Branch | fs:ReadBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | GET /repositories/{repositoryId}/branches/{branchId} | - | . | Create Branch | fs:CreateBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | POST /repositories/{repositoryId}/branches | - | . | Delete Branch | fs:DeleteBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | DELETE /repositories/{repositoryId}/branches/{branchId} | - | . | Merge branches | fs:CreateCommit | arn:lakefs:fs:::repository/{repositoryId}/branch/{destinationBranchId} | POST /repositories/{repositoryId}/refs/{sourceBranchId}/merge/{destinationBranchId} | - | . | Diff branch uncommitted changes | fs:ListObjects | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/branches/{branchId}/diff | - | . | Diff refs | fs:ListObjects | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/refs/{leftRef}/diff/{rightRef} | - | . | Stat object | fs:ReadObject | arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey} | GET /repositories/{repositoryId}/refs/{ref}/objects/stat | HeadObject | . | Get Object | fs:ReadObject | arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey} | GET /repositories/{repositoryId}/refs/{ref}/objects | GetObject | . | List Objects | fs:ListObjects | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/refs/{ref}/objects/ls | ListObjects, ListObjectsV2 (no delimiter, or “/” + non-empty prefix) | . | Upload Object | fs:WriteObject | arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey} | POST /repositories/{repositoryId}/branches/{branchId}/objects | PutObject, CreateMultipartUpload, UploadPart, CompleteMultipartUpload | . | Delete Object | fs:DeleteObject | arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey} | DELETE /repositories/{repositoryId}/branches/{branchId}/objects | DeleteObject, DeleteObjects, AbortMultipartUpload | . | Revert Branch | fs:RevertBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | PUT /repositories/{repositoryId}/branches/{branchId} | - | . | Create User | auth:CreateUser | arn:lakefs:auth:::user/{userId} | POST /auth/users | - | . | List Users | auth:ListUsers | * | GET /auth/users | - | . | Get User | auth:ReadUser | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId} | - | . | Delete User | auth:DeleteUser | arn:lakefs:auth:::user/{userId} | DELETE /auth/users/{userId} | - | . | Get Group | auth:ReadGroup | arn:lakefs:auth:::group/{groupId} | GET /auth/groups/{groupId} | - | . | List Groups | auth:ListGroups | * | GET /auth/groups | - | . | Create Group | auth:CreateGroup | arn:lakefs:auth:::group/{groupId} | POST /auth/groups | - | . | Delete Group | auth:DeleteGroup | arn:lakefs:auth:::group/{groupId} | DELETE /auth/groups/{groupId} | - | . | List Policies | auth:ListPolicies | * | GET /auth/policies | - | . | Create Policy | auth:CreatePolicy | arn:lakefs:auth:::policy/{policyId} | POST /auth/policies | - | . | Update Policy | auth:UpdatePolicy | arn:lakefs:auth:::policy/{policyId} | POST /auth/policies | - | . | Delete Policy | auth:DeletePolicy | arn:lakefs:auth:::policy/{policyId} | DELETE /auth/policies/{policyId} | - | . | Get Policy | auth:ReadPolicy | arn:lakefs:auth:::policy/{policyId} | GET /auth/policies/{policyId} | - | . | List Group Members | auth:ReadGroup | arn:lakefs:auth:::group/{groupId} | GET /auth/groups/{groupId}/members | - | . | Add Group Member | auth:AddGroupMember | arn:lakefs:auth:::group/{groupId} | PUT /auth/groups/{groupId}/members/{userId} | - | . | Remove Group Member | auth:RemoveGroupMember | arn:lakefs:auth:::group/{groupId} | DELETE /auth/groups/{groupId}/members/{userId} | - | . | List User Credentials | auth:ListCredentials | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId}/credentials | - | . | Create User Credentials | auth:CreateCredentials | arn:lakefs:auth:::user/{userId} | POST /auth/users/{userId}/credentials | - | . | Delete User Credentials | auth:DeleteCredentials | arn:lakefs:auth:::user/{userId} | DELETE /auth/users/{userId}/credentials/{accessKeyId} | - | . | Get User Credentials | auth:ReadCredentials | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId}/credentials/{accessKeyId} | - | . | List User Groups | auth:ReadUser | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId}/groups | - | . | List User Policies | auth:ReadUser | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId}/policies | - | . | Attach Policy To User | auth:AttachPolicy | arn:lakefs:auth:::user/{userId} | PUT /auth/users/{userId}/policies/{policyId} | - | . | Detach Policy From User | auth:DetachPolicy | arn:lakefs:auth:::user/{userId} | DELETE /auth/users/{userId}/policies/{policyId} | - | . | List Group Policies | auth:ReadGroup | arn:lakefs:auth:::group/{groupId} | GET /auth/groups/{groupId}/policies | - | . | Attach Policy To Group | auth:AttachPolicy | arn:lakefs:auth:::group/{groupId} | PUT /auth/groups/{groupId}/policies/{policyId} | - | . | Detach Policy From Group | auth:DetachPolicy | arn:lakefs:auth:::group/{groupId} | DELETE /auth/groups/{groupId}/policies/{policyId} | - | . | Read Storage Config | fs:ReadConfig | * | GET /config/storage | - | . | Get Garbage Collection Rules | retention:GetGarbageCollectionRules | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/gc/rules | - | . | Set Garbage Collection Rules | retention:SetGarbageCollectionRules | arn:lakefs:fs:::repository/{repositoryId} | POST /repositories/{repositoryId}/gc/rules | - | . | Prepare Garbage Collection Commits | retention:PrepareGarbageCollectionCommits | arn:lakefs:fs:::repository/{repositoryId} | POST /repositories/{repositoryId}/gc/prepare_commits | - | . | List Repository Action Runs | ci:ReadAction | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repository}/actions/runs | - | . | Get Action Run | ci:ReadAction | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repository}/actions/runs/{run_id} | - | . | List Action Run Hooks | ci:ReadAction | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repository}/actions/runs/{run_id}/hooks | - | . | Get Action Run Hook Output | ci:ReadAction | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repository}/actions/runs/{run_id}/hooks/{hook_run_id}/output | - | . Some APIs may require more than one action. For instance, in order to create a repository (POST /repositories) you need permission to fs:CreateRepository for the name of the repository and also fs:AttachStorageNamespace for the storage namespace used. Preconfigured Policies . The following Policies are created during initial setup: . FSFullAccess . Policy: . { \"statement\": [ { \"action\": [ \"fs:*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . FSReadAll . Policy: . { \"statement\": [ { \"action\": [ \"fs:List*\", \"fs:Read*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . FSReadWriteAll . Policy: . { \"statement\": [ { \"action\": [ \"fs:ListRepositories\", \"fs:ReadRepository\", \"fs:ReadCommit\", \"fs:ListBranches\", \"fs:ListObjects\", \"fs:ReadObject\", \"fs:WriteObject\", \"fs:DeleteObject\", \"fs:RevertBranch\", \"fs:ReadBranch\", \"fs:CreateBranch\", \"fs:DeleteBranch\", \"fs:CreateCommit\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . AuthFullAccess . Policy: . { \"statement\": [ { \"action\": [ \"auth:*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . AuthManageOwnCredentials . Policy: . { \"statement\": [ { \"action\": [ \"auth:CreateCredentials\", \"auth:DeleteCredentials\", \"auth:ListCredentials\", \"auth:ReadCredentials\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:auth:::user/${user}\" } ] } . RepoManagementFullAccess . Policy: . { \"statement\": [ { \"action\": [ \"ci:*\" ], \"effect\": \"allow\", \"resource\": \"*\" }, { \"action\": [ \"retention:*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . RepoManagementReadAll . Policy: . { \"statement\": [ { \"action\": [ \"ci:Read*\" ], \"effect\": \"allow\", \"resource\": \"*\" }, { \"action\": [ \"retention:Get*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . ExportSetConfiguration . Additional Policies . The following examples can be used to create additional policies to further limit user access. Use the web UI or the lakectl auth command to create policies. Read/write access for a specific repository . Policy: . { \"statement\": [ { \"action\": [ \"fs:ReadRepository\", \"fs:ReadCommit\", \"fs:ListBranches\", \"fs:ListTags\", \"fs:ListObjects\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;\" }, { \"action\": [ \"fs:RevertBranch\", \"fs:ReadBranch\", \"fs:CreateBranch\", \"fs:DeleteBranch\", \"fs:CreateCommit\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;/branch/*\" }, { \"action\": [ \"fs:ListObjects\", \"fs:ReadObject\", \"fs:WriteObject\", \"fs:DeleteObject\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;/object/*\" }, { \"action\": [ \"fs:ReadTag\", \"fs:CreateTag\", \"fs:DeleteTag\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;/tag/*\" }, { \"action\": [\"fs:ReadConfig\"], \"effect\": \"allow\", \"resource\": \"*\" } ] } . Preconfigured Groups . Admins . Policies: [\"FSFullAccess\", \"AuthFullAccess\", \"RepoManagementFullAccess\", \"ExportSetConfiguration\"] . SuperUsers . Policies: [\"FSFullAccess\", \"AuthManageOwnCredentials\", \"RepoManagementReadAll\"] . Developers . Policies: [\"FSReadWriteAll\", \"AuthManageOwnCredentials\", \"RepoManagementReadAll\"] . Viewers . Policies: [\"FSReadAll\", \"AuthManageOwnCredentials\"] . ",
    "url": "/reference/authorization.html",
    "relUrl": "/reference/authorization.html"
  },"31": {
    "doc": "On AWS",
    "title": "Deploy lakeFS on AWS",
    "content": "Expected deployment time: 25min . ",
    "url": "/deploy/aws.html#deploy-lakefs-on-aws",
    "relUrl": "/deploy/aws.html#deploy-lakefs-on-aws"
  },"32": {
    "doc": "On AWS",
    "title": "Table of contents",
    "content": ". | Prerequisites | Creating the Database on AWS RDS | Installation Options . | On EC2 | On ECS | On EKS | . | Load balancing | Next Steps | . ",
    "url": "/deploy/aws.html#table-of-contents",
    "relUrl": "/deploy/aws.html#table-of-contents"
  },"33": {
    "doc": "On AWS",
    "title": "Prerequisites",
    "content": "Users that require S3 access using virtual host addressing should configure an S3 Gateway domain. ",
    "url": "/deploy/aws.html#prerequisites",
    "relUrl": "/deploy/aws.html#prerequisites"
  },"34": {
    "doc": "On AWS",
    "title": "Creating the Database on AWS RDS",
    "content": "lakeFS requires a PostgreSQL database to synchronize actions on your repositories. We will show you how to create a database on AWS RDS, but you can use any PostgreSQL database as long as it’s accessible by your lakeFS installation. If you already have a database, take note of the connection string and skip to the next step . | Follow the official AWS documentation on how to create a PostgreSQL instance and connect to it. You may use the default PostgreSQL engine, or Aurora PostgreSQL. Make sure you’re using PostgreSQL version &gt;= 11. | Once your RDS is set up and the server is in Available state, take note of the endpoint and port. | Make sure your security group rules allow you to connect to the database instance. | . ",
    "url": "/deploy/aws.html#creating-the-database-on-aws-rds",
    "relUrl": "/deploy/aws.html#creating-the-database-on-aws-rds"
  },"35": {
    "doc": "On AWS",
    "title": "Installation Options",
    "content": "On EC2 . | Save the following configuration file as config.yaml: . --- database: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: # replace this with a randomly-generated string: secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: s3 s3: region: us-east-1 # optional, fallback in case discover from bucket is not supported . | Download the binary to the EC2 instance. | Run the lakefs binary on the EC2 instance: lakefs --config config.yaml run . Note: it is preferable to run the binary as a service using systemd or your operating system’s facilities. | . On ECS . To support container-based environments like AWS ECS, lakeFS can be configured using environment variables. Here is a docker run command to demonstrate starting lakeFS using Docker: . docker run \\ --name lakefs \\ -p 8000:8000 \\ -e LAKEFS_DATABASE_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\ -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\ -e LAKEFS_BLOCKSTORE_TYPE=\"s3\" \\ treeverse/lakefs:latest run . See the reference for a complete list of environment variables. On EKS . See Kubernetes Deployment. ",
    "url": "/deploy/aws.html#installation-options",
    "relUrl": "/deploy/aws.html#installation-options"
  },"36": {
    "doc": "On AWS",
    "title": "Load balancing",
    "content": "Depending on how you chose to install lakeFS, you should have a load balancer direct requests to the lakeFS server. By default, lakeFS operates on port 8000, and exposes a /_health endpoint which you can use for health checks. Notes for using an AWS Application Load Balancer . | Your security groups should allow the load balancer to access the lakeFS server. | Create a target group with a listener for port 8000. | Setup TLS termination using the domain names you wish to use (e.g. lakefs.example.com and potentially s3.lakefs.example.com, *.s3.lakefs.example.com if using virtual-host addressing). | Configure the health-check to use the exposed /_health URL | . ",
    "url": "/deploy/aws.html#load-balancing",
    "relUrl": "/deploy/aws.html#load-balancing"
  },"37": {
    "doc": "On AWS",
    "title": "Next Steps",
    "content": "Your next step is to prepare your storage. If you already have a storage bucket/container, you are ready to create your first lakeFS repository. ",
    "url": "/deploy/aws.html#next-steps",
    "relUrl": "/deploy/aws.html#next-steps"
  },"38": {
    "doc": "On AWS",
    "title": "On AWS",
    "content": " ",
    "url": "/deploy/aws.html",
    "relUrl": "/deploy/aws.html"
  },"39": {
    "doc": "AWS CLI",
    "title": "Using lakeFS with AWS CLI",
    "content": "The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts. We could use the file commands for S3 to access lakeFS . ",
    "url": "/integrations/aws_cli.html#using-lakefs-with-aws-cli",
    "relUrl": "/integrations/aws_cli.html#using-lakefs-with-aws-cli"
  },"40": {
    "doc": "AWS CLI",
    "title": "Table of contents",
    "content": ". | Configuration | Path convention | Usage | Examples . | List directory | Copy from lakeFS to lakeFS | Copy from lakeFS to a local path | Copy from a local path to lakeFS | Delete file | Delete directory | . | Adding an alias | . ",
    "url": "/integrations/aws_cli.html#table-of-contents",
    "relUrl": "/integrations/aws_cli.html#table-of-contents"
  },"41": {
    "doc": "AWS CLI",
    "title": "Configuration",
    "content": "We would like to configure an AWS profile for lakeFS. In order to configure the lakeFS credentials run: . aws configure --profile lakefs . we will be prompted to enter AWS Access Key ID , AWS Secret Access Key . It should look like this: . aws configure --profile lakefs # output: # AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE # AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # Default region name [None]: # Default output format [None]: . ",
    "url": "/integrations/aws_cli.html#configuration",
    "relUrl": "/integrations/aws_cli.html#configuration"
  },"42": {
    "doc": "AWS CLI",
    "title": "Path convention",
    "content": "When accessing objects in s3 we will need to use the lakeFS path convention s3://[REPOSITORY]/[BRANCH]/PATH/TO/OBJECT . ",
    "url": "/integrations/aws_cli.html#path-convention",
    "relUrl": "/integrations/aws_cli.html#path-convention"
  },"43": {
    "doc": "AWS CLI",
    "title": "Usage",
    "content": "After configuring the credentials, This is how a command should look: . aws s3 --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ ls s3://example-repo/main/example-directory . We could use an alias to make it shorter and more convenient. ",
    "url": "/integrations/aws_cli.html#usage",
    "relUrl": "/integrations/aws_cli.html#usage"
  },"44": {
    "doc": "AWS CLI",
    "title": "Examples",
    "content": "List directory . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 ls s3://example-repo/main/example-directory . Copy from lakeFS to lakeFS . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 cp s3://example-repo/main/example-file-1 s3://example-repo/main/example-file-2 . Copy from lakeFS to a local path . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 cp s3://example-repo/main/example-file-1 /path/to/local/file . Copy from a local path to lakeFS . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 cp /path/to/local/file s3://example-repo/main/example-file-1 . Delete file . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 rm s3://example-repo/main/example-directory/example-file . Delete directory . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 rm s3://example-repo/main/example-directory/ --recursive . ",
    "url": "/integrations/aws_cli.html#examples",
    "relUrl": "/integrations/aws_cli.html#examples"
  },"45": {
    "doc": "AWS CLI",
    "title": "Adding an alias",
    "content": "In order to make the command shorter and more convenient we can create an alias: . alias awslfs='aws --endpoint https://lakefs.example.com --profile lakefs' . Now, the ls command using the alias will be: . awslfs s3 ls s3://example-repo/main/example-directory . ",
    "url": "/integrations/aws_cli.html#adding-an-alias",
    "relUrl": "/integrations/aws_cli.html#adding-an-alias"
  },"46": {
    "doc": "AWS CLI",
    "title": "AWS CLI",
    "content": " ",
    "url": "/integrations/aws_cli.html",
    "relUrl": "/integrations/aws_cli.html"
  },"47": {
    "doc": "On Azure",
    "title": "Deploy lakeFS on Azure",
    "content": "Expected deployment time: 25min . ",
    "url": "/deploy/azure.html#deploy-lakefs-on-azure",
    "relUrl": "/deploy/azure.html#deploy-lakefs-on-azure"
  },"48": {
    "doc": "On Azure",
    "title": "Table of contents",
    "content": ". | Prerequisites | Creating the Database on Azure Database | Installation Options . | On Azure VM | On Azure Container instances | On AKS | . | Load balancing | Next Steps | . ",
    "url": "/deploy/azure.html#table-of-contents",
    "relUrl": "/deploy/azure.html#table-of-contents"
  },"49": {
    "doc": "On Azure",
    "title": "Prerequisites",
    "content": "Users that require S3 access using virtual host addressing should configure an S3 Gateway domain. ",
    "url": "/deploy/azure.html#prerequisites",
    "relUrl": "/deploy/azure.html#prerequisites"
  },"50": {
    "doc": "On Azure",
    "title": "Creating the Database on Azure Database",
    "content": "lakeFS requires a PostgreSQL database to synchronize actions on your repositories. We will show you how to create a database on Azure Database, but you can use any PostgreSQL database as long as it’s accessible by your lakeFS installation. If you already have a database, take note of the connection string and skip to the next step . | Follow the official Azure documentation on how to create a PostgreSQL instance and connect to it. Make sure you’re using PostgreSQL version &gt;= 11. | Once your Azure Database for PostgreSQL server is set up and the server is in Available state, take note of the endpoint and username. | Make sure your Access control roles allow you to connect to the database instance. | . ",
    "url": "/deploy/azure.html#creating-the-database-on-azure-database",
    "relUrl": "/deploy/azure.html#creating-the-database-on-azure-database"
  },"51": {
    "doc": "On Azure",
    "title": "Installation Options",
    "content": "On Azure VM . | Save the following configuration file as config.yaml: . --- database: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: # replace this with a randomly-generated string: secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: azure azure: auth_method: msi # msi for active directory, access-key for access key # In case you chose to authenticate via access key unmark the following rows and insert the values from the previous step # storage_account: [your storage account] # storage_access_key: [your access key] . | Download the binary to the Azure Virtual Machine. | Run the lakefs binary on the machine: lakefs --config config.yaml run . Note: it is preferable to run the binary as a service using systemd or your operating system’s facilities. | To support Azure AD authentication go to Identity tab and switch Status toggle to on, then add the `Storage Blob Data Contributor’ role on the container you created. | . On Azure Container instances . To support container-based environments like Azure Container Instances, lakeFS can be configured using environment variables. Here is a docker run command to demonstrate starting lakeFS using Docker: . docker run \\ --name lakefs \\ -p 8000:8000 \\ -e LAKEFS_DATABASE_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\ -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\ -e LAKEFS_BLOCKSTORE_TYPE=\"azure\" \\ -e LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCOUNT=\"[YOUR_STORAGE_ACCOUNT]\" \\ -e LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCESS_KEY=\"[YOUR_ACCESS_KEY]\" \\ treeverse/lakefs:latest run . See the reference for a complete list of environment variables. On AKS . See Kubernetes Deployment. ",
    "url": "/deploy/azure.html#installation-options",
    "relUrl": "/deploy/azure.html#installation-options"
  },"52": {
    "doc": "On Azure",
    "title": "Load balancing",
    "content": "Depending on how you chose to install lakeFS, you should have a load balancer direct requests to the lakeFS server. By default, lakeFS operates on port 8000, and exposes a /_health endpoint which you can use for health checks. ",
    "url": "/deploy/azure.html#load-balancing",
    "relUrl": "/deploy/azure.html#load-balancing"
  },"53": {
    "doc": "On Azure",
    "title": "Next Steps",
    "content": "Your next step is to prepare your storage. If you already have a storage bucket/container, you are ready to create your first lakeFS repository. ",
    "url": "/deploy/azure.html#next-steps",
    "relUrl": "/deploy/azure.html#next-steps"
  },"54": {
    "doc": "On Azure",
    "title": "On Azure",
    "content": " ",
    "url": "/deploy/azure.html",
    "relUrl": "/deploy/azure.html"
  },"55": {
    "doc": "Azure Blob Storage",
    "title": "Prepare Your Blob Storage Container",
    "content": "Create a container in Azure portal: . | From the Azure portal, Storage Accounts, choose your account, then in the container tab click + Container. | Make sure you block public access | . ",
    "url": "/setup/storage/blob.html#prepare-your-blob-storage-container",
    "relUrl": "/setup/storage/blob.html#prepare-your-blob-storage-container"
  },"56": {
    "doc": "Azure Blob Storage",
    "title": "Authenticate with Secret Key",
    "content": "If you want lakeFS to authenticate with your storage using the storage account key, go to the Access Keys tab and click Show Keys. Use the values under Storage account name and Key in the lakeFS configuration. ",
    "url": "/setup/storage/blob.html#authenticate-with-secret-key",
    "relUrl": "/setup/storage/blob.html#authenticate-with-secret-key"
  },"57": {
    "doc": "Azure Blob Storage",
    "title": "Authenticate with Active Directory",
    "content": "In case you want your lakeFS Installation (we will install in the next step) to access this Container using Active Directory authentication, First go to the container you created in step 1. | Go to Access Control (IAM) | Go to the Role assignments tab | Add the Storage Blob Data Contributor role to the Installation running lakeFS. | . You are now ready to create your first lakeFS repository. ",
    "url": "/setup/storage/blob.html#authenticate-with-active-directory",
    "relUrl": "/setup/storage/blob.html#authenticate-with-active-directory"
  },"58": {
    "doc": "Azure Blob Storage",
    "title": "Azure Blob Storage",
    "content": " ",
    "url": "/setup/storage/blob.html",
    "relUrl": "/setup/storage/blob.html"
  },"59": {
    "doc": "Boto (Python)",
    "title": "Using lakeFS with Boto (Python)",
    "content": "To use Boto with lakeFS alongside S3, check out Boto S3 Router. It will route requests to either S3 or lakeFS according to the provided bucket name. lakeFS exposes an S3-compatible API, so you can use Boto to interact with your objects on lakeFS. ",
    "url": "/integrations/boto.html#using-lakefs-with-boto-python",
    "relUrl": "/integrations/boto.html#using-lakefs-with-boto-python"
  },"60": {
    "doc": "Boto (Python)",
    "title": "Table of contents",
    "content": ". | Creating a Boto client | Usage Examples . | Put Object | List Objects | Head Object | . | . ",
    "url": "/integrations/boto.html#table-of-contents",
    "relUrl": "/integrations/boto.html#table-of-contents"
  },"61": {
    "doc": "Boto (Python)",
    "title": "Creating a Boto client",
    "content": "Create a Boto3 S3 client with your lakeFS endpoint and key-pair: . import boto3 s3 = boto3.client('s3', endpoint_url='https://lakefs.example.com', aws_access_key_id='AKIAIOSFODNN7EXAMPLE', aws_secret_access_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY') . The client is now configured to operate on your lakeFS installation. ",
    "url": "/integrations/boto.html#creating-a-boto-client",
    "relUrl": "/integrations/boto.html#creating-a-boto-client"
  },"62": {
    "doc": "Boto (Python)",
    "title": "Usage Examples",
    "content": "Put Object . Use a branch name and a path to put an object in lakeFS: . with open('/local/path/to/file_0', 'rb') as f: s3.put_object(Body=f, Bucket='example-repo', Key='main/example-file.parquet') . You can now commit this change using the lakeFS UI or CLI. List Objects . List branch objects starting with a prefix: . list_resp = s3.list_objects_v2(Bucket='example-repo', Prefix='main/example-prefix') for obj in list_resp['Contents']: print(obj['Key']) . Or, use a lakeFS commit ID to list objects for a specific commit: . list_resp = s3.list_objects_v2(Bucket='example-repo', Prefix='c7a632d74f/example-prefix') for obj in list_resp['Contents']: print(obj['Key']) . Head Object . Get object metadata using branch and path: . s3.head_object(Bucket='example-repo', Key='main/example-file.parquet') # output: # {'ResponseMetadata': {'RequestId': '72A9EBD1210E90FA', # 'HostId': '', # 'HTTPStatusCode': 200, # 'HTTPHeaders': {'accept-ranges': 'bytes', # 'content-length': '1024', # 'etag': '\"2398bc5880e535c61f7624ad6f138d62\"', # 'last-modified': 'Sun, 24 May 2020 10:42:24 GMT', # 'x-amz-request-id': '72A9EBD1210E90FA', # 'date': 'Sun, 24 May 2020 10:45:42 GMT'}, # 'RetryAttempts': 0}, # 'AcceptRanges': 'bytes', # 'LastModified': datetime.datetime(2020, 5, 24, 10, 42, 24, tzinfo=tzutc()), # 'ContentLength': 1024, # 'ETag': '\"2398bc5880e535c61f7624ad6f138d62\"', # 'Metadata': {}} . ",
    "url": "/integrations/boto.html#usage-examples",
    "relUrl": "/integrations/boto.html#usage-examples"
  },"63": {
    "doc": "Boto (Python)",
    "title": "Boto (Python)",
    "content": " ",
    "url": "/integrations/boto.html",
    "relUrl": "/integrations/boto.html"
  },"64": {
    "doc": "Branching Model",
    "title": "Branching Model",
    "content": "At its core, lakeFS uses a Git-like branching model. ",
    "url": "/understand/branching-model.html",
    "relUrl": "/understand/branching-model.html"
  },"65": {
    "doc": "Branching Model",
    "title": "Table of contents",
    "content": ". | Repositories | Branches | Commits | Objects | . Repositories . In lakeFS, a repository is a logical namespace used to group together objects, branches and commits. It can be considered the lakeFS analog of a bucket in an object store. Since it has version control qualities, it is also analogous to a repository in Git. Branches . Branches are similar in concept to Git branches. When creating a new branch in lakeFS, we are actually creating a consistent snapshot of the entire repository, which is isolated from other branches and their changes. Another way to think of branches is like a very long-lived database transaction, providing us with Snapshot Isolation. Once we’ve made the necessary changes to our data within our isolated branch, we can merge it back to the branch we branched from. This operation is atomic in lakeFS - readers will either see all our committed changes or none at all. Isolation and Atomicity are very powerful tools: it allows us to do things that are otherwise extremely hard to get right: replace data in-place, add or update multiple objects and collections as a single piece, run tests and validations before exposing data to others and more. Commits . Commits are immutable “checkpoints”, containing an entire snapshot of a repository at a given point in time. This is again very similar to commits in Git. Each commit contains metadata - who performed it, timestamp, a commit message as well as arbitrary key/value pairs we can choose to add. Using commits, we can view our Data Lake at a certain point in its history and we are guaranteed that the data we see is exactly is it was at the point of committing it. In lakeFS, different users can view different branches (or even commits, directly) at the same time on the same repository. there’s no “checkout” process that copies data around. All live branches and commits are immediately available at all times. Objects . Objects in lakeFS are very similar to those found in S3 (or other object stores, for that matter). lakeFS is agnostic to what these objects contain: Parquet, CSV, ORC and even JPEG or other forms of unstructured data. Unlike Git, lakeFS does not care about the contents of an object - if we try to merge two branches that both update the same file, it is up to the user to resolve this conflict. This is because lakeFS doesn’t assume anything about the structure of the object and so cannot try to merge both changesets into a single object (additionally, this operation makes little sense for machine generated files, and data in general). The actual data itself is not stored inside lakeFS directly, but rather stored in an underlying object store. lakeFS will manage these writes, and will store a pointer to the object in its metadata database. ",
    "url": "/understand/branching-model.html#table-of-contents",
    "relUrl": "/understand/branching-model.html#table-of-contents"
  },"66": {
    "doc": "During Deployment",
    "title": "During Deployment",
    "content": "Every day we introduce new data to the lake. And even if the code and infra doesn’t change, the data might, and those changes introduce potential quality issues. This is one of the complexities of a data product; the data we consume changes over the course of a month, a week, day, hour, or even minute-to-minute. Examples of changes to data that may occur: . | A client-side bug in the data collection of website events | A new Android version that interferes with the collecting events from your App | COVID-19 abrupt impact on consumers’ behavior, and its effect on the accuracy of ML models. | During a change to Salesforce interface, the validation requirement from a certain field had been lost | . lakeFS enable CI/CD-inspired workflows to help validate expectations and assumptions about the data before it goes live in production or lands in the data environment. Example 1: Data update safety . Continuous deployment of existing data we expect to consume, flowing from ingest-pipelines into the lake. We merge data from an ingest branch (“events-data”), which allows us to create tests using data analysis tools or data quality services (e.g. Great Expectations, Monte Carlo) to ensure reliability of the data we merge to the main branch. Since merge is atomic, no performance issue will be introduced by using lakeFS, but your main branch will only include quality data. Each merge to the main branch creates a new commit on the main branch, which serves as a new version of the data. This allows us to easily revert to previous states of the data if a newer change introduces data issues. Example 2: Test - Validate new data . Examples of common validation checks enforced in organizations: . | No user_* columns except under /private/… | Only (*.parquet | *.orc | _delta_log/*.json) files allowed | Under /production, only backward-compatible schema changes are allowed | New tables on main must be registered in our metadata repository first, with owner and SLA | . lakeFS will assist in enforcing best practices by giving you a designated branch to ingest new data (“new-data-1” in the drawing). You may run automated tests to validate predefined best practices as pre-merge hooks. If the validation passes, the new data will be automatically and atomically merged to the main branch. However, if the validation fails, you will be alerted and the new data will not be exposed to consumers. By using this branching model and implementing best practices as pre merge hooks, you ensure the main lake is never compromised. ",
    "url": "/usecases/ci.html",
    "relUrl": "/usecases/ci.html"
  },"67": {
    "doc": "Command (CLI) Reference",
    "title": "Commands (CLI) Reference",
    "content": " ",
    "url": "/reference/commands.html#commands-cli-reference",
    "relUrl": "/reference/commands.html#commands-cli-reference"
  },"68": {
    "doc": "Command (CLI) Reference",
    "title": "Table of contents",
    "content": ". | Installing the lakectl command locally . | Configuring credentials and API endpoint | lakectl | lakectl abuse | lakectl abuse create-branches | lakectl abuse help | lakectl abuse random-read | lakectl abuse random-write | lakectl actions | lakectl actions help | lakectl actions runs | lakectl actions runs describe | lakectl actions runs help | lakectl actions runs list | lakectl actions validate | lakectl annotate | lakectl auth | lakectl auth groups | lakectl auth groups create | lakectl auth groups delete | lakectl auth groups help | lakectl auth groups list | lakectl auth groups members | lakectl auth groups members add | lakectl auth groups members help | lakectl auth groups members list | lakectl auth groups members remove | lakectl auth groups policies | lakectl auth groups policies attach | lakectl auth groups policies detach | lakectl auth groups policies help | lakectl auth groups policies list | lakectl auth help | lakectl auth policies | lakectl auth policies create | lakectl auth policies delete | lakectl auth policies help | lakectl auth policies list | lakectl auth policies show | lakectl auth users | lakectl auth users create | lakectl auth users credentials | lakectl auth users credentials create | lakectl auth users credentials delete | lakectl auth users credentials help | lakectl auth users credentials list | lakectl auth users delete | lakectl auth users groups | lakectl auth users groups help | lakectl auth users groups list | lakectl auth users help | lakectl auth users list | lakectl auth users policies | lakectl auth users policies attach | lakectl auth users policies detach | lakectl auth users policies help | lakectl auth users policies list | lakectl branch | lakectl branch create | lakectl branch delete | lakectl branch help | lakectl branch list | lakectl branch reset | lakectl branch revert | lakectl branch show | lakectl branch-protect | lakectl branch-protect add | lakectl branch-protect delete | lakectl branch-protect help | lakectl branch-protect list | lakectl cat-hook-output | lakectl cat-sst | lakectl commit | lakectl completion | lakectl config | lakectl dbt | lakectl dbt create-branch-schema | lakectl dbt generate-schema-macro | lakectl dbt help | lakectl diff | lakectl docs | lakectl doctor | lakectl fs | lakectl fs cat | lakectl fs help | lakectl fs ls | lakectl fs rm | lakectl fs stage | lakectl fs stat | lakectl fs upload | lakectl gc | lakectl gc get-config | lakectl gc help | lakectl gc set-config | lakectl help | lakectl ingest | lakectl log | lakectl merge | lakectl metastore | lakectl metastore copy | lakectl metastore copy-all | lakectl metastore copy-schema | lakectl metastore create-symlink | lakectl metastore diff | lakectl metastore help | lakectl metastore import-all | lakectl refs-dump | lakectl refs-restore | lakectl repo | lakectl repo create | lakectl repo create-bare | lakectl repo delete | lakectl repo help | lakectl repo list | lakectl show | lakectl tag | lakectl tag create | lakectl tag delete | lakectl tag help | lakectl tag list | lakectl tag show | . | . ",
    "url": "/reference/commands.html#table-of-contents",
    "relUrl": "/reference/commands.html#table-of-contents"
  },"69": {
    "doc": "Command (CLI) Reference",
    "title": "Installing the lakectl command locally",
    "content": "lakectl is distributed as a single binary, with no external dependencies - and is available for MacOS, Windows and Linux. Download lakectl . Configuring credentials and API endpoint . Once you’ve installed the lakectl command, run: . lakectl config # output: # Config file /home/janedoe/.lakectl.yaml will be used # Access key ID: AKIAIOSFODNN7EXAMPLE # Secret access key: **************************************** # Server endpoint URL: http://localhost:8000/api/v1 . This will setup a $HOME/.lakectl.yaml file with the credentials and API endpoint you’ve supplied. When setting up a new installation and creating initial credentials (see Quick start), the UI will provide a link to download a preconfigured configuration file for you. lakectl configuration items can each be controlled by an environment variable. The variable name will have a prefix of LAKECTL_, followed by the name of the configuration, replacing every ‘.’ with a ‘_’. Example: LAKECTL_SERVER_ENDPOINT_URL controls server.endpoint_url. lakectl . A cli tool to explore manage and work with lakeFS . Synopsis . lakeFS is data lake management solution, allowing Git-like semantics over common object stores . lakectl is a CLI tool allowing exploration and manipulation of a lakeFS environment . Options . --base-uri string base URI used for lakeFS address parse -c, --config string config file (default is $HOME/.lakectl.yaml) -h, --help help for lakectl --log-format string set logging output format --log-level string set logging level (default \"none\") --log-output strings set logging output(s) --no-color don't use fancy output colors (default when not attached to an interactive terminal) --verbose run in verbose mode . note: The base-uri option can be controlled with the LAKECTL_BASE_URI environment variable. Example usage . $ export LAKECTL_BASE_URI=\"lakefs://my-repo/my-branch\" # Once set, use relative lakefs uri's: $ lakectl fs ls /path . lakectl abuse . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Abuse a running lakeFS instance. See sub commands for more info. Options . -h, --help help for abuse . lakectl abuse create-branches . Create a lot of branches very quickly. lakectl abuse create-branches &lt;source ref uri&gt; [flags] . Options . --amount int amount of things to do (default 1000000) --branch-prefix string prefix to create branches under (default \"abuse-\") --clean-only only clean up past runs -h, --help help for create-branches --parallelism int amount of things to do in parallel (default 100) . lakectl abuse help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type abuse help [path to command] for full details. lakectl abuse help [command] [flags] . Options . -h, --help help for help . lakectl abuse random-read . Read keys from a file and generate random reads from the source ref for those keys. lakectl abuse random-read &lt;source ref uri&gt; [flags] . Options . --amount int amount of reads to do (default 1000000) --from-file string read keys from this file (\"-\" for stdin) -h, --help help for random-read --parallelism int amount of reads to do in parallel (default 100) . lakectl abuse random-write . Generate random writes to the source branch . lakectl abuse random-write &lt;source branch uri&gt; [flags] . Options . --amount int amount of writes to do (default 1000000) -h, --help help for random-write --parallelism int amount of writes to do in parallel (default 100) --prefix string prefix to create paths under (default \"abuse/\") . lakectl actions . Manage Actions commands . Options . -h, --help help for actions . lakectl actions help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type actions help [path to command] for full details. lakectl actions help [command] [flags] . Options . -h, --help help for help . lakectl actions runs . Explore runs information . Options . -h, --help help for runs . lakectl actions runs describe . Describe run results . Synopsis . Show information about the run and all the hooks that were executed as part of the run . lakectl actions runs describe [flags] . Examples . lakectl actions runs describe lakefs://&lt;repository&gt; &lt;run_id&gt; . Options . --after string show results after this value (used for pagination) --amount int number of results to return. By default, all results are returned. -h, --help help for describe . lakectl actions runs help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type runs help [path to command] for full details. lakectl actions runs help [command] [flags] . Options . -h, --help help for help . lakectl actions runs list . List runs . Synopsis . List all runs on a repository optional filter by branch or commit . lakectl actions runs list [flags] . Examples . lakectl actions runs list lakefs://&lt;repository&gt; [--branch &lt;branch&gt;] [--commit &lt;commit_id&gt;] . Options . --after string show results after this value (used for pagination) --amount int number of results to return (default 100) --branch string show results for specific branch --commit string show results for specific commit ID -h, --help help for list . lakectl actions validate . Validate action file . Synopsis . Tries to parse the input action file as lakeFS action file . lakectl actions validate [flags] . Examples . lakectl actions validate &lt;path&gt; . Options . -h, --help help for validate . lakectl annotate . List entries under a given path, annotating each with the latest modifying commit . lakectl annotate &lt;path uri&gt; [flags] . Options . -h, --help help for annotate -r, --recursive recursively annotate all entries under a given path or prefix . lakectl auth . Manage authentication and authorization . Synopsis . manage authentication and authorization including users, groups and policies . Options . -h, --help help for auth . lakectl auth groups . Manage groups . Options . -h, --help help for groups . lakectl auth groups create . Create a group . lakectl auth groups create [flags] . Options . -h, --help help for create --id string group identifier . lakectl auth groups delete . Delete a group . lakectl auth groups delete [flags] . Options . -h, --help help for delete --id string group identifier . lakectl auth groups help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type groups help [path to command] for full details. lakectl auth groups help [command] [flags] . Options . -h, --help help for help . lakectl auth groups list . List groups . lakectl auth groups list [flags] . Options . --after string show results after this value (used for pagination) --amount int how many results to return (default 100) -h, --help help for list . lakectl auth groups members . Manage group user memberships . Options . -h, --help help for members . lakectl auth groups members add . Add a user to a group . lakectl auth groups members add [flags] . Options . -h, --help help for add --id string group identifier --user string user identifier to add to the group . lakectl auth groups members help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type members help [path to command] for full details. lakectl auth groups members help [command] [flags] . Options . -h, --help help for help . lakectl auth groups members list . List users in a group . lakectl auth groups members list [flags] . Options . --after string show results after this value (used for pagination) --amount int how many results to return (default 100) -h, --help help for list --id string group identifier . lakectl auth groups members remove . Remove a user from a group . lakectl auth groups members remove [flags] . Options . -h, --help help for remove --id string group identifier --user string user identifier to add to the group . lakectl auth groups policies . Manage group policies . Options . -h, --help help for policies . lakectl auth groups policies attach . Attach a policy to a group . lakectl auth groups policies attach [flags] . Options . -h, --help help for attach --id string user identifier --policy string policy identifier . lakectl auth groups policies detach . Detach a policy from a group . lakectl auth groups policies detach [flags] . Options . -h, --help help for detach --id string user identifier --policy string policy identifier . lakectl auth groups policies help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type policies help [path to command] for full details. lakectl auth groups policies help [command] [flags] . Options . -h, --help help for help . lakectl auth groups policies list . List policies for the given group . lakectl auth groups policies list [flags] . Options . --after string show results after this value (used for pagination) --amount int how many results to return (default 100) -h, --help help for list --id string group identifier . lakectl auth help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type auth help [path to command] for full details. lakectl auth help [command] [flags] . Options . -h, --help help for help . lakectl auth policies . Manage policies . Options . -h, --help help for policies . lakectl auth policies create . Create a policy . lakectl auth policies create [flags] . Options . -h, --help help for create --id string policy identifier --statement-document string JSON statement document path (or \"-\" for stdin) . lakectl auth policies delete . Delete a policy . lakectl auth policies delete [flags] . Options . -h, --help help for delete --id string policy identifier . lakectl auth policies help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type policies help [path to command] for full details. lakectl auth policies help [command] [flags] . Options . -h, --help help for help . lakectl auth policies list . List policies . lakectl auth policies list [flags] . Options . --after string show results after this value (used for pagination) --amount int how many results to return (default 100) -h, --help help for list . lakectl auth policies show . Show a policy . lakectl auth policies show [flags] . Options . -h, --help help for show --id string policy identifier . lakectl auth users . Manage users . Options . -h, --help help for users . lakectl auth users create . Create a user . lakectl auth users create [flags] . Options . -h, --help help for create --id string user identifier . lakectl auth users credentials . Manage user credentials . Options . -h, --help help for credentials . lakectl auth users credentials create . Create user credentials . lakectl auth users credentials create [flags] . Options . -h, --help help for create --id string user identifier (default: current user) . lakectl auth users credentials delete . Delete user credentials . lakectl auth users credentials delete [flags] . Options . --access-key-id string access key ID to delete -h, --help help for delete --id string user identifier (default: current user) . lakectl auth users credentials help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type credentials help [path to command] for full details. lakectl auth users credentials help [command] [flags] . Options . -h, --help help for help . lakectl auth users credentials list . List user credentials . lakectl auth users credentials list [flags] . Options . --after string show results after this value (used for pagination) --amount int how many results to return (default 100) -h, --help help for list --id string user identifier (default: current user) . lakectl auth users delete . Delete a user . lakectl auth users delete [flags] . Options . -h, --help help for delete --id string user identifier . lakectl auth users groups . Manage user groups . Options . -h, --help help for groups . lakectl auth users groups help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type groups help [path to command] for full details. lakectl auth users groups help [command] [flags] . Options . -h, --help help for help . lakectl auth users groups list . List groups for the given user . lakectl auth users groups list [flags] . Options . --after string show results after this value (used for pagination) --amount int how many results to return (default 100) -h, --help help for list --id string user identifier . lakectl auth users help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type users help [path to command] for full details. lakectl auth users help [command] [flags] . Options . -h, --help help for help . lakectl auth users list . List users . lakectl auth users list [flags] . Options . --after string show results after this value (used for pagination) --amount int how many results to return (default 100) -h, --help help for list . lakectl auth users policies . Manage user policies . Options . -h, --help help for policies . lakectl auth users policies attach . Attach a policy to a user . lakectl auth users policies attach [flags] . Options . -h, --help help for attach --id string user identifier --policy string policy identifier . lakectl auth users policies detach . Detach a policy from a user . lakectl auth users policies detach [flags] . Options . -h, --help help for detach --id string user identifier --policy string policy identifier . lakectl auth users policies help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type policies help [path to command] for full details. lakectl auth users policies help [command] [flags] . Options . -h, --help help for help . lakectl auth users policies list . List policies for the given user . lakectl auth users policies list [flags] . Options . --after string show results after this value (used for pagination) --amount int how many results to return (default 100) --effective list all distinct policies attached to the user, even through group memberships -h, --help help for list --id string user identifier . lakectl branch . Create and manage branches within a repository . Synopsis . Create delete and list branches within a lakeFS repository . Options . -h, --help help for branch . lakectl branch create . Create a new branch in a repository . lakectl branch create &lt;branch uri&gt; -s &lt;source ref uri&gt; [flags] . Examples . lakectl branch create lakefs://example-repo/new-branch -s lakefs://example-repo/main . Options . -h, --help help for create -s, --source string source branch uri . lakectl branch delete . Delete a branch in a repository, along with its uncommitted changes (CAREFUL) . lakectl branch delete &lt;branch uri&gt; [flags] . Examples . lakectl branch delete lakefs://example-repo/example-branch . Options . -h, --help help for delete -y, --yes Automatically say yes to all confirmations . lakectl branch help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type branch help [path to command] for full details. lakectl branch help [command] [flags] . Options . -h, --help help for help . lakectl branch list . List branches in a repository . lakectl branch list &lt;repository uri&gt; [flags] . Examples . lakectl branch list lakefs://&lt;repository&gt; . Options . --after string show results after this value (used for pagination) --amount int number of results to return (default 100) -h, --help help for list . lakectl branch reset . Reset uncommitted changes - all of them, or by path . Synopsis . reset changes. There are four different ways to reset changes: . | reset all uncommitted changes - reset lakefs://myrepo/main | reset uncommitted changes under specific path - reset lakefs://myrepo/main –prefix path | reset uncommitted changes for specific object - reset lakefs://myrepo/main –object path | . lakectl branch reset &lt;branch uri&gt; [--prefix|--object] [flags] . Examples . lakectl branch reset lakefs://example-repo/example-branch . Options . -h, --help help for reset --object string path to object to be reset --prefix string prefix of the objects to be reset -y, --yes Automatically say yes to all confirmations . lakectl branch revert . Given a commit, record a new commit to reverse the effect of this commit . Synopsis . The commits will be reverted in left-to-right order . lakectl branch revert &lt;branch uri&gt; &lt;commit ref to revert&gt; [&lt;more commits&gt;...] [flags] . Examples . lakectl branch revert lakefs://example-repo/example-branch commitA Revert the changes done by commitA in example-branch branch revert lakefs://example-repo/example-branch HEAD~1 HEAD~2 HEAD~3 Revert the changes done by the second last commit to the fourth last commit in example-branch . Options . -h, --help help for revert -m, --parent-number int the parent number (starting from 1) of the mainline. The revert will reverse the change relative to the specified parent. -y, --yes Automatically say yes to all confirmations . lakectl branch show . Show branch latest commit reference . lakectl branch show &lt;branch uri&gt; [flags] . Examples . lakectl branch show lakefs://example-repo/example-branch . Options . -h, --help help for show . lakectl branch-protect . Create and manage branch protection rules . Synopsis . Define branch protection rules to prevent direct changes. Changes to protected branches can only be done by merging from other branches. Options . -h, --help help for branch-protect . lakectl branch-protect add . Add a branch protection rule . Synopsis . Add a branch protection rule for a given branch name pattern . lakectl branch-protect add &lt;repo uri&gt; &lt;pattern&gt; [flags] . Examples . lakectl branch-protect add lakefs://&lt;repository&gt; 'stable_*' . Options . -h, --help help for add . lakectl branch-protect delete . Delete a branch protection rule . Synopsis . Delete a branch protection rule for a given branch name pattern . lakectl branch-protect delete &lt;repo uri&gt; &lt;pattern&gt; [flags] . Examples . lakectl branch-protect delete lakefs://&lt;repository&gt; stable_* . Options . -h, --help help for delete . lakectl branch-protect help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type branch-protect help [path to command] for full details. lakectl branch-protect help [command] [flags] . Options . -h, --help help for help . lakectl branch-protect list . List all branch protection rules . lakectl branch-protect list &lt;repo uri&gt; [flags] . Examples . lakectl branch-protect list lakefs://&lt;repository&gt; . Options . -h, --help help for list . lakectl cat-hook-output . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Cat actions hook output . lakectl cat-hook-output [flags] . Examples . lakectl cat-hook-output lakefs://&lt;repository&gt; &lt;run_id&gt; &lt;run_hook_id&gt; . Options . -h, --help help for cat-hook-output . lakectl cat-sst . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Explore lakeFS .sst files . lakectl cat-sst &lt;sst-file&gt; [flags] . Options . --amount int how many records to return, or -1 for all records (default -1) -f, --file string path to an sstable file, or \"-\" for stdin -h, --help help for cat-sst . lakectl commit . Commit changes on a given branch . lakectl commit &lt;branch uri&gt; [flags] . Options . --allow-empty-message allow an empty commit message -h, --help help for commit -m, --message string commit message --meta strings key value pair in the form of key=value . lakectl completion . Generate completion script . Synopsis . To load completions: . Bash: . $ source &lt;(lakectl completion bash) . To load completions for each session, execute once: Linux: . $ lakectl completion bash &gt; /etc/bash_completion.d/lakectl . MacOS: . $ lakectl completion bash &gt; /usr/local/etc/bash_completion.d/lakectl . Zsh: . If shell completion is not already enabled in your environment you will need to enable it. You can execute the following once: . $ echo \"autoload -U compinit; compinit\" &gt;&gt; ~/.zshrc . To load completions for each session, execute once: . $ lakectl completion zsh &gt; \"${fpath[1]}/_lakectl\" . You will need to start a new shell for this setup to take effect. Fish: . $ lakectl completion fish | source . To load completions for each session, execute once: . $ lakectl completion fish &gt; ~/.config/fish/completions/lakectl.fish . lakectl completion &lt;bash|zsh|fish&gt; . Options . -h, --help help for completion . lakectl config . Create/update local lakeFS configuration . lakectl config [flags] . Options . -h, --help help for config . lakectl dbt . Integration with dbt commands . Options . -h, --help help for dbt . lakectl dbt create-branch-schema . Creates a new schema dedicated for branch and clones all dbt models to new schema . lakectl dbt create-branch-schema [flags] . Examples . lakectl dbt create-branch-schema --branch &lt;branch-name&gt; . Options . --branch string requested branch --continue-on-error prevent command from failing when a single table fails --continue-on-schema-exists allow running on existing schema --create-branch create a new branch for the schema --dbfs-location string -h, --help help for create-branch-schema --project-root string location of dbt project (default \".\") --skip-views --to-schema string destination schema name [default is branch] . lakectl dbt generate-schema-macro . generates the a macro allowing lakectl to run dbt on dynamic schemas . lakectl dbt generate-schema-macro [flags] . Examples . lakectl dbt generate-schema-macro . Options . -h, --help help for generate-schema-macro --project-root string location of dbt project (default \".\") . lakectl dbt help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type dbt help [path to command] for full details. lakectl dbt help [command] [flags] . Options . -h, --help help for help . lakectl diff . Show changes between two commits, or the currently uncommitted changes . lakectl diff &lt;ref uri&gt; [ref uri] [flags] . Examples . lakectl diff lakefs://example-repo/example-branch Show uncommitted changes in example-branch. lakectl diff lakefs://example-repo/main lakefs://example-repo/dev This shows the differences between master and dev starting at the last common commit. This is similar to the three-dot (...) syntax in git. Uncommitted changes are not shown. lakectl diff --two-way lakefs://example-repo/main lakefs://example-repo/dev Show changes between the tips of the main and dev branches. This is similar to the two-dot (..) syntax in git. Uncommitted changes are not shown. lakectl diff --two-way lakefs://example-repo/main lakefs://example-repo/dev$ Show changes between the tip of the main and the dev branch, including uncommitted changes on dev. Options . -h, --help help for diff --two-way Use two-way diff: show difference between the given refs, regardless of a common ancestor. lakectl docs . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. lakectl docs [outfile] [flags] . Options . -h, --help help for docs . lakectl doctor . Run a basic diagnosis of the LakeFS configuration . lakectl doctor [flags] . Options . -h, --help help for doctor . lakectl fs . View and manipulate objects . Options . -h, --help help for fs . lakectl fs cat . Dump content of object to stdout . lakectl fs cat &lt;path uri&gt; [flags] . Options . -d, --direct read directly from backing store (faster but requires more credentials) -h, --help help for cat . lakectl fs help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type fs help [path to command] for full details. lakectl fs help [command] [flags] . Options . -h, --help help for help . lakectl fs ls . List entries under a given tree . lakectl fs ls &lt;path uri&gt; [flags] . Options . -h, --help help for ls --recursive list all objects under the specified prefix . lakectl fs rm . Delete object . lakectl fs rm &lt;path uri&gt; [flags] . Options . -C, --concurrency int max concurrent single delete operations to send to the lakeFS server (default 50) -h, --help help for rm -r, --recursive recursively delete all objects under the specified path . lakectl fs stage . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Stage a reference to an existing object, to be managed in lakeFS . lakectl fs stage &lt;path uri&gt; [flags] . Options . --checksum string Object MD5 checksum as a hexadecimal string --content-type string MIME type of contents -h, --help help for stage --location string fully qualified storage location (i.e. \"s3://bucket/path/to/object\") --meta strings key value pairs in the form of key=value --mtime int Object modified time (Unix Epoch in seconds). Defaults to current time --size int Object size in bytes . lakectl fs stat . View object metadata . lakectl fs stat &lt;path uri&gt; [flags] . Options . -h, --help help for stat . lakectl fs upload . Upload a local file to the specified URI . lakectl fs upload &lt;path uri&gt; [flags] . Options . --content-type string MIME type of contents -d, --direct write directly to backing store (faster but requires more credentials) -h, --help help for upload -r, --recursive recursively copy all files under local source -s, --source string local file to upload, or \"-\" for stdin . lakectl gc . Manage garbage collection configuration . Options . -h, --help help for gc . lakectl gc get-config . Show garbage collection configuration JSON . lakectl gc get-config [flags] . Examples . lakectl gc get-config &lt;repository uri&gt; . Options . -h, --help help for get-config -p, --json get rules as JSON . lakectl gc help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type gc help [path to command] for full details. lakectl gc help [command] [flags] . Options . -h, --help help for help . lakectl gc set-config . Set garbage collection configuration JSON . Synopsis . Sets the garbage collection configuration JSON. Example configuration file: { “default_retention_days”: 21, “branches”: [ { “branch_id”: “main”, “retention_days”: 28 }, { “branch_id”: “dev”, “retention_days”: 14 } ] } . lakectl gc set-config [flags] . Examples . lakectl gc set-config &lt;repository uri&gt; -f config.json . Options . -f, --filename string file containing the GC configuration -h, --help help for set-config . lakectl help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type lakectl help [path to command] for full details. lakectl help [command] [flags] . Options . -h, --help help for help . lakectl ingest . Ingest objects from an external source into a lakeFS branch (without actually copying them) . lakectl ingest --from &lt;object store URI&gt; --to &lt;lakeFS path URI&gt; [--dry-run] [flags] . Options . -C, --concurrency int max concurrent API calls to make to the lakeFS server (default 64) --dry-run only print the paths to be ingested --from string prefix to read from (e.g. \"s3://bucket/sub/path/\"). must not be in a storage namespace -h, --help help for ingest --s3-endpoint-url string URL to access S3 storage API (by default, use regular AWS S3 endpoint --to string lakeFS path to load objects into (e.g. \"lakefs://repo/branch/sub/path/\") . lakectl log . Show log of commits . Synopsis . Show log of commits for a given branch . lakectl log &lt;branch uri&gt; [flags] . Options . --after string show results after this value (used for pagination) --amount int number of results to return. By default, all results are returned -h, --help help for log --objects strings show results that contains changes to at least one path in that list of objects. Use comma separator to pass all objects together --prefixes strings show results that contains changes to at least one path in that list of prefixes. Use comma separator to pass all prefixes together --show-meta-range-id also show meta range ID . lakectl merge . Merge &amp; commit changes from source branch into destination branch . Synopsis . Merge &amp; commit changes from source branch into destination branch . lakectl merge &lt;source ref&gt; &lt;destination ref&gt; [flags] . Options . -h, --help help for merge --strategy string In case of a merge conflict, this option will force the merge process to automatically favor changes from the dest branch (\"dest-wins\") or from the source branch(\"source-wins\"). In case no selection is made, the merge process will fail in case of a conflict . lakectl metastore . Manage metastore commands . Options . -h, --help help for metastore . lakectl metastore copy . Copy or merge table . Synopsis . Copy or merge table. the destination table will point to the selected branch . lakectl metastore copy [flags] . Options . --catalog-id string Glue catalog ID --dbfs-root dbfs:/ dbfs location root will replace dbfs:/ in the location before transforming --from-client-type string metastore type [hive, glue] --from-schema string source schema name --from-table string source table name -h, --help help for copy --metastore-uri string Hive metastore URI -p, --partition strings partition to copy --serde string serde to set copy to [default is to-table] --to-branch string lakeFS branch name --to-client-type string metastore type [hive, glue] --to-schema string destination schema name [default is from-branch] --to-table string destination table name [default is from-table] . lakectl metastore copy-all . Copy from one metastore to another . Synopsis . copy or merge requested tables between hive metastores. the destination tables will point to the selected branch . lakectl metastore copy-all [flags] . Options . --branch string lakeFS branch name --continue-on-error prevent copy-all from failing when a single table fails --dbfs-root dbfs:/ dbfs location root will replace dbfs:/ in the location before transforming --from-address string source metastore address --from-client-type string metastore type [hive, glue] -h, --help help for copy-all --schema-filter string filter for schemas to copy in metastore pattern (default \".*\") --table-filter string filter for tables to copy in metastore pattern (default \".*\") --to-address string destination metastore address --to-client-type string metastore type [hive, glue] . lakectl metastore copy-schema . Copy schema . Synopsis . Copy schema (without tables). the destination schema will point to the selected branch . lakectl metastore copy-schema [flags] . Options . --catalog-id string Glue catalog ID --dbfs-root dbfs:/ dbfs location root will replace dbfs:/ in the location before transforming --from-client-type string metastore type [hive, glue] --from-schema string source schema name -h, --help help for copy-schema --metastore-uri string Hive metastore URI --to-branch string lakeFS branch name --to-client-type string metastore type [hive, glue] --to-schema string destination schema name [default is from-branch] . lakectl metastore create-symlink . Create symlink table and data . Synopsis . create table with symlinks, and create the symlinks in s3 in order to access from external services that could only access s3 directly (e.g athena) . lakectl metastore create-symlink [flags] . Options . --branch string lakeFS branch name --catalog-id string Glue catalog ID --from-client-type string metastore type [hive, glue] --from-schema string source schema name --from-table string source table name -h, --help help for create-symlink --path string path to table on lakeFS --repo string lakeFS repository name --to-schema string destination schema name --to-table string destination table name . lakectl metastore diff . Show column and partition differences between two tables . lakectl metastore diff [flags] . Options . --catalog-id string Glue catalog ID --from-address string source metastore address --from-client-type string metastore type [hive, glue] --from-schema string source schema name --from-table string source table name -h, --help help for diff --metastore-uri string Hive metastore URI --to-address string destination metastore address --to-client-type string metastore type [hive, glue] --to-schema string destination schema name --to-table string destination table name [default is from-table] . lakectl metastore help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type metastore help [path to command] for full details. lakectl metastore help [command] [flags] . Options . -h, --help help for help . lakectl metastore import-all . Import from one metastore to another . Synopsis . import requested tables between hive metastores. the destination tables will point to the selected repository and branch table with location s3://my-s3-bucket/path/to/table will be transformed to location s3://repo-param/bucket-param/path/to/table . lakectl metastore import-all [flags] . Options . --branch string lakeFS branch name --continue-on-error prevent import-all from failing when a single table fails --dbfs-root dbfs:/ dbfs location root will replace dbfs:/ in the location before transforming --from-address string source metastore address --from-client-type string metastore type [hive, glue] -h, --help help for import-all --repo string lakeFS repo name --schema-filter string filter for schemas to copy in metastore pattern (default \".*\") --table-filter string filter for tables to copy in metastore pattern (default \".*\") --to-address string destination metastore address --to-client-type string metastore type [hive, glue] . lakectl refs-dump . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Dumps refs (branches, commits, tags) to the underlying object store . lakectl refs-dump &lt;repository uri&gt; [flags] . Options . -h, --help help for refs-dump . lakectl refs-restore . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Restores refs (branches, commits, tags) from the underlying object store to a bare repository . Synopsis . restores refs (branches, commits, tags) from the underlying object store to a bare repository. This command is expected to run on a bare repository (i.e. one created with ‘lakectl repo create-bare’). Since a bare repo is expected, in case of transient failure, delete the repository and recreate it as bare and retry. lakectl refs-restore &lt;repository uri&gt; [flags] . Examples . aws s3 cp s3://bucket/_lakefs/refs_manifest.json - | lakectl refs-restore lakefs://my-bare-repository --manifest - . Options . -h, --help help for refs-restore --manifest refs-dump path to a refs manifest json file (as generated by refs-dump). Alternatively, use \"-\" to read from stdin . lakectl repo . Manage and explore repos . Options . -h, --help help for repo . lakectl repo create . Create a new repository . lakectl repo create &lt;repository uri&gt; &lt;storage namespace&gt; [flags] . Examples . lakectl repo create lakefs://some-repo-name s3://some-bucket-name . Options . -d, --default-branch string the default branch of this repository (default \"main\") -h, --help help for create . lakectl repo create-bare . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Create a new repository with no initial branch or commit . lakectl repo create-bare &lt;repository uri&gt; &lt;storage namespace&gt; [flags] . Examples . lakectl create-bare lakefs://some-repo-name s3://some-bucket-name . Options . -d, --default-branch string the default branch name of this repository (will not be created) (default \"main\") -h, --help help for create-bare . lakectl repo delete . Delete existing repository . lakectl repo delete &lt;repository uri&gt; [flags] . Options . -h, --help help for delete -y, --yes Automatically say yes to all confirmations . lakectl repo help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type repo help [path to command] for full details. lakectl repo help [command] [flags] . Options . -h, --help help for help . lakectl repo list . List repositories . lakectl repo list [flags] . Options . --after string show results after this value (used for pagination) --amount int number of results to return (default 100) -h, --help help for list . lakectl show . See detailed information about an entity by ID (commit, user, etc) . lakectl show &lt;repository uri&gt; [flags] . Options . --commit string commit ID to show -h, --help help for show --show-meta-range-id when showing commits, also show meta range ID . lakectl tag . Create and manage tags within a repository . Synopsis . Create delete and list tags within a lakeFS repository . Options . -h, --help help for tag . lakectl tag create . Create a new tag in a repository . lakectl tag create &lt;tag uri&gt; &lt;commit uri&gt; [flags] . Examples . lakectl tag create lakefs://example-repo/example-tag lakefs://example-repo/2397cc9a9d04c20a4e5739b42c1dd3d8ba655c0b3a3b974850895a13d8bf9917 . Options . -f, --force override the tag if it exists -h, --help help for create . lakectl tag delete . Delete a tag from a repository . lakectl tag delete &lt;tag uri&gt; [flags] . Options . -h, --help help for delete . lakectl tag help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type tag help [path to command] for full details. lakectl tag help [command] [flags] . Options . -h, --help help for help . lakectl tag list . List tags in a repository . lakectl tag list &lt;repository uri&gt; [flags] . Examples . lakectl tag list lakefs://&lt;repository&gt; . Options . --after string show results after this value (used for pagination) --amount int number of results to return (default 100) -h, --help help for list . lakectl tag show . Show tag’s commit reference . lakectl tag show &lt;tag uri&gt; [flags] . Options . -h, --help help for show . ",
    "url": "/reference/commands.html#installing-the-lakectl-command-locally",
    "relUrl": "/reference/commands.html#installing-the-lakectl-command-locally"
  },"70": {
    "doc": "Command (CLI) Reference",
    "title": "Command (CLI) Reference",
    "content": " ",
    "url": "/reference/commands.html",
    "relUrl": "/reference/commands.html"
  },"71": {
    "doc": "Our commitment to open source",
    "title": "Our commitment to open source",
    "content": "lakeFS is an open source project under the Apache 2.0 license. The project was created and is supported by Treeverse, a commercial company, founded by engineers passionate about providing solutions to the evolving world of data engineering. Why did we choose to open the source of our core capabilities? . We believe in bottom up adoption of technologies. We believe collaborative communities have the power to bring the best solutions to the community. We believe that every engineer should be able to use, contribute and influence cutting edge technologies, so they can innovate in their domain. What is our commitment to open source? . We created lakeFS, our open source project, to provide a Git-like interface on top of object stores - that you can fully take advantage of with any data application at any scale. For that reason, we commit that the following capabilities are and will remain open source as part of lakeFS: . | All versioning capabilities | Git-Like interface for the versioning operations | Support for public object store APIs | Integrations with publicly available applications accessing an object store | CLI, API and GUI interfaces | . We also commit to keeping lakeFS scalable in throughput and performance. We are deeply committed to our community of engineers who use and contribute to the project. We are and will continue to be highly responsive and shape lakeFS together to provide the data versioning capabilities we are all looking for. What is lakeFS Cloud? . Treeverse offers lakeFS Cloud, which provides all the same benefits of the Git-like interface on top of object stores as a fully-managed service. The vision of lakeFS Cloud is to provide a managed data versioning and management solution for data practitioners. lakeFS Cloud will leverage the lakeFS open source technology, integrate capabilities and unique features, and lead its users to implement best practices. As part of our commitment to the open source values of transparency and interoperability, we believe everyone should be able to enjoy these benefits, regardless of whether or not they choose to use the managed offering. Because of that, we will not intentionally make it harder to build these features independently on top of the open source solution. ",
    "url": "/commitment.html",
    "relUrl": "/commitment.html"
  },"72": {
    "doc": "Configuration Reference",
    "title": "Configuration Reference",
    "content": " ",
    "url": "/reference/configuration.html",
    "relUrl": "/reference/configuration.html"
  },"73": {
    "doc": "Configuration Reference",
    "title": "Table of contents",
    "content": ". | Reference | Using Environment Variables | Example: Local Development | Example: AWS Deployment | Example: Google Storage | Example: MinIO | Example: Azure blob storage | . Configuring lakeFS is done using a yaml configuration file and/or environment variable. The configuration file location can be set with the ‘–config’ flag. If not specified, the the first file found in the following order will be used: . | ./config.yaml | $HOME/lakefs/config.yaml | /etc/lakefs/config.yaml | $HOME/.lakefs.yaml | . Configuration items can each be controlled by an environment variable. The variable name will have a prefix of LAKEFS_, followed by the name of the configuration, replacing every ‘.’ with a ‘_’. Example: LAKEFS_LOGGING_LEVEL controls logging.level. This reference uses . to denote the nesting of values. ",
    "url": "/reference/configuration.html#table-of-contents",
    "relUrl": "/reference/configuration.html#table-of-contents"
  },"74": {
    "doc": "Configuration Reference",
    "title": "Reference",
    "content": ". | logging.format (one of [\"json\", \"text\"] : \"text\") - Format to output log message in | logging.level (one of [\"TRACE\", \"DEBUG\", \"INFO\", \"WARN\", \"ERROR\", \"NONE\"] : \"DEBUG\") - Logging level to output | logging.output (string : \"-\") - A path or paths to write logs to. A - means the standard output, = means the standard error. | logging.file_max_size_mb (int : 100) - Output file maximum size in megabytes. | logging.files_keep (int : 0) - Numbe of log files to keep, default is all. | actions.enabled (bool : true) - Setting this to false will block hooks from being executed | database.connection_string (string : \"postgres://localhost:5432/postgres?sslmode=disable\") - PostgreSQL connection string to use | database.max_open_connections (int : 25) - Maximum number of open connections to the database | database.max_idle_connections (int : 25) - Sets the maximum number of connections in the idle connection pool | database.connection_max_lifetime (duration : 5m) - Sets the maximum amount of time a connection may be reused | listen_address (string : \"0.0.0.0:8000\") - A &lt;host&gt;:&lt;port&gt; structured string representing the address to listen on | auth.cache.enabled (bool : true) - Whether to cache access credentials and user policies in-memory. Can greatly improve throughput when enabled. | auth.cache.size (int : 1024) - How many items to store in the auth cache. Systems with a very high user count should use a larger value at the expense of ~1kb of memory per cached user. | auth.cache.ttl (time duration : \"20s\") - How long to store an item in the auth cache. Using a higher value reduces load on the database, but will cause changes longer to take effect for cached users. | auth.cache.jitter (time duration : \"3s\") - A random amount of time between 0 and this value is added to each item’s TTL. This is done to avoid a large bulk of keys expiring at once and overwhelming the database. | auth.encrypt.secret_key (string : required) - A random (cryptographically safe) generated string that is used for encryption and HMAC signing | auth.cookie_domain (string : \"\") - Domain attribute to set the access_token cookie on (the default is an empty string which defaults to the same host that sets the cookie) | auth.api.endpoint (string: https://external.service/api/v1) - URL to external Authorization Service described at authorization.yml; | auth.api.token (string: eyJhbGciOiJIUzI1NiIsInR5...) - API token used to authenticate requests to api endpoint . Note: It is best to keep this somewhere safe such as KMS or Hashicorp Vault, and provide it to the system at run time . | auth.ldap.server_endpoint (string : required) - If specified, also authenticate users via this LDAP server | auth.ldap.bind_dn (string : required) - Use this DN to bind lakeFS on the LDAP server for searching for users. | auth.ldap.bind_password (string : ) - If set, use this password for binding bind_dn. | auth.ldap.username_attribute (string : required) - Attribute holding login username on LDAP users, e.g. cn or uid. | auth.ldap.user_base_dn (string : required) - Base DN for searching for users. Search looks for users in the subtree below this. | auth.ldap.default_user_group (string : ) - Create all LDAP users in this group. Defaults to Viewers. | auth.ldap.user_filter (string : ) - Additional filter for users. | blockstore.type (one of [\"local\", \"s3\", \"gs\", \"azure\", \"mem\"] : required). Block adapter to use. This controls where the underlying data will be stored | blockstore.default_namespace_prefix (string : ) - Use this to help your users choose a storage namespace for their repositories. If specified, the storage namespace will be filled with this default value as a prefix, when creating a repository from the UI. The user may still change it to something else. | blockstore.local.path (string: \"~/lakefs/data\") - When using the local Block Adapter, which directory to store files in | blockstore.gs.credentials_file (string : ) - If specified will be used as a file path of the JSON file that contains your Google service account key | blockstore.gs.credentials_json (string : ) - If specified will be used as JSON string that contains your Google service account key (when credentials_file is not set) | blockstore.azure.storage_account (string : ) - If specified, will be used as the Azure storage account | blockstore.azure.storage_access_key (string : ) - If specified, will be used as the Azure storage access key | blockstore.azure.auth_method (one of [\"msi\", \"access-key\"]: \"access-key\" ) - Authentication method to use (msi is used for Azure AD authentication). | blockstore.s3.region (string : \"us-east-1\") - Default region for lakeFS to use when interacting with S3. | blockstore.s3.profile (string : ) - If specified, will be used as a named credentials profile | blockstore.s3.credentials_file (string : ) - If specified, will be used as a credentials file | blockstore.s3.credentials.access_key_id (string : ) - If specified, will be used as a static set of credential | blockstore.s3.credentials.secret_access_key (string : ) - If specified, will be used as a static set of credential | blockstore.s3.credentials.session_token (string : ) - If specified, will be used as a static session token | blockstore.s3.endpoint (string : ) - If specified, custom endpoint for the AWS S3 API (https://s3_compatible_service_endpoint:port) | blockstore.s3.force_path_style (boolean : false) - When true, use path-style S3 URLs (https:/// instead of https://.) . | blockstore.s3.streaming_chunk_size (int : 1048576) - Object chunk size to buffer before streaming to blockstore (use a lower value for less reliable networks). Minimum is 8192. | blockstore.s3.streaming_chunk_timeout (time duration : \"60s\") - Per object chunk timeout for blockstore streaming operations (use a larger value for less reliable networks). | blockstore.s3.discover_bucket_region (boolean : true) - (Can be turned off if the underlying S3 bucket doesn’t support the GetBucketRegion API). | committed.local_cache - an object describing the local (on-disk) cache of metadata from permanent storage: . | committed.local_cache.size_bytes (int : 1073741824) - bytes for local cache to use on disk. The cache may use more storage for short periods of time. | committed.local_cache.dir (string, ~/lakefs/local_tier) - directory to store local cache. | committed.local_cache.range_proportion (float : 0.9) - proportion of local cache to use for storing ranges (leaves of committed metadata storage). | committed.local_cache.range.open_readers (int : 500) - maximal number of unused open SSTable readers to keep for ranges. | committed.local_cache.range.num_shards (int : 30) - sharding factor for open SSTable readers for ranges. Should be at least sqrt(committed.local_cache.range.open_readers). | committed.local_cache.metarange_proportion (float : 0.1) - proportion of local cache to use for storing metaranges (roots of committed metadata storage). | committed.local_cache.metarange.open_readers (int : 50) - maximal number of unused open SSTable readers to keep for metaranges. | committed.local_cache.metarange.num_shards (int : 10) - sharding factor for open SSTable readers for metaranges. Should be at least sqrt(committed.local_cache.metarange.open_readers). | . | committed.block_storage_prefix (string : _lakefs) - Prefix for metadata file storage in each repository’s storage namespace | committed.permanent.min_range_size_bytes (int : 0) - Smallest allowable range in metadata. Increase to somewhat reduce random access time on committed metadata, at the cost of increased committed metadata storage cost. | committed.permanent.max_range_size_bytes (int : 20971520) - Largest allowable range in metadata. Should be close to the size at which fetching from remote storage becomes linear. | committed.permanent.range_raggedness_entries (int : 50_000) - Average number of object pointers to store in each range (subject to min_range_size_bytes and max_range_size_bytes). | committed.sstable.memory.cache_size_bytes (int : 200_000_000) - maximal size of in-memory cache used for each SSTable reader. | email.smtp_host (string) - A string representing the URL of the SMTP host. | email.smtp_port (int) - An integer representing the port of the SMTP service (465, 587, 993, 25 are some standard ports) | email.use_ssl (bool : false) - Use SSL connection with SMTP host. | email.username (string) - A string representing the username of the specific account at the SMTP. It’s recommended to provide this value at runtime from a secret vault of some sort. | email.password (string) - A string representing the password of the account. It’s recommended to provide this value at runtime from a secret vault of some sort. | email.local_name (string) - A string representing the hostname sent to the SMTP server with the HELO command. By default, “localhost” is sent. | email.sender (string) - A string representing the email account which is set as the sender. | email.limit_every_duration (duration : 1m) - The average time between sending emails. If zero is entered, there is no limit to the amount of emails that can be sent. | email.burst (int: 10) - Maximal burst of emails before applying limit_every_duration. The zero value means no burst and therefore no emails can be sent. | email.lakefs_base_url (string : \"http://localhost:8000\") - A string representing the base lakefs endpoint to be directed to when emails are sent inviting users, reseting passwords etc. | gateways.s3.domain_name (string : \"s3.local.lakefs.io\") - a FQDN representing the S3 endpoint used by S3 clients to call this server (*.s3.local.lakefs.io always resolves to 127.0.0.1, useful for local development, if using virtual-host addressing. | gateways.s3.region (string : \"us-east-1\") - AWS region we’re pretending to be. Should match the region configuration used in AWS SDK clients | gateways.s3.fallback_url (string) - If specified, requests with a non-existing repository will be forwarded to this url. This can be useful for using lakeFS side-by-side with S3, with the URL pointing at an S3Proxy instance. | stats.enabled (boolean : true) - Whether or not to periodically collect anonymous usage statistics | security.audit_check_interval (duration : 12h) - Duration in which we check for security audit | . ",
    "url": "/reference/configuration.html#reference",
    "relUrl": "/reference/configuration.html#reference"
  },"75": {
    "doc": "Configuration Reference",
    "title": "Using Environment Variables",
    "content": "All configuration variables can be set or overridden using environment variables. To set an environment variable, prepend LAKEFS_ to its name, convert it to upper case, and replace . with _: . For example, logging.format becomes LAKEFS_LOGGING_FORMAT, blockstore.s3.region becomes LAKEFS_BLOCKSTORE_S3_REGION, etc. ",
    "url": "/reference/configuration.html#using-environment-variables",
    "relUrl": "/reference/configuration.html#using-environment-variables"
  },"76": {
    "doc": "Configuration Reference",
    "title": "Example: Local Development",
    "content": "--- listen_address: \"0.0.0.0:8000\" database: connection_string: \"postgres://localhost:5432/postgres?sslmode=disable\" logging: format: text level: DEBUG output: \"-\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc09e90b6641\" blockstore: type: local local: path: \"~/lakefs/dev/data\" gateways: s3: region: us-east-1 . ",
    "url": "/reference/configuration.html#example-local-development",
    "relUrl": "/reference/configuration.html#example-local-development"
  },"77": {
    "doc": "Configuration Reference",
    "title": "Example: AWS Deployment",
    "content": "--- logging: format: json level: WARN output: \"-\" database: connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: s3 s3: region: us-east-1 # optional, fallback in case discover from bucket is not supported credentials_file: /secrets/aws/credentials profile: default . ",
    "url": "/reference/configuration.html#example-aws-deployment",
    "relUrl": "/reference/configuration.html#example-aws-deployment"
  },"78": {
    "doc": "Configuration Reference",
    "title": "Example: Google Storage",
    "content": "--- logging: format: json level: WARN output: \"-\" database: connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: gs gs: credentials_file: /secrets/lakefs-service-account.json . ",
    "url": "/reference/configuration.html#example-google-storage",
    "relUrl": "/reference/configuration.html#example-google-storage"
  },"79": {
    "doc": "Configuration Reference",
    "title": "Example: MinIO",
    "content": "--- logging: format: json level: WARN output: \"-\" database: connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: s3 s3: force_path_style: true endpoint: http://localhost:9000 discover_bucket_region: false credentials: access_key_id: minioadmin secret_access_key: minioadmin . ",
    "url": "/reference/configuration.html#example-minio",
    "relUrl": "/reference/configuration.html#example-minio"
  },"80": {
    "doc": "Configuration Reference",
    "title": "Example: Azure blob storage",
    "content": "--- logging: format: json level: WARN output: \"-\" database: connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: azure azure: auth_method: access-key storage_account: exampleStorageAcount storage_access_key: ExampleAcessKeyMD7nkPOWgV7d4BUjzLw== . ",
    "url": "/reference/configuration.html#example-azure-blob-storage",
    "relUrl": "/reference/configuration.html#example-azure-blob-storage"
  },"81": {
    "doc": "Contributing",
    "title": "Contributing to lakeFS",
    "content": "Thank you for your interest in contributing to our project. Whether it’s a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution. Don’t know where to start? Reach out on the #dev channel on our Slack and we will help you get started. We also recommend this free series about contributing to OSS projects. ",
    "url": "/contributing.html#contributing-to-lakefs",
    "relUrl": "/contributing.html#contributing-to-lakefs"
  },"82": {
    "doc": "Contributing",
    "title": "Getting Started",
    "content": "Before you get started, we ask that you: . | Check out the code of conduct. | Sign the lakeFS CLA when making your first pull request (individual / corporate) | Submit any security issues directly to security@treeverse.io. | Contributions should have an associated GitHub issue. | Before making major contributions, please reach out to us on the #dev channel on Slack. We will make sure no one else is working on the same feature. | . ",
    "url": "/contributing.html#getting-started",
    "relUrl": "/contributing.html#getting-started"
  },"83": {
    "doc": "Contributing",
    "title": "Setting up an Environment",
    "content": "This section was tested on macOS and Linux (Fedora 32, Ubuntu 20.04) - Your mileage may vary . Our Go release workflow holds the Go and Node.js versions we currently use under go-version and node-version compatibly. The Java workflows use Maven 3.8.1 (but any recent version of Maven should work). | Install the required dependencies for your OS: . | Git | GNU make (probably best to install from your OS package manager such as apt or brew) | Docker | Go | Node.js &amp; npm | Maven to build and test Spark client codes. | Optional - PostgreSQL 11 (useful for running and debugging locally) | . | With Apple M1, you can install Java from Azul Zulu Builds for Java JDK. | . | Clone the repository from https://github.com/treeverse/lakeFS (gives you read-only access to the repository. To contribute, see the next section). | Build the project: . make build . | Make sure tests are passing: . make test . | . ",
    "url": "/contributing.html#setting-up-an-environment",
    "relUrl": "/contributing.html#setting-up-an-environment"
  },"84": {
    "doc": "Contributing",
    "title": "Before creating a pull request",
    "content": ". | Review this document in full | Make sure there’s an open issue on GitHub that this pull request addresses, and that it isn’t labeled x/wontfix | Fork the lakeFS repository | If you’re adding new functionality, create a new branch named feature/&lt;DESCRIPTIVE NAME&gt; | If you’re fixing a bug, create a new branch named fix/&lt;DESCRIPTIVE NAME&gt;-&lt;ISSUE NUMBER&gt; | . ",
    "url": "/contributing.html#before-creating-a-pull-request",
    "relUrl": "/contributing.html#before-creating-a-pull-request"
  },"85": {
    "doc": "Contributing",
    "title": "Testing your change",
    "content": "Once you’ve made the necessary changes to the code, make sure tests pass: . Run unit tests: . make test . Check linting rules are passing: . make checks-validator . lakeFS uses go fmt as a style guide for Go code. Run system-tests: . make system-tests . Want to dive deeper into our system tests infrastructure? Need to debug the tests? Follow this documentation. ",
    "url": "/contributing.html#testing-your-change",
    "relUrl": "/contributing.html#testing-your-change"
  },"86": {
    "doc": "Contributing",
    "title": "Submitting a pull request",
    "content": "Open a GitHub pull request with your change. The PR description should include a brief explanation of your change. You should also mention the related GitHub issue. If the issue should be automatically closed after the merge, please link it to the PR. After submitting your pull request, GitHub Actions will automatically run tests on your changes and make sure that your updated code builds and runs on Go 1.17.x. Check back shortly after submitting your pull request to make sure that your code passes these checks. If any of the checks come back with a red X, then do your best to address the errors. A developer from our team will review your pull request, and may request some changes to it. After the request is approved, it will be merged to our main branch. ",
    "url": "/contributing.html#submitting-a-pull-request",
    "relUrl": "/contributing.html#submitting-a-pull-request"
  },"87": {
    "doc": "Contributing",
    "title": "Documentation",
    "content": "Documentation of features and changes in behaviour should be included in the pull-request. You can create separate pull requests for documentation changes only. Documentation site customizations should be performed in accordance with the Just The Docs Customization guide, which is applied during the site creation process. CHANGELOG.md . Any user-facing change should be labeled with include-changelog. The PR title should contain a concise summary of the feature or fix, and the description should have the GitHub issue number. When we publish a new version of lakeFS, we will add this to the relevant version section of the changelog. If the change should not be included in the changelog, label it with exclude-changelog. ",
    "url": "/contributing.html#documentation",
    "relUrl": "/contributing.html#documentation"
  },"88": {
    "doc": "Contributing",
    "title": "Contributing",
    "content": " ",
    "url": "/contributing.html",
    "relUrl": "/contributing.html"
  },"89": {
    "doc": "Create a Repository",
    "title": "Create a Repository",
    "content": "A repository contains all of your objects, including the revision history. It can be considered the lakeFS analog of a bucket in an object store. Since it has version control qualities, it is also analogous to a repository in Git. ",
    "url": "/setup/create-repo.html",
    "relUrl": "/setup/create-repo.html"
  },"90": {
    "doc": "Create a Repository",
    "title": "Create the first user",
    "content": "When you first open the lakeFS UI, you will be asked to create an initial admin user. | In your browser, open the address of your lakeFS server. Depending on how you deployed lakeFS, this can be a custom address pointing at your server (e.g. https://lakefs.example.com), the address of a load balancer, or something else. You should see the following page, prompting you to set up an admin user. Note: If you already have lakeFS credentials, log in and skip to creating the repository. | Follow the steps to create an initial administrator user. Save the credentials you’ve received somewhere safe, you won’t be able to see them again! . | Follow the link and go to the login screen. | Use the credentials to login as an administrator. | . ",
    "url": "/setup/create-repo.html#create-the-first-user",
    "relUrl": "/setup/create-repo.html#create-the-first-user"
  },"91": {
    "doc": "Create a Repository",
    "title": "Create the repository",
    "content": ". | Click Create Repository. | Fill in a repository name. | Set the Storage Namespace to a location in the bucket you’ve configured in a previous step. The storage namespace is a location in the underlying storage where data for this repository will be stored. | Click Create Repository. | . ",
    "url": "/setup/create-repo.html#create-the-repository",
    "relUrl": "/setup/create-repo.html#create-the-repository"
  },"92": {
    "doc": "Create a Repository",
    "title": "Next steps",
    "content": "You just created your first lakeFS repository! . | You may now want to import data into your repository. | Check out how lakeFS easily integrates with your other tools. | Join us on Slack to introduce yourself, discover best practices and share your own! | . ",
    "url": "/setup/create-repo.html#next-steps",
    "relUrl": "/setup/create-repo.html#next-steps"
  },"93": {
    "doc": "In Development",
    "title": "In Development",
    "content": "As part of our routine work with data we develop new code, improve and upgrade old code, upgrade infrastructures, and test new technologies. lakeFS enables a safe development environment on your data lake without the need to copy or mock data, work on the pipelines or involve DevOps. Creating a branch provides you an isolated environment with a snapshot of your repository (any part of your data lake you chose to manage on lakeFS). While working on your own branch in isolation, all other data users will be looking at the repository’s main branch. They can’t see your changes, and you don’t see changes to main done after you created the branch. No worries, no data duplication is done, it’s all metadata management behind the scenes. Let’s look at 3 examples of a development environment and their branching models. Example 1: Upgrading Spark and using Reset action . You installed the latest version of Apache Spark. As a first step you’ll test your Spark jobs to see that the upgrade doesn’t have any undesired side effects. For this purpose, you may create a branch (testing-spark-3.0) which will only be used to test the Spark upgrade, and discarded later. Jobs may run smoothly (the theoretical possibility exists!), or they may fail halfway through, leaving you with some intermediate partitions, data and metadata. In this case, you can simply reset the branch to its original state, without worrying about the intermediate results of your last experiment, and perform another (hopefully successful) test in an isolated branch. Reset actions are atomic and immediate, so no manual cleanup is required. Once testing is completed, and you have achieved the desired result, you can delete this experimental branch, and all data not used on any other branch will be deleted with it. Creating a testing branch: . lakectl branch create \\ lakefs://example-repo/testing-spark-3 \\ --source lakefs://example-repo/main # output: # created branch 'testing-spark-3' . Resetting changes to a branch: . lakectl branch reset lakefs://example-repo/testing-spark-3 # are you sure you want to reset all uncommitted changes?: y█ . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. Example 2: Collaborate &amp; Compare - Which option is better? . Easily compare by testing which one performs better on your data set. Examples may be: . | Different computation tools, e.g Spark vs. Presto | Different compression algorithms | Different Spark configurations | Different code versions of an ETL | . Run each experiment on its own independent branch, while the main remains untouched. Once both experiments are done, create a comparison query (using Hive or Presto or any other tool of your choice) to compare data characteristics, performance or any other metric you see fit. With lakeFS you don’t need to worry about creating data paths for the experiments, copying data, and remembering to delete it. It’s substantially easier to avoid errors and maintain a clean lake after. Reading from and comparing branches using Spark: . val dfExperiment1 = sc.read.parquet(\"s3a://example-repo/experiment-1/events/by-date\") val dfExperiment2 = sc.read.parquet(\"s3a://example-repo/experiment-2/events/by-date\") dfExperiment1.groupBy(\"...\").count() dfExperiment2.groupBy(\"...\").count() // now we can compare the properties of the data itself . ",
    "url": "/usecases/data-devenv.html",
    "relUrl": "/usecases/data-devenv.html"
  },"94": {
    "doc": "Databricks",
    "title": "Using lakeFS with Databricks",
    "content": "Databricks is an Apache Spark-based analytics platform. ",
    "url": "/integrations/databricks.html#using-lakefs-with-databricks",
    "relUrl": "/integrations/databricks.html#using-lakefs-with-databricks"
  },"95": {
    "doc": "Databricks",
    "title": "Table of contents",
    "content": ". | Configuration . | When running lakeFS inside your VPC . | Using multi-cluster writes | . | . | Reading Data | Writing Data | Case Study: SimilarWeb | . ",
    "url": "/integrations/databricks.html#table-of-contents",
    "relUrl": "/integrations/databricks.html#table-of-contents"
  },"96": {
    "doc": "Databricks",
    "title": "Configuration",
    "content": "For Databricks to work with lakeFS, set the S3 Hadoop configuration to the lakeFS endpoint and credentials: . | In databricks, go to your cluster configuration page. | Click Edit. | Expand Advanced Options | Under the Spark tab, add the following configurations, replacing &lt;repo-name&gt; with your lakeFS repository name. Also replace the credentials and endpoint with those of your lakeFS installation. | . spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.access.key AKIAIOSFODNN7EXAMPLE spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.secret.key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.endpoint https://lakefs.example.com spark.hadoop.fs.s3a.path.style.access true . When using DeltaLake tables, the following is also needed in some versions of Databricks: . spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.aws.credentials.provider shaded.databricks.org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.session.token lakefs . For more information, see the documentation from Databricks. When running lakeFS inside your VPC . When lakeFS runs inside your private network, your Databricks cluster needs to be able to access it. This can be done by setting up a VPC peering between the two VPCs (the one where lakeFS runs, and the one where Databricks runs). For this to work on DeltaLake tables, you would also have to disable multi-cluster writes with: . spark.databricks.delta.multiClusterWrites.enabled false . Using multi-cluster writes . When using multi-cluster writes, Databricks overrides Delta’s s3-commit action. The new action tries to contact lakeFS from servers on Databricks own AWS account, which of course will not be able to access your private network. So, if you must use multi-cluster writes, your will have to allow access from Databricks’ AWS account to lakeFS. We are researching for the best ways to achieve that, and will update here soon. ",
    "url": "/integrations/databricks.html#configuration",
    "relUrl": "/integrations/databricks.html#configuration"
  },"97": {
    "doc": "Databricks",
    "title": "Reading Data",
    "content": "In order for us to access objects in lakeFS we will need to use the lakeFS path conventions: s3a://[REPOSITORY]/[BRANCH]/PATH/TO/OBJECT . Here is an example for reading a parquet file from lakeFS to a Spark DataFrame: . val repo = \"example-repo\" val branch = \"main\" val dataPath = s\"s3a://${repo}/${branch}/example-path/example-file.parquet\" val df = spark.read.parquet(dataPath) . You can now use this DataFrame like you would normally do. ",
    "url": "/integrations/databricks.html#reading-data",
    "relUrl": "/integrations/databricks.html#reading-data"
  },"98": {
    "doc": "Databricks",
    "title": "Writing Data",
    "content": "Now simply write your results back to a lakeFS path: . df.write .partitionBy(\"example-column\") .parquet(s\"s3a://${repo}/${branch}/output-path/\") . The data is now created in lakeFS as new changes in your branch. You can now commit these changes, or revert them. ",
    "url": "/integrations/databricks.html#writing-data",
    "relUrl": "/integrations/databricks.html#writing-data"
  },"99": {
    "doc": "Databricks",
    "title": "Case Study: SimilarWeb",
    "content": "See how SimilarWeb integrated lakeFS with DataBricks. ",
    "url": "/integrations/databricks.html#case-study-similarweb",
    "relUrl": "/integrations/databricks.html#case-study-similarweb"
  },"100": {
    "doc": "Databricks",
    "title": "Databricks",
    "content": " ",
    "url": "/integrations/databricks.html",
    "relUrl": "/integrations/databricks.html"
  },"101": {
    "doc": "dbt",
    "title": "Maintaining environments with dbt and lakeFS",
    "content": "dbt can run on lakeFS with the Spark adapter or the Presto/Trino adapter. Both Spark and Presto use Hive metastore or Glue in order to manage tables and views. When creating a branch in lakeFS we receive a logical copy of the data that could be accessed by s3://my-repo/branch/... In order to run our dbt project on a new created branch we need to have a copy of the metadata as well. The lakectl dbt command generates all the metadata needed in order to work on the new created branch, continuing from the last state in the source branch. The dbt lakectl command does this using dbt commands and lakectl metastore commands. ",
    "url": "/integrations/dbt.html#maintaining-environments-with-dbt-and-lakefs",
    "relUrl": "/integrations/dbt.html#maintaining-environments-with-dbt-and-lakefs"
  },"102": {
    "doc": "dbt",
    "title": "Table of contents",
    "content": ". | Configuration . | Hive metastore | Glue | . | Views . | Using lakectl | Manually | . | Create Schema | . ",
    "url": "/integrations/dbt.html#table-of-contents",
    "relUrl": "/integrations/dbt.html#table-of-contents"
  },"103": {
    "doc": "dbt",
    "title": "Configuration",
    "content": "In order to run the lakectl-dbt commands you need to configure both dbt and lakectl. Assuming dbt is already configured using either a Spark or Presto/Trino target you will need to add configurations to give lakeFS access to your catalog (metastore). This is done by adding the following configurations to the lakectl configuration file (by default ~/.lakectl.yaml) . Hive metastore . metastore: type: hive hive: uri: hive-metastore:9083 . Glue . metastore: type: glue glue: catalog-id: 123456789012 region: us-east-1 profile: default # optional, implies using a credentials file credentials: access_key_id: AKIAIOSFODNN7EXAMPLE secret_access_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY . ",
    "url": "/integrations/dbt.html#configuration",
    "relUrl": "/integrations/dbt.html#configuration"
  },"104": {
    "doc": "dbt",
    "title": "Views",
    "content": "lakectl copies all models materialized as tables and incremental directly on your metastore. However, copying views should be done manually or with lakectl. Using lakectl . The generate_schema_name macro could be used by lakectl to create models using dbt on a dynamic schema. The following command will add a macro to your project allowing lakectl to run dbt on the destination schema using an environment variable . lakectl dbt generate-schema-macro . Manually . In case you don’t want to add the generate_schema_name macro to your project you could create the views on the destination schema manually. For every run: . | use the --skip-views flag | change the default schema to be the branch schema in your dbt configuration file | run dbt on all views | . dbt run --select config.materialized:view . ",
    "url": "/integrations/dbt.html#views",
    "relUrl": "/integrations/dbt.html#views"
  },"105": {
    "doc": "dbt",
    "title": "Create Schema",
    "content": "Creating the schema From your dbt project run: . lakectl dbt create-branch-schema --branch my-branch --to-schema my_branch . Advanced options could be found here . ",
    "url": "/integrations/dbt.html#create-schema",
    "relUrl": "/integrations/dbt.html#create-schema"
  },"106": {
    "doc": "dbt",
    "title": "dbt",
    "content": " ",
    "url": "/integrations/dbt.html",
    "relUrl": "/integrations/dbt.html"
  },"107": {
    "doc": "Delta Lake",
    "title": "Using lakeFS with Delta Lake",
    "content": "Delta Lake is an open file format designed to improve performance and provide transactional guarantees to data lake tables. lakeFS is format-agnostic, so you can save data in Delta format within a lakeFS repository to get the benefits of both technologies. Specifically: . | ACID operations can now span across many Delta tables. | CI/CD hooks can validate Delta table contents, schema, or even referential integrity. | lakeFS supports zero-copy branching for quick experimentation with full isolation. | . ",
    "url": "/integrations/delta.html#using-lakefs-with-delta-lake",
    "relUrl": "/integrations/delta.html#using-lakefs-with-delta-lake"
  },"108": {
    "doc": "Delta Lake",
    "title": "Table of contents",
    "content": ". | Configuration | Limitations | Read more | . ",
    "url": "/integrations/delta.html#table-of-contents",
    "relUrl": "/integrations/delta.html#table-of-contents"
  },"109": {
    "doc": "Delta Lake",
    "title": "Configuration",
    "content": "Most commonly Delta tables are interacted with in a Spark environment given the native integration between Delta Lake and Spark. To configure a Spark environment to read from and write to a Delta table within a lakeFS repository, we need to set the proper credentials and endpoint in the S3 Hadoop configuration, like we do with any Spark script. sc.hadoopConfiguration.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") sc.hadoopConfiguration.set(\"spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.access.key\", \"AKIAIOSFODNN7EXAMPLE\") sc.hadoopConfiguration.set(\"spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\") sc.hadoopConfiguration.set(\"spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.endpoint\", \"https://lakefs.example.com\") . Once set, you can now interact with Delta tables using regular Spark path URIs. Make sure you include the lakeFS repository and branch name: . data.write.format(\"delta\").save(\"s3a://&lt;repo-name&gt;/&lt;branch-name&gt;/path/to/delta-table\") . Note: If using the Databricks Analytics Platform, see the integration guide for configuring a Databricks cluster to use lakeFS. ",
    "url": "/integrations/delta.html#configuration",
    "relUrl": "/integrations/delta.html#configuration"
  },"110": {
    "doc": "Delta Lake",
    "title": "Limitations",
    "content": "The Delta log is an auto-generated sequence of text files used to keep track of transactions on a Delta table sequentially. Writing to one Delta table from multiple lakeFS branches is possible, but note that it will result in conflicts if later attempting to merge one branch into the other. For that reason, production workflows should ideally write to a single lakeFS branch that could then be safely merged into main. ",
    "url": "/integrations/delta.html#limitations",
    "relUrl": "/integrations/delta.html#limitations"
  },"111": {
    "doc": "Delta Lake",
    "title": "Read more",
    "content": "See this post on the lakeFS blog that shows how to guarantee data quality in a Delta table by utilizing lakeFS branches. ",
    "url": "/integrations/delta.html#read-more",
    "relUrl": "/integrations/delta.html#read-more"
  },"112": {
    "doc": "Delta Lake",
    "title": "Delta Lake",
    "content": " ",
    "url": "/integrations/delta.html",
    "relUrl": "/integrations/delta.html"
  },"113": {
    "doc": "Copying Data with DistCp",
    "title": "Copying Data to/from lakeFS with DistCp",
    "content": "Apache Hadoop DistCp (distributed copy) is a tool used for large inter/intra-cluster copying. You can easily use it with your lakeFS repositories. ",
    "url": "/integrations/distcp.html#copying-data-tofrom-lakefs-with-distcp",
    "relUrl": "/integrations/distcp.html#copying-data-tofrom-lakefs-with-distcp"
  },"114": {
    "doc": "Copying Data with DistCp",
    "title": "Table of contents",
    "content": ". | Copying from lakeFS to lakeFS | Copying between S3 and lakeFS . | From S3 to lakeFs | From lakeFS to S3 | . | . Note In the following examples we set AWS credentials on the command line, for clarity. In production, these properties should be set using one of Hadoop’s standard ways of Authenticating with S3. ",
    "url": "/integrations/distcp.html#table-of-contents",
    "relUrl": "/integrations/distcp.html#table-of-contents"
  },"115": {
    "doc": "Copying Data with DistCp",
    "title": "Copying from lakeFS to lakeFS",
    "content": "You can use DistCP to copy between two different lakeFS repositories. Replace the access key pair with your lakeFS access key pair: . hadoop distcp \\ -Dfs.s3a.path.style.access=true \\ -Dfs.s3a.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\ -Dfs.s3a.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\ -Dfs.s3a.endpoint=\"https://lakefs.example.com\" \\ \"s3a://example-repo-1/main/example-file.parquet\" \\ \"s3a://example-repo-2/main/example-file.parquet\" . val workDir = s”s3a://${repo}/${branch}/collection/shows” val dataPath = s”$workDir/title.basics.parquet” . ",
    "url": "/integrations/distcp.html#copying-from-lakefs-to-lakefs",
    "relUrl": "/integrations/distcp.html#copying-from-lakefs-to-lakefs"
  },"116": {
    "doc": "Copying Data with DistCp",
    "title": "Copying between S3 and lakeFS",
    "content": "In order to copy between an S3 bucket and lakeFS repository, use Hadoop’s per-bucket configuration. In the following examples, replace the first access key pair with your lakeFS key pair, and the second one with your AWS IAM key pair: . From S3 to lakeFs . hadoop distcp \\ -Dfs.s3a.path.style.access=true \\ -Dfs.s3a.bucket.example-repo.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\ -Dfs.s3a.bucket.example-repo.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\ -Dfs.s3a.bucket.example-repo.endpoint=\"https://lakefs.example.com\" \\ -Dfs.s3a.bucket.example-bucket.access.key=\"AKIAIOSFODNN3EXAMPLE\" \\ -Dfs.s3a.bucket.example-bucket.secret.key=\"wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY\" \\ \"s3a://example-bucket/example-file.parquet\" \\ \"s3a://example-repo/main/example-file.parquet\" . From lakeFS to S3 . hadoop distcp \\ -Dfs.s3a.path.style.access=true \\ -Dfs.s3a.bucket.example-repo.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\ -Dfs.s3a.bucket.example-repo.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\ -Dfs.s3a.bucket.example-repo.endpoint=\"https://lakefs.example.com\" \\ -Dfs.s3a.bucket.example-bucket.access.key=\"AKIAIOSFODNN3EXAMPLE\" \\ -Dfs.s3a.bucket.example-bucket.secret.key=\"wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY\" \\ \"s3a://example-repo/main/myfile\" \\ \"s3a://example-bucket/myfile\" . ",
    "url": "/integrations/distcp.html#copying-between-s3-and-lakefs",
    "relUrl": "/integrations/distcp.html#copying-between-s3-and-lakefs"
  },"117": {
    "doc": "Copying Data with DistCp",
    "title": "Copying Data with DistCp",
    "content": " ",
    "url": "/integrations/distcp.html",
    "relUrl": "/integrations/distcp.html"
  },"118": {
    "doc": "With Docker",
    "title": "Deploy lakeFS on Docker",
    "content": " ",
    "url": "/deploy/docker.html#deploy-lakefs-on-docker",
    "relUrl": "/deploy/docker.html#deploy-lakefs-on-docker"
  },"119": {
    "doc": "With Docker",
    "title": "Database",
    "content": "lakeFS requires a PostgreSQL database to synchronize actions on your repositories. This section assumes you already have a PostgreSQL database accessible from where you intend to install lakeFS. Instructions for creating the database can be found on the deployment instructions for AWS, Azure and GCP. ",
    "url": "/deploy/docker.html#database",
    "relUrl": "/deploy/docker.html#database"
  },"120": {
    "doc": "With Docker",
    "title": "Table of contents",
    "content": ". | Prerequisites | Installing on Docker | Load balancing | Next Steps | . ",
    "url": "/deploy/docker.html#table-of-contents",
    "relUrl": "/deploy/docker.html#table-of-contents"
  },"121": {
    "doc": "With Docker",
    "title": "Prerequisites",
    "content": "Users that require S3 access using virtual host addressing should configure an S3 Gateway domain. ",
    "url": "/deploy/docker.html#prerequisites",
    "relUrl": "/deploy/docker.html#prerequisites"
  },"122": {
    "doc": "With Docker",
    "title": "Installing on Docker",
    "content": "To deploy using Docker, create a yaml configuration file. Here is a minimal example, but you can see the reference for the full list of configurations. | AWS | Google Cloud | Microsoft Azure | . database: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: s3 . database: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: gs # Uncomment the following lines to give lakeFS access to your buckets using a service account: # gs: # credentials_json: [YOUR SERVICE ACCOUNT JSON STRING] . database: connection_string: \"postgres://user:pass@&lt;AZURE_POSTGRES_SERVER_NAME&gt;...\" auth: encrypt: secret_key: \"&lt;RANDOM_GENERATED_STRING&gt;\" blockstore: type: azure azure: auth_method: msi # msi for active directory, access-key for access key # In case you chose to authenticate via access key replace unmark the following rows and insert the values from the previous step # storage_account: &lt;your storage account&gt; # storage_access_key: &lt;your access key&gt; . Save the configuration file locally as lakefs-config.yaml and run the following command: . docker run \\ --name lakefs \\ -p 8000:8000 \\ -v $(pwd)/lakefs-config.yaml:/etc/lakefs/config.yaml \\ treeverse/lakefs:latest run --config /etc/lakefs/config.yaml . ",
    "url": "/deploy/docker.html#installing-on-docker",
    "relUrl": "/deploy/docker.html#installing-on-docker"
  },"123": {
    "doc": "With Docker",
    "title": "Load balancing",
    "content": "You should have a load balancer direct requests to the lakeFS server. By default, lakeFS operates on port 8000, and exposes a /_health endpoint which you can use for health checks. ",
    "url": "/deploy/docker.html#load-balancing",
    "relUrl": "/deploy/docker.html#load-balancing"
  },"124": {
    "doc": "With Docker",
    "title": "Next Steps",
    "content": "Your next step is to prepare your storage. If you already have a storage bucket/container, you are ready to create your first lakeFS repository. ",
    "url": "/deploy/docker.html#next-steps",
    "relUrl": "/deploy/docker.html#next-steps"
  },"125": {
    "doc": "With Docker",
    "title": "With Docker",
    "content": " ",
    "url": "/deploy/docker.html",
    "relUrl": "/deploy/docker.html"
  },"126": {
    "doc": "Dremio",
    "title": "Using lakeFS with Dremio",
    "content": "Dremio is a next-generation data lake engine that liberates your data with live, interactive queries directly on cloud data lake storage, including S3 and lakeFS. ",
    "url": "/integrations/dremio.html#using-lakefs-with-dremio",
    "relUrl": "/integrations/dremio.html#using-lakefs-with-dremio"
  },"127": {
    "doc": "Dremio",
    "title": "Configuration",
    "content": "Starting from 3.2.3, Dremio supports Minio as an experimental S3-compatible plugin. Similarly, we can connect lakeFS with Dremio. Suppose you already have both lakeFS and Dremio deployed, and want to utilize Dremio to query your data in the lakeFS repositories. You can follow below steps to configure on Dremio UI: . | click Add Data Lake. | Under File Stores, choose Amazon S3. | Under Advanced Options, check Enable compatibility mode (experimental). | Under Advanced Options &gt; Connection Properties, add fs.s3a.path.style.access and set the value to true. | Under Advanced Options &gt; Connection Properties, add fs.s3a.endpoint and set lakeFS S3 endpoint to the value. | Under the General tab, specify the access_key_id and secret_access_key provided by lakeFS server. | Click Save, and now you should be able to browse lakeFS repositories on Dremio. | . ",
    "url": "/integrations/dremio.html#configuration",
    "relUrl": "/integrations/dremio.html#configuration"
  },"128": {
    "doc": "Dremio",
    "title": "Dremio",
    "content": " ",
    "url": "/integrations/dremio.html",
    "relUrl": "/integrations/dremio.html"
  },"129": {
    "doc": "EMR",
    "title": "Using lakeFS with EMR",
    "content": "Amazon EMR is a managed cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark. ",
    "url": "/integrations/emr.html#using-lakefs-with-emr",
    "relUrl": "/integrations/emr.html#using-lakefs-with-emr"
  },"130": {
    "doc": "EMR",
    "title": "Configuration",
    "content": "In order to configure Spark on EMR to work with lakeFS we will set the lakeFS credentials and endpoint in the appropriate fields. The exact configuration keys depends on the application running in EMR, but their format is of the form: . lakeFS endpoint: *.fs.s3a.endpoint . lakeFS access key: *.fs.s3a.access.key . lakeFS secret key: *.fs.s3a.secret.key . EMR will encourage users to use s3:// with Spark as it will use EMR’s proprietary driver. Users need to use s3a:// for this guide to work. The Spark job reads and writes will be directed to the lakeFS instance, using the s3 gateway. There are 2 options for configuring an EMR cluster to work with lakeFS: . | When you create a cluster - All steps will use the cluster configuration. No specific configuration needed when adding a step. | Configuring on each step - cluster is created with the default s3 configuration. Each step using lakeFS should pass the appropriate config params. | . ",
    "url": "/integrations/emr.html#configuration",
    "relUrl": "/integrations/emr.html#configuration"
  },"131": {
    "doc": "EMR",
    "title": "Configuration on cluster creation",
    "content": "Use the below configuration when creating the cluster. You may delete any app configuration which is not suitable for your use-case. [{ \"Classification\": \"presto-connector-hive\", \"Properties\": { \"hive.s3.aws-access-key\": \"AKIAIOSFODNN7EXAMPLE\", \"hive.s3.aws-secret-key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"hive.s3.endpoint\": \"https://lakefs.example.com\", \"hive.s3.path-style-access\": \"true\", \"hive.s3-file-system-type\": \"PRESTO\" } }, { \"Classification\": \"hive-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://lakefs.example.com\", \"fs.s3.path.style.access\": \"true\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://lakefs.example.com\", \"fs.s3a.path.style.access\": \"true\" } }, { \"Classification\": \"hdfs-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://lakefs.example.com\", \"fs.s3.path.style.access\": \"true\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://lakefs.example.com\", \"fs.s3a.path.style.access\": \"true\" } }, { \"Classification\": \"core-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://lakefs.example.com\", \"fs.s3.path.style.access\": \"true\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://lakefs.example.com\", \"fs.s3a.path.style.access\": \"true\" } }, { \"Classification\": \"emrfs-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://lakefs.example.com\", \"fs.s3.path.style.access\": \"true\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://lakefs.example.com\", \"fs.s3a.path.style.access\": \"true\" } }, { \"Classification\": \"mapred-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://lakefs.example.com\", \"fs.s3.path.style.access\": \"true\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://lakefs.example.com\", \"fs.s3a.path.style.access\": \"true\" } }, { \"Classification\": \"spark-defaults\", \"Properties\": { \"spark.sql.catalogImplementation\": \"hive\" } } ] . ",
    "url": "/integrations/emr.html#configuration-on-cluster-creation",
    "relUrl": "/integrations/emr.html#configuration-on-cluster-creation"
  },"132": {
    "doc": "EMR",
    "title": "Configuration on adding a step",
    "content": "When a cluster was created without the above configuration, you can still use lakeFS when adding a step. For example, when creating a Spark job: . aws emr add-steps --cluster-id j-197B3AEGQ9XE4 \\ --steps=\"Type=Spark,Name=SparkApplication,ActionOnFailure=CONTINUE, \\ Args=[--conf,spark.hadoop.fs.s3a.access.key=AKIAIOSFODNN7EXAMPLE, \\ --conf,spark.hadoop.fs.s3a.secret.key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY, \\ --conf,spark.hadoop.fs.s3a.endpoint=https://lakefs.example.com, \\ --conf,spark.hadoop.fs.s3a.path.style.access=true, \\ s3a://&lt;lakefs-repo&gt;/&lt;lakefs-branch&gt;/path/to/jar]\" . The Spark context in the running job will already be initialized to use the provided lakeFS configuration. There’s no need to repeat the configuration steps mentioned in Using lakeFS with Spark . ",
    "url": "/integrations/emr.html#configuration-on-adding-a-step",
    "relUrl": "/integrations/emr.html#configuration-on-adding-a-step"
  },"133": {
    "doc": "EMR",
    "title": "EMR",
    "content": " ",
    "url": "/integrations/emr.html",
    "relUrl": "/integrations/emr.html"
  },"134": {
    "doc": "Exporting Data",
    "title": "Exporting Data",
    "content": "The export operation copies all data from a given lakeFS commit to a designated object store location. For instance, the contents lakefs://example/main might be exported on s3://company-bucket/example/latest. Clients entirely unaware of lakeFS could use that base URL to access latest files on main. Clients aware of lakeFS can continue to use the lakeFS S3 endpoint to access repository files on s3://example/main, as well as other versions and uncommitted versions. Possible use-cases: . | External consumers of data don’t have access to your lakeFS installation. | Some data pipelines in the organization are not fully migrated to lakeFS. | You want to experiment with lakeFS as a side-by-side installation first. | Create copies of your data lake in other regions (taking into account read pricing). | . ",
    "url": "/reference/export.html",
    "relUrl": "/reference/export.html"
  },"135": {
    "doc": "Exporting Data",
    "title": "Table of contents",
    "content": ". | Exporting Data With Spark . | Using spark-submit | Using custom code (notebook/spark) | . | Success/Failure Indications | Export Rounds (Spark success files) | Example | Exporting Data With Docker | . ",
    "url": "/reference/export.html#table-of-contents",
    "relUrl": "/reference/export.html#table-of-contents"
  },"136": {
    "doc": "Exporting Data",
    "title": "Exporting Data With Spark",
    "content": "Using spark-submit . You can use the export main in 3 different modes: . | Export all objects from branch example-branch on example-repo repository to s3 location s3://example-bucket/prefix/: .... example-repo s3://example-bucket/prefix/ --branch=example-branch . | Export all objects from a commit c805e49bafb841a0875f49cd555b397340bbd9b8 on example-repo repository to s3 location s3://example-bucket/prefix/: .... example-repo s3://example-bucket/prefix/ --commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8 . | Export only the diff between branch example-branch and commit c805e49bafb841a0875f49cd555b397340bbd9b8 on example-repo repository to s3 location s3://example-bucket/prefix/: .... example-repo s3://example-bucket/prefix/ --branch=example-branch --prev_commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8 . | . The complete spark-submit command would look like: . spark-submit --conf spark.hadoop.lakefs.api.url=https://&lt;LAKEFS_ENDPOINT&gt;/api/v1 \\ --conf spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY_ID&gt; \\ --conf spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_ACCESS_KEY&gt; \\ --packages io.lakefs:lakefs-spark-client-301_2.12:0.1.6 \\ --class io.treeverse.clients.Main export-app example-repo s3://example-bucket/prefix \\ --branch=example-branch . The command assumes the Spark cluster has permissions to write to s3://example-bucket/prefix. Otherwise, add spark.hadoop.fs.s3a.access.key and spark.hadoop.fs.s3a.secret.key with the proper credentials. Using custom code (notebook/spark) . Set up lakeFS Spark metadata client with the endpoint and credentials as instructed in the previous page. The client exposes the Exporter object with 3 export options: . | Export all objects at the HEAD of a given branch. Does not include files that were added to that branch, but were not committed. | . exportAllFromBranch(branch: String) . | Export ALL objects from a commit: | . exportAllFromCommit(commitID: String) . | Export just the diff between a commit and the HEAD of a branch. This is the ideal option for continuous exports of a branch, as it will change only the files that have been changed since the previous commit. exportFrom(branch: String, prevCommitID: String) . | . ",
    "url": "/reference/export.html#exporting-data-with-spark",
    "relUrl": "/reference/export.html#exporting-data-with-spark"
  },"137": {
    "doc": "Exporting Data",
    "title": "Success/Failure Indications",
    "content": "When the Spark export operation ends, an additional status file will be added to the root object storage destination. If all files were exported successfully the file path will be of form: EXPORT_&lt;commitID&gt;_&lt;ISO-8601-time-UTC&gt;_SUCCESS. For failures: the form will beEXPORT_&lt;commitID&gt;_&lt;ISO-8601-time-UTC&gt;_FAILURE, and the file will include a log of the failed files operations. ",
    "url": "/reference/export.html#successfailure-indications",
    "relUrl": "/reference/export.html#successfailure-indications"
  },"138": {
    "doc": "Exporting Data",
    "title": "Export Rounds (Spark success files)",
    "content": "Some files should be exported before others, e.g. a Spark _SUCCESS file exported before other files under the same prefix might send the wrong indication. The export operation may contain several rounds within the same export. A failing round will stop the export of all the files of the next rounds. By default, lakeFS will use the SparkFilter and have 2 rounds for each export. The first round will export any non Spark _SUCCESS files. Second round will export all Spark’s _SUCCESS files. You may override the default behaviour by passing a custom filter to the Exporter. ",
    "url": "/reference/export.html#export-rounds-spark-success-files",
    "relUrl": "/reference/export.html#export-rounds-spark-success-files"
  },"139": {
    "doc": "Exporting Data",
    "title": "Example",
    "content": ". | First configure the Exporter instance: . import io.treeverse.clients.{ApiClient, Exporter} import org.apache.spark.sql.SparkSession val endpoint = \"http://&lt;LAKEFS_ENDPOINT&gt;/api/v1\" val accessKey = \"&lt;LAKEFS_ACCESS_KEY_ID&gt;\" val secretKey = \"&lt;LAKEFS_SECRET_ACCESS_KEY&gt;\" val repo = \"example-repo\" val spark = SparkSession.builder().appName(\"I can export\").master(\"local\").getOrCreate() val sc = spark.sparkContext sc.hadoopConfiguration.set(\"lakefs.api.url\", endpoint) sc.hadoopConfiguration.set(\"lakefs.api.access_key\", accessKey) sc.hadoopConfiguration.set(\"lakefs.api.secret_key\", secretKey) // Add any required spark context configuration for s3 val rootLocation = \"s3://company-bucket/example/latest\" val apiClient = new ApiClient(endpoint, accessKey, secretKey) val exporter = new Exporter(spark, apiClient, repo, rootLocation) . | Now you can export all objects from main branch to s3://company-bucket/example/latest: . val branch = \"main\" exporter.exportAllFromBranch(branch) . | Assuming a previous successful export on commit f3c450d8cd0e84ac67e7bc1c5dcde9bef82d8ba7, you can alternatively export just the difference between main branch and the commit: . val branch = \"main\" val commit = \"f3c450d8cd0e84ac67e7bc1c5dcde9bef82d8ba7\" exporter.exportFrom(branch, commit) . | . ",
    "url": "/reference/export.html#example",
    "relUrl": "/reference/export.html#example"
  },"140": {
    "doc": "Exporting Data",
    "title": "Exporting Data With Docker",
    "content": "This option is recommended if you don’t have Spark at your tool-set. It does not support distribution across machines, therefore may have a lower performance. Using this method, you can export data from lakeFS to S3 using 3 export options (in a similar way to the Spark export): . | Export all objects from a branch example-branch on example-repo repository to s3 location s3://destination-bucket/prefix/: .... example-repo s3://destination-bucket/prefix/ --branch=\"example-branch\" . | Export all objects from a commit c805e49bafb841a0875f49cd555b397340bbd9b8 on example-repo repository to s3 location s3://destination-bucket/prefix/: .... example-repo s3://destination-bucket/prefix/ --commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8 . | Export only the diff between branch example-branch and commit c805e49bafb841a0875f49cd555b397340bbd9b8 on example-repo repository to s3 location s3://destination-bucket/prefix/: .... example-repo s3://destination-bucket/prefix/ --branch=\"example-branch\" --prev_commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8 . | . You will need to add the relevant environment variables. The complete docker run command would look like: . docker run \\ -e LAKEFS_ACCESS_KEY=XXX -e LAKEFS_SECRET_KEY=YYY -e LAKEFS_ENDPOINT=https://&lt;LAKEFS_ENDPOINT&gt;/ \\ -e S3_ACCESS_KEY=XXX -e S3_SECRET_KEY=YYY \\ -it treeverse/lakefs-rclone-export:latest example-repo s3://destination-bucket/prefix/ --branch=\"example-branch\" . Note: This feature uses rclone, and specifically rclone sync. This can change the destination path, therefore the s3 destination location must be designated to lakeFS export. ",
    "url": "/reference/export.html#exporting-data-with-docker",
    "relUrl": "/reference/export.html#exporting-data-with-docker"
  },"141": {
    "doc": "FAQ",
    "title": "FAQ",
    "content": "1. Is lakeFS open source? . lakeFS is completely free and open source and licensed under the Apache 2.0 License. We maintain a public product roadmap and Slack channel for open discussions. 2. How does lakeFS data versioning work? . lakeFS uses a copy-on-write mechanism to avoid data duplication. For example, creating a new branch is a metadata-only operation: no objects are actually copied. Only when an object changes does lakeFS create another version of the data in the storage. For more information, see Versioning internals. 3. How do I get support for my lakeFS installation? . We are extremely responsive on our slack channel, and we make sure to prioritize and with the community the issues most urgent for it. For SLA based support, please contact us at support@treeverse.io. 4. Do you collect data from your active installations? . We collect anonymous usage statistics in order to understand the patterns of use and to detect product gaps we may have so we can fix them. This is completely optional and may be turned off by setting stats.enabled to false. See the configuration reference for more details. The data we gather is limited to the following: . | A UUID which is generated when setting up lakeFS for the first time and contains no personal or otherwise identifiable information | The lakeFS version currently running | The OS and architecture lakeFS is running on | Metadata regarding the database used (version, installed extensions and parameters such as DB Timezone and work memory) | Periodic aggregated action counters (e.g. how many “get_object” operations occurred). | . 5. How is lakeFS different from Delta Lake / Hudi / Iceberg? . Delta Lake, Hudi and Iceberg all define dedicated, structured data formats that allow deletes and upserts. lakeFS is format-agnostic and enables consistent cross-collection versioning of your data using git-like operations. Read our blog for a more detailed comparison. 6. What inspired the lakeFS logo? . The Axolotl – a species of salamander, also known as the Mexican Lake Monster or the Peter Pan of the animal kingdom. It’s a magical creature, living in a lake, just like us :-). copyright . ",
    "url": "/faq.html",
    "relUrl": "/faq.html"
  },"142": {
    "doc": "Commit the Changes",
    "title": "Commit the Changes",
    "content": " ",
    "url": "/quickstart/first_commit.html",
    "relUrl": "/quickstart/first_commit.html"
  },"143": {
    "doc": "Commit the Changes",
    "title": "Install lakectl",
    "content": "lakeFS comes with its own native CLI client, lakectl. Most importantly, you can use it to perform Git-like operations like committing, reverting and merging. Follow the below tutorial video to get started with the CLI, or follow the instructions on this page. Here’s how to get started with the CLI: . | Download the CLI binary: . The CLI binary can be downloaded from the official lakeFS releases published on GitHub under “Assets”. Unless you need a specific previous version of the CLI, it is recommended to download the most recently released version. The Operating System of the computer being used determines whether you should pick the binary Asset compiled for a Windows, Linux, or Mac (Darwin). For Mac and Linux Operating Systems, the processor determines whether you should download the x64 or arm binary. | Once unzipped, inside the downloaded asset, you’ll see a file named lakectl. It’s recommended that you place this file somewhere in your PATH (this is OS dependant but for *NIX systems , /usr/local/bin is usually a safe bet). Once in your PATH, you’ll be able to open a Terminal program and run lakectl commands! . | We recommend starting with lakectl config to configure the CLI to use the credentials created earlier: . lakectl config # output: # Config file /home/janedoe/.lakectl.yaml will be used # Access key ID: AKIAJVHTOKZWGCD2QQYQ # Secret access key: **************************************** # Server endpoint URL: http://localhost:8000 . Note The first time you run a lakectl command, depending on your computer’s security settings, you may need to respond to a prompt to allow the program to run. | To verify lakectl is properly configured, we can list the branches in our repository: . lakectl branch list lakefs://example-repo # output: # +----------+------------------------------------------------------------------+ # | REF NAME | COMMIT ID | # +----------+------------------------------------------------------------------+ # | main | a91f56a7e11be1348fc405053e5234e4af7d6da01ed02f3d9a8ba7b1f71499c8 | # +----------+------------------------------------------------------------------+ . | . ",
    "url": "/quickstart/first_commit.html#install-lakectl",
    "relUrl": "/quickstart/first_commit.html#install-lakectl"
  },"144": {
    "doc": "Commit the Changes",
    "title": "Perform your first commit",
    "content": "Let’s commit the file we’ve added in the previous section: . lakectl commit lakefs://example-repo/main -m 'added our first file!' # output: # Commit for branch \"main\" done. # # ID: 901f7b21e1508e761642b142aea0ccf28451675199655381f65101ea230ebb87 # Timestamp: 2021-06-15 13:48:37 +0300 IDT # Parents: a91f56a7e11be1348fc405053e5234e4af7d6da01ed02f3d9a8ba7b1f71499c8 . Note: lakeFS versions &lt;= v0.33.1 used ‘@’ (instead of ‘/’) as separator between repository and branch. And finally we can view the log to see the new commit: . lakectl log lakefs://example-repo/main # output: # commit 901f7b21e1508e761642b142aea0ccf28451675199655381f65101ea230ebb87 # Author: Example User &lt;user@example.com&gt; # Date: 2021-06-15 13:48:37 +0300 IDT added our first file! . Congratulations! You have completed your first commit in lakeFS. Next steps . | Learn how to deploy lakeFS lakeFS on your cloud. | Join us on Slack to introduce yourself, discover best practices and share your own! | . ",
    "url": "/quickstart/first_commit.html#perform-your-first-commit",
    "relUrl": "/quickstart/first_commit.html#perform-your-first-commit"
  },"145": {
    "doc": "Garbage Collection",
    "title": "Garbage Collection",
    "content": "By default, lakeFS keeps all your objects forever. This allows you to travel back in time to previous versions of your data. However, sometimes you may want to hard-delete your objects, namely delete them from the underlying storage. Reasons for this include cost-reduction and privacy policies. Garbage collection rules in lakeFS define for how long to retain objects after they have been deleted (see more information below). lakeFS provides a Spark program to hard-delete objects that have been deleted and whose retention period has ended according to the GC rules. The GC job does not remove any commits: you will still be able to use commits containing hard-deleted objects, but trying to read these objects from lakeFS will result in a 410 Gone HTTP status. Note At this point, lakeFS supports Garbage Collection only on S3, but we have concrete plans to extend the support to Azure. ",
    "url": "/reference/garbage-collection.html",
    "relUrl": "/reference/garbage-collection.html"
  },"146": {
    "doc": "Garbage Collection",
    "title": "Understanding Garbage Collection",
    "content": "For every branch, the GC job retains deleted objects for the number of days defined for the branch. In the absence of a branch-specific rule, the default rule for the repository is used. If an object is present in more than one branch ancestry, it is retained according to the rule with the largest number of days between those branches. That is, it is hard-deleted only after the retention period has ended for all relevant branches. Example GC rules for a repository: . { \"default_retention_days\": 21, \"branches\": [ {\"branch_id\": \"main\", \"retention_days\": 28}, {\"branch_id\": \"dev\", \"retention_days\": 7} ] } . In the above example, objects are retained for 21 days after deletion by default. However, if they are present in the branch main, they are retained for 28 days. Objects present in the dev branch (but not in any other branch), are retained for 7 days after they are deleted. ",
    "url": "/reference/garbage-collection.html#understanding-garbage-collection",
    "relUrl": "/reference/garbage-collection.html#understanding-garbage-collection"
  },"147": {
    "doc": "Garbage Collection",
    "title": "Configuring GC rules",
    "content": "Use the lakectl CLI to define the GC rules: . cat &lt;&lt;EOT &gt;&gt; example_repo_gc_rules.json { \"default_retention_days\": 21, \"branches\": [ {\"branch_id\": \"main\", \"retention_days\": 28}, {\"branch_id\": \"dev\", \"retention_days\": 7} ] } EOT lakectl gc set-config lakefs://example-repo -f example_repo_gc_rules.json . ",
    "url": "/reference/garbage-collection.html#configuring-gc-rules",
    "relUrl": "/reference/garbage-collection.html#configuring-gc-rules"
  },"148": {
    "doc": "Garbage Collection",
    "title": "Running the GC job",
    "content": "The GC job is a Spark program that can be run using spark-submit (or using your preferred method of running Spark programs). The job will hard-delete objects that were deleted and whose retention period has ended according to the GC rules. First, you’ll have to download the lakeFS Spark client Uber-jar. The Uber-Jar can be found on a public S3 location: . For Spark 2.4.7: http://treeverse-clients-us-east.s3-website-us-east-1.amazonaws.com/lakefs-spark-client-247/${CLIENT_VERSION}/lakefs-spark-client-247-assembly-${CLIENT_VERSION}.jar . For Spark 3.0.1: http://treeverse-clients-us-east.s3-website-us-east-1.amazonaws.com/lakefs-spark-client-301/${CLIENT_VERSION}/lakefs-spark-client-301-assembly-${CLIENT_VERSION}.jar . CLIENT_VERSIONs for Spark 2.4.7 can be found here, and for Spark 3.0.1 they can be found here. Second, you should specify the Uber-jar path instead of &lt;APPLICATION-JAR-PATH&gt; and run the following command to make the garbage collector start running: . spark-submit --class io.treeverse.clients.GarbageCollector \\ -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1 \\ -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\ -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\ -c spark.hadoop.fs.s3a.access.key=&lt;S3_ACCESS_KEY&gt; \\ -c spark.hadoop.fs.s3a.secret.key=&lt;S3_SECRET_KEY&gt; \\ &lt;APPLICATION-JAR-PATH&gt; \\ example-repo us-east-1 . ",
    "url": "/reference/garbage-collection.html#running-the-gc-job",
    "relUrl": "/reference/garbage-collection.html#running-the-gc-job"
  },"149": {
    "doc": "Garbage Collection",
    "title": "Considerations",
    "content": ". | In order for an object to be hard-deleted, it must be deleted from all branches. You should remove stale branches to prevent them from retaining old objects. For example, consider a branch that has been merged to main and has become stale. An object which is later deleted from main will always be present in the stale branch, preventing it from being hard-deleted. | lakeFS will never delete objects outside your repository’s storage namespace. In particular, objects that were imported using lakefs import or lakectl ingest will not be affected by GC jobs. | In cases where deleted objects are brought back to life while a GC job is running, said objects may or may not be deleted. Such actions include: . | Reverting a commit in which a file was deleted. | Branching out from an old commit. | Expanding the retention period of a branch. | Creating a branch from an existing branch, where the new branch has a longer retention period. | . | . ",
    "url": "/reference/garbage-collection.html#considerations",
    "relUrl": "/reference/garbage-collection.html#considerations"
  },"150": {
    "doc": "On GCP",
    "title": "Deploy lakeFS on GCP",
    "content": "Expected deployment time: 25min . ",
    "url": "/deploy/gcp.html#deploy-lakefs-on-gcp",
    "relUrl": "/deploy/gcp.html#deploy-lakefs-on-gcp"
  },"151": {
    "doc": "On GCP",
    "title": "Table of contents",
    "content": ". | Prerequisites | Creating the Database on GCP SQL | Installation Options . | On Google Compute Engine | On Google Cloud Run | On GKE | . | Load balancing | Next Steps | . ",
    "url": "/deploy/gcp.html#table-of-contents",
    "relUrl": "/deploy/gcp.html#table-of-contents"
  },"152": {
    "doc": "On GCP",
    "title": "Prerequisites",
    "content": "Users that require S3 access using virtual host addressing should configure an S3 Gateway domain. ",
    "url": "/deploy/gcp.html#prerequisites",
    "relUrl": "/deploy/gcp.html#prerequisites"
  },"153": {
    "doc": "On GCP",
    "title": "Creating the Database on GCP SQL",
    "content": "lakeFS requires a PostgreSQL database to synchronize actions on your repositories. We will show you how to create a database on Google Cloud SQL, but you can use any PostgreSQL database as long as it’s accessible by your lakeFS installation. If you already have a database, take note of the connection string and skip to the next step . | Follow the official Google documentation on how to create a PostgreSQL instance. Make sure you’re using PostgreSQL version &gt;= 11. | On the Users tab in the console, create a user. The lakeFS installation will use it to connect to your database. | Choose the method by which lakeFS will connect to your database. Google recommends using the SQL Auth Proxy. | . Depending on the chosen lakeFS installation method, you will need to make sure lakeFS can access your database. For example, if you install lakeFS on GKE, you need to deploy the SQL Auth Proxy from this Helm chart, or as a sidecar container in your lakeFS pod. ",
    "url": "/deploy/gcp.html#creating-the-database-on-gcp-sql",
    "relUrl": "/deploy/gcp.html#creating-the-database-on-gcp-sql"
  },"154": {
    "doc": "On GCP",
    "title": "Installation Options",
    "content": "On Google Compute Engine . | Save the following configuration file as config.yaml: . --- database: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: # replace this with a randomly-generated string: secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: gs # Uncomment the following lines to give lakeFS access to your buckets using a service account: # gs: # credentials_json: [YOUR SERVICE ACCOUNT JSON STRING] . | Download the binary to the GCE instance. | Run the lakefs binary on the GCE machine: lakefs --config config.yaml run . Note: it is preferable to run the binary as a service using systemd or your operating system’s facilities. | . On Google Cloud Run . To support container-based environments like Google Cloud Run, lakeFS can be configured using environment variables. Here is a docker run command to demonstrate starting lakeFS using Docker: . docker run \\ --name lakefs \\ -p 8000:8000 \\ -e LAKEFS_DATABASE_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\ -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\ -e LAKEFS_BLOCKSTORE_TYPE=\"gs\" \\ treeverse/lakefs:latest run . See the reference for a complete list of environment variables. On GKE . See Kubernetes Deployment. ",
    "url": "/deploy/gcp.html#installation-options",
    "relUrl": "/deploy/gcp.html#installation-options"
  },"155": {
    "doc": "On GCP",
    "title": "Load balancing",
    "content": "Depending on how you chose to install lakeFS, you should have a load balancer direct requests to the lakeFS server. By default, lakeFS operates on port 8000, and exposes a /_health endpoint which you can use for health checks. ",
    "url": "/deploy/gcp.html#load-balancing",
    "relUrl": "/deploy/gcp.html#load-balancing"
  },"156": {
    "doc": "On GCP",
    "title": "Next Steps",
    "content": "Your next step is to prepare your storage. If you already have a storage bucket/container, you are ready to create your first lakeFS repository. ",
    "url": "/deploy/gcp.html#next-steps",
    "relUrl": "/deploy/gcp.html#next-steps"
  },"157": {
    "doc": "On GCP",
    "title": "On GCP",
    "content": " ",
    "url": "/deploy/gcp.html",
    "relUrl": "/deploy/gcp.html"
  },"158": {
    "doc": "Google Cloud Storage",
    "title": "Prepare Your GCS Bucket",
    "content": ". | On the Google Cloud Storage console, click Create Bucket. Follow the instructions. | On the Permissions tab, add the service account you intend to use lakeFS with. Give it a role that allows reading and writing to the bucket, e.g. Storage Object Creator. | . You are now ready to create your first lakeFS repository. ",
    "url": "/setup/storage/gcs.html#prepare-your-gcs-bucket",
    "relUrl": "/setup/storage/gcs.html#prepare-your-gcs-bucket"
  },"159": {
    "doc": "Google Cloud Storage",
    "title": "Google Cloud Storage",
    "content": " ",
    "url": "/setup/storage/gcs.html",
    "relUrl": "/setup/storage/gcs.html"
  },"160": {
    "doc": "Glue ETL",
    "title": "Using lakeFS with Glue ETL",
    "content": "AWS Glue is a fully managed extract, transform, and load (ETL) service. With AWS Glue ETL you can run your ETL jobs as soon as new data becomes available in Amazon S3 by invoking your AWS Glue ETL jobs from an AWS Lambda function. ",
    "url": "/integrations/glue_etl.html#using-lakefs-with-glue-etl",
    "relUrl": "/integrations/glue_etl.html#using-lakefs-with-glue-etl"
  },"161": {
    "doc": "Glue ETL",
    "title": "Configuration",
    "content": "Since Glue ETL is essentially running Spark jobs, to configure Glue ETL to work with lakeFS, you should apply the lakeFS Spark configuration to your Glue ETL script. ",
    "url": "/integrations/glue_etl.html#configuration",
    "relUrl": "/integrations/glue_etl.html#configuration"
  },"162": {
    "doc": "Glue ETL",
    "title": "Glue ETL",
    "content": " ",
    "url": "/integrations/glue_etl.html",
    "relUrl": "/integrations/glue_etl.html"
  },"163": {
    "doc": "Glue / Hive metastore",
    "title": "Table of contents",
    "content": ". | Glue / Hive Metastore Intro | Managing Tables With lakeFS Branches . | Motivation | Configurations | Suggested Model | Commands . | Copy | Diff | . | . | . ",
    "url": "/integrations/glue_hive_metastore.html#table-of-contents",
    "relUrl": "/integrations/glue_hive_metastore.html#table-of-contents"
  },"164": {
    "doc": "Glue / Hive metastore",
    "title": "Glue / Hive Metastore Intro",
    "content": "This part contains a brief explanation about how Glue/Hive metastore work with lakeFS . Glue and Hive Metastore stores metadata related to Hive and other services (such as Spark and Trino). They contain metadata such as the location of the table, information about columns, partitions and many more. ",
    "url": "/integrations/glue_hive_metastore.html#glue--hive-metastore-intro",
    "relUrl": "/integrations/glue_hive_metastore.html#glue--hive-metastore-intro"
  },"165": {
    "doc": "Glue / Hive metastore",
    "title": "Without lakeFS",
    "content": "In order to query the table my_table, Spark will: . | Request the metadata from Hive metastore (steps 1,2) | Use the location from the metadata to access the data in S3 (steps 3,4). | . ",
    "url": "/integrations/glue_hive_metastore.html#without-lakefs",
    "relUrl": "/integrations/glue_hive_metastore.html#without-lakefs"
  },"166": {
    "doc": "Glue / Hive metastore",
    "title": "With lakeFS",
    "content": "When using lakeFS, the flow stays exactly the same. Note that the location of the table my_table now contains the branch s3://example/main/path/to/table . ",
    "url": "/integrations/glue_hive_metastore.html#with-lakefs",
    "relUrl": "/integrations/glue_hive_metastore.html#with-lakefs"
  },"167": {
    "doc": "Glue / Hive metastore",
    "title": "Managing Tables With lakeFS Branches",
    "content": " ",
    "url": "/integrations/glue_hive_metastore.html#managing-tables-with-lakefs-branches",
    "relUrl": "/integrations/glue_hive_metastore.html#managing-tables-with-lakefs-branches"
  },"168": {
    "doc": "Glue / Hive metastore",
    "title": "Motivation",
    "content": "When creating a table in Glue/Hive metastore (using a client such as Spark, Hive, Presto), we specify the table location. Consider the table my_table which was created with the location s3://example/main/path/to/table. Assume we created a new branch called DEV with main as the source branch. The data from s3://example/main/path/to/table is now accessible in s3://example/DEV/path/to/table. The metadata is not managed in lakeFS, meaning we don’t have any table pointing to s3://example/DEV/path/to/table. To address this, lakeFS introduces lakectl metastore commands. The case above could be handled using the copy command: it can create a copy of my_table with data located in s3://example/DEV/path/to/table. Note that this is a fast, metadata-only operation. ",
    "url": "/integrations/glue_hive_metastore.html#motivation",
    "relUrl": "/integrations/glue_hive_metastore.html#motivation"
  },"169": {
    "doc": "Glue / Hive metastore",
    "title": "Configurations",
    "content": "The lakectl metastore commands could run on Glue or Hive metastore. Add the following to the lakectl configuration file (by default ~/.lakectl.yaml): . Hive . metastore: type: hive hive: uri: hive-metastore:9083 . Glue . metastore: type: glue glue: catalog-id: 123456789012 region: us-east-1 profile: default # optional, implies using a credentials file credentials: access_key_id: AKIAIOSFODNN7EXAMPLE secret_access_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY . Notice: It’s recommended to set type and catalog-id/metastore-uri in the lakectl configuration file. ",
    "url": "/integrations/glue_hive_metastore.html#configurations",
    "relUrl": "/integrations/glue_hive_metastore.html#configurations"
  },"170": {
    "doc": "Glue / Hive metastore",
    "title": "Suggested Model",
    "content": "For simplicity, we recommend creating a schema for each branch, this way you can use the same table name across different schemas. For example: after creating branch example_branch also create a schema named example_branch. For a table named my_table under the schema main, create a new table by the same name under the schema example_branch. You now have two my_tables, one in the main schema and one, in the branch schema. ",
    "url": "/integrations/glue_hive_metastore.html#suggested-model",
    "relUrl": "/integrations/glue_hive_metastore.html#suggested-model"
  },"171": {
    "doc": "Glue / Hive metastore",
    "title": "Commands",
    "content": "Metastore tools support three commands: copy, diff and create-symlink. copy and diff could work both on Glue and on Hive. create-symlink works only on Glue. Notice: If to-schema or to-table are not specified, the destination branch and source table names will be used as per the suggested model. Notice: Metastore commands can only run on tables located in lakeFS, you should not use tables that are not located in lakeFS. Copy . The copy command creates a copy of a table pointing to the defined branch. In case the destination table already exists, the command will only merge the changes. Example: . Suppose we created the table inventory on branch main on schema default. CREATE EXTERNAL TABLE `inventory`( `inv_item_sk` int, `inv_warehouse_sk` int, `inv_quantity_on_hand` int) PARTITIONED BY ( `inv_date_sk` int) STORED AS ORC LOCATION 's3a://my_repo/main/path/to/table'; . We create a new lakeFS branch example_branch: . lakectl branch create lakefs://my_repo/example_branch --source lakefs://my_repo/main . The data from s3://my_repo/main/path/to/table is now accessible in s3://my_repo/DEV/path/to/table. In order to query the data in s3://my_repo/DEV/path/to/table we would like to create a copy of the table inventory in schema example_branch pointing to the new branch. lakectl metastore copy --from-schema default --from-table inventory --to-schema example_branch --to-table inventory --to-branch example_branch . After running this command, query the table example_branch.inventory to get the data from s3://my_repo/DEV/path/to/table . Copy Partition . After adding a partition to the branch table, we may want to copy the partition to the main table. For example, for the new partition 2020-08-01, run the following in order to copy the partition to the main table: . lakectl metastore copy --type hive --from-schema example_branch --from-table inventory --to-schema default --to-table inventory --to-branch main -p 2020-08-01 . For a table partitioned by more than one column, specify the partition flag for every column. For example for the partition (year='2020',month='08',day='01'): . lakectl metastore copy --from-schema example_branch --from-table branch_inventory --to-schema default --to-branch main -p 2020 -p 08 -p 01 . Diff . Provides a 2-way diff between two tables. Shows added+ , removed- and changed~ partitions and columns. Example: . Suppose that we made some changes on the copied table inventory on schema example_branch and we want to view the changes before merging back to inventory on schema default. Hive: . lakectl metastore diff --type hive --address thrift://hive-metastore:9083 --from-schema example_branch --from-table branch --to-schema default --to-table inventory . The output will be something like: . Columns are identical Partitions - 2020-07-04 + 2020-07-05 + 2020-07-06 ~ 2020-07-08 . ",
    "url": "/integrations/glue_hive_metastore.html#commands",
    "relUrl": "/integrations/glue_hive_metastore.html#commands"
  },"172": {
    "doc": "Glue / Hive metastore",
    "title": "Glue / Hive metastore",
    "content": " ",
    "url": "/integrations/glue_hive_metastore.html",
    "relUrl": "/integrations/glue_hive_metastore.html"
  },"173": {
    "doc": "Hive",
    "title": "Using lakeFS with Hive",
    "content": "The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive. ",
    "url": "/integrations/hive.html#using-lakefs-with-hive",
    "relUrl": "/integrations/hive.html#using-lakefs-with-hive"
  },"174": {
    "doc": "Hive",
    "title": "Table of contents",
    "content": ". | Configuration | Examples . | Example with schema | Example with external table | . | . ",
    "url": "/integrations/hive.html#table-of-contents",
    "relUrl": "/integrations/hive.html#table-of-contents"
  },"175": {
    "doc": "Hive",
    "title": "Configuration",
    "content": "In order to configure hive to work with lakeFS we will set the lakeFS credentials in the corresponding S3 credential fields. lakeFS endpoint: fs.s3a.endpoint . lakeFS access key: fs.s3a.access.key . lakeFS secret key: fs.s3a.secret.key . Note In the following examples we set AWS credentials at runtime, for clarity. In production, these properties should be set using one of Hadoop’s standard ways of Authenticating with S3. For example, we could add the configurations to the file hdfs-site.xml: . &lt;configuration&gt; ... &lt;property&gt; &lt;name&gt;fs.s3a.secret.key&lt;/name&gt; &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.access.key&lt;/name&gt; &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;https://lakefs.example.com&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . Note In this example we set fs.s3a.path.style.access to true, in order to remove the need of additional DNS records for virtual hosting fs.s3a.path.style.access was introduced in Hadoop 2.8.0 . ",
    "url": "/integrations/hive.html#configuration",
    "relUrl": "/integrations/hive.html#configuration"
  },"176": {
    "doc": "Hive",
    "title": "Examples",
    "content": "Example with schema . CREATE SCHEMA example LOCATION 's3a://example/main/' ; CREATE TABLE example.request_logs ( request_time timestamp, url string, ip string, user_agent string ); . Example with external table . CREATE EXTERNAL TABLE request_logs ( request_time timestamp, url string, ip string, user_agent string ) LOCATION 's3a://example/main/request_logs' ; . ",
    "url": "/integrations/hive.html#examples",
    "relUrl": "/integrations/hive.html#examples"
  },"177": {
    "doc": "Hive",
    "title": "Hive",
    "content": " ",
    "url": "/integrations/hive.html",
    "relUrl": "/integrations/hive.html"
  },"178": {
    "doc": "Hooks",
    "title": "Configurable Hooks",
    "content": " ",
    "url": "/setup/hooks.html#configurable-hooks",
    "relUrl": "/setup/hooks.html#configurable-hooks"
  },"179": {
    "doc": "Hooks",
    "title": "Table of contents",
    "content": ". | Example use-cases | Terminology | Uploading Action files | Runs API &amp; CLI | Hook types . | Webhooks . | Action file Webhook properties | Request body schema | . | Airflow Hooks . | Action file Airflow hook properties | Hook Record in configuration field | . | . | Experimentation | . Like other version control systems, lakeFS allows the configuration of Actions to trigger when predefined events occur. Supported Events: . | Event | Description | . | pre_commit | Runs when the commit occurs, before the commit is finalized | . | post_commit | Runs after the commit is finalized | . | pre_merge | Runs on the source branch when the merge occurs, before the merge is finalized | . | post_merge | Runs on the merge result, after the merge is finalized | . | pre_create_branch | Runs on the source branch prior to creating a new branch | . | post_create_branch | Runs on the new branch after the branch was created | . | pre_delete_branch | Runs prior to deleting a branch | . | post_delete_branch | Runs after the branch was deleted | . | pre_create_tag | Runs prior to creating a new tag | . | post_create_tag | Runs after the tag was created | . | pre_delete_tag | Runs prior to deleting a tag | . | post_delete_tag | Runs after the tag was deleted | . lakeFS Actions are handled per repository and cannot be shared between repositories. Failure of any Hook under any Action of a pre_* event will result in aborting the lakeFS operation that is taking place. Hook failures under any Action of a post_* event will not revert the operation. Hooks are managed by Action files that are written to a prefix in the lakeFS repository. This allows configuration-as-code inside lakeFS, where Action files are declarative and written in YAML. ",
    "url": "/setup/hooks.html#table-of-contents",
    "relUrl": "/setup/hooks.html#table-of-contents"
  },"180": {
    "doc": "Hooks",
    "title": "Example use-cases",
    "content": ". | Format Validator: A webhook that checks new files to ensure they are of a set of allowed data format. | Schema Validator: A webhook that reads new Parquet and ORC files to ensure they don’t contain a block list of column names (or name prefixes). This is useful when we want to avoid accidental PII exposure. | . For more examples and configuration samples, check out lakeFS-hooks example repo. ",
    "url": "/setup/hooks.html#example-use-cases",
    "relUrl": "/setup/hooks.html#example-use-cases"
  },"181": {
    "doc": "Hooks",
    "title": "Terminology",
    "content": "Action . An Action is a list of Hooks with the same trigger configuration, i.e. an event will trigger all Hooks under an Action, or none at all. The Hooks under an Action are ordered and so is their execution. A Hook will only be executed if all previous Hooks that were triggered with it, had passed. Hook . A Hook is the basic building block of an Action. Failure of a single Hook will stop the execution of the containing Action and fail the Run. Action file . Schema of the Action file: . | Property | Description | Data Type | Required | Default Value | . | name | Identify the Action file | String | false | If missing, filename is used instead | . | on | List of events that will trigger the hooks | List | true |   | . | on.branches | Glob pattern list of branches that triggers the hooks | List | false | Not applicable to Tag events. If empty, Action runs on all branches | . | hooks | List of hooks to be executed | List | true |   | . | hook.id | ID of the hook, must be unique within the Action | String | true |   | . | hook.type | Type of the hook (types) | String | true |   | . | hook.properties | Hook’s specific configuration | Dictionary | true |   | . Example: . name: Good files check description: set of checks to verify that branch is good on: pre-commit: pre-merge: branches: - main hooks: - id: no_temp type: webhook description: checking no temporary files found properties: url: \"https://your.domain.io/webhook?notmp=true?t=1za2PbkZK1bd4prMuTDr6BeEQwWYcX2R\" - id: no_freeze type: webhook description: check production is not in dev freeze properties: url: \"https://your.domain.io/webhook?nofreeze=true?t=1za2PbkZK1bd4prMuTDr6BeEQwWYcX2R\" . Note: lakeFS will validate action files only when an Event occurred. Use lakectl actions validate &lt;path&gt; to validate your action files locally. Run . A Run is an instantiation of the repository’s Action files when the triggering event occurs. For example, if our repository contains a pre-commit hook, every commit would generate a Run for that specific commit. lakeFS will fetch, parse and filter the repository Action files and start to execute the Hooks under each Action. All executed Hooks (each with hook_run_id) exists in the context of that Run (run_id). ",
    "url": "/setup/hooks.html#terminology",
    "relUrl": "/setup/hooks.html#terminology"
  },"182": {
    "doc": "Hooks",
    "title": "Uploading Action files",
    "content": "Action files should be uploaded with the prefix _lakefs_actions/ to the lakeFS repository. When an actionable event (see Supported Events above) takes place, lakeFS will read all files with prefix _lakefs_actions/ in the repository branch where the action occurred. A failure to parse an Action file will result with a failing Run. For example, lakeFS will search and execute all matching Action files with the prefix lakefs://repo1/feature-1/_lakefs_actions/ on: . | Commit to feature-1 branch on repo1 repository. | Merge to main branch from feature-1 branch on repo1 repository. | . ",
    "url": "/setup/hooks.html#uploading-action-files",
    "relUrl": "/setup/hooks.html#uploading-action-files"
  },"183": {
    "doc": "Hooks",
    "title": "Runs API &amp; CLI",
    "content": "The lakeFS API and lakectl expose the results of executions per repository, branch, commit and specific Action. The endpoint also allows to download the execution log of any executed Hook under each Run for observability. Result Files . There are 2 types of files that are stored in the metadata section of lakeFS repository with each Run: . | _lakefs/actions/log/&lt;runID&gt;/&lt;hookRunID&gt;.log - Execution log of the specific Hook run. | _lakefs/actions/log/&lt;runID&gt;/run.manifest - Manifest with all Hooks execution for the run with their results and additional metadata. | . Note: Metadata section of a lakeFS repository is where lakeFS keeps its metadata, like commits and metaranges. Metadata files stored in the metadata section aren’t accessible like user stored files. ",
    "url": "/setup/hooks.html#runs-api--cli",
    "relUrl": "/setup/hooks.html#runs-api--cli"
  },"184": {
    "doc": "Hooks",
    "title": "Hook types",
    "content": "Currently, there are two types of Hooks that are supported by lakeFS: Webhook and Airflow. Webhooks . A Webhook is a Hook type that sends an HTTP POST request to the configured URL. Any non 2XX response by the responding endpoint will fail the Hook, cancel the execution of the following Hooks under the same Action. For pre_* hooks, the triggering operation will also be aborted. Warning: You should not use pre_* webhooks for long-running tasks, since they block the performed operation. Moreover, the branch is locked during the execution of pre_* hooks, so the webhook server cannot perform any write operations (like uploading or commits) on the branch. Action file Webhook properties . | Property | Description | Data Type | Required | Default Value | Env Vars Support | . | url | The URL address of the request | String | true |   | no | . | timeout | Time to wait for response before failing the hook | String (golang’s Duration representation) | false | 1 minute | no | . | query_params | List of query params that will be added to the request | Dictionary(String:String or String:List(String) | false |   | yes | . | headers | Headers to add to the request | Dictionary(String:String) | false |   | yes | . Secrets &amp; Environment Variables lakeFS Actions supports secrets by using environment variables. The following format {{ ENV.SOME_ENV_VAR }} will be replaced with the value of SOME_ENV_VAR during the execution of the action. If that environment variable doesn’t exist in the lakeFS server environment, the action run will fail. Example: ... hooks: - id: prevent_user_columns type: webhook description: Ensure no user_* columns under public/ properties: url: \"http://&lt;host:port&gt;/webhooks/schema\" timeout: 1m30s query_params: disallow: [\"user_\", \"private_\"] prefix: public/ headers: secret_header: \"{{ ENV.MY_SECRET }}\" ... Request body schema . Upon execution, a webhook will send a request containing a JSON object with the following fields: . | Field | Description | Type | . | event_type | Type of the event that triggered the Action | string | . | event_time | Time of the event that triggered the Action (RFC3339) | string | . | action_name | Containing Hook Action’s Name | string | . | hook_id | ID of the Hook | string | . | repository_id | ID of the Repository | string | . | branch_id1 | ID of the Branch | string | . | source_ref | Reference to the source on which the event was triggered | string | . | commit_message2 | The message for the commit (or merge) that is taking place | string | . | committer2 | Name of the committer | string | . | commit_metadata2 | The metadata for the commit that is taking place | string | . | tag_id3 | The ID of the created/deleted tag | string | . Example: . { \"event_type\": \"pre-merge\", \"event_time\": \"2021-02-28T14:03:31Z\", \"action_name\": \"test action\", \"hook_id\": \"prevent_user_columns\", \"repository_id\": \"repo1\", \"branch_id\": \"feature-1\", \"source_ref\": \"feature-1\", \"commit_message\": \"merge commit message\", \"committer\": \"committer\", \"commit_metadata\": { \"key\": \"value\" } } . Airflow Hooks . Airflow Hook triggers a DAG run in an Airflow installation using Airflow’s REST API. The hook run succeeds if the DAG was triggered, and fails otherwise. Action file Airflow hook properties . | Property | Description | Data Type | Example | Required | Env Vars Support | . | url | The URL of the Airflow instance | String | “http://localhost:8080” | true | no | . | dag_id | The DAG to trigger | String | “example_dag” | true | no | . | username | The name of the Airflow user performing the request | String | “admin” | true | no | . | password | The password of the Airflow user performing the request | String | “admin” | true | yes | . | dag_conf | DAG run configuration that will be passed as is | JSON |   | false | no | . | wait_for_dag | Wait for DAG run to complete and reflect state (default: false) | Boolean |   | false | no | . | timeout | Time to wait for the DAG run to complete (default: 1m) | String (golang’s Duration representation) |   | false | no | . Example: ... hooks: - id: trigger_my_dag type: airflow description: Trigger an example_dag properties: url: \"http://localhost:8000\" dag_id: \"example_dag\" username: \"admin\" password: \"{{ ENV.AIRFLOW_SECRET }}\" dag_conf: some: \"additional_conf\" ... Hook Record in configuration field . lakeFS will add an entry to the Airflow request configuration property (conf) with the event that triggered the action. The key of the record will be lakeFS_event and the value will match the one described here . ",
    "url": "/setup/hooks.html#hook-types",
    "relUrl": "/setup/hooks.html#hook-types"
  },"185": {
    "doc": "Hooks",
    "title": "Experimentation",
    "content": "It’s sometimes easier to start experimenting with lakeFS webhooks, even before you have a running server to receive the calls. There are a couple of online tools that can intercept and display the webhook requests, one of them is Svix. | Go to play.svix.com and copy the URL address supplied by Svix. It should look like https://api.relay.svix.com/api/v1/play/receive/&lt;Random_Gen_String&gt;/ . | Upload the following action file to lakeFS under the path _lakefs_actions/test.yaml in the default branch: . name: Sending everything to Svix description: Experimenting with webhooks on: pre-commit: branches: pre-merge: branches: post-commit: branches: post-merge: branches: hooks: - id: svix type: webhook properties: url: \"https://api.relay.svix.com/api/v1/play/receive/&lt;Random_Gen_String&gt;/\" . by using: . lakectl fs upload lakefs://example-repo/main/_lakefs_actions/test.yaml -s path/to/action/file . or the UI. | Commit that file to the branch. lakectl commit lakefs://example-repo/main -m 'added webhook action file' . | Every time you commit or merge to a branch, the relevant pre_* and post_* requests will be available in the Svix endpoint you provided. You can also check the Actions tab in the lakeFS UI for more details. | . | N\\A for Tag events &#8617; . | N\\A for Tag and Create/Delete Branch events &#8617; &#8617;2 &#8617;3 . | Applicable only for Tag events &#8617; . | . ",
    "url": "/setup/hooks.html#experimentation",
    "relUrl": "/setup/hooks.html#experimentation"
  },"186": {
    "doc": "Hooks",
    "title": "Hooks",
    "content": " ",
    "url": "/setup/hooks.html",
    "relUrl": "/setup/hooks.html"
  },"187": {
    "doc": "Import data into lakeFS",
    "title": "Import data into lakeFS",
    "content": " ",
    "url": "/setup/import.html",
    "relUrl": "/setup/import.html"
  },"188": {
    "doc": "Import data into lakeFS",
    "title": "Table of contents",
    "content": ". | Use external tools | Zero-copy import . | UI Import . | Prerequisite | Limitations | . | lakectl ingest | lakefs import | . | Importing from public buckets | . ",
    "url": "/setup/import.html#table-of-contents",
    "relUrl": "/setup/import.html#table-of-contents"
  },"189": {
    "doc": "Import data into lakeFS",
    "title": "Use external tools",
    "content": "In order to import existing data to lakeFS, you may choose to copy it using S3 CLI or using tools like Apache DistCp. This is the most straightforward way, and we recommend it if it’s applicable for you. ",
    "url": "/setup/import.html#use-external-tools",
    "relUrl": "/setup/import.html#use-external-tools"
  },"190": {
    "doc": "Import data into lakeFS",
    "title": "Zero-copy import",
    "content": "lakeFS supports 3 ways to ingest objects from the object store without copying the data. They differ by the outcome, scale and ease of use: . | Import from the UI - A UI dialog to trigger an import to a designated import branch. It creates a commit from all imported objects. Easy to use and scales well. | lakectl ingest - Use a simple CLI command to create uncommitted objects in a branch. It will make sequential calls between the CLI and the server, which make scaling difficult. | lakefs import - Use the lakeFS binary to create a commit from an S3 inventory file. Supported only with lakeFS with S3 storage adapters. Requires the most setup - lakeFS binary with DB access and an S3 inventory file. The most scalable option. | . UI Import . Clicking the Import button from any branch will open the following dialog: . If it’s the first import to my-branch, it will create the import branch named _my-branch_imported. lakeFS will import all objects from the Source URI to the import branch under the given prefix. Hang tight! It might take several minutes for the operation to complete: . Once the import is completed, you can merge the changes from the import branch to the source branch. Prerequisite . lakeFS must have permissions to list the objects at the source object store, and on the same region of your destination bucket. Limitations . Import is feasible only from source object storage that matches the storage namespace of the current repository. lakeFS S3 installation cannot import from Azure bucket. Although created by lakeFS, import branches are just like any other branch. Authorization policies, CI/CD triggering, branch protection rules and all other lakeFS concepts apply to them as they apply to any other branch. lakectl ingest . For cases where copying data is not feasible, the lakectl command supports ingesting objects from a source object store without actually copying the data itself. This is done by listing the source bucket (and optional prefix), and creating pointers to the returned objects in lakeFS. By doing this, it’s possible to take even large sets of objects, and have them appear as objects in a lakeFS branch, as if they were written directly to it. For this to work, make sure that: . | The user calling lakectl ingest must have permissions to list the objects at the source object store | The lakeFS installation has read permissions to the objects being ingested. | The source path is not a storage namespace used by lakeFS. e.g. if lakefs://my-repo created with storage namespace s3://my-bucket, then s3://my-bucket/* cannot be an ingestion source. | . | AWS S3 or S3 API Compatible storage | Azure Blob | Google Cloud Storage | . lakectl ingest \\ --from s3://bucket/optional/prefix/ \\ --to lakefs://my-repo/ingest-branch/optional/path/ . The lakectl ingest command will attempt to use the current user’s existing credentials and will respect instance profiles, environment variables and credential files in the same way that the AWS cli does Specify an endpoint to ingest from other S3 compatible storage solutions, e.g. add --s3-endpoint-url https://play.min.io. export AZURE_STORAGE_ACCOUNT=\"storageAccountName\" export AZURE_STORAGE_ACCESS_KEY=\"EXAMPLEroozoo2gaec9fooTieWah6Oshai5Sheofievohthapob0aidee5Shaekahw7loo1aishoonuuquahr3==\" lakectl ingest \\ --from https://storageAccountName.blob.core.windows.net/container/optional/prefix/ \\ --to lakefs://my-repo/ingest-branch/optional/path/ . The lakectl ingest command currently supports storage accounts configured through environment variables as shown above. Note: Currently lakectl import supports the http:// and https:// schemes for Azure storage URIs. wasb, abfs or adls are currently not supported. export GOOGLE_APPLICATION_CREDENTIALS=\"$HOME/.gcs_credentials.json\" # Optional, will fallback to the default configured credentials lakectl ingest \\ --from gs://bucket/optional/prefix/ \\ --to lakefs://my-repo/ingest-branch/optional/path/ . The lakectl ingest command currently supports the standard GOOGLE_APPLICATION_CREDENTIALS environment variable as described in Google Cloud’s documentation. lakefs import . Importing a very large amount of objects (&gt; ~250M) might take some time using lakectl ingest as described above, since it has to paginate through all the objects in the source using API calls. For S3, we provide a utility as part of the lakefs binary, called lakefs import. The lakeFS import tool will use the S3 Inventory feature to create lakeFS metadata. The imported metadata will be committed to a special branch, called import-from-inventory. You should not make any changes or commit anything to branch import-from-inventory: it will be operated on only by lakeFS. After importing, you will be able to merge this branch into your main branch. How it works . The imported data is not copied to the repository’s dedicated bucket. Rather, it will be read directly from your existing bucket when you access it through lakeFS. Files created or replaced through lakeFS will then be stored in the repository’s dedicated bucket. It is important to note that due to the deduplication feature of lakeFS, data will be read from your original bucket even when accessing it through other branches. In a sense, your original bucket becomes an initial snapshot of your data. Note: lakeFS will never make any changes to the import source bucket. Prerequisites . | Your bucket should have S3 Inventory enabled. | The inventory should be in Parquet or ORC format. | The inventory must contain (at least) the size, last-modified-at, and e-tag columns. | The S3 credentials you provided to lakeFS should have GetObject permissions on the source bucket and on the bucket where the inventory is stored. | If you want to use the tool for gradual import, you should not delete the data for the most recently imported inventory, until a more recent inventory is successfully imported. | . For a step-by-step walkthrough of this process, see the post 3 Ways to Add Data to lakeFS on our blog. Usage . Import is performed by the lakefs import command. Assuming your manifest.json is at s3://example-bucket/path/to/inventory/YYYY-MM-DDT00-00Z/manifest.json, and your lakeFS configuration yaml is at config.yaml (see notes below), run the following command to start the import: . lakefs import lakefs://example-repo -m s3://example-bucket/path/to/inventory/YYYY-MM-DDT00-00Z/manifest.json --config config.yaml . You will see the progress of your import as it is performed. After the import is finished, a summary will be printed along with suggestions for commands to access your data. Added or changed objects: 565000 Deleted objects: 0 Commit ref: cf349ded0a0e65e20bd3b25ea8d9b656c2870b7f1f32f60eb1d90ca5873b6c03 Import to branch import-from-inventory finished successfully. To list imported objects, run: $ lakectl fs ls lakefs://example-repo/cf349ded0a0e65e20bd3b25ea8d9b656c2870b7f1f32f60eb1d90ca5873b6c03/ To merge the changes to your main branch, run: $ lakectl merge lakefs://example-repo/import-from-inventory lakefs://goo/main . Merging imported data to the main branch . As previously mentioned, the above command imports data to the dedicated import-from-inventory branch. By adding the --with-merge flag to the import command, this branch will be automatically merged to your main branch immediately after the import. lakefs import --with-merge lakefs://example-repo -m s3://example-bucket/path/to/inventory/YYYY-MM-DDT00-00Z/manifest.json --config config.yaml . Notes . | Perform the import from a machine with access to your database, and on the same region of your destination bucket. | You can download the lakefs binary from here. Make sure you choose one compatible with your installation of lakeFS. | Use a configuration file like the one used to start your lakeFS installation. This will be used to access your database. An example can be found here. | . Warning: the import-from-inventory branch should only be used by lakeFS. You should not make any operations on it. Gradual Import . Once you switch to using the lakeFS S3-compatible endpoint in all places, you can stop making changes to your original bucket. However, if your operation still requires that you work on the original bucket, you can repeat using the import API with up-to-date inventories every day, until you complete the onboarding process. You can specify only the prefixes that require import. lakeFS will merge those prefixes with the previous imported inventory. For example, a prefixes-file that contains only the prefix new/data/. The new commit to import-from-inventory branch will include all objects from the HEAD of that branch, except for objects with prefix new/data/ that is imported from the inventory. Limitations . Note that lakeFS cannot manage your metadata if you make changes to data in the original bucket. The following table describes the results of making changes in the original bucket, without importing it to lakeFS: . | Object action in the original bucket | ListObjects result in lakeFS | GetObject result in lakeFS | . | Create | Object not visible | Object not accessible | . | Overwrite | Object visible with outdated metadata | Updated object accessible | . | Delete | Object visible | Object not accessible | . ",
    "url": "/setup/import.html#zero-copy-import",
    "relUrl": "/setup/import.html#zero-copy-import"
  },"191": {
    "doc": "Import data into lakeFS",
    "title": "Importing from public buckets",
    "content": "lakeFS needs access to the imported location to first list the files to import and later to read the files upon users request. There are some use-cases where the user would like to import from a destination which isn’t owned by the account running lakeFS. For example, importing public datasets to experiment with lakeFS and Spark. lakeFS will require additional permissions to read from public buckets. For example, for S3 public buckets, the following policy needs to be attached to the lakeFS S3 service-account to allow access to public buckets, while blocking access to other owned buckets: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"PubliclyAccessibleBuckets\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetBucketVersioning\", \"s3:ListBucket\", \"s3:GetBucketLocation\", \"s3:ListBucketMultipartUploads\", \"s3:ListBucketVersions\", \"s3:GetObject\", \"s3:GetObjectVersion\", \"s3:AbortMultipartUpload\", \"s3:ListMultipartUploadParts\" ], \"Resource\": [\"*\"], \"Condition\": { \"StringNotEquals\": { \"s3:ResourceAccount\": \"&lt;YourAccountID&gt;\" } } } ] } . ",
    "url": "/setup/import.html#importing-from-public-buckets",
    "relUrl": "/setup/import.html#importing-from-public-buckets"
  },"192": {
    "doc": "Deploy lakeFS",
    "title": "Deploy lakeFS",
    "content": "This page contains a collection of practical step-by-step instructions to help you set up lakeFS on your preferred cloud environemnt. If you just want to try out lakeFS locally, see Quickstart. ",
    "url": "/deploy/",
    "relUrl": "/deploy/"
  },"193": {
    "doc": "Integrations",
    "title": "Integrations",
    "content": " ",
    "url": "/integrations/",
    "relUrl": "/integrations/"
  },"194": {
    "doc": "Understanding lakeFS",
    "title": "Understanding lakeFS",
    "content": "This section includes all the details about the lakeFS open source project. ",
    "url": "/understand/",
    "relUrl": "/understand/"
  },"195": {
    "doc": "Using lakeFS",
    "title": "Using lakeFS",
    "content": " ",
    "url": "/usecases/",
    "relUrl": "/usecases/"
  },"196": {
    "doc": "Slack",
    "title": "Slack",
    "content": " ",
    "url": "/slack/",
    "relUrl": "/slack/"
  },"197": {
    "doc": "Quickstart",
    "title": "Quickstart",
    "content": " ",
    "url": "/quickstart/",
    "relUrl": "/quickstart/"
  },"198": {
    "doc": "Quickstart",
    "title": "lakeFS Playground",
    "content": "Experience lakeFS first hand with your own isolated environment. You can easily integrate it with your existing tools, and feel lakeFS in action in an environment similar to your own. Try lakeFS now without installing . ",
    "url": "/quickstart/#lakefs-playground",
    "relUrl": "/quickstart/#lakefs-playground"
  },"199": {
    "doc": "Quickstart",
    "title": "Katacoda Tutorial",
    "content": "Learn how to use lakeFS using the CLI and an interactive Spark shell - all from your browser, without installing anything. In the tutorial we cover: . | Basic lakectl command line usage | How to read, write, list and delete objects from lakeFS using the lakectl command | Read from, and write to lakeFS using its S3 API interface using Spark | Diff, commit and merge the changes created by Spark | Track commit history to understand changes to your data over time | . The web based environment provides a full working lakeFS and Spark environment, so feel free to explore it on your own. Start Katacoda Tutorial Now . ",
    "url": "/quickstart/#katacoda-tutorial",
    "relUrl": "/quickstart/#katacoda-tutorial"
  },"200": {
    "doc": "Quickstart",
    "title": "Next Steps",
    "content": "After getting acquainted with lakeFS, easily install it on your computer or deploy it on your cloud account. ",
    "url": "/quickstart/#next-steps",
    "relUrl": "/quickstart/#next-steps"
  },"201": {
    "doc": "Prepare Your Storage",
    "title": "Prepare Your Storage",
    "content": "A production installation of lakeFS will usually use your cloud provider’s object storage as the underlying storage layer. You can choose to create a new bucket/container (recommended), or use an existing one with a path prefix. The path under the existing bucket/container should be empty. If you already have a bucket/container configured, you are ready to create your first lakeFS repository. Choose your storage provider to configure your storage: . ",
    "url": "/setup/storage/",
    "relUrl": "/setup/storage/"
  },"202": {
    "doc": "Setup lakeFS",
    "title": "Setup lakeFS",
    "content": "Once lakeFS is installed, a few simple steps are required to start working. The first step is to create a bucket/container in your underlying storage. If you already have one, you are ready to create your first lakeFS repository. ",
    "url": "/setup/",
    "relUrl": "/setup/"
  },"203": {
    "doc": "Reference",
    "title": "Reference",
    "content": " ",
    "url": "/reference/",
    "relUrl": "/reference/"
  },"204": {
    "doc": "What is lakeFS",
    "title": "What is lakeFS",
    "content": "lakeFS transforms object storage buckets into data lake repositories that expose a Git-like interface. By design, it works with data of any size. The Git-like interface means users of lakeFS can use the same development workflows for code and data. Git workflows greatly improved software development practices; we designed lakeFS to bring the same benefits to data. In this way, lakeFS brings a unique combination of performance and manageability to data lakes. To learn more about applying Git principles to data, see here. The open source lakeFS project supports AWS S3, Azure Blob Storage and Google Cloud Storage (GCS) as its underlying storage service. It is API compatible with S3 and integrates seamlessly with popular data frameworks such as Spark, Hive, dbt, Trino, and many others. ",
    "url": "/",
    "relUrl": "/"
  },"205": {
    "doc": "What is lakeFS",
    "title": "New! lakeFS Playground",
    "content": "Experience lakeFS first hand with your own isolated environment. You can easily integrate it with your existing tools, and feel lakeFS in action in an environment similar to your own. Try lakeFS now without installing . ",
    "url": "/#new-lakefs-playground",
    "relUrl": "/#new-lakefs-playground"
  },"206": {
    "doc": "What is lakeFS",
    "title": "How do I use lakeFS?",
    "content": "lakeFS maintains compatibility with the S3 API to minimize adoption friction. Use it as a drop-in replacement for S3 from the perspective of any tool interacting with a data lake. For example, take the common operation of reading a collection of data from object storage into a Spark DataFrame. For data outside a lakeFS repo, the code will look like: . df = spark.read.parquet(\"s3a://my-bucket/collections/foo/\") . After adding the data collections in my-bucket to a repository, the same operation becomes: . df = spark.read.parquet(\"s3a://my-repo/main-branch/collections/foo/\") . You can use the same methods and syntax you already use to read and write data when using a lakeFS repository. This simplifies adoption of lakeFS: minimal changes are needed to get started, making further changes an incremental process. ",
    "url": "/#how-do-i-use-lakefs",
    "relUrl": "/#how-do-i-use-lakefs"
  },"207": {
    "doc": "What is lakeFS",
    "title": "Why is lakeFS the data solution you’ve been missing?",
    "content": "Working with data in a lakeFS repository — as opposed to a bucket — enables simplified workflows when developing data lake pipelines. lakeFS performs all these operations safely and efficiently: . | Copying objects between prefixes to promote new data | Deleting specific objects to recover from data errors | Maintaining auxilliary jobs that populate a development environment with data | . If today you spend time performing any of these actions, adopting lakeFS will speed up your development and deployment cycles, reduce the chance of incorrect data making it into production, and make recovery less painful if it does. Through its versioning engine, lakeFS enables the following built-in operations familiar from Git: . | branch: a consistent copy of a repository, isolated from other branches and their changes. Initial creation of a branch is a metadata operation that does not duplicate objects. | commit: an immutable checkpoint containing a complete snapshot of a repository. | merge: performed between two branches — merges atomically update one branch with the changes from another. | revert: return a repo to the exact state of a previous commit. | tag: a pointer to a single immutable commit with a readable, meaningful name. | . See the object model for an in-depth definition of these,and the CLI reference for the full list of commands. Incorporating these operations into your data lake pipelines provides the same collaboration and organizational benefits you get when managing application code with source control. The lakeFS promotion workflow . Here’s how lakeFS branches and merges improve the universal process of updating collections with the latest data. | First, create a new branch from main to instantly generate a complete “copy” of your production data. | Apply changes or make updates on the isolated branch to understand their impact prior to exposure. | And finally, perform a merge from the feature branch back to main to atomically promote the updates into production. | . Following this pattern, lakeFS facilitates a streamlined data deployment workflow that consistently produces data assets you can have total confidence in. ",
    "url": "/#why-is-lakefs-the-data-solution-youve-been-missing",
    "relUrl": "/#why-is-lakefs-the-data-solution-youve-been-missing"
  },"208": {
    "doc": "What is lakeFS",
    "title": "What else does lakeFS do?",
    "content": "lakeFS helps you maintain a tidy data lake in several other ways, including: . Recovery from data errors . Human error, misconfiguration, or wide-ranging systematic effects are unavoidable. When they do happen, erroneous data may make it into production, or critical data assets might accidentally by deleted. By their nature, backups are the wrong tool for recovering from such events. Backups are periodic events that are usually not tied to performing erroneous operations. So they may be out of date, and they will require sifting through data at the object level. This process is inefficient and can take hours, days, or in some cases, weeks to complete. By quickly committing entire snapshots of data at well-defined times, recovering data in deletion or corruption events becomes an instant one-line operation with lakeFS: just identify a good historical commit, and then restore to it or copy from it. Reverting your data lake back to previous version using our command-line tool is explained here. Data reprocessing and backfills . Occasionally, we might need to reprocess historical data. This can be due to several reasons: . | Implementing new logic. | Late arriving data that wasn’t included in previous analysis, and need to be backfilled after the fact. | . This is tricky first and foremost because it often involves huge volumes of historical data. In addition, auditing requirements may necessitate keeping the old version of the data. lakeFS allows you to manage the reprocess on an isolated branch before merging to ensure the reprocessed data is exposed atomically. It also allows you to easily access the different versions of reprocessed data, using any tag or a historical commit ID. Cross-collection consistency guarantees . Data engineers typically need to implement custom logic in scripts to guarantee two or more data assets are updated synchronously. This logic often requires extensive rewrites or periods during which data is unavailable. The lakeFS merge operation from one branch into another removes the need to implement this logic yourself. Instead, make updates to the desired data assets on a branch, and then utilize a lakeFS merge to atomically expose the data to downstream consumers. To learn more about atomic cross-collection updates, this video describes the concept in more detail. Troubleshooting production problems . Data engineers are often asked to validate the data. A user might report inconsistencies, question the accuracy, or simply report it to be incorrect. Since the data continuously changes, it is challenging to understand its state at the time of the error. The best way to investigate, therefore, is to have a snapshot of the data as close as possible to the time of the error. Once implementing a regular commit cadence in lakeFS, each commit represents an accessible historical snapshot of the data. When needed, a branch may be created from a commit ID to debug an issue in isolation. To learn more on how to access a specific historical commit in a repository, see our seminal post on data reproducibility. Establishing data quality guarantees . The best way to deal with mistakes is to avoid them. A data source that is ingested into the lake introducing low-quality data should be blocked before exposure if possible. With lakeFS, you can achieve this by tying data quality tests to commit and merge operations via lakeFS hooks. Additional things you should know about lakeFS: . | It is format agnostic | Your data stays in place | It minimizes data duplication via a copy-on-write mechanism | It maintains high performance over data lakes of any size | It includes configurable garbage collection capabilities | It is highly available and production ready | . Downloads . Binary Releases . Binary packages are available for Linux/macOS/Windows on GitHub Releases . Docker Images . Official Docker images are available at https://hub.docker.com/r/treeverse/lakefs . Next steps . Get started and set up lakeFS on your preferred cloud environemnt . ",
    "url": "/#what-else-does-lakefs-do",
    "relUrl": "/#what-else-does-lakefs-do"
  },"209": {
    "doc": "Install lakeFS",
    "title": "Install lakeFS",
    "content": "Note: The quickstart section is for learning purposes. The installations below will not persist your data. Instead, it will spin-up a database in a docker container, which will be discarded later. For a production suitable deployment, learn how to deploy lakeFS on your cloud. ",
    "url": "/quickstart/installing.html",
    "relUrl": "/quickstart/installing.html"
  },"210": {
    "doc": "Install lakeFS",
    "title": "Using Docker Compose",
    "content": "To run a local lakeFS instance using Docker Compose: . | Ensure you have Docker and Docker Compose installed on your computer, and that Compose version is 1.25.04 or higher. For more information, please see this issue. | Run the following command in your terminal: . curl https://compose.lakefs.io | docker-compose -f - up . | Check your installation by opening http://127.0.0.1:8000/setup in your web browser. | Create your first repository in lakeFS. | . ",
    "url": "/quickstart/installing.html#using-docker-compose",
    "relUrl": "/quickstart/installing.html#using-docker-compose"
  },"211": {
    "doc": "Install lakeFS",
    "title": "Other methods",
    "content": "You can try lakeFS: . | On Kubernetes. | With docker-compose on Windows. | By running the binary directly. | . ",
    "url": "/quickstart/installing.html#other-methods",
    "relUrl": "/quickstart/installing.html#other-methods"
  },"212": {
    "doc": "Install lakeFS",
    "title": "Next steps",
    "content": "Now that your lakeFS is running, try creating a repository. ",
    "url": "/quickstart/installing.html#next-steps",
    "relUrl": "/quickstart/installing.html#next-steps"
  },"213": {
    "doc": "Tutorial – Isolated Environment",
    "title": "Isolated Environments",
    "content": " ",
    "url": "/quickstart/iso_env.html#isolated-environments",
    "relUrl": "/quickstart/iso_env.html#isolated-environments"
  },"214": {
    "doc": "Tutorial – Isolated Environment",
    "title": "Why Do I Need Multiple Environments?",
    "content": "When developing over a data lake, it is useful to have replicas of your production environment. These replicas allow you to test and understand changes to your data without impacting consumers of the production data. Running ETL and transformation jobs directly in production is a guaranteed way to have data issues flow into dashboards, ML models, and other consumers sooner or later. The most common approach to avoid making changes directly in production is to create and maintain a second data environment called development (or dev) where updates are implemented first. The issue with this approach is that it is time-consuming and costly to maintain this separate dev environment. And for larger teams it forces multiple people to share one environment, requiring co-ordination. ",
    "url": "/quickstart/iso_env.html#why-do-i-need-multiple-environments",
    "relUrl": "/quickstart/iso_env.html#why-do-i-need-multiple-environments"
  },"215": {
    "doc": "Tutorial – Isolated Environment",
    "title": "How do I create isolated environments with lakeFS?",
    "content": "lakeFS makes it instantaneous to create isolated development environments. This frees you from spending time on environment maintenance and makes it possible to create as many environments as needed. In a lakeFS repository, data is always located on a branch. You can think of each branch in lakeFS as its own environment. This is because branches are isolated, meaning changes on one branch have no effect other branches. Objects that are unchanged between two branches are not copied, but rather shared to both branches via metadata pointers that lakeFS manages. If you make a change on one branch and want it reflected on another, you can perform a merge operation to update one branch with the changes from another. Let’s show an example of using multiple lakeFS branches for isolation. ",
    "url": "/quickstart/iso_env.html#how-do-i-create-isolated-environments-with-lakefs",
    "relUrl": "/quickstart/iso_env.html#how-do-i-create-isolated-environments-with-lakefs"
  },"216": {
    "doc": "Tutorial – Isolated Environment",
    "title": "Using Branches as Environments",
    "content": "The key difference when using lakeFS for isolated data environments is you can create them immediately before testing a change. And once new data is merged into production, you can delete the branch, effectively deleting the old environment. This is different from creating a long-living dev environment that is used as a staging area to test all updates. With lakeFS, we create a new branch for each change to production we want to make. (One benefit of this is the ability to test multiple changes at one time). Setup . To get a working lakeFS environment, we’re going to run a pre-configured Docker environment on our local (Mac) machine. This environment (which we call the “Everything Bagel”) includes lakeFS and other common data tools like Spark, dbt, Trino, Hive, and Jupyter. The following commands can be run in your terminal to get the bagel running: . | Clone the lakeFS repo: git clone https://github.com/treeverse/lakeFS.git | Start the Docker containers: cd lakeFS/deployments/compose &amp;&amp; docker compose up -d | . Once you have your Docker environment running, it is helpful to pull up the UI for lakeFS. To do this navigate to http://localhost:8000 in your browser. The access key and secret to login are found in the docker_compose.yml file in the lakefs-setup section. Once you are logged in, you should see a page that looks like below. The first thing to notice is in this environment, lakeFS comes with a repository called example already created, and the repo’s default branch is main. If your lakeFS installation doesn’t have the example repo created, you can use the green Create Repository button to do so: . Next it’ll be useful to add some data into this lakeFS repo. We’ll use an Amazon review dataset from a public S3 bucket. First we’ll download the file to our local computer using the AWS CLI. Then, we’ll upload it into lakeFS using the Upload Object button in the UI. To install the AWS CLI, follow these instructions . Download the file . aws s3 cp s3://amazon-reviews-pds/parquet/product_category=Sports/part-00000-495c48e6-96d6-4650-aa65-3c36a3516ddd.c000.snappy.parquet $HOME/ . Next, on the Objects tab of the example repo, click Upload Object then Choose File and find it in the Finder window. Once it is uploaded, we’ll see the file in the repository on the main branch. Currently it is in an uncommitted state. Let’s commit it! . To do this we can go to the Uncommitted Changes tab and click the green Commit Changes button in the top right. Add a commit message and the file is in the version history of our lakeFS repo. As the final setup step, we’re going to create a new branch called double-branch. To do this we can use the lakeFS UI by going to the Branches tab and clicking Create Branch. Once we create it, we’ll see two branches, main and double-branch. This new branch serves as an isolated environment on which we can make changes that have no effect on main. Let’s see that in action by using… . Data Manipulation with Jupyter &amp; Spark . The Everything Bagel comes with Spark and Jupyter installed. Let’s use them to manipulate the data on one branch, showing how it has no effect on the other. To access the Jupyter notebook UI, go to http://localhost:8888 in your browser and type in “lakefs” when prompted for a password. Next, create a new notebook and start a spark context: . from pyspark.context import SparkContext from pyspark.sql.session import SparkSession sc = SparkContext('local') spark = SparkSession(sc) . Now we can use spark to read in the parquet file we added to the main branch of our lakeFS repo: . df = spark.read.parquet('s3a://example/main/') . To see the Dataframe, run display(df.show()). If we run display(df.count()) we’ll get returned that the Dataframe has 486k rows. Changing one branch . Let’s accidentally write the DataFrame back to the double-branch branch, creating a duplicate object on that branch. df.write.mode('append').parquet('s3a://example/double-branch/') . What happens if we re-read in the data on both branches and perform a count on the resulting DataFrames? . As expected, there are now twice as many rows, 972k, on the double-branch branch. On the main branch however, there is still just the origin 486k rows. This shows the utility of branch-based isolated environments with lakeFS. ",
    "url": "/quickstart/iso_env.html#using-branches-as-environments",
    "relUrl": "/quickstart/iso_env.html#using-branches-as-environments"
  },"217": {
    "doc": "Tutorial – Isolated Environment",
    "title": "Tutorial – Isolated Environment",
    "content": " ",
    "url": "/quickstart/iso_env.html",
    "relUrl": "/quickstart/iso_env.html"
  },"218": {
    "doc": "With Kubernetes",
    "title": "Deploy lakeFS on Kubernetes",
    "content": " ",
    "url": "/deploy/k8s.html#deploy-lakefs-on-kubernetes",
    "relUrl": "/deploy/k8s.html#deploy-lakefs-on-kubernetes"
  },"219": {
    "doc": "With Kubernetes",
    "title": "Database",
    "content": "lakeFS requires a PostgreSQL database to synchronize actions on your repositories. This section assumes you already have a PostgreSQL database accessible from your Kubernetes cluster. Instructions for creating the database can be found on the deployment instructions for AWS, Azure and GCP. ",
    "url": "/deploy/k8s.html#database",
    "relUrl": "/deploy/k8s.html#database"
  },"220": {
    "doc": "With Kubernetes",
    "title": "Table of contents",
    "content": ". | Prerequisites | Installing on Kubernetes | Load balancing | Next Steps | . ",
    "url": "/deploy/k8s.html#table-of-contents",
    "relUrl": "/deploy/k8s.html#table-of-contents"
  },"221": {
    "doc": "With Kubernetes",
    "title": "Prerequisites",
    "content": "Users that require S3 access using virtual host addressing should configure an S3 Gateway domain. ",
    "url": "/deploy/k8s.html#prerequisites",
    "relUrl": "/deploy/k8s.html#prerequisites"
  },"222": {
    "doc": "With Kubernetes",
    "title": "Installing on Kubernetes",
    "content": "lakeFS can be easily installed on Kubernetes using a Helm chart. To install lakeFS with Helm: . | Copy the Helm values file relevant to your storage provider: | . | S3 | GCS | Azure Blob | . secrets: # replace DATABASE_CONNECTION_STRING with the connection string of the database you created in a previous step. # e.g. postgres://postgres:myPassword@my-lakefs-db.rds.amazonaws.com:5432/lakefs databaseConnectionString: [DATABASE_CONNECTION_STRING] # replace this with a randomly-generated string authEncryptSecretKey: [ENCRYPTION_SECRET_KEY] lakefsConfig: | blockstore: type: s3 s3: region: us-east-1 # optional, fallback in case discover from bucket is not supported . secrets: # replace DATABASE_CONNECTION_STRING with the connection string of the database you created in a previous step. # e.g.: postgres://postgres:myPassword@localhost/postgres:5432 databaseConnectionString: [DATABASE_CONNECTION_STRING] # replace this with a randomly-generated string authEncryptSecretKey: [ENCRYPTION_SECRET_KEY] lakefsConfig: | blockstore: type: gs # Uncomment the following lines to give lakeFS access to your buckets using a service account: # gs: # credentials_json: [YOUR SERVICE ACCOUNT JSON STRING] . Notes for running lakeFS on GKE . | To connect to your database, you need to use one of the ways of connecting GKE to Cloud SQL. | To give lakeFS access to your bucket, you can start the cluster in storage-rw mode. Alternatively, you can use a service account JSON string by uncommenting the gs.credentials_json property in the following yaml. | . secrets: # replace this with the connection string of the database you created in a previous step: databaseConnectionString: [DATABASE_CONNECTION_STRING] # replace this with a randomly-generated string authEncryptSecretKey: [ENCRYPTION_SECRET_KEY] lakefsConfig: | blockstore: type: azure azure: auth_method: msi # msi for active directory, access-key for access key # In case you chose to authenticate via access key unmark the following rows and insert the values from the previous step # storage_account: [your storage account] # storage_access_key: [your access key] . | Fill in the missing values and save the file as conf-values.yaml. For more configuration options, see our Helm chart README. The lakefsConfig parameter is the lakeFS configuration documented here, but without sensitive information. Sensitive information like databaseConnectionString is given through separate parameters, and the chart will inject them into Kubernetes secrets. | In the directory where you created conf-values.yaml, run the following commands: . # Add the lakeFS repository helm repo add lakefs https://charts.lakefs.io # Deploy lakeFS helm install example-lakefs lakefs/lakefs -f conf-values.yaml . example-lakefs is the Helm Release name. | . You should give your Kubernetes nodes access to all buckets/containers you intend to use lakeFS with. If you can’t provide such access, lakeFS can be configured to use an AWS key-pair, an Azure access key, or a Google Cloud credentials file to authenticate (part of the lakefsConfig YAML below). ",
    "url": "/deploy/k8s.html#installing-on-kubernetes",
    "relUrl": "/deploy/k8s.html#installing-on-kubernetes"
  },"223": {
    "doc": "With Kubernetes",
    "title": "Load balancing",
    "content": "You should have a load balancer direct requests to the lakeFS server. Options to do so include a Kubernetes Service of type LoadBalancer, or a Kubernetes Ingress. By default, lakeFS operates on port 8000, and exposes a /_health endpoint which you can use for health checks. The NGINX Ingress Controller by default limits the client body size to 1 MiB. Some clients use bigger chunks to upload objects, for example multipart upload to lakeFS using the S3 Gateway or a simple PUT request using the OpenAPI Server. Checkout Nginx documentation for increasing the limit, or an example of Nginx configuration with MinIO. ",
    "url": "/deploy/k8s.html#load-balancing",
    "relUrl": "/deploy/k8s.html#load-balancing"
  },"224": {
    "doc": "With Kubernetes",
    "title": "Next Steps",
    "content": "Your next step is to prepare your storage. If you already have a storage bucket/container, you are ready to create your first lakeFS repository. ",
    "url": "/deploy/k8s.html#next-steps",
    "relUrl": "/deploy/k8s.html#next-steps"
  },"225": {
    "doc": "With Kubernetes",
    "title": "With Kubernetes",
    "content": " ",
    "url": "/deploy/k8s.html",
    "relUrl": "/deploy/k8s.html"
  },"226": {
    "doc": "Kafka",
    "title": "Using lakeFS with Kafka",
    "content": "Apache Kafka provides a unified, high-throughput, low-latency platform for handling real-time data feeds. Different distributions of Kafka have different methods for exporting data to s3, called Kafka Sink Connectors. Most commonly used for S3 is Confluent’s S3 Sink Connector. Add the following to connector.properties file for lakeFS support: . # Your lakeFS repository s3.bucket.name=example-repo # Your lakeFS S3 endpoint and credentials store.url=https://lakefs.example.com aws.access.key.id=AKIAIOSFODNN7EXAMPLE aws.secret.access.key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # main being the branch we want to write to topics.dir=main/topics . ",
    "url": "/integrations/kakfa.html#using-lakefs-with-kafka",
    "relUrl": "/integrations/kakfa.html#using-lakefs-with-kafka"
  },"227": {
    "doc": "Kafka",
    "title": "Kafka",
    "content": " ",
    "url": "/integrations/kakfa.html",
    "relUrl": "/integrations/kakfa.html"
  },"228": {
    "doc": "Kubeflow",
    "title": "Using lakeFS with Kubeflow pipelines",
    "content": "Kubeflow is a project dedicated to making deployments of ML workflows on Kubernetes simple, portable and scalable. A Kubeflow pipeline is a portable and scalable definition of an ML workflow composed of steps. Each step in the pipeline is an instance of a component represented as an instance of ContainerOp. ",
    "url": "/integrations/kubeflow.html#using-lakefs-with-kubeflow-pipelines",
    "relUrl": "/integrations/kubeflow.html#using-lakefs-with-kubeflow-pipelines"
  },"229": {
    "doc": "Kubeflow",
    "title": "Table of contents",
    "content": ". | Add pipeline steps for lakeFS operations . | Function-based ContainerOps | Non-function-based ContainerOps | . | Add the lakeFS steps to your pipeline | . ",
    "url": "/integrations/kubeflow.html#table-of-contents",
    "relUrl": "/integrations/kubeflow.html#table-of-contents"
  },"230": {
    "doc": "Kubeflow",
    "title": "Add pipeline steps for lakeFS operations",
    "content": "To integrate lakeFS onto your Kubeflow pipeline, we will need to create Kubeflow components that perform lakeFS operations. Currently, there are two methods to create lakeFS ContainerOps: . | Implement a function-based ContainerOp that uses lakeFS’s Python API to invoke lakeFS operations. | Implement a ContainerOp that uses the lakectl CLI docker image to invoke lakeFS operations. | . Function-based ContainerOps . To implement a function-based component that invokes lakeFS operations, you should use the Python OpenAPI client lakeFS has. See the example below that demonstrates how to make the client’s package available to your ContainerOp. Example operations . Create new branch: A function-based ContainerOp that creates a branch called example-branch based on the main branch of example-repo. from kfp import components def create_branch(repo_name, branch_name, source_branch): import lakefs_client from lakefs_client import models from lakefs_client.client import LakeFSClient # lakeFS credentials and endpoint configuration = lakefs_client.Configuration() configuration.username = 'AKIAIOSFODNN7EXAMPLE' configuration.password = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' configuration.host = 'https://lakefs.example.com' client = LakeFSClient(configuration) client.branches.create_branch(repository=repo_name, branch_creation=models.BranchCreation(name=branch_name, source=source_branch)) # Convert the function to a lakeFS pipeline step. create_branch_op = components.func_to_container_op( func=create_branch, packages_to_install=['lakefs_client==&lt;lakeFS version&gt;']) # Type in the lakeFS version you are using . You can invoke any lakeFS operation supported by lakeFS OpenAPI, for example, you could implement a commit and merge function-based ContainerOps. Check out the full API reference. Non-function-based ContainerOps . To implement a non-function based ContainerOp, you should use the treeverse/lakectl docker image. With this image you can run lakeFS CLI commands to execute the desired lakeFS operation. For lakectl to work with Kubeflow, you will need to pass your lakeFS configurations as environment variables named: . | LAKECTL_CREDENTIALS_ACCESS_KEY_ID: AKIAIOSFODNN7EXAMPLE | LAKECTL_SECRET_ACCESS_KEY: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY | LAKECTL_SERVER_ENDPOINT_URL: https://lakefs.example.com | . Example operations . | Commit changes to a branch: A ContainerOp that commits uncommitted changes to example-branch on example-repo. from kubernetes.client.models import V1EnvVar def commit_op(): return dsl.ContainerOp( name='commit', image='treeverse/lakectl', arguments=['commit', 'lakefs://example-repo/example-branch', '-m', 'commit message']).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_ACCESS_KEY_ID',value='AKIAIOSFODNN7EXAMPLE')).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY',value='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')).add_env_variable(V1EnvVar(name='LAKECTL_SERVER_ENDPOINT_URL',value='https://lakefs.example.com')) . | Merge two lakeFS branches: A ContainerOp that merges example-branch into the main branch of example-repo. def merge_op(): return dsl.ContainerOp( name='merge', image='treeverse/lakectl', arguments=['merge', 'lakefs://example-repo/example-branch', 'lakefs://example-repo/main']).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_ACCESS_KEY_ID',value='AKIAIOSFODNN7EXAMPLE')).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY',value='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')).add_env_variable(V1EnvVar(name='LAKECTL_SERVER_ENDPOINT_URL',value='https://lakefs.example.com')) . | . You can invoke any lakeFS operation supported by lakectl by implementing it as a ContainerOp. Check out the complete CLI reference for the list of supported operations. Note The lakeFS Kubeflow integration that uses lakectl is supported on lakeFS version &gt;= v0.43.0. ",
    "url": "/integrations/kubeflow.html#add-pipeline-steps-for-lakefs-operations",
    "relUrl": "/integrations/kubeflow.html#add-pipeline-steps-for-lakefs-operations"
  },"231": {
    "doc": "Kubeflow",
    "title": "Add the lakeFS steps to your pipeline",
    "content": "Add the steps created on the previous step to your pipeline before compiling it. Example pipeline . A pipeline that implements a simple ETL, that has steps for branch creation and commits. def lakectl_pipeline(): create_branch_task = create_branch_op('example-repo', 'example-branch', 'main') # A function-based component extract_task = example_extract_op() commit_task = commit_op() transform_task = example_transform_op() commit_task = commit_op() load_task = example_load_op() . Note It is recommended to store credentials as kubernetes secrets and pass them as environment variables to Kubeflow operations using V1EnvVarSource. ",
    "url": "/integrations/kubeflow.html#add-the-lakefs-steps-to-your-pipeline",
    "relUrl": "/integrations/kubeflow.html#add-the-lakefs-steps-to-your-pipeline"
  },"232": {
    "doc": "Kubeflow",
    "title": "Kubeflow",
    "content": " ",
    "url": "/integrations/kubeflow.html",
    "relUrl": "/integrations/kubeflow.html"
  },"233": {
    "doc": "MapReduce",
    "title": "Using lakeFS with MapReduce",
    "content": " ",
    "url": "/integrations/mapreduce.html#using-lakefs-with-mapreduce",
    "relUrl": "/integrations/mapreduce.html#using-lakefs-with-mapreduce"
  },"234": {
    "doc": "MapReduce",
    "title": "MapReduce",
    "content": " ",
    "url": "/integrations/mapreduce.html",
    "relUrl": "/integrations/mapreduce.html"
  },"235": {
    "doc": "MinIO",
    "title": "Using lakeFS with MinIO",
    "content": "MinIO is a high performance, distributed object storage system. You can use lakeFS to add git-like capabilities over it. For learning purposes, it is recommended to follow our step-by-step guide on how to deploy lakeFS locally over MinIO. If you already know how to install lakeFS, and want to configure it to use MinIO as the underlying storage, your lakeFS configuration should contain the following: . blockstore: type: s3 s3: force_path_style: true endpoint: http://&lt;minio_endpoint&gt;:9000 discover_bucket_region: false credentials: access_key_id: &lt;minio_access_key&gt; secret_access_key: &lt;minio_secret_key&gt; . The full example can be found here. Note that lakeFS can also be configured using environment variables. ",
    "url": "/integrations/minio.html#using-lakefs-with-minio",
    "relUrl": "/integrations/minio.html#using-lakefs-with-minio"
  },"236": {
    "doc": "MinIO",
    "title": "MinIO",
    "content": " ",
    "url": "/integrations/minio.html",
    "relUrl": "/integrations/minio.html"
  },"237": {
    "doc": "Monitoring using Prometheus",
    "title": "Monitoring using Prometheus",
    "content": " ",
    "url": "/reference/monitor.html",
    "relUrl": "/reference/monitor.html"
  },"238": {
    "doc": "Monitoring using Prometheus",
    "title": "Table of contents",
    "content": ". | Example prometheus.yml | Metrics exposed by lakeFS | Example queries . | 99th percentile of API request latencies | 50th percentile of S3-compatible API latencies | Number of errors in outgoing S3 requests | Number of open connections to the database | Example Grafana dashboard | . | . ",
    "url": "/reference/monitor.html#table-of-contents",
    "relUrl": "/reference/monitor.html#table-of-contents"
  },"239": {
    "doc": "Monitoring using Prometheus",
    "title": "Example prometheus.yml",
    "content": "lakeFS exposes metrics through the same port used by the lakeFS service, using the standard /metrics path. An example prometheus.yml could look like this: . scrape_configs: - job_name: lakeFS scrape_interval: 10s metrics_path: /metrics static_configs: - targets: - lakefs.example.com:8000 . ",
    "url": "/reference/monitor.html#example-prometheusyml",
    "relUrl": "/reference/monitor.html#example-prometheusyml"
  },"240": {
    "doc": "Monitoring using Prometheus",
    "title": "Metrics exposed by lakeFS",
    "content": "By default, Prometheus exports metrics with OS process information like memory and CPU. It also includes Go-specific metrics like details about GC and number of goroutines. You can learn about these default metrics in this post. In addition, lakeFS exposes the following metrics to help monitor your deployment: . | Name in Prometheus | Description | Labels | . | api_requests_total | lakeFS API requests (counter) | code: http statusmethod: http method | . | api_request_duration_seconds | Durations of lakeFS API requests (histogram) | operation: name of API operationcode: http status | . | gateway_request_duration_seconds | lakeFS S3-compatible endpoint request (histogram) | operation: name of gateway operationcode: http status | . | s3_operation_duration_seconds | Outgoing S3 operations (histogram) | operation: operation nameerror: “true” if error, “false” otherwise | . | gs_operation_duration_seconds | Outgoing Google Storage operations (histogram) | operation: operation nameerror: “true” if error, “false” otherwise | . | azure_operation_duration_seconds | Outgoing Azure storage operations (histogram) | operation: operation nameerror: “true” if error, “false” otherwise | . | go_sql_stats_* | Go DB stats metrics have this prefix.dlmiddlecote/sqlstats is used to expose them. |   | . ",
    "url": "/reference/monitor.html#metrics-exposed-by-lakefs",
    "relUrl": "/reference/monitor.html#metrics-exposed-by-lakefs"
  },"241": {
    "doc": "Monitoring using Prometheus",
    "title": "Example queries",
    "content": "Note: when using Prometheus functions like rate or increase, results are extrapolated and may not be exact. 99th percentile of API request latencies . sum by (operation)(histogram_quantile(0.99, rate(api_request_duration_seconds_bucket[1m]))) . 50th percentile of S3-compatible API latencies . sum by (operation)(histogram_quantile(0.5, rate(gateway_request_duration_seconds_bucket[1m]))) . Number of errors in outgoing S3 requests . sum by (operation) (increase(s3_operation_duration_seconds_count{error=\"true\"}[1m])) . Number of open connections to the database . go_sql_stats_connections_open . Example Grafana dashboard . ",
    "url": "/reference/monitor.html#example-queries",
    "relUrl": "/reference/monitor.html#example-queries"
  },"242": {
    "doc": "More Quickstart Options",
    "title": "More Quickstart Options",
    "content": "Note: The quickstart section is for learning purposes. The installations below will not persist your data. Instead, it will spin-up a database in a docker container, which will be discarded later. For a production suitable deployment, learn how to deploy lakeFS on your cloud. ",
    "url": "/quickstart/more_quickstart_options.html",
    "relUrl": "/quickstart/more_quickstart_options.html"
  },"243": {
    "doc": "More Quickstart Options",
    "title": "Table of contents",
    "content": ". | Docker on Windows | On Kubernetes with Helm | Using the Binary | . ",
    "url": "/quickstart/more_quickstart_options.html#table-of-contents",
    "relUrl": "/quickstart/more_quickstart_options.html#table-of-contents"
  },"244": {
    "doc": "More Quickstart Options",
    "title": "Docker on Windows",
    "content": "To run a local lakeFS instance using Docker Compose: . | Ensure you have Docker installed on your computer, and that compose version is 1.25.04 or higher. For more information, please see this issue. | Run the following command in your terminal: . Invoke-WebRequest https://compose.lakefs.io | Select-Object -ExpandProperty Content | docker-compose -f - up . | Check your installation by opening http://127.0.0.1:8000/setup in your web browser. | You are now ready to creating your first repository in lakeFS. | . ",
    "url": "/quickstart/more_quickstart_options.html#docker-on-windows",
    "relUrl": "/quickstart/more_quickstart_options.html#docker-on-windows"
  },"245": {
    "doc": "More Quickstart Options",
    "title": "On Kubernetes with Helm",
    "content": ". | Install lakeFS on a Kubernetes cluster using Helm: # Add the lakeFS Helm repository helm repo add lakefs https://charts.lakefs.io # Deploy lakeFS with helm release \"my-lakefs\" helm install my-lakefs lakefs/lakefs . | The printed output will help you forward a port to lakeFS, so you can access it from your browser at http://127.0.0.1:8000/setup. | Move on to create your first repository in lakeFS. | . ",
    "url": "/quickstart/more_quickstart_options.html#on-kubernetes-with-helm",
    "relUrl": "/quickstart/more_quickstart_options.html#on-kubernetes-with-helm"
  },"246": {
    "doc": "More Quickstart Options",
    "title": "Using the Binary",
    "content": "Alternatively, you may opt to run the lakefs binary directly on your computer. | Download the lakeFS binary for your operating system: . Download lakefs . | Install and configure PostgreSQL . | Create a configuration file: . --- database: connection_string: \"postgres://localhost:5432/postgres?sslmode=disable\" blockstore: type: \"local\" local: path: \"~/lakefs_data\" auth: encrypt: secret_key: \"a random string that should be kept secret\" . | Create a local directory to store objects: . mkdir ~/lakefs_data . | Run the server: ./lakefs --config /path/to/config.yaml run . | Check your installation by opening http://127.0.0.1:8000/setup in your web browser. | You are now ready to create your first repository in lakeFS. | . ",
    "url": "/quickstart/more_quickstart_options.html#using-the-binary",
    "relUrl": "/quickstart/more_quickstart_options.html#using-the-binary"
  },"247": {
    "doc": "Object Model",
    "title": "Object Model",
    "content": " ",
    "url": "/understand/object-model.html",
    "relUrl": "/understand/object-model.html"
  },"248": {
    "doc": "Object Model",
    "title": "Table of contents",
    "content": ". | Introduction | lakeFS the object store | lakeFS the version control system . | Repository | Commits | Identifying commits . | Tags | Branches | Ref expressions | . | History | Three way merge | . | Concepts unique to lakeFS . | lakefs protocol URIs | . | . ",
    "url": "/understand/object-model.html#table-of-contents",
    "relUrl": "/understand/object-model.html#table-of-contents"
  },"249": {
    "doc": "Object Model",
    "title": "Introduction",
    "content": "lakeFS blends concepts from object stores such as S3 with concepts from Git. This reference defines the common concepts of lakeFS. Every concept appearing in italic text is its definition. ",
    "url": "/understand/object-model.html#introduction",
    "relUrl": "/understand/object-model.html#introduction"
  },"250": {
    "doc": "Object Model",
    "title": "lakeFS the object store",
    "content": "lakeFS is an object store, and borrows concepts from S3. An object store links objects to paths. An object holds: . | Some contents, with unlimited size and format. | Some metadata, including . | size in bytes | the creation time, a timestamp with seconds resolution | a checksum string which uniquely identifies the contents | some user metadata, a small map of strings to strings. | . | . Similarly to many object stores, lakeFS objects are immutable and never rewritten. They can be entirely replaced or deleted, but not modified. A path is a readable string, typically decoded as UTF-8. lakeFS maps paths to their objects according to specific rules. lakeFS paths use the lakefs protocol, described below. ",
    "url": "/understand/object-model.html#lakefs-the-object-store",
    "relUrl": "/understand/object-model.html#lakefs-the-object-store"
  },"251": {
    "doc": "Object Model",
    "title": "lakeFS the version control system",
    "content": "lakeFS borrows its concepts for version control from Git. Repository . A repository is a collection of objects with common history tracking. lakeFS manages versions of the repository, identified by their commits. A commit is a collection of object metadata and data, including especially all paths and the object contents and metadata at that commit. Commits have their own commit metadata, which includes a textual comment and additional user metadata. Commits . Commits are organized into a history using their parent commits. Every repository has exactly one initial commit with no parents. Note that a Git repository may have multiple initial commits. A commit with more than one parent is a merge commit. Currently lakeFS only supports merge commits with two parents. Identifying commits . A commit is identified by its commit ID, a digest of all contents of the commit. Commit IDs are by nature long, so a unique prefix may be used to abbreviate them (but note that short prefixes can become non-unique as the repository grows, so prefer to avoid abbreviating when storing commits for the long term). A commit may also be identified by using a textual definition, called a ref. Examples of refs include tags, branches, and expressions. The state of the repository at any commit is always readable. Tags . A tag is an immutable pointer to a single commit. Tags have readable names. Because tags are commits, a repository can be read from any tag. Example tags: . | v2.3 to mark a release | dev:jane-before-v2.3-merge to mark Jane’s private temporary point. | . Branches . A branch is a mutable pointer to a commit and its staging area. Repositories are readable from any branch, but they are also writable to a branch. The staging area associated with a branch is mutable storage where objects can be created, updated or deleted. These objects are readable when reading from the branch. To create a commit from a branch, all files from the staging area are merged into the contents of the current branch, creating the new set of objects. The parent of the commit is the previous branch tip, and the new branch tip is set to this new commit. Example branches: . | main, the trunk | staging, maybe ahead of main | dev:joe-bugfix-1234 for Joe to fix issue 1234. | . Ref expressions . lakeFS also supports expressions for creating a ref. These are similar to revisions in Git; indeed all ~ and ^ examples at the end of that section will work unchanged in lakeFS. | A branch or a tag are ref expressions. | If &lt;ref&gt; is a ref expression, then: . | &lt;ref&gt;^ is a ref expression referring to its first parent. | &lt;ref&gt;^N is a ref expression referring to its N’th parent; in particular &lt;ref&gt;^1 is the same as &lt;ref&gt;^. | &lt;ref&gt;~ is a ref expression referring to its first parent; in particular &lt;ref&gt;~ is the same as &lt;ref&gt;^ and &lt;ref&gt;~. | &lt;ref&gt;~N is a ref expression referring to its N’th parent, always traversing to the first parent. So &lt;ref&gt;~N is the same as &lt;ref&gt;^^...^ with N consecutive carets ^. | . | . History . The history of the branch is the list of commits from the branch tip through the first parent of each commit. Histories go back in time. The other way to create a commit is to merge an existing commit onto a branch. To merge a source commit into a branch, lakeFS finds the best common ancestor of that source commit and the branch tip, called the “base”. Then it performs a 3-way merge. The “best” ancestor is exactly that defined in the documentation for git-merge-base. The result of a merge is a new commit, with the destination as the first parent and the source as the second. Thus the previous tip of the merge destination is part of the history of the merged object. Three way merge . To merge a merge source (a commit) into a merge destination (another commit), lakeFS first finds the merge base, the nearest common parent of the two commits. It can now perform a three-way merge, by examining the presence and identity of files in each commit. In the table below, “A”, “B” and “C” are possible file contents, “X” is a missing file, and “conflict” (which only appears as a result) is a merge failure. | In base | In source | In destination | Result | Comment | . | A | A | A | A | Unchanged file | . | A | B | B | B | Files changed on both sides in same way | . | A | B | C | conflict | Files changed on both sides differently | . | A | A | B | B | File changed only on one branch | . | A | B | A | B | File changed only on one branch | . | A | X | X | X | Files deleted on both sides | . | A | B | X | conflict | File changed on one side, deleted on the other | . | A | X | B | conflict | File changed on one side, deleted on the other | . | A | A | X | X | File deleted on one side | . | A | X | A | X | File deleted on one side | . The API and lakectl allow passing an optional strategy flag with the following values: . | dest-wins - in case of a conflict, merge will pick the destination object. | source-wins - in case of a conflict, merge will pick the source object. If the strategy is set, it will affect all the objects in the merge, there is currently no way to treat each conflict differently. | . As a format-agnostic system, lakeFS currently merges by complete files. Format-specific and other user-defined merge strategies for handling conflicts are on the roadmap. ",
    "url": "/understand/object-model.html#lakefs-the-version-control-system",
    "relUrl": "/understand/object-model.html#lakefs-the-version-control-system"
  },"252": {
    "doc": "Object Model",
    "title": "Concepts unique to lakeFS",
    "content": "Underlying storage is the area on some other object store that lakeFS uses to store object contents and some of its metadata. We sometimes refer to underlying storage as physical. The path used to store the contents of an object is then termed a physical path. The object itself on underlying storage is never modified, except to remove it entirely during some cleanups. When creating a lakeFS repository, you assign it with a storage namespace. The repository’s storage namespace is the prefix in the underlying storage where data for this repository will be stored. A lot of what lakeFS does is to manage how lakeFS paths translate to physical paths on the object store. This mapping is generally not straightforward. Importantly (and unlike many object stores), lakeFS may map multiple paths to the same object on backing storage, and always does this for objects that are unchanged across versions. lakefs protocol URIs . lakeFS uses a specific format for path URIs. The URI lakefs://&lt;REPO&gt;/&lt;REF&gt;/&lt;KEY&gt; is a path to objects in the given repo and ref expression under key. This is used both for path prefixes and for full paths. In similar fashion, lakefs://&lt;REPO&gt;/&lt;REF&gt; identifies the repository at a ref expression, and lakefs://&lt;REPO&gt; identifes a repo. ",
    "url": "/understand/object-model.html#concepts-unique-to-lakefs",
    "relUrl": "/understand/object-model.html#concepts-unique-to-lakefs"
  },"253": {
    "doc": "Migrating away from lakeFS",
    "title": "Migrating away from lakeFS",
    "content": " ",
    "url": "/reference/offboarding.html",
    "relUrl": "/reference/offboarding.html"
  },"254": {
    "doc": "Migrating away from lakeFS",
    "title": "Copying data from a lakeFS repository to an S3 bucket",
    "content": "The simplest way to migrate away from lakeFS is to copy data from a lakeFS repository to an S3 bucket (or any other object store). For smaller repositories, this could be done using the AWS cli or rclone. For larger repositories, running distcp with lakeFS as the source is also an option. ",
    "url": "/reference/offboarding.html#copying-data-from-a-lakefs-repository-to-an-s3-bucket",
    "relUrl": "/reference/offboarding.html#copying-data-from-a-lakefs-repository-to-an-s3-bucket"
  },"255": {
    "doc": "Presto/Trino",
    "title": "Using lakeFS with Presto/Trino",
    "content": "Presto and Trino are a distributed SQL query engine designed to query large data sets distributed over one or more heterogeneous data sources. ",
    "url": "/integrations/presto_trino.html#using-lakefs-with-prestotrino",
    "relUrl": "/integrations/presto_trino.html#using-lakefs-with-prestotrino"
  },"256": {
    "doc": "Presto/Trino",
    "title": "Table of contents",
    "content": ". | Using lakeFS with Presto/Trino . | | Configuration . | Configure Hive connector | Configure Hive | . | Examples . | Example with schema | Example with External table | Example of copying a table with metastore tools: | . | . | . Querying data in lakeFS from Presto/Trino is the same as querying data in S3 from Presto/Trino. It is done using the Presto Hive connector or Trino Hive connector. Note In the following examples we set AWS credentials at runtime, for clarity. In production, these properties should be set using one of Hadoop’s standard ways of Authenticating with S3. ",
    "url": "/integrations/presto_trino.html#table-of-contents",
    "relUrl": "/integrations/presto_trino.html#table-of-contents"
  },"257": {
    "doc": "Presto/Trino",
    "title": "Configuration",
    "content": "Configure Hive connector . Create /etc/catalog/hive.properties with the following contents to mount the hive-hadoop2 connector as the hive catalog, replacing example.net:9083 with the correct host and port for your Hive metastore Thrift service: . connector.name=hive-hadoop2 hive.metastore.uri=thrift://example.net:9083 . Add to /etc/catalog/hive.properties the lakeFS configurations in the corresponding S3 configuration properties: . hive.s3.aws-access-key=AKIAIOSFODNN7EXAMPLE hive.s3.aws-secret-key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY hive.s3.endpoint=https://lakefs.example.com hive.s3.path-style-access=true . Configure Hive . Presto/Trino uses Hive metastore service (HMS), or a compatible implementation of the Hive metastore, such as AWS Glue Data Catalog to write data to S3. In case you are using Hive metastore, you will need to configure Hive as well. In file hive-site.xml add to the configuration: . &lt;configuration&gt; ... &lt;property&gt; &lt;name&gt;fs.s3a.access.key&lt;/name&gt; &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.secret.key&lt;/name&gt; &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;https://lakefs.example.com&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . ",
    "url": "/integrations/presto_trino.html#configuration",
    "relUrl": "/integrations/presto_trino.html#configuration"
  },"258": {
    "doc": "Presto/Trino",
    "title": "Examples",
    "content": "Here are some examples based on examples from the Presto Hive connector examples and Trino Hive connector examples . Example with schema . Create a new schema named main that will store tables in a lakeFS repository named example branch: master: . CREATE SCHEMA main WITH (location = 's3a://example/main') . Create a new Hive table named page_views in the web schema that is stored using the ORC file format, partitioned by date and country, and bucketed by user into 50 buckets (note that Hive requires the partition columns to be the last columns in the table): . CREATE TABLE main.page_views ( view_time timestamp, user_id bigint, page_url varchar, ds date, country varchar ) WITH ( format = 'ORC', partitioned_by = ARRAY['ds', 'country'], bucketed_by = ARRAY['user_id'], bucket_count = 50 ) . Example with External table . Create an external Hive table named request_logs that points at existing data in lakeFS: . CREATE TABLE main.request_logs ( request_time timestamp, url varchar, ip varchar, user_agent varchar ) WITH ( format = 'TEXTFILE', external_location = 's3a://example/main/data/logs/' ) . Example of copying a table with metastore tools: . Copy the created table page_views on schema main to schema example_branch with location s3a://example/example_branch/page_views/ . lakectl metastore copy --from-schema main --from-table page_views --to-branch example_branch . ",
    "url": "/integrations/presto_trino.html#examples",
    "relUrl": "/integrations/presto_trino.html#examples"
  },"259": {
    "doc": "Presto/Trino",
    "title": "Presto/Trino",
    "content": " ",
    "url": "/integrations/presto_trino.html",
    "relUrl": "/integrations/presto_trino.html"
  },"260": {
    "doc": "In Production",
    "title": "In Production",
    "content": "Errors with data in production inevitably occur. When they do, they best thing we can do is remove the erroneous data, understand why the issue happened, and deploy changes that prevent it from occurring again. Example 1: RollBack! - Data ingested from a Kafka stream . If you introduce a new code version to production and discover it has a critical bug, you can simply roll back to the previous version. But you also need to roll back the results of running it. Similar to Git, lakeFS allows you to revert your commits in case they introduced low-quality data. Revert in lakeFS is an atomic action that prevents the data consumers from receiving low quality data until the issue is resolved. As previously mentioned, with lakeFS the recommended branching schema is to ingest data to a dedicated branch. When streaming data, we can decide to merge the incoming data to main at a given time interval or checkpoint, depending on how we chose to write it from Kafka. You can run quality tests for each merge (as discussed in the During Deployment section). Alas, tests are not perfect and we might still introduce low quality data to our main branch at some point. In such a case, we can revert the bad commits from main to the last known high quality commit. This will record new commits reversing the effect of the bad commits. Reverting commits using the CLI . lakectl branch revert lakefs://example-repo/main 20c30c96 ababea32 . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. Example 2: Troubleshoot - Reproduce a bug in production . You upgraded spark and deployed changes in production. A few days or weeks later, you identify a data quality issue, a performance degradation, or an increase to your infra costs. Something that requires investigation and fixing (aka, a bug). lakeFS allows you to open a branch of your lake from the specific merge/commit that introduced the changes to production. Using the metadata saved on the merge/commit you can reproduce all aspects of the environment, then reproduce the issue on the branch and debug it. Meanwhile, you can revert the main to a previous point in time, or keep it as is, depending on the use case . Reading from a historic version (a previous commit) using Spark . // represents the data as existed at commit \"11eef40b\": spark.read.parquet(\"s3://example-repo/11eef40b/events/by-date\") . Example 3: Cross collection consistency . We often need consistency between different data collections. A few examples may be: . | To join different collections in order to create a unified view of an account, a user or another entity we measure. | To introduce the same data in different formats | To introduce the same data with a different leading index or sorting due to performance considerations | . lakeFS will help ensure you introduce only consistent data to your consumers by exposing the new collections and their join in one atomic action to main. Once you consumed the collections on a different branch, and only when both are synchronized, we calculated the join and merged to main. In this example you can see two data sets (Sales data and Marketing data) consumed each to its own independent branch, and after the write of both data sets is completed, they are merged to a different branch (leads branch) where the join ETL runs and creates a joined collection by account. The joined table is then merged to main. The same logic can apply if the data is ingested in streaming, using standard formats, or formats that allow upsert/delete such as Apache Hudi, Delta Lake or Iceberg. ",
    "url": "/usecases/production.html",
    "relUrl": "/usecases/production.html"
  },"261": {
    "doc": "In Production",
    "title": "Case Study: Windward",
    "content": "See how Windward is using lakeFS’ isolation and atomic commits to achieve consistency on top of S3. ",
    "url": "/usecases/production.html#case-study-windward",
    "relUrl": "/usecases/production.html#case-study-windward"
  },"262": {
    "doc": "Protected Branches",
    "title": "Branch Protection Rules",
    "content": "Define branch protection rules to prevent direct changes and commits to specific branches. Only merges are allowed into protected branches. Together with the power of pre-merge hooks, you can run validations on your data before it reaches your important branches and is exposed to consumers. You can create rules for a specific branch, or to any branch that matches a name pattern you specify with glob syntax (supporting ? and * wildcards). ",
    "url": "/reference/protected_branches.html#branch-protection-rules",
    "relUrl": "/reference/protected_branches.html#branch-protection-rules"
  },"263": {
    "doc": "Protected Branches",
    "title": "How it works",
    "content": "When at least one protection rules applies to a branch, the branch is protected. The following operations will fail on protected branches: . | Object write operations: upload and delete objects. | Branch operations: commit and reset uncommitted changes. | . To operate on a protected branch, merge commits from other branches into it. Use pre-merge hooks to validate the changes before they are merged. Reverting a previous commit using lakectl branch revert is allowed on a protected branch. ",
    "url": "/reference/protected_branches.html#how-it-works",
    "relUrl": "/reference/protected_branches.html#how-it-works"
  },"264": {
    "doc": "Protected Branches",
    "title": "Managing branch protection rules",
    "content": "This section explains how to use the lakeFS UI to manage rules. You can also use the command line. Reaching the branch protection rules page . | On lakeFS, navigate to the main page of the repository. | Click on the Settings tab. | In the left menu, click Branches. | . Adding a rule . To add a new rule, click the Add button. In the dialog, enter the branch name pattern and then click Create. Deleting a rule . To delete a rule, click the Delete button next to it. ",
    "url": "/reference/protected_branches.html#managing-branch-protection-rules",
    "relUrl": "/reference/protected_branches.html#managing-branch-protection-rules"
  },"265": {
    "doc": "Protected Branches",
    "title": "Protected Branches",
    "content": " ",
    "url": "/reference/protected_branches.html",
    "relUrl": "/reference/protected_branches.html"
  },"266": {
    "doc": "Python",
    "title": "Calling the lakeFS API from Python",
    "content": "The lakeFS API is OpenAPI 3.0 compliant, allowing the generation of clients from multiple languages or directly accessed by any HTTP client. For Python, this example uses lakeFS’s python package. The lakefs-client pacakge was created by OpenAPI Generator using our OpenAPI definition served by a lakeFS server. ",
    "url": "/integrations/python.html#calling-the-lakefs-api-from-python",
    "relUrl": "/integrations/python.html#calling-the-lakefs-api-from-python"
  },"267": {
    "doc": "Python",
    "title": "Table of contents",
    "content": ". | Install lakeFS Python Client API | Working with the Client API | Using the generated client . | Creating a repository | Creating a branch, uploading files, committing changes | Merging changes from a branch into main | . | Python client documentation | Full API reference | . ",
    "url": "/integrations/python.html#table-of-contents",
    "relUrl": "/integrations/python.html#table-of-contents"
  },"268": {
    "doc": "Python",
    "title": "Install lakeFS Python Client API",
    "content": "Install the Python client using pip: . pip install 'lakefs_client==&lt;lakeFS version&gt;' . The package is available from version &gt;= 0.34.0. ",
    "url": "/integrations/python.html#install-lakefs-python-client-api",
    "relUrl": "/integrations/python.html#install-lakefs-python-client-api"
  },"269": {
    "doc": "Python",
    "title": "Working with the Client API",
    "content": "How to instantiate a client: . import lakefs_client from lakefs_client import models from lakefs_client.client import LakeFSClient # lakeFS credentials and endpoint configuration = lakefs_client.Configuration() configuration.username = 'AKIAIOSFODNN7EXAMPLE' configuration.password = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' configuration.host = 'http://localhost:8000' client = LakeFSClient(configuration) . ",
    "url": "/integrations/python.html#working-with-the-client-api",
    "relUrl": "/integrations/python.html#working-with-the-client-api"
  },"270": {
    "doc": "Python",
    "title": "Using the generated client",
    "content": "Now that we have a client object, we can use it to interact with the API. Creating a repository . repo = models.RepositoryCreation(name='example-repo', storage_namespace='s3://storage-bucket/repos/example-repo', default_branch='main') client.repositories.create_repository(repo) # output: # {'creation_date': 1617532175, # 'default_branch': 'main', # 'id': 'example-repo', # 'storage_namespace': 's3://storage-bucket/repos/example-repo'} . Creating a branch, uploading files, committing changes . List repository branches: . client.branches.list_branches('example-repo') # output: # [{'commit_id': 'cdd673a4c5f42d33acdf3505ecce08e4d839775485990d231507f586ebe97656', 'id': 'main'}] . Create a new branch: . client.branches.create_branch(repository='example-repo', branch_creation=models.BranchCreation(name='experiment-aggregations1', source='main')) # output: # 'cdd673a4c5f42d33acdf3505ecce08e4d839775485990d231507f586ebe97656' . Let’s list again, to see our newly created branch: . client.branches.list_branches('example-repo').results # output: # [{'commit_id': 'cdd673a4c5f42d33acdf3505ecce08e4d839775485990d231507f586ebe97656', 'id': 'experiment-aggregations1'}, {'commit_id': 'cdd673a4c5f42d33acdf3505ecce08e4d839775485990d231507f586ebe97656', 'id': 'main'}] . Great. Now, let’s upload a file into our new branch: . with open('file.csv', 'rb') as f: client.objects.upload_object(repository='example-repo', branch='experiment-aggregations1', path='path/to/file.csv', content=f) # output: # {'checksum': '0d3b39380e2500a0f60fb3c09796fdba', # 'mtime': 1617534834, # 'path': 'path/to/file.csv', # 'path_type': 'object', # 'physical_address': 'local://example-repo/1865650a296c42e28183ad08e9b068a3', # 'size_bytes': 18} . Diffing a single branch will show all uncommitted changes on that branch: . client.branches.diff_branch(repository='example-repo', branch='experiment-aggregations1').results # output: # [{'path': 'path/to/file.csv', 'path_type': 'object', 'type': 'added'}] . As expected, our change appears here. Let’s commit it, and attach some arbitrary metadata: . client.commits.commit( repository='example-repo', branch='experiment-aggregations1', commit_creation=models.CommitCreation(message='Added a CSV file!', metadata={'using': 'python_api'})) # output: # {'committer': 'barak', # 'creation_date': 1617535120, # 'id': 'e80899a5709509c2daf797c69a6118be14733099f5928c14d6b65c9ac2ac841b', # 'message': 'Added a CSV file!', # 'meta_range_id': '', # 'metadata': {'using': 'python_api'}, # 'parents': ['cdd673a4c5f42d33acdf3505ecce08e4d839775485990d231507f586ebe97656']} . Diffing again, this time there should be no uncommitted files: . client.branches.diff_branch(repository='example-repo', branch='experiment-aggregations1').results # output: # [] . Merging changes from a branch into main . Let’s diff between our branch and the main branch: . client.refs.diff_refs(repository='example-repo', left_ref='main', right_ref='experiment-aggregations1').results # output: # [{'path': 'path/to/file.csv', 'path_type': 'object', 'type': 'added'}] . Looks like we have a change. Let’s merge it: . client.refs.merge_into_branch(repository='example-repo', source_ref='experiment-aggregations1', destination_branch='main') # output: # {'reference': 'd0414a3311a8c1cef1ef355d6aca40db72abe545e216648fe853e25db788fa2e', # 'summary': {'added': 1, 'changed': 0, 'conflict': 0, 'removed': 0}} . Let’s diff again - there should be no changes as all changes are on our main branch already: . client.refs.diff_refs(repository='example-repo', left_ref='main', right_ref='experiment-aggregations1').results # output: # [] . ",
    "url": "/integrations/python.html#using-the-generated-client",
    "relUrl": "/integrations/python.html#using-the-generated-client"
  },"271": {
    "doc": "Python",
    "title": "Python client documentation",
    "content": "For the documentation of lakeFS’s python package, see https://pydocs.lakefs.io . ",
    "url": "/integrations/python.html#python-client-documentation",
    "relUrl": "/integrations/python.html#python-client-documentation"
  },"272": {
    "doc": "Python",
    "title": "Full API reference",
    "content": "For a full reference of the lakeFS API, see lakeFS API . ",
    "url": "/integrations/python.html#full-api-reference",
    "relUrl": "/integrations/python.html#full-api-reference"
  },"273": {
    "doc": "Python",
    "title": "Python",
    "content": " ",
    "url": "/integrations/python.html",
    "relUrl": "/integrations/python.html"
  },"274": {
    "doc": "Copying data with Rclone",
    "title": "Copying data with rclone",
    "content": "Rclone is a command line program to sync files and directories between cloud providers. To use it with lakeFS, just create an Rclone remote as describe below, and then use it as you would any other Rclone remote. ",
    "url": "/integrations/rclone.html#copying-data-with-rclone",
    "relUrl": "/integrations/rclone.html#copying-data-with-rclone"
  },"275": {
    "doc": "Copying data with Rclone",
    "title": "Table of contents",
    "content": ". | Creating a remote for lakeFS in Rclone . | Option 1: add an entry in your Rclone configuration file | Option 2: use Rclone interactive config command | . | Examples . | Syncing your data from S3 to lakeFS | Syncing a local directory to lakeFS | . | . ",
    "url": "/integrations/rclone.html#table-of-contents",
    "relUrl": "/integrations/rclone.html#table-of-contents"
  },"276": {
    "doc": "Copying data with Rclone",
    "title": "Creating a remote for lakeFS in Rclone",
    "content": "To add the remote to Rclone, choose one of the following options: . Option 1: add an entry in your Rclone configuration file . | Find the path to your Rclone configuration file and copy it for the next step. rclone config file # output: # Configuration file is stored at: # /home/myuser/.config/rclone/rclone.conf . | If your lakeFS access key is already set in an AWS profile or environment variables, just run the following command, replacing the endpoint property with your lakeFS endpoint: . cat &lt;&lt;EOT &gt;&gt; /home/myuser/.config/rclone/rclone.conf [lakefs] type = s3 provider = AWS endpoint = https://lakefs.example.com no_check_bucket = true EOT . | Otherwise, also include your lakeFS access key pair in the Rclone configuration file: . cat &lt;&lt;EOT &gt;&gt; /home/myuser/.config/rclone/rclone.conf [lakefs] type = s3 provider = AWS env_auth = false access_key_id = AKIAIOSFODNN7EXAMPLE secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY endpoint = https://lakefs.example.com no_check_bucket = true EOT . | . Option 2: use Rclone interactive config command . Run this command and follow the instructions: . rclone config . Choose AWS S3 as your type of storage, and enter your lakeFS endpoint as your S3 endpoint. You will have to choose whether you use your environment for authentication (recommended), or to enter the lakeFS access key pair into the Rclone configuration. Select “Edit advanced config” and accept defaults for all values except no_check_bucket: . If set, don't attempt to check the bucket exists or create it This can be useful when trying to minimise the number of transactions rclone does if you know the bucket exists already. It can also be needed if the user you are using does not have bucket creation permissions. Before v1.52.0 this would have passed silently due to a bug. Enter a boolean value (true or false). Press Enter for the default (\"false\"). no_check_bucket&gt; yes . ",
    "url": "/integrations/rclone.html#creating-a-remote-for-lakefs-in-rclone",
    "relUrl": "/integrations/rclone.html#creating-a-remote-for-lakefs-in-rclone"
  },"277": {
    "doc": "Copying data with Rclone",
    "title": "Examples",
    "content": "Syncing your data from S3 to lakeFS . rclone sync mys3remote://mybucket/path/ lakefs:example-repo/main/path . Syncing a local directory to lakeFS . rclone sync /home/myuser/path/ lakefs:example-repo/main/path . ",
    "url": "/integrations/rclone.html#examples",
    "relUrl": "/integrations/rclone.html#examples"
  },"278": {
    "doc": "Copying data with Rclone",
    "title": "Copying data with Rclone",
    "content": " ",
    "url": "/integrations/rclone.html",
    "relUrl": "/integrations/rclone.html"
  },"279": {
    "doc": "Create a Repository",
    "title": "Create a Repository",
    "content": "A repository contains all of your objects, including the revision history. It can be considered the lakeFS analog of a bucket in an object store. Since it has version control qualities, it is also analogous to a repository in Git. ",
    "url": "/quickstart/repository.html",
    "relUrl": "/quickstart/repository.html"
  },"280": {
    "doc": "Create a Repository",
    "title": "Create the first user",
    "content": "When you first open the lakeFS UI, you will be asked to create an initial admin user. | Open http://127.0.0.1:8000/ in your web browser. | Follow the steps to create an initial administrator user. Save the credentials you’ve received somewhere safe, you won’t be able to see them again! . | Follow the link and go to the login screen: . | . ",
    "url": "/quickstart/repository.html#create-the-first-user",
    "relUrl": "/quickstart/repository.html#create-the-first-user"
  },"281": {
    "doc": "Create a Repository",
    "title": "Create the repository",
    "content": ". | Use the credentials from the previous step to log in as an administrator. | Click Create Repository. | Fill in a repository name. | Under Storage Namespace, enter local://. In this tutorial, the underlying storage for lakeFS is the local disk. Accordingly, the value for Storage Namespace should simply be local://. For a deployment that uses an object store as the underlying storage, this would be a location in the store, e.g. s3://example-bucket/prefix. | Click Create Repository. | . Next steps . You just created your first lakeFS repository! You can now add some data to it. ",
    "url": "/quickstart/repository.html#create-the-repository",
    "relUrl": "/quickstart/repository.html#create-the-repository"
  },"282": {
    "doc": "Roadmap",
    "title": "Roadmap",
    "content": " ",
    "url": "/understand/roadmap.html",
    "relUrl": "/understand/roadmap.html"
  },"283": {
    "doc": "Roadmap",
    "title": "Table of contents",
    "content": ". | Ecosystem . | Snowflake Support: External tables on lakeFS Requires Discussion | Pluggable diff/merge operators . | Delta Lake merges and diffs across branches | Iceberg merges and diffs across branches High Priority | . | Native Spark OutputCommitter | Native connector: Trino | Improved streaming support for Apache Kafka | . | Versioning Capabilities . | Git-lakeFS integration | Support asyncronous hooks High Priority | Partial Commits | Rebase | Support Garbage Collection on Azure High Priority | . | Architecture . | Decouple ref-store from PostgreSQL High Priority | Ref-store implementation for DynamoDB | Ref-store implementation for RocksDB (for testing and experimentation) | . | . ",
    "url": "/understand/roadmap.html#table-of-contents",
    "relUrl": "/understand/roadmap.html#table-of-contents"
  },"284": {
    "doc": "Roadmap",
    "title": "Ecosystem",
    "content": "Snowflake Support: External tables on lakeFS Requires Discussion . Since Snowflake supports reading external tables from an object store, we’d like to extend this support to work with lakeFS repositories that are hosted on top of supported object stores. This could be done by utilizing Snowflake’s support for SymlinkInputFormat similar to how Delta Lake support is implemented, and later on by having a native integration with Snowflake itself. If you’d like to hear more: . Contact us, we’d love to talk about it! . Pluggable diff/merge operators . Currently, lakeFS supports merging and comparing references by doing an object-wise comparison. For unstructured data and some forms of tabluar data (namely Hive structured tables), this works fine. However, in some cases, simply creating a union of object modifications from both references isn’t good enough. Modern table formats such as Delta Lake, Hudi and Iceberg rely on a set of manifest or log files that describe the logical structure of the table. In those cases, a merge operation might have to be aware of the structure of the data: generate a new manifest or re-order the log in order for the output to make sense. Additionally, the definition of a conflict is also a bit different: simply looking at object names to determine whether or not a conflict occured might not be good enough. With that in mind, we plan on making the diff and merge operations pluggable. lakeFS already supports injecting custom behavior using hooks. Ideally, we can support this by introducing on-diff and on-merge hooks that allow implementing hooks in different languages, possibly utilizing existing code and libraries to aid with understanding these formats. Once implemented we could support: . Delta Lake merges and diffs across branches . Delta lake stores metadata files that represent a logical transaction log that relies on numerical ordering. Currently, when trying to modify a Delta table from 2 different branches, lakeFS would correctly recognize a conflict: this log diverged into 2 different copies, representing different changes. Users would then have to forgo one of the change sets, by either retaining the destination’s branch set of changes, or the source’s branch. A much better user experience would be to allow merging this log into a new unified set of changes, representing changes made in both branches as a new set of log files (and potentially, data files too!). Track and discuss on GitHub . Iceberg merges and diffs across branches High Priority . Iceberg stores metadata files (“manifests”) that represent a snapshot of a given Iceberg table. Currently, when trying to modify an Iceberg table from 2 different branches, lakeFS would not be able to create a logical snapshot that consists of the changes applied in both references. Users would then have to forgo one of the change sets, by either retaining the destination’s branch set of changes, or the source’s branch. A much better user experience would be to allow merging snapshots into a new unified set of changes, representing changes made in both refs as a new snapshot. Track and discuss on GitHub . Native Spark OutputCommitter . Add a Hadoop OutputCommitter that uses existing lakeFS operations for atomic commits that are efficient and safely concurrent. This has several benefits: . | Performance: This committer does metadata operations only, and doesn’t rely on copying data | Atomicity: A commit in lakeFS is guaranteed to either succeed or fail, but will not leave any intermediate state on failure. | Traceability: Attaching metadata to each commit means we get quite a lot of information on where data is coming from, how it’s generated, etc. This allows building reproducible pipelines in an easier way. | Resilience: Since every Spark write is a commit, it is also undoable by reverting it. | . Track and discuss on GitHub . Native connector: Trino . Currently, the Trino integration works well using the lakeFS s3 Gateway. While easy to integrate and useful out of the box, due to the S3 protocol, it means that the data itself must pass through the lakeFS server. For larger installations, a native integration where lakeFS handles metadata, returning locations in the underlying object store that Trino can then access directly, would allow reducing the operational overhead and increasing the scalability of lakeFS. This would be done in a similar way to the Native Spark integration using the Hadoop Filesystem implementation. Track and discuss on GitHub . Improved streaming support for Apache Kafka . Committing (along with attaching useful information to the commit) makes a lot of sense for batch workloads: . | run a job or a pipeline on a separate branch and commit | record information such as the git hash of the code executed, the versions of frameworks used, and information about the data artifacts | once the pipeline has completed successfully, commit and attach the recorded information as metadata | . For streaming however, this is currently less clear: There’s no obvious point in time to commit, as things never actually “finish successfully”. The recommended pattern would be to ingest from a stream on a separate branch, periodically committing - storing not only the data added since last commit but also capturing the offset read from the stream, for reproducibility. These commits can then be merged into a main branch given they pass all relevant quality checks and other validations using hooks, exposing consumers to validated, clean data. In practice, implementing such a workflow is a little challenging. Users need to: . | Orchestrate the commits and merge operations. | figure out how to attach the correct offset read from the stream broker | Handle writes coming in while the commit is taking place | . Ideally, lakeFS should provide tools to automate this, with native support for Apache Kafka. Track and discuss on GitHub . ",
    "url": "/understand/roadmap.html#ecosystem",
    "relUrl": "/understand/roadmap.html#ecosystem"
  },"285": {
    "doc": "Roadmap",
    "title": "Versioning Capabilities",
    "content": "Git-lakeFS integration . The ability to connect Git commits with lakeFS commits. Especially useful for reproducibility: By looking at a set of changes to the data, be able to reference (or ever run) the job that produced it. Track and discuss on GitHub . Support asyncronous hooks High Priority . support running hooks that might possibly take many minutes to complete. This is useful for things such as data quality checks - where we might want to do big queries or scans to ensure the data being merged adheres to certain business rules. Currently, pre-commit and pre-merge hooks in lakeFS are tied to the lifecycle of the API request that triggers the said commit or merge operation. In order to support long running hooks, there are enhancements to make to lakeFS APIs in order to support an asynchronous commit and merge operations that are no longer tied to the HTTP request that triggered them. Partial Commits . In some cases, lakeFS users might want to commit only a set of staged objects instead of all modifications made to a branch This is especially useful when experimenting with multiple data sources, but we only care about an output as opposed to intermediate state. Track and discuss on GitHub . Rebase . Since some users are interested in achieving parity between their Git workflow and their lakeFS workflow, extending the merge behavior to support history rewriting in which the changes that occured in the source branch are then reapplied to the HEAD of the destination branch (aka “rebase”) is desired. Track and discuss on GitHub . Support Garbage Collection on Azure High Priority . The lakeFS Garbage Collection capability hard-deletes objects deleted from branches, helping users reduce costs and comply with data privacy policies. Currently, lakeFS only supports Garbage Collection of S3 objects managed by lakeFS. Extending the support to Azure will allow lakeFS users that use Azure as their underlying storage to use this feature. Track and discuss on GitHub . ",
    "url": "/understand/roadmap.html#versioning-capabilities",
    "relUrl": "/understand/roadmap.html#versioning-capabilities"
  },"286": {
    "doc": "Roadmap",
    "title": "Architecture",
    "content": "Decouple ref-store from PostgreSQL High Priority . Currently lakeFS requires a PostgreSQL database. Internally, it is used to store references (branches, tags, etc), uncommitted objects metadata and other metadata such as user management. Making this store a pluggable component would allow the following: . | Simpler quickstart using only an object store: allow running lakeFS without any dependencies. This ref-store will use the underlying object store to also store the references. For S3 (or any object store that doesn’t support any native transaction/compare-and-swap semantics) this will be available only when running in single-instance mode. This is still beneficial for running lakeFS in POC or development mode, removing the need to run and connect multiple Docker containers. | Flexible production setup: A PostgreSQL option will still be available, but additional implementations will also be possible: Using other RDBMS types such as MySQL &amp;emdash; or using managed services such as DynamoDB that lakeFS will be able to manage itself | Easier scalability: Scaling RDBMS for very high throughput while keeping it predictable in performance for different loads and access patterns has a very high operational cost. | . This release will mark the completion of project “lakeFS on the Rocks” . Track and discuss on GitHub . Ref-store implementation for DynamoDB . Once we’ve decoupled the ref-store from PostgreSQL, we’d like to create a ref-store implementation that supports DynamoDB. This has several advantages for users looking to run lakeFS on AWS: . | DynamoDB is fast to provision and requires very little configuration | The operational overhead of maintaining a serverless database is very small | Scaling according to usage is much more fine grained, which eliminates a lot of the cost for smaller installations (as opposed to RDS) | . Track and discuss on GitHub . Ref-store implementation for RocksDB (for testing and experimentation) . Once we’ve decoupled the ref-store from PostgreSQL, we’d like to create a ref-store implementation that supports running with an embedded RocksDB database. While not fit for real world production use, it makes it much easier to try lakeFS when running locally, either by directly executing the binary, or by doing a single docker run with the right configuration (as opposed to having to use docker-compose or run PostgreSQL locally). Track and discuss on GitHub . ",
    "url": "/understand/roadmap.html#architecture",
    "relUrl": "/understand/roadmap.html#architecture"
  },"287": {
    "doc": "AWS S3",
    "title": "Prepare Your AWS S3 Bucket",
    "content": ". | From the S3 Administration console, choose Create Bucket. | Make sure you: . | Block public access | Disable Object Locking | . | lakeFS requires permissions to interact with your bucket. Following is a minimal bucket policy. To add it, go to the Permissions tab, and paste it as : . { \"Id\": \"Policy1590051531320\", \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Stmt1590051522178\", \"Action\": [ \"s3:GetObject\", \"s3:GetObjectVersion\", \"s3:PutObject\", \"s3:AbortMultipartUpload\", \"s3:ListMultipartUploadParts\", \"s3:GetBucketVersioning\", \"s3:ListBucket\", \"s3:GetBucketLocation\", \"s3:ListBucketMultipartUploads\", \"s3:ListBucketVersions\" ], \"Effect\": \"Allow\", \"Resource\": [\"arn:aws:s3:::&lt;BUCKET_NAME&gt;\", \"arn:aws:s3:::&lt;BUCKET_NAME_WITH_PATH_PREFIX&gt;/*\"], \"Principal\": { \"AWS\": [\"arn:aws:iam::&lt;ACCOUNT_ID&gt;:role/&lt;IAM_ROLE&gt;\"] } } ] } . Replace &lt;ACCOUNT_ID&gt;, &lt;BUCKET_NAME&gt; and &lt;IAM_ROLE&gt; with values relevant to your environment. IAM_ROLE should be the role assumed by your lakeFS installation. Alternatively, if you use an AWS user’s key-pair to authenticate lakeFS to AWS, change the policy’s Principal to be the user: . \"Principal\": { \"AWS\": [\"arn:aws:iam::&lt;ACCOUNT_ID&gt;:user/&lt;IAM_USER&gt;\"] } . | . You are now ready to create your first lakeFS repository. ",
    "url": "/setup/storage/s3.html#prepare-your-aws-s3-bucket",
    "relUrl": "/setup/storage/s3.html#prepare-your-aws-s3-bucket"
  },"288": {
    "doc": "AWS S3",
    "title": "AWS S3",
    "content": " ",
    "url": "/setup/storage/s3.html",
    "relUrl": "/setup/storage/s3.html"
  },"289": {
    "doc": "S3 Supported API",
    "title": "S3 Supported API",
    "content": "The S3 Gateway emulates a subset of the API exposed by S3. This subset includes all API endpoints relevant to data systems. for more information, see architecture . lakeFS supports the following API operations: . | Identity and authorization . | SIGv2 | SIGv4 | . | Bucket operations: . | HEAD bucket | . | Object operations: . | DeleteObject | DeleteObjects | GetObject . | Support for caching headers, ETag | Support for range requests | No support for SSE | No support for SelectObject operations | . | HeadObject | PutObject . | Support multi-part uploads | No support for storage classes | No object level tagging | . | CopyObject | . | Object Listing: . | ListObjects | ListObjectsV2 | Delimiter support (for \"/\" only) | . | Multipart Uploads: . | AbortMultipartUpload | CompleteMultipartUpload | CreateMultipartUpload | ListParts | Upload Part | UploadPartCopy | . | . ",
    "url": "/reference/s3.html",
    "relUrl": "/reference/s3.html"
  },"290": {
    "doc": "SageMaker",
    "title": "Using lakeFS with SageMaker",
    "content": "Amazon SageMaker helps prepare, build, train and deploy ML models quickly by bringing together a broad set of capabilities purpose-built for ML. ",
    "url": "/integrations/sagemaker.html#using-lakefs-with-sagemaker",
    "relUrl": "/integrations/sagemaker.html#using-lakefs-with-sagemaker"
  },"291": {
    "doc": "SageMaker",
    "title": "Table of contents",
    "content": ". | Initializing session and client | Usage Examples . | Upload train and test data | Download objects | . | . ",
    "url": "/integrations/sagemaker.html#table-of-contents",
    "relUrl": "/integrations/sagemaker.html#table-of-contents"
  },"292": {
    "doc": "SageMaker",
    "title": "Initializing session and client",
    "content": "Initialize a Sagemaker session and an s3 client with lakeFS as the endpoint: . import sagemaker import boto3 endpoint_url = '&lt;LAKEFS_ENDPOINT&gt;' aws_access_key_id = '&lt;LAKEFS_ACCESS_KEY_ID&gt;' aws_secret_access_key = '&lt;LAKEFS_SECRET_ACCESS_KEY&gt;' repo = 'example-repo' sm = boto3.client('sagemaker', endpoint_url=endpoint_url, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key) s3_resource = boto3.resource('s3', endpoint_url=endpoint_url, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key) session = sagemaker.Session(boto3.Session(), sagemaker_client=sm, default_bucket=repo) session.s3_resource = s3_resource . ",
    "url": "/integrations/sagemaker.html#initializing-session-and-client",
    "relUrl": "/integrations/sagemaker.html#initializing-session-and-client"
  },"293": {
    "doc": "SageMaker",
    "title": "Usage Examples",
    "content": "Upload train and test data . Let’s use the created session for uploading data to the ‘main’ branch: . prefix = \"/prefix-within-branch\" branch = 'main' train_file = 'train_data.csv'; train_data.to_csv(train_file, index=False, header=True) train_data_s3_path = session.upload_data(path=train_file, key_prefix=branch + prefix + \"/train\") test_file = 'test_data.csv'; test_data_no_target.to_csv(test_file, index=False, header=False) test_data_s3_path = session.upload_data(path=test_file, key_prefix=branch + prefix + \"/test\") . Download objects . We can use the integration with lakeFS to download a portion of the data we see fit: . repo = 'example-repo' prefix = \"/prefix-to-download\" branch = 'main' localpath = './' + branch session.download_data(path=localpath, bucket=repo, key_prefix = branch + prefix) . Note: Advanced AWS SageMaker features, like Autopilot jobs, are encapsulated and don’t have the option to override the S3 endpoint. However, it is possible to export the required inputs from lakeFS to S3. If you’re using SageMaker features that aren’t supported by lakeFS, we’d love to hear from you. ",
    "url": "/integrations/sagemaker.html#usage-examples",
    "relUrl": "/integrations/sagemaker.html#usage-examples"
  },"294": {
    "doc": "SageMaker",
    "title": "SageMaker",
    "content": " ",
    "url": "/integrations/sagemaker.html",
    "relUrl": "/integrations/sagemaker.html"
  },"295": {
    "doc": "Sizing Guide",
    "title": "Sizing guide",
    "content": " ",
    "url": "/understand/sizing-guide.html#sizing-guide",
    "relUrl": "/understand/sizing-guide.html#sizing-guide"
  },"296": {
    "doc": "Sizing Guide",
    "title": "Table of contents",
    "content": ". | System Requirements . | Operating Systems and ISA | Memory and CPU requirements | Network | Disk | PostgreSQL database . | Storage | RAM and shared_buffers | CPU | . | . | Scaling factors . | Understanding latency and throughput considerations | . | Benchmarks . | Random reads | Random Writes | Branch creation | . | Important metrics | Reference architectures . | Reference Architecture: Data Science/Research environment | Reference Architecture: Automated Production Pipelines | . | . ",
    "url": "/understand/sizing-guide.html#table-of-contents",
    "relUrl": "/understand/sizing-guide.html#table-of-contents"
  },"297": {
    "doc": "Sizing Guide",
    "title": "System Requirements",
    "content": "Operating Systems and ISA . lakeFS can run on MacOS and Linux (Windows binaries are available but not rigorously tested - we don’t recommend deploying lakeFS to production on Windows). x86_64 and arm64 architectures are supported for both MacOS and Linux. Memory and CPU requirements . lakeFS servers require a minimum of 512mb of RAM and 1 CPU core. For high throughput, additional CPUs help scale requests across different cores. “Expensive” operations such as large diff or commit operations can take advantage of multiple cores. Network . If using the data APIs such as the S3 Gateway, lakeFS will require enough network bandwidth to support the planned concurrent network upload/download operations. For most cloud providers, more powerful machines (i.e. more expensive and usually with more CPU cores) also provide increased network bandwidth. If using only the metadata APIs (for example, only using the Hadoop/Spark clients), network bandwidth is minimal, at roughly 1Kb per request. Disk . lakeFS greatly benefits from fast local disks. A lakeFS instance doesn’t require any strong durability guarantees from the underlying storage, as the disk is only ever used as a local caching layer for lakeFS metadata, and not for long-term storage. lakeFS is designed to work with ephemeral disks - these are usually based on NVMe and are tied to the machine’s lifecycle. Using ephemeral disks lakeFS can provide a very high throughput/cost ratio, probably the best that could be achieved on a public cloud, so we recommend those. A local cache of at least 512 MiB should be provided. For large installations (managing &gt;100 concurrently active branches, with &gt;100M objects per commit), we recommend allocating at least 10 GiB - since it’s a caching layer over a relatively slow storage (the object store), see Important metrics below to understand how to size this: it should be big enough to hold all commit metadata for actively referenced commits. PostgreSQL database . lakeFS uses a PostgreSQL instance to manage branch references, authentication and authorization information and to keep track of currently uncommitted data across branches. Storage . The dataset stored in PostgreSQL is relatively modest, as most metadata is pushed down into the object store. Required storage is mostly a factor of the amount of uncommitted writes across all branches at any given point in time: in the range of 150 MiB per every 100,000 uncommitted writes. We recommend starting at 10 GiB for a production deployment, as it will likely be more than enough. RAM and shared_buffers . Since the data size is small, it is recommended to provide enough memory to hold the vast majority of that data in RAM: Ideally configure shared_buffers of your PostgreSQL instances to be large enough to contain the currently active dataset. Pick a database instance with enough RAM to accommodate this buffer size, at roughly x4 the size given for shared_buffers (so for example, if an installation has ~500,000 uncommitted writes at any given time, it would require about 750 MiB of shared_buffers, that would require about 3 GiB of RAM). Cloud providers will save you the need to tune this parameter. It will be set to a fixed percentage the chosen instance’s available RAM (25% on AWS RDS, 30% on Google Cloud SQL). CPU . PostgreSQL CPU cores help scale concurrent requests. 1 CPU core for every 5,000 requests/second is ideal. ",
    "url": "/understand/sizing-guide.html#system-requirements",
    "relUrl": "/understand/sizing-guide.html#system-requirements"
  },"298": {
    "doc": "Sizing Guide",
    "title": "Scaling factors",
    "content": "Scaling lakeFS, like most data systems, moves across 2 axes: throughput of requests (amount per given timeframe), and latency (time to complete a single request). Understanding latency and throughput considerations . Most lakeFS operations are designed to be very low in latency. Assuming a well tuned local disk cache (see Storage above), most critical path operations (writing objects, requesting objects, deleting objects) are designed to complete in &lt;25ms at p90. Listing objects obviously requires accessing more data, but should always be on-par with what the underlying object store can provide, and in most cases, it is actually faster. At the worst case for directory listing with 1,000 common prefixes returned, expect a latency of 75ms at p90. Managing branches (creating them, listing them and deleting them) are all constant-time operations, generally taking &lt;30ms at p90. Committing and merging can take longer, as they are proportional to the amount of changes introduced. This is what makes lakeFS optimal for large Data Lakes - the amount of changes introduced per commit usually stays relatively stable, while the entire data set usually grows over time. This means lakeFS will provide predictable performance: committing 100 changes will take roughly the same amount of time whether the resulting commit contains 500 or 500 million objects. See Data Model for more information. Scaling throughput depends very much on the amount of CPU cores available to lakeFS. In many cases it is easier to scale lakeFS across a fleet of smaller cloud instances (or containers) than it is to scale up with machines that have many cores, and in fact, lakeFS works well in both cases. Most critical path operations scale very well across machines. ",
    "url": "/understand/sizing-guide.html#scaling-factors",
    "relUrl": "/understand/sizing-guide.html#scaling-factors"
  },"299": {
    "doc": "Sizing Guide",
    "title": "Benchmarks",
    "content": "All benchmarks below were measured using 2 x c5ad.4xlarge instances on AWS us-east-1. Similar results can be achieved on Google Cloud using a c2-standard-16 machine type, with an attached local SSD. On Azure, a Standard_F16s_v2 virtual machine can be used. The PostgreSQL instance that was used is a db.m6g.2xlarge (8 vCPUs, 32 GB RAM). Equivalent machines on Google Cloud or Azure should yield similar results. The example repository we tested against contains the metadata of a large lakeFS installation, where each commit contains ~180,000,000 objects (representing ~7.5 Petabytes of data). All tests are reproducible using the lakectl abuse command, so do use it to properly size and tune your setup. All tests are accompanied by the relevant lakectl abuse command that generated them. Random reads . This test generates random read requests to lakeFS, in a given commit. Paths are requested randomly from a file containing a set of preconfigured (and existing) paths. command executed: . lakectl abuse random-read \\ --from-file randomly_selected_paths.txt \\ --amount 500000 \\ --parallelism 128 \\ lakefs://example-repo/&lt;commit hash&gt; . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and commit hash. Result Histogram (raw): . Histogram (ms): 1 0 2 0 5 37945 7 179727 10 296964 15 399682 25 477502 50 499625 75 499998 100 499998 250 500000 350 500000 500 500000 750 500000 1000 500000 5000 500000 min 3 max 222 total 500000 . So 50% of all requests took &lt;10ms, while 99.9% of them took &lt;50ms . throughput: . Average throughput during the experiment was 10851.69 requests/second . Random Writes . This test generates random write requests to a given lakeFS branch. All paths are pre-generated and do not overwrite each other (as overwrites are relatively rare in a Data Lake setup). command executed: . lakectl abuse random-write \\ --amount 500000 \\ --parallelism 64 \\ lakefs://example-repo/main . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. Result Histogram (raw): . Histogram (ms): 1 0 2 0 5 30715 7 219647 10 455807 15 498144 25 499535 50 499742 75 499784 100 499802 250 500000 350 500000 500 500000 750 500000 1000 500000 5000 500000 min 3 max 233 total 500000 . So 50% of all requests took &lt;10ms, while 99.9% of them took &lt;25ms . throughput: . Average throughput during the experiment was 7595.46 requests/second . Branch creation . This test creates branches from a given reference . command executed: . lakectl abuse create-branches \\ --amount 500000 \\ --branch-prefix \"benchmark-\" \\ --parallelism 256 \\ lakefs://example-repo/&lt;commit hash&gt; . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and commit hash. Result Histogram (raw): . Histogram (ms): 1 0 2 1 5 5901 7 39835 10 135863 15 270201 25 399895 50 484932 75 497180 100 499303 250 499996 350 500000 500 500000 750 500000 1000 500000 5000 500000 min 2 max 304 total 500000 . So 50% of all requests took &lt;15ms, while 99.9% of them took &lt;100ms . throughput: . Average throughput during the experiment was 7069.03 requests/second . ",
    "url": "/understand/sizing-guide.html#benchmarks",
    "relUrl": "/understand/sizing-guide.html#benchmarks"
  },"300": {
    "doc": "Sizing Guide",
    "title": "Important metrics",
    "content": "lakeFS exposes metrics using the Prometheus protocol. Every lakeFS instance exposes a /metrics endpoint that could be used to extract them. Here are a few notable metrics to keep track of when sizing lakeFS: . api_requests_total - Tracks throughput of API requests over time . api_request_duration_seconds - Histogram of latency per operation type . gateway_request_duration_seconds - Histogram of latency per S3 Gateway operation . go_sql_stats_* - Important client-side metrics collected from the PostgreSQL driver. See The full reference here. ",
    "url": "/understand/sizing-guide.html#important-metrics",
    "relUrl": "/understand/sizing-guide.html#important-metrics"
  },"301": {
    "doc": "Sizing Guide",
    "title": "Reference architectures",
    "content": "Below are a few example architectures for lakeFS deployment . Reference Architecture: Data Science/Research environment . Use case: Manage Machine learning or algorithms development. Use lakeFS branches to achieve both isolation and reproducibility of experiments. Data being managed by lakeFS is both structured, tabular data; as well as unstructured sensor and image data used for training. Assuming a team of 20-50 researchers, with a dataset size of 500 TiB across 20M objects. Environment: lakeFS will be deployed on Kubernetes managed by AWS EKS with PostgreSQL on AWS RDS Aurora . Sizing: Since most of the work is done by humans (vs automated pipelines), most experiments tend to be small in scale, reading and writing 10s to 1000s of objects. The expected amount of branches active in parallel is relatively low, around 1-2 per user, each representing a small amount of uncommitted changes at any given point in time. Let’s assume 5,000 uncommitted writes per branch = ~500k. To support the expected throughput, a single moderate lakeFS instance should be more than enough, since requests per second would be on the order of 10s to 100s. For high availability, we’ll deploy 2 pods with 1 CPU core and 1 GiB of RAM each.. Since the PostgreSQL instance is expected to hold a very small dataset (at 500k, expected dataset size is 150MiB (for 100k records) * 5 = 750MiB). To ensure we have enough RAM to hold this, we’ll need 3 GiB of RAM, so a very moderate Aurora instance db.t3.large (2 vCPUs, 8 GB RAM) will be more than enough. An equivalent database instance on GCP or Azure should give similar results. Reference Architecture: Automated Production Pipelines . Use case: Manage multiple concurrent data pipelines using Apache Spark and Airflow. Airflow DAGs start by creating a branch for isolation and for CI/CD. Data being managed by lakeFS is structured, tabular data. Total dataset size is 10 PiB, spanning across 500M objects. Expected throughput is 10k reads/second + 2k writes per second across 100 concurrent branches. Environment: lakeFS will be deployed on Kubernetes managed by AWS EKS with PostgreSQL on AWS RDS . Sizing: Data pipelines tend to be bursty in nature: reading in a lot of objects concurrently, doing some calculation or aggregation and then writing many objects concurrently. The expected amount of branches active in parallel is high, with many Airflow DAGs running per day, each representing a moderate amount of uncommitted changes at any given point in time. Let’s assume 1,000 uncommitted writes/branch * 2500 branches = ~2.5M records. To support the expected throughput, looking the benchmarking numbers above, we’re doing roughly 625 requests/core, so 24 cores should cover our peak traffic. We can deploy 6 * 4 CPU core pods. The PostgreSQL instance (at 500k, expected dataset size is 150MiB (for 100k records) * 25 = 3750 MiB). To ensure we have enough RAM to hold this, we’ll need at least 15 GiB of RAM, so we’ll go with a db.r5.xlarge (4 vCPUs, 32GB RAM) Aurora instance. An equivalent database instance on GCP or Azure should give similar results. ",
    "url": "/understand/sizing-guide.html#reference-architectures",
    "relUrl": "/understand/sizing-guide.html#reference-architectures"
  },"302": {
    "doc": "Sizing Guide",
    "title": "Sizing Guide",
    "content": " ",
    "url": "/understand/sizing-guide.html",
    "relUrl": "/understand/sizing-guide.html"
  },"303": {
    "doc": "Spark Client",
    "title": "lakeFS Spark Client",
    "content": "Utilize the power of Spark to interact with the metadata on lakeFS. Possible use-cases include: . | Create a DataFrame for listing the objects in a specific commit or branch. | Compute changes between two commits. | Export your data for consumption outside lakeFS. | Bulk operations on underlying storage. | . ",
    "url": "/reference/spark-client.html#lakefs-spark-client",
    "relUrl": "/reference/spark-client.html#lakefs-spark-client"
  },"304": {
    "doc": "Spark Client",
    "title": "Getting Started",
    "content": "Start Spark Shell / PySpark with the --packages flag: . | Spark 2.x | Spark 3.x | Spark 3.x on Hadoop 3.x | . This client is compiled for Spark 2.4.7 and tested with it, but can work for higher versions. spark-shell --packages io.lakefs:lakefs-spark-client-247_2.11:0.1.7 . Alternatively an assembled jar is available on S3, at s3://treeverse-clients-us-east/lakefs-spark-client-247/0.1.7/lakefs-spark-client-247-assembly-0.1.7.jar . This client is compiled for Spark 3.0.1 with Hadoop 2 and tested with it, but can work for higher versions. spark-shell --packages io.lakefs:lakefs-spark-client-301_2.12:0.1.7 . Alternatively an assembled jar is available on S3, at s3://treeverse-clients-us-east/lakefs-spark-client-301/0.1.7/lakefs-spark-client-301-assembly-0.1.7.jar . This client is compiled for Spark 3.1.2 with Hadoop 3.2.1, but can work for other Spark versions and higher Hadoop versions. spark-shell --packages io.lakefs:lakefs-spark-client-312-hadoop3-assembly_2.12:0.1.7 . Alternatively an assembled jar is available on S3, at s3://treeverse-clients-us-east/lakefs-spark-client-312-hadoop3/0.1.7/lakefs-spark-client-312-hadoop3-assembly-0.1.7.jar . ",
    "url": "/reference/spark-client.html#getting-started",
    "relUrl": "/reference/spark-client.html#getting-started"
  },"305": {
    "doc": "Spark Client",
    "title": "Configuration",
    "content": ". | To read metadata from lakeFS, the client should be configured with your lakeFS endpoint and credentials, using the following Hadoop configurations: . | Configuration | Description | . | spark.hadoop.lakefs.api.url | lakeFS API endpoint, e.g: http://lakefs.example.com/api/v1 | . | spark.hadoop.lakefs.api.access_key | The access key to use for fetching metadata from lakeFS | . | spark.hadoop.lakefs.api.secret_key | Corresponding lakeFS secret key | . | The client will also directly interact with your storage using Hadoop FileSystem. Therefore, your Spark session must be able to access the underlying storage of your lakeFS repository. There are various ways to do this, but for a non-production environment you can use the following Hadoop configurations: . | Configuration | Description | . | spark.hadoop.fs.s3a.access.key | Access key to use for accessing underlying storage on S3 | . | spark.hadoop.fs.s3a.secret.key | Corresponding secret key to use with S3 access key | . Assuming role on S3 (Hadoop 3 only) . The client includes support for assuming a separate role on S3 when running on Hadoop 3. It uses the same configuration used by S3AFileSystem to assume the role on S3A. Apache Hadoop AWS documentation has details under “Working with IAM Assumed Roles”. You will need to use the following Hadoop configurations: . | Configuration | Description | . | fs.s3a.aws.credentials.provider | Set to org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider | . | fs.s3a.assumed.role.arn | Set to the ARN of the role to assume | . | . ",
    "url": "/reference/spark-client.html#configuration",
    "relUrl": "/reference/spark-client.html#configuration"
  },"306": {
    "doc": "Spark Client",
    "title": "Examples",
    "content": ". | Get a DataFrame for listing all objects in a commit: . import io.treeverse.clients.LakeFSContext val commitID = \"a1b2c3d4\" val df = LakeFSContext.newDF(spark, \"example-repo\", commitID) df.show /* output example: +------------+--------------------+--------------------+-------------------+----+ | key | address| etag| last_modified|size| +------------+--------------------+--------------------+-------------------+----+ | file_1 |791457df80a0465a8...|7b90878a7c9be5a27...|2021-03-05 11:23:30| 36| file_2 |e15be8f6e2a74c329...|95bee987e9504e2c3...|2021-03-05 11:45:25| 36| file_3 |f6089c25029240578...|32e2f296cb3867d57...|2021-03-07 13:43:19| 36| file_4 |bef38ef97883445c8...|e920efe2bc220ffbb...|2021-03-07 13:43:11| 13| +------------+--------------------+--------------------+-------------------+----+ */ . | Run SQL queries on your metadata: . df.createOrReplaceTempView(\"files\") spark.sql(\"SELECT DATE(last_modified), COUNT(*) FROM files GROUP BY 1 ORDER BY 1\") /* output example: +----------+--------+ | dt|count(1)| +----------+--------+ |2021-03-05| 2|2021-03-07| 2| +----------+--------+ */ . | . ",
    "url": "/reference/spark-client.html#examples",
    "relUrl": "/reference/spark-client.html#examples"
  },"307": {
    "doc": "Spark Client",
    "title": "Spark Client",
    "content": " ",
    "url": "/reference/spark-client.html",
    "relUrl": "/reference/spark-client.html"
  },"308": {
    "doc": "Spark",
    "title": "Using lakeFS with Spark",
    "content": "Apache Spark is a unified analytics engine for big data processing, with built-in modules for streaming, SQL, machine learning and graph processing. ",
    "url": "/integrations/spark.html#using-lakefs-with-spark",
    "relUrl": "/integrations/spark.html#using-lakefs-with-spark"
  },"309": {
    "doc": "Spark",
    "title": "Table of contents",
    "content": ". | Two-tiered Spark support | Access lakeFS using the S3A gateway . | Configuration . | Per-bucket configuration | . | Reading Data | Writing Data | . | Access lakeFS using the lakeFS-specific Hadoop FileSystem . | Configuration . | Load the FileSystem JARs | Configure the lakeFS FileSystem and the underlying S3A FileSystem | Per-bucket and per-repo configuration | . | Reading Data | Writing Data | . | Case Study: SimilarWeb | . Note In all following examples we set AWS and lakeFS credentials at runtime, for clarity. In production, properties defining AWS credentials should be set using one of Hadoop’s standard ways of authenticating with S3. Similarly, properties defining lakeFS credentials should be configured in secure site files, not on the command line or inlined in code where they might be exposed. ",
    "url": "/integrations/spark.html#table-of-contents",
    "relUrl": "/integrations/spark.html#table-of-contents"
  },"310": {
    "doc": "Spark",
    "title": "Two-tiered Spark support",
    "content": "lakeFS support in Spark has two tiers: . | Access lakeFS using the S3A gateway. | Access lakeFS using the lakeFS-specific Hadoop FileSystem. | . Using the S3A gateway is easier to configure and may be more suitable for legacy or small-scale applications. Using the lakeFS FileSystem requires somewhat more complex configuration, but offers greatly increased performance. ",
    "url": "/integrations/spark.html#two-tiered-spark-support",
    "relUrl": "/integrations/spark.html#two-tiered-spark-support"
  },"311": {
    "doc": "Spark",
    "title": "Access lakeFS using the S3A gateway",
    "content": "To use this mode you configure the Spark application to use S3A using the S3-compatible endpoint which the lakeFS server provides. Accordingly all data flows through the lakeFS server. Accessing data in lakeFS from Spark is the same as accessing S3 data from Spark. The only changes we need to consider are: . | Setting the configurations to access lakeFS. | Accessing objects using the lakeFS S3 path convention. | . Configuration . In order to configure Spark to work with lakeFS, we set S3 Hadoop configuration to the lakeFS endpoint and credentials: . | Hadoop Configuration | Value | . | fs.s3a.access.key | Set to the lakeFS access key | . | fs.s3a.secret.key | Set to the lakeFS secret key | . | fs.s3a.endpoint | Set to the lakeFS S3-compatible API endpoint | . | fs.s3a.path.style.access | Set to true | . Here is how to do it: . | CLI | Scala | XML Configuration | . spark-shell --conf spark.hadoop.fs.s3a.access.key='AKIAlakefs12345EXAMPLE' \\ --conf spark.hadoop.fs.s3a.secret.key='abc/lakefs/1234567bPxRfiCYEXAMPLEKEY' \\ --conf spark.hadoop.fs.s3a.path.style.access=true \\ --conf spark.hadoop.fs.s3a.endpoint='https://lakefs.example.com' ... spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", \"AKIAlakefs12345EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.endpoint\", \"https://lakefs.example.com\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.path.style.access\", \"true\") . Add these into a configuration file, e.g. $SPARK_HOME/conf/hdfs-site.xml: . &lt;?xml version=\"1.0\"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.s3a.access.key&lt;/name&gt; &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.secret.key&lt;/name&gt; &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;https://lakefs.example.com&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . Per-bucket configuration . The above configuration will use lakeFS as the sole S3 endpoint. To use lakeFS in parallel with S3, you can configure Spark to use lakeFS only for specific bucket names. For example, to configure only example-repo to use lakeFS, set the following configurations: . | CLI | Scala | XML Configuration | . spark-shell --conf spark.hadoop.fs.s3a.bucket.example-repo.access.key='AKIAlakefs12345EXAMPLE' \\ --conf spark.hadoop.fs.s3a.bucket.example-repo.secret.key='abc/lakefs/1234567bPxRfiCYEXAMPLEKEY' \\ --conf spark.hadoop.fs.s3a.bucket.example-repo.endpoint='https://lakefs.example.com' \\ --conf spark.hadoop.fs.s3a.path.style.access=true . spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.access.key\", \"AKIAlakefs12345EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.endpoint\", \"https://lakefs.example.com\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.path.style.access\", \"true\") . Add these into a configuration file, e.g. $SPARK_HOME/conf/hdfs-site.xml: . &lt;?xml version=\"1.0\"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.s3a.bucket.example-repo.access.key&lt;/name&gt; &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.bucket.example-repo.secret.key&lt;/name&gt; &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.bucket.example-repo.endpoint&lt;/name&gt; &lt;value&gt;https://lakefs.example.com&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . With this configuration set , reading s3a paths with example-repo as the bucket will use lakeFS, while all other buckets will use AWS S3. Reading Data . In order for us to access objects in lakeFS we will need to use the lakeFS S3 gateway path conventions: . s3a://[REPOSITORY]/[BRANCH]/PATH/TO/OBJECT . Here is an example for reading a parquet file from lakeFS to a Spark DataFrame: . val repo = \"example-repo\" val branch = \"main\" val dataPath = s\"s3a://${repo}/${branch}/example-path/example-file.parquet\" val df = spark.read.parquet(dataPath) . You can now use this DataFrame like you would normally do. Writing Data . Now simply write your results back to a lakeFS path: . df.write.partitionBy(\"example-column\").parquet(s\"s3a://${repo}/${branch}/output-path/\") . The data is now created in lakeFS as new changes in your branch. You can now commit these changes, or revert them. ",
    "url": "/integrations/spark.html#access-lakefs-using-the-s3a-gateway",
    "relUrl": "/integrations/spark.html#access-lakefs-using-the-s3a-gateway"
  },"312": {
    "doc": "Spark",
    "title": "Access lakeFS using the lakeFS-specific Hadoop FileSystem",
    "content": "To use this mode you configure the Spark application to perform metadata operations on the lakeFS server, and all data operations directly through the same underlying object store that lakeFS uses. The lakeFS FileSystem currently supports Spark with Hadoop Apache 2.7 using only the S3A Hadoop FileSystem for data access. In this mode the Spark application will directly read and write from the underlying object store, significantly increasing application scalability and performance by reducing the load on the lakeFS server. Accessing data in lakeFS from Spark is the same as accessing S3 data from Spark. The only changes we need to perform are: . | Configure Spark to access lakeFS for metadata and S3 or a compatible underlying object store to access data. | Use lakefs://repo/ref/path/to/data URIs to read and write data on lakeFS, rather than s3a://... URIs. | . Configuration . In order to configure Spark to work using the lakeFS Hadoop FileSystem, you will need to load the filesystem JARs and then configure both that FileSystem and the underlying data access FileSystem. Load the FileSystem JARs . Add the package io.lakefs:hadoop-lakefs-assembly:&lt;VERSION&gt; to your Spark command: . --packages io.lakefs:hadoop-lakefs-assembly:0.1.6 . The jar is also available on a public S3 location: s3://treeverse-clients-us-east/hadoop/hadoop-lakefs-assembly-0.1.6.jar . Configure the lakeFS FileSystem and the underlying S3A FileSystem . Add Hadoop configuration to the underlying storage and additionally to lakeFS credentials. When using this mode, do not set the S3A endpoint URL to point at lakeFS – it should point at the underlying storage. | Hadoop Configuration | Value | . | fs.s3a.access.key | Set to the AWS S3 access key | . | fs.s3a.secret.key | Set to the AWS S3 secret key | . | fs.s3a.endpoint | Set to the AWS S3-compatible endpoint | . | fs.lakefs.impl | io.lakefs.LakeFSFileSystem | . | fs.lakefs.access.key | Set to the lakeFS access key | . | fs.lakefs.secret.key | Set to the lakeFS secret key | . | fs.lakefs.endpoint | Set to the lakeFS API URL | . When using AWS S3 itself, the default configuration works with us-east-1, so you may still need to configure fs.s3a.endpoint. Amazon provides these S3 endpoints you can use. Note: If not running on AWS, all s3a configuration properties are required! Unlike when using the S3 gateway, when using the lakeFS-specific Hadoop FileSystem you configure s3a to access the S3 underlying object storage, and lakefs to access the lakeFS server. When running on AWS you do not need to configure credentials if the instance profile has sufficient permissions. Here is how to do it: . | CLI | Scala | XML Configuration | . spark-shell --conf spark.hadoop.fs.s3a.access.key='AKIAIOSFODNN7EXAMPLE' \\ --conf spark.hadoop.fs.s3a.secret.key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' \\ --conf spark.hadoop.fs.s3a.endpoint='https://s3.eu-central-1.amazonaws.com' \\ --conf spark.hadoop.fs.lakefs.impl=io.lakefs.LakeFSFileSystem \\ --conf spark.hadoop.fs.lakefs.access.key=AKIAlakefs12345EXAMPLE \\ --conf spark.hadoop.fs.lakefs.secret.key=abc/lakefs/1234567bPxRfiCYEXAMPLEKEY \\ --conf spark.hadoop.fs.lakefs.endpoint=https://lakefs.example.com/api/v1 \\ --packages io.lakefs:hadoop-lakefs-assembly:0.1.6 ... Ensure you load the lakeFS FileSystem into Spark by running it with --packages or --jars, and then run: . spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", \"AKIAIOSFODNN7EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.endpoint\", \"https://s3.eu-central-1.amazonaws.com\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.access.key\", \"AKIAlakefs12345EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.endpoint\", \"https://lakefs.example.com/api/v1\") . Ensure you load the lakeFS FileSystem into Spark by running it with --packages or --jars, and then add these into a configuration file, e.g. $SPARK_HOME/conf/hdfs-site.xml: . &lt;?xml version=\"1.0\"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.s3a.access.key&lt;/name&gt; &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.secret.key&lt;/name&gt; &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;https://s3.eu-central-1.amazonaws.com&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.impl&lt;/name&gt; &lt;value&gt;io.lakefs.LakeFSFileSystem&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.access.key&lt;/name&gt; &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.secret.key&lt;/name&gt; &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.endpoint&lt;/name&gt; &lt;value&gt;https://lakefs.example.com/api/v1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . Per-bucket and per-repo configuration . As above, S3 allows for per-bucket configuration. You can use this if: . | You need to use S3A directly to access data in an S3 outside of lakeFS, and | different credentials are required to access data inside that bucket. | . Refer to the Hadoop AWS guide on Configuring different S3 buckets with Per-Bucket Configuration. There is no need for per-repo configurations in lakeFS when all repositories are on the same lakeFS server. If you need to access repositories that are on multiple lakeFS servers, configure multiple prefixes. For instance, you might configure both fs.lakefs.impl and fs.lakefs2.impl to be io.lakefs.LakeFSFileSystem, place separate endpoints and credentials under fs.lakefs.* and fs.lakefs2.*, and access the two servers using lakefs://... and lakefs2://... URLs. Reading Data . In order for us to access objects in lakeFS we will need to use the lakeFS path conventions: . lakefs://[REPOSITORY]/[BRANCH]/PATH/TO/OBJECT . Here is an example for reading a parquet file from lakeFS to a Spark DataFrame: . val repo = \"example-repo\" val branch = \"main\" val dataPath = s\"lakefs://${repo}/${branch}/example-path/example-file.parquet\" val df = spark.read.parquet(dataPath) . You can now use this DataFrame like you would normally do. Writing Data . Now simply write your results back to a lakeFS path: . df.write.partitionBy(\"example-column\").parquet(s\"lakefs://${repo}/${branch}/output-path/\") . The data is now created in lakeFS as new changes in your branch. You can now commit these changes, or revert them. ",
    "url": "/integrations/spark.html#access-lakefs-using-the-lakefs-specific-hadoop-filesystem",
    "relUrl": "/integrations/spark.html#access-lakefs-using-the-lakefs-specific-hadoop-filesystem"
  },"313": {
    "doc": "Spark",
    "title": "Case Study: SimilarWeb",
    "content": "See how SimilarWeb is using lakeFS with Spark to manage algorithm changes in data pipelines. ",
    "url": "/integrations/spark.html#case-study-similarweb",
    "relUrl": "/integrations/spark.html#case-study-similarweb"
  },"314": {
    "doc": "Spark",
    "title": "Spark",
    "content": " ",
    "url": "/integrations/spark.html",
    "relUrl": "/integrations/spark.html"
  },"315": {
    "doc": "Try without installing",
    "title": "Try lakeFS without installing",
    "content": " ",
    "url": "/quickstart/try.html#try-lakefs-without-installing",
    "relUrl": "/quickstart/try.html#try-lakefs-without-installing"
  },"316": {
    "doc": "Try without installing",
    "title": "lakeFS Playground",
    "content": "Experience lakeFS first hand with your own isolated environment. You can easily integrate it with your existing tools, and feel lakeFS in action in an environment similar to your own. Try lakeFS now without installing . ",
    "url": "/quickstart/try.html#lakefs-playground",
    "relUrl": "/quickstart/try.html#lakefs-playground"
  },"317": {
    "doc": "Try without installing",
    "title": "Katacoda Tutorial",
    "content": "Learn how to use lakeFS using the CLI and an interactive Spark shell - all from your browser, without installing anything. In the tutorial we cover: . | Basic lakectl command line usage | How to read, write, list and delete objects from lakeFS using the lakectl command | Read from, and write to lakeFS using its S3 API interface using Spark | Diff, commit and merge the changes created by Spark | Track commit history to understand changes to your data over time | . The web based environment provides a full working lakeFS and Spark environment, so feel free to explore it on your own. Start Katacoda Tutorial Now . ",
    "url": "/quickstart/try.html#katacoda-tutorial",
    "relUrl": "/quickstart/try.html#katacoda-tutorial"
  },"318": {
    "doc": "Try without installing",
    "title": "Next Steps",
    "content": "After getting acquainted with lakeFS, easily install it on your computer or deploy it on your cloud account. ",
    "url": "/quickstart/try.html#next-steps",
    "relUrl": "/quickstart/try.html#next-steps"
  },"319": {
    "doc": "Try without installing",
    "title": "Try without installing",
    "content": " ",
    "url": "/quickstart/try.html",
    "relUrl": "/quickstart/try.html"
  },"320": {
    "doc": "Upgrade lakeFS",
    "title": "Upgrading lakeFS",
    "content": "Upgrading lakeFS from a previous version usually just requires re-deploying with the latest image (or downloading the latest version, if you’re using the binary). There are cases where the database will require a migration - check whether the release you are upgrading to requires that. ",
    "url": "/reference/upgrade.html#upgrading-lakefs",
    "relUrl": "/reference/upgrade.html#upgrading-lakefs"
  },"321": {
    "doc": "Upgrade lakeFS",
    "title": "When DB migrations are required",
    "content": "lakeFS 0.30.0 or greater . In case a migration is required, first stop the running lakeFS service. Using the lakefs binary for the new version, run the following: . lakefs migrate up . Deploy (or run) the new version of lakeFS. Note that an older version of lakeFS cannot run on a migrated database. Prior to lakeFS 0.30.0 . Note: with lakeFS &lt; 0.30.0, you should first upgrade to 0.30.0 following this guide. Then, proceed to upgrade to the newest version. Starting version 0.30.0, lakeFS handles your committed metadata in a new way, which is more robust and has better performance. To move your existing data, you will need to run the following upgrade commands. Verify lakeFS version == 0.30.0 (can skip if using Docker) . lakefs --version . Migrate data from previous format: . lakefs migrate db . Or migrate using Docker image: . docker run --rm -it -e LAKEFS_DATABASE_CONNECTION_STRING=&lt;database connection string&gt; treeverse/lakefs:rocks-migrate migrate db . Once migrated, it is possible to now use more recent lakeFS versions. Please refer to their release notes for more information on ugrading and usage). If you want to start over, discarding your existing data, you need to explicitly state this in your lakeFS configuration file. To do so, add the following to your configuration (relevant only for 0.30.0): . cataloger: type: rocks . ",
    "url": "/reference/upgrade.html#when-db-migrations-are-required",
    "relUrl": "/reference/upgrade.html#when-db-migrations-are-required"
  },"322": {
    "doc": "Upgrade lakeFS",
    "title": "Data Migration for Version v0.50.0",
    "content": "We discovered a bug in the way lakeFS is storing objects in the underlying object store. It affects only repositories on Azure and GCP, and not all of these. Issue #2397 describes the repository storage namespaces patterns which are affected by this bug. When first upgrading to a version greater or equal to v0.50.0, you must follow these steps: . | Stop lakeFS. | Perform a data-migration (details below) | Start lakeFS with the new version. | After a successful run of the new version, and after validating the objects are accessible, you can delete the old data prefix. | . Note: Migrating data is a delicate procedure. The lakeFS team is here to help, reach out to us on Slack. We’ll be happy to walk you through the process. Data migration . The following patterns have been impacted by the bug: . | Type | Storage Namespace pattern | Copy From | Copy To | . | gs | gs://bucket/prefix | gs://bucket//prefix/* | gs://bucket/prefix/* | . | gs | gs://bucket/prefix/ | gs://bucket//prefix/* | gs://bucket/prefix/* | . | azure | https://account.blob.core.windows.net/containerid | https://account.blob.core.windows.net/containerid//* | https://account.blob.core.windows.net/containerid/* | . | azure | https://account.blob.core.windows.net/containerid/ | https://account.blob.core.windows.net/containerid//* | https://account.blob.core.windows.net/containerid/* | . | azure | https://account.blob.core.windows.net/containerid/prefix/ | https://account.blob.core.windows.net/containerid/prefix// | https://account.blob.core.windows.net/containerid/prefix/* | . You can find the repositories storage namespaces with: . lakectl repo list . Or the settings tab in the UI. Migrating Google Storage data with gsutil . gsutil is a Python application that lets you access Cloud Storage from the command line. We can use it for copying the data between the prefixes in the Google bucket, and later on removing it. For every affected repository, copy its data with: . gsutil -m cp -r gs://&lt;BUCKET&gt;//&lt;PREFIX&gt;/ gs://&lt;BUCKET&gt;/ . Note the double slash after the bucket name. Migrating Azure Blob Storage data with AzCopy . AzCopy is a command-line utility that you can use to copy blobs or files to or from a storage account. We can use it for copying the data between the prefixes in the Azure storage account container, and later on removing it. First, you need to acquire an Account SAS. Using the Azure CLI: . az storage container generate-sas \\ --account-name &lt;ACCOUNT&gt; \\ --name &lt;CONTAINER&gt; \\ --permissions cdrw \\ --auth-mode key \\ --expiry 2021-12-31 . With the resulted SAS, use AzCopy to copy the files. If a prefix exists after the container: . azcopy copy \\ \"https://&lt;ACCOUNT&gt;.blob.core.windows.net/&lt;CONTAINER&gt;/&lt;PREFIX&gt;//?&lt;SAS_TOKEN&gt;\" \\ \"https://&lt;ACCOUNT&gt;.blob.core.windows.net/&lt;CONTAINER&gt;?&lt;SAS_TOKEN&gt;\" \\ --recursive=true . Or when using the container without a prefix: . azcopy copy \\ \"https://&lt;ACCOUNT&gt;.blob.core.windows.net/&lt;CONTAINER&gt;//?&lt;SAS_TOKEN&gt;\" \\ \"https://&lt;ACCOUNT&gt;.blob.core.windows.net/&lt;CONTAINER&gt;/./?&lt;SAS_TOKEN&gt;\" \\ --recursive=true . ",
    "url": "/reference/upgrade.html#data-migration-for-version-v0500",
    "relUrl": "/reference/upgrade.html#data-migration-for-version-v0500"
  },"323": {
    "doc": "Upgrade lakeFS",
    "title": "Upgrade lakeFS",
    "content": " ",
    "url": "/reference/upgrade.html",
    "relUrl": "/reference/upgrade.html"
  },"324": {
    "doc": "Versioning Internals",
    "title": "Versioning Internals",
    "content": " ",
    "url": "/understand/versioning-internals.html",
    "relUrl": "/understand/versioning-internals.html"
  },"325": {
    "doc": "Versioning Internals",
    "title": "Table of contents",
    "content": ". | Overview | SSTable File Format (“Graveler File”) | Constructing a consistent view of the keyspace (i.e., a commit) | Representing references and uncommitted metadata | . ",
    "url": "/understand/versioning-internals.html#table-of-contents",
    "relUrl": "/understand/versioning-internals.html#table-of-contents"
  },"326": {
    "doc": "Versioning Internals",
    "title": "Overview",
    "content": "Since commits in lakeFS are immutable, they are easy to store on an immutable object store. Older commits are rarely accessed, while newer commits are accessed very frequently, a tiered storage approach can work very well - The object store is the source of truth, while local disk and even RAM can be used to cache the more frequently accessed ones. Since they are immutable - once cached, we only need to evict them when space is running out. There’s no complex invalidation that needs to happen. In terms of storage format, commits are be stored as SSTables, compatible with RocksDB. SSTables were chosen as a storage format for 3 major reasons: . | Extremely high read throughput on modern hardware: using commits representing a 200m object repository (modeled after the S3 inventory of one of our design partners), we were able to achieve close to 500k random GetObject calls / second. This provides a very high throughput/cost ratio, probably as high as can be achieved on public clouds. | Being a known storage format means it’s relatively easy to generate and consume. Storing it in the object store makes it accessible to data engineering tools for analysis and distributed computation, effectively reducing the silo effect of storing it in an operational database. | The SSTable format supports delta encoding for keys which makes them very space efficient for data lakes where many keys share the same common prefixes. | . Each lakeFS commit is represented as a set of contiguous, non-overlapping SSTables that make up the entire keyspace of a repository at that commit. ",
    "url": "/understand/versioning-internals.html#overview",
    "relUrl": "/understand/versioning-internals.html#overview"
  },"327": {
    "doc": "Versioning Internals",
    "title": "SSTable File Format (“Graveler File”)",
    "content": "lakeFS metadata is encoded into a format called “Graveler” - a standardized way to encode content-addressable key value pairs. This is what a Graveler file looks like: . Each Key/Value pair (“ValueRecord”) is constructed of a key, an identity and a value. A simple identity could be, for example, a sha256 hash of the value’s bytes, but it could be any sequence of bytes that uniquely identifies the value. As far as Graveler is concerned, two ValueRecords are considered identical, if their key and identity fields are equal. A Graveler file itself is content-addressable, i.e. similarly to Git, the name of the file is its identity. File identity is calculated based on the identity of the ValueRecords the file contains: . valueRecordID = h(h(valueRecord.key) || h(valueRecord.Identity)) fileID = h(valueRecordID1 + … + valueRecordIDN) . ",
    "url": "/understand/versioning-internals.html#sstable-file-format-graveler-file",
    "relUrl": "/understand/versioning-internals.html#sstable-file-format-graveler-file"
  },"328": {
    "doc": "Versioning Internals",
    "title": "Constructing a consistent view of the keyspace (i.e., a commit)",
    "content": "We have 2 additional requirements for the storage format: . | Be space and time efficient when creating a commit - assuming a commit changes a single object out of a billion, we don’t want to write a full snapshot of the entire repository. Ideally, we’ll be able to reuse some data files that haven’t changed to make the commit operations (in both space and time) proportional to the size of the difference as opposed to the total size of the repository. | Allow an efficient diff between commits which runs in time proportional to the size of their difference and not their absolute sizes. | . To support these requirements, we decided to essentially build a 2-layer Merkle tree composed of a set of leaf nodes (“Range”) addressed by their content address, and a “Meta Range”, which is a special range containing all ranges, thus representing an entire consistent view of the keyspace: . Assuming commit B is derived from commit A, and only changed files in range e-f, it can reuse all ranges except for SSTable #N (the one containing the modified range of keys), which will be recreated with a new hash, representing the state as exists after applying commit B’s changes. This will in turn, also create a new Metarange since its hash is now changed as well (as it is derived from the hash of all contained ranges). Assuming most commits usually change related objects (i.e. that are likely to share some common prefix), the reuse ratio could be very high. We tested this assumption using S3 inventory from 2 design partners - we partitioned the keyspace to an arbitrary number of simulated blocks and measured their change over time. We saw a daily change rate of about 5-20%. Given the size of the repositories, it is safe to assume that a single day would translate into multiple commits. At a modest 20 commits per day, a commit is expected to reuse &gt;= 99% of the previous commit blocks, so acceptable in terms of write amplification generated on commit. On the object store, ranges are stored in the following hierarchy: . &lt;lakefs root&gt; _lakefs/ &lt;range hash1&gt; &lt;range hash2&gt; &lt;range hashN&gt; ... &lt;metarange hash1&gt; &lt;metarange hash2&gt; &lt;metarange hashN&gt; ... &lt;data object hash1&gt; &lt;data object hash2&gt; &lt;data object hashN&gt; ... Note: this relatively flat structure could be modified in the future: looking at the diagram above, it imposes no real limitations on the depth of the tree. A tree could easily be made recursive by having Meta Ranges point to other Meta Ranges - and still provide all the same characteristics. For simplicity, we decided to start with a fixed 2-level hierarchy. ",
    "url": "/understand/versioning-internals.html#constructing-a-consistent-view-of-the-keyspace-ie-a-commit",
    "relUrl": "/understand/versioning-internals.html#constructing-a-consistent-view-of-the-keyspace-ie-a-commit"
  },"329": {
    "doc": "Versioning Internals",
    "title": "Representing references and uncommitted metadata",
    "content": "lakeFS always stores the object data in the storage namespace in the user’s object store, committed and uncommitted data alike. However, the lakeFS object metadata might be stored in either the object store or PostgresSQL. Unlike committed metadata which is immutable, uncommitted (or “staged”) metadata experiences frequent random writes and is very mutable in nature. This is also true for “refs” - in particular, branches, which are simply pointers to an underlying commit, are modified frequently: on every commit or merge operation. Both these types of metadata are not only mutable, but also require strong consistency guarantees while also being fault tolerant. If we can’t access the current pointer of the main branch, a big portion of the system is essentially down. Luckily, this is also much smaller set of metadata, compared to the committed metadata. References and uncommitted metadata are currently stored on PostgreSQL for its strong consistency and transactional guarantees. In the future we plan on eliminating the need for an RDBMS by using a pluggable Key-Value store interface that would allow the use of many databases that meet its naive requirements. Non-production single server installations can leverage an embedded key-value store like RocksDB, that will allow running with only a single container. ",
    "url": "/understand/versioning-internals.html#representing-references-and-uncommitted-metadata",
    "relUrl": "/understand/versioning-internals.html#representing-references-and-uncommitted-metadata"
  },"330": {
    "doc": "S3 Virtual-host addressing (advanced)",
    "title": "Configuring lakeFS to use S3 Virtual-Host addressing",
    "content": " ",
    "url": "/setup/virtual-host-addressing.html#configuring-lakefs-to-use-s3-virtual-host-addressing",
    "relUrl": "/setup/virtual-host-addressing.html#configuring-lakefs-to-use-s3-virtual-host-addressing"
  },"331": {
    "doc": "S3 Virtual-host addressing (advanced)",
    "title": "Understanding virtual-host addressing",
    "content": "Some systems require S3 endpoints (such as lakeFS’ S3 Gateway) to support virtual-host style addressing. lakeFS supports this, but requires some configuration in order to extract the bucket name (used as the lakeFS repository ID) from the host address. For example: . GET http://foo.example.com/some/location . In this case, there’s no way for lakeFS to determine whether this is a virtual-host request where the endpoint url is example.com, the bucket name is foo and the path is /some/location, or a path-based request where the endpoint is foo.example.com, the bucket name is some and the path is location. This requires an extra step: Defining an explicit set of DNS record for lakeFS S3 gateway. ",
    "url": "/setup/virtual-host-addressing.html#understanding-virtual-host-addressing",
    "relUrl": "/setup/virtual-host-addressing.html#understanding-virtual-host-addressing"
  },"332": {
    "doc": "S3 Virtual-host addressing (advanced)",
    "title": "Adding an explicit S3 domain name to the S3 Gateway configuration",
    "content": "The first step would be to tell the lakeFS installation which hostnames are used for the S3 Gateway. This should be a different DNS record from the one used for e.g. the UI or API. Typically, if the lakeFS installation is served under lakefs.example.com, a good choice would be s3.lakefs.example.com. This could be done using either an environment variable: . LAKEFS_GATEWAYS_S3_DOMAIN_NAME=\"s3.lakefs.example.com\" . Or by adding the gateways.s3.domain_name setting to the lakeFS config.yaml file: . --- database: connection_string: \"...\" ... # This section defines an explict S3 gateway address that supports virtual-host addressing gateways: s3: domain_name: s3.lakefs.example.com . For more information on how to configure lakeFS, check out the configuration reference . ",
    "url": "/setup/virtual-host-addressing.html#adding-an-explicit-s3-domain-name-to-the-s3-gateway-configuration",
    "relUrl": "/setup/virtual-host-addressing.html#adding-an-explicit-s3-domain-name-to-the-s3-gateway-configuration"
  },"333": {
    "doc": "S3 Virtual-host addressing (advanced)",
    "title": "Setting up the appropriate DNS records",
    "content": "Once our lakeFS installation is configured with an explicit S3 gateway endpoint address, we need to define 2 DNS records and have them point at our lakeFS installation. This requires 2 CNAME records: . | s3.lakefs.example.com - CNAME to lakefs.example.com. This would be used as the S3 endpoint when configuring clients and will serve as our bare domain. | *.s3.lakefs.example.com - Also a CNAME to lakefs.example.com. This will resolve virtual-host requests such as example-repo.s3.lakefs.example.com that lakeFS would now know how to parse. | . For more information on how to configure these, see the official documentation of your DNS provider. On AWS, This could also be done using ALIAS records for a load balancer. ",
    "url": "/setup/virtual-host-addressing.html#setting-up-the-appropriate-dns-records",
    "relUrl": "/setup/virtual-host-addressing.html#setting-up-the-appropriate-dns-records"
  },"334": {
    "doc": "S3 Virtual-host addressing (advanced)",
    "title": "S3 Virtual-host addressing (advanced)",
    "content": " ",
    "url": "/setup/virtual-host-addressing.html",
    "relUrl": "/setup/virtual-host-addressing.html"
  }
}
