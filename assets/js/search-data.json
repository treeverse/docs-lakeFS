{"0": {
    "doc": "Page not found",
    "title": "Page not found",
    "content": " ",
    "url": "/404.html",
    "relUrl": "/404.html"
  },"1": {
    "doc": "Page not found",
    "title": "We can’t seem to find the page you’re looking for.",
    "content": "Error code: 404 . Back to Docs . ",
    "url": "/404.html#we-cant-seem-to-find-the-page-youre-looking-for",
    "relUrl": "/404.html#we-cant-seem-to-find-the-page-youre-looking-for"
  },"2": {
    "doc": "Airbyte",
    "title": "Table of contents",
    "content": ". | Using lakeFS with Airbyte | Use-cases | S3 Connector . | Configuring lakeFS using the connector | . | . ",
    "url": "/integrations/airbyte.html#table-of-contents",
    "relUrl": "/integrations/airbyte.html#table-of-contents"
  },"3": {
    "doc": "Airbyte",
    "title": "Using lakeFS with Airbyte",
    "content": "The integration between the two open-source projects brings resilience and manageability when using Airbyte connectors to sync data to your S3 buckets by leveraging lakeFS branches and atomic commits and merges. ",
    "url": "/integrations/airbyte.html#using-lakefs-with-airbyte",
    "relUrl": "/integrations/airbyte.html#using-lakefs-with-airbyte"
  },"4": {
    "doc": "Airbyte",
    "title": "Use-cases",
    "content": "You can leverage lakeFS ACID guarantees and CI/CD capabilities when ingesting data to S3 using lakeFS: . | Consolidate many data source into a single branch and expose them to the consumers simultaneously when merging to the main branch. | Test the incoming data for breaking schema changes, using lakeFS hooks. | Avoid having consumers reading partial data from connectors which failed half-way through the sync operation. | Experiment with ingested data before exposing it. | . ",
    "url": "/integrations/airbyte.html#use-cases",
    "relUrl": "/integrations/airbyte.html#use-cases"
  },"5": {
    "doc": "Airbyte",
    "title": "S3 Connector",
    "content": "lakeFS exposes an S3 Gateway that enables applications to communicate with lakeFS the same way they would with Amazon S3. You can use Airbyte’s S3 Destination for uploading the data to lakeFS. Configuring lakeFS using the connector . Set the following parameters when creating a new Destination of type S3: . | Name | Value | Example | . | Endpoint | The lakeFS S3 gateway URL | http://s3.lakefs.example.com | . | S3 Bucket Name | The lakeFS repository where the data will be written | example-repo | . | S3 Bucket Path | The branch and the path where the data will be written | main/data/from/airbyte Where main is the branch name, and data/from/airbyte is the path under the branch. | . | S3 Bucket Region | Not applicable to lakeFS, use us-east-1 | us-east-1 | . | S3 Key ID | The lakeFS access key id used to authenticate to lakeFS. | AKIAlakefs12345EXAMPLE | . | S3 Access Key | The lakeFS secret access key used to authenticate to lakeFS. | abc/lakefs/1234567bPxRfiCYEXAMPLEKEY | . Note S3 Destination connector supports custom S3 endpoints strating from Airbyte’s version v0.26.0-alpha released on Jun 17th 2021 . The UI configuration will look like: . ",
    "url": "/integrations/airbyte.html#s3-connector",
    "relUrl": "/integrations/airbyte.html#s3-connector"
  },"6": {
    "doc": "Airbyte",
    "title": "Airbyte",
    "content": "Airbyte is an open-source platform to sync data from applications, APIs &amp; databases to warehouses, lakes and other destinations. Using Airbyte’s connectors you can get your data pipelines running to consolidate many input sources. ",
    "url": "/integrations/airbyte.html",
    "relUrl": "/integrations/airbyte.html"
  },"7": {
    "doc": "Airflow",
    "title": "Using lakeFS with Airflow",
    "content": "Apache Airflow is a platform to programmatically author, schedule and monitor workflows. There are some aspects we will need to handle in order to run Airflow with lakeFS: . ",
    "url": "/integrations/airflow.html#using-lakefs-with-airflow",
    "relUrl": "/integrations/airflow.html#using-lakefs-with-airflow"
  },"8": {
    "doc": "Airflow",
    "title": "Creating the lakeFS connection",
    "content": "For authenticating to the lakeFS server, you need to create a new Airflow Connection of type HTTP and pass it to your DAG. You can do that using the Airflow UI or the cli. Here’s an example Airflow command that does just that: . airflow connections add conn_lakefs --conn-type=HTTP --conn-host=http://&lt;LAKEFS_ENDPOINT&gt; \\ --conn-extra='{\"access_key_id\":\"&lt;LAKEFS_ACCESS_KEY_ID&gt;\",\"secret_access_key\":\"&lt;LAKEFS_SECRET_ACCESS_KEY&gt;\"}' . ",
    "url": "/integrations/airflow.html#creating-the-lakefs-connection",
    "relUrl": "/integrations/airflow.html#creating-the-lakefs-connection"
  },"9": {
    "doc": "Airflow",
    "title": "Installing lakeFS Airflow package",
    "content": "Installing the package using pip: . pip install airflow-provider-lakefs . ",
    "url": "/integrations/airflow.html#installing-lakefs-airflow-package",
    "relUrl": "/integrations/airflow.html#installing-lakefs-airflow-package"
  },"10": {
    "doc": "Airflow",
    "title": "Using the package",
    "content": "The package exposes several operations for interacting with a lakeFS server: . | CreateBranchOperator creates a new lakeFS branch from the source branch (defaults to main). task_create_branch = CreateBranchOperator( task_id='create_branch', repo='example-repo', branch='example-branch', source_branch='main' ) . | CommitOperator commits uncommitted changes to a branch. task_commit = CommitOperator( task_id='commit', repo='example-repo', branch='example-branch', msg='committing to lakeFS using airflow!', metadata={'committed_from\": \"airflow-operator'} ) . | MergeOperator merges 2 lakeFS branches. task_merge = MergeOperator( task_id='merge_branches', source_ref='example-branch', destination_branch='main', msg='merging job outputs', metadata={'committer': 'airflow-operator'} ) . | . Sensors are also available if you want to synchronize a running DAG with external operations: . | CommitSensor waits until a commit has been applied to the branch . task_sense_commit = CommitSensor( repo='example-repo', branch='example-branch', task_id='sense_commit' ) . | FileSensor waits until a given file is present in a branch. task_sense_file = FileSensor( task_id='sense_file', repo='example-repo', branch='example-branch', path=\"file/to/sense\" ) . | . For a DAG example that uses all the above, check out the example DAG in the airflow-provider-lakeFS repository. Performing other operations . To perform other operations that are not yet supported by the package, you can use: . | SimpleHttpOperator to send API requests to lakeFS. | BashOperator with lakeCTL commands. For example, deleting a branch using BashOperator: commit_extract = BashOperator( task_id='delete_branch', bash_command='lakectl branch delete lakefs://example-repo/example-branch', dag=dag, ) . | . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. ",
    "url": "/integrations/airflow.html#using-the-package",
    "relUrl": "/integrations/airflow.html#using-the-package"
  },"11": {
    "doc": "Airflow",
    "title": "Airflow",
    "content": " ",
    "url": "/integrations/airflow.html",
    "relUrl": "/integrations/airflow.html"
  },"12": {
    "doc": "API Reference",
    "title": "API Reference",
    "content": "| ",
    "url": "/reference/api.html",
    "relUrl": "/reference/api.html"
  },"13": {
    "doc": "Architecture",
    "title": "Architecture Overview",
    "content": " ",
    "url": "/understand/architecture.html#architecture-overview",
    "relUrl": "/understand/architecture.html#architecture-overview"
  },"14": {
    "doc": "Architecture",
    "title": "Table of contents",
    "content": ". | Overview | lakeFS Components . | S3 Gateway | OpenAPI Server | S3 Storage Adapter | Metadata Index | Authentication &amp; Authorization Service | Frontend UI | . | . ",
    "url": "/understand/architecture.html#table-of-contents",
    "relUrl": "/understand/architecture.html#table-of-contents"
  },"15": {
    "doc": "Architecture",
    "title": "Overview",
    "content": "lakeFS is distributed as a single binary encapsulating several logical services: . The server itself is stateless, meaning you can easily add more instances to handle bigger load. lakeFS stores data in an underlying S3 bucket with some of its metadata stored in PostgreSQL. (see Data Model) . ",
    "url": "/understand/architecture.html#overview",
    "relUrl": "/understand/architecture.html#overview"
  },"16": {
    "doc": "Architecture",
    "title": "lakeFS Components",
    "content": "S3 Gateway . The API Gateway implements lakeFS’ compatibility with S3. It implements a compatible subset of the S3 API to ensure most data systems can use lakeFS as a drop-in replacement for S3. To achieve this, the gateway exposes an HTTP listener on a dedicated host and port. See the S3 API Reference section for information on supported API operations. OpenAPI Server . The Swagger (OpenAPI) Server exposes the full set of lakeFS operations (see Reference). This includes basic CRUD operations against repositories and objects, as well as versioning related operations such as branching, merging, committing and reverting changes to data. S3 Storage Adapter . The S3 Storage Adapter is the component in charge of communication with the underlying S3 bucket. It is logically decoupled from the S3 Gateway to allow for future compatibility with other types of underlying storage such as HDFS or S3-Compatible storage providers. See the roadmap for information on future plans for storage compatibility. Metadata Index . To learn about the data model used to store lakeFS metadata, see the data model section. Authentication &amp; Authorization Service . The Auth service handles creation, management and validation of user credentials and RBAC policies. The credential scheme, along with the request signing logic are compatible with AWS IAM (both SIGv2 and SIGv4). Currently, the auth service manages its own database of users and credentials and does not use IAM in any way. Frontend UI . The UI layer is a simple browser-based client that uses the OpenAPI server. It allows management, exploration and data access to repositories, branches, commits and objects in the system. ",
    "url": "/understand/architecture.html#lakefs-components",
    "relUrl": "/understand/architecture.html#lakefs-components"
  },"17": {
    "doc": "Architecture",
    "title": "Architecture",
    "content": " ",
    "url": "/understand/architecture.html",
    "relUrl": "/understand/architecture.html"
  },"18": {
    "doc": "Amazon Athena",
    "title": "Using lakeFS with Amazon Athena",
    "content": "Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Amazon Athena works directly above S3 and can’t access lakeFS. In order to support querying data from lakeFS with Amazon Athena, we will use create-symlink, one of the metastore commands in lakectl. create-symlink receives a table in glue pointing to lakeFS and creates a copy of the table in glue pointing to the underlying S3 bucket. We can then query the new created table with Athena . ",
    "url": "/integrations/athena.html#using-lakefs-with-amazon-athena",
    "relUrl": "/integrations/athena.html#using-lakefs-with-amazon-athena"
  },"19": {
    "doc": "Amazon Athena",
    "title": "Amazon Athena",
    "content": " ",
    "url": "/integrations/athena.html",
    "relUrl": "/integrations/athena.html"
  },"20": {
    "doc": "Authentication & Authorization",
    "title": "Authentication &amp; Authorization",
    "content": " ",
    "url": "/reference/authorization.html#authentication--authorization",
    "relUrl": "/reference/authorization.html#authentication--authorization"
  },"21": {
    "doc": "Authentication & Authorization",
    "title": "Table of contents",
    "content": ". | Authentication . | API Server Authentication | S3 Gateway Authentication | . | Authorization . | Authorization Model | Authorization process | Policy Precedence | Resource naming - ARNs | Actions and Permissions | Preconfigured Policies . | FSFullAccess | FSReadAll | FSReadWriteAll | AuthFullAccess | AuthManageOwnCredentials | RepoManagementFullAccess | RepoManagementReadAll | ExportSetConfiguration | . | Preconfigured Groups . | Admins | SuperUsers | Developers | Viewers | . | . | . ",
    "url": "/reference/authorization.html#table-of-contents",
    "relUrl": "/reference/authorization.html#table-of-contents"
  },"22": {
    "doc": "Authentication & Authorization",
    "title": "Authentication",
    "content": "API Server Authentication . Authenticating against the API server is done using a key-pair, passed via Basic Access Authentication. All HTTP requests must carry an Authorization header with the following structure: . Authorization: Basic &lt;base64 encoded access_key_id:secret_access_key&gt; . For example, assuming my access_key_id is my_access_key_id and my secret_access_key is my_secret_access_key, we’d send the following header with every request: . Authorization: Basic bXlfYWNjZXNzX2tleV9pZDpteV9hY2Nlc3Nfc2VjcmV0X2tleQ== . S3 Gateway Authentication . To provide API compatibility with Amazon S3, authentication with the S3 Gateway supports both SIGv2 and SIGv4. Clients such as the AWS SDK that implement these authentication methods should work without modification. See this example for authenticating with the AWS CLI. ",
    "url": "/reference/authorization.html#authentication",
    "relUrl": "/reference/authorization.html#authentication"
  },"23": {
    "doc": "Authentication & Authorization",
    "title": "Authorization",
    "content": "Authorization Model . Access to resources is managed very much like AWS IAM. There are 4 basic components to the system: . Users - Representing entities that access and use the system. A user is given one or more Access Credentials for authentication. Actions - Representing a logical action within the system - reading a file, creating a repository, etc. Resources - A unique identifier representing a specific resource in the system - a repository, an object, a user, etc. Policies - Representing a set of Actions, a Resource and an effect: whether or not these actions are allowed or denied for the given resource(s). Groups - A named collection of users. Users can belong to multiple groups. Controlling access is done by attaching Policies, either directly to Users, or to Groups they belong to. Authorization process . Every action in the system, be it an API request, UI interaction, S3 Gateway call or CLI command, requires a set of actions to be allowed for one or more resources. When a user makes a request to perform that action, the following process takes place: . | Authentication - The credentials passed in the request are evaluated, and the user’s identity is extracted. | Action permission resolution - lakeFS would then calculate the set of allowed actions and resources that this request requires. | Effective policy resolution - the user’s policies (either attached directly or through group memberships) are calculated | Policy/Permission evaluation - lakeFS will compare the given user policies with the request actions and determine whether or not the request is allowed to continue | . Policy Precedence . Each policy attached to a user or a group has an Effect - either allow or deny. During evaluation of a request, deny would take precedence over any other allow policy. This helps us compose policies together. For example, we could attach a very permissive policy to a user and use deny rules to then selectively restrict what that user can do. Resource naming - ARNs . lakeFS uses ARN identifier - very similar in structure to those used by AWS. The resource segment of the ARN supports wildcards: use * to match 0 or more characters, or ? to match exactly one character. Additionally, the current user’s ID is interpolated in runtime into the ARN using the ${user} placeholder. Here are a few examples of valid ARNs within lakeFS: . arn:lakefs:auth:::user/jane.doe arn:lakefs:auth:::user/* arn:lakefs:fs:::repository/myrepo/* arn:lakefs:fs:::repository/myrepo/object/foo/bar/baz arn:lakefs:fs:::repository/myrepo/object/* arn:lakefs:fs:::repository/* arn:lakefs:fs:::* . this allows us to create fine-grained policies affecting only a specific subset of resources. See below for a full reference of ARNs and actions . Actions and Permissions . For the full list of actions and their required permissions see the following table: . | Action name | required action | Resource | API endpoint | S3 gateway operation | . | List Repositories | fs:ListRepositories | * | GET /repositories | ListBuckets | . | Get Repository | fs:ReadRepository | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId} | HeadBucket | . | Get Commit | fs:ReadCommit | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/commits/{commitId} | - | . | Create Commit | fs:CreateCommit | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | POST /repositories/{repositoryId}/branches/{branchId}/commits | - | . | Get Commit log | fs:ReadBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | GET /repositories/{repositoryId}/branches/{branchId}/commits | - | . | Create Repository | fs:CreateRepository | arn:lakefs:fs:::repository/{repositoryId} | POST /repositories | - | . | Delete Repository | fs:DeleteRepository | arn:lakefs:fs:::repository/{repositoryId} | DELETE /repositories/{repositoryId} | - | . | List Branches | fs:ListBranches | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/branches | ListObjects/ListObjectsV2 (with delimiter = / and empty prefix) | . | Get Branch | fs:ReadBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | GET /repositories/{repositoryId}/branches/{branchId} | - | . | Create Branch | fs:CreateBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | POST /repositories/{repositoryId}/branches | - | . | Delete Branch | fs:DeleteBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | DELETE /repositories/{repositoryId}/branches/{branchId} | - | . | Merge branches | fs:CreateCommit | arn:lakefs:fs:::repository/{repositoryId}/branch/{destinationBranchId} | POST /repositories/{repositoryId}/refs/{sourceBranchId}/merge/{destinationBranchId} | - | . | Diff branch uncommitted changes | fs:ListObjects | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/branches/{branchId}/diff | - | . | Diff refs | fs:ListObjects | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/refs/{leftRef}/diff/{rightRef} | - | . | Stat object | fs:ReadObject | arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey} | GET /repositories/{repositoryId}/refs/{ref}/objects/stat | HeadObject | . | Get Object | fs:ReadObject | arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey} | GET /repositories/{repositoryId}/refs/{ref}/objects | GetObject | . | List Objects | fs:ListObjects | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/refs/{ref}/objects/ls | ListObjects, ListObjectsV2 (no delimiter, or “/” + non-empty prefix) | . | Upload Object | fs:WriteObject | arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey} | POST /repositories/{repositoryId}/branches/{branchId}/objects | PutObject, CreateMultipartUpload, UploadPart, CompleteMultipartUpload | . | Delete Object | fs:DeleteObject | arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey} | DELETE /repositories/{repositoryId}/branches/{branchId}/objects | DeleteObject, DeleteObjects, AbortMultipartUpload | . | Revert Branch | fs:RevertBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | PUT /repositories/{repositoryId}/branches/{branchId} | - | . | Create User | auth:CreateUser | arn:lakefs:auth:::user/{userId} | POST /auth/users | - | . | List Users | auth:ListUsers | * | GET /auth/users | - | . | Get User | auth:ReadUser | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId} | - | . | Delete User | auth:DeleteUser | arn:lakefs:auth:::user/{userId} | DELETE /auth/users/{userId} | - | . | Get Group | auth:ReadGroup | arn:lakefs:auth:::group/{groupId} | GET /auth/groups/{groupId} | - | . | List Groups | auth:ListGroups | * | GET /auth/groups | - | . | Create Group | auth:CreateGroup | arn:lakefs:auth:::group/{groupId} | POST /auth/groups | - | . | Delete Group | auth:DeleteGroup | arn:lakefs:auth:::group/{groupId} | DELETE /auth/groups/{groupId} | - | . | List Policies | auth:ListPolicies | * | GET /auth/policies | - | . | Create Policy | auth:CreatePolicy | arn:lakefs:auth:::policy/{policyId} | POST /auth/policies | - | . | Update Policy | auth:UpdatePolicy | arn:lakefs:auth:::policy/{policyId} | POST /auth/policies | - | . | Delete Policy | auth:DeletePolicy | arn:lakefs:auth:::policy/{policyId} | DELETE /auth/policies/{policyId} | - | . | Get Policy | auth:ReadPolicy | arn:lakefs:auth:::policy/{policyId} | GET /auth/policies/{policyId} | - | . | List Group Members | auth:ReadGroup | arn:lakefs:auth:::group/{groupId} | GET /auth/groups/{groupId}/members | - | . | Add Group Member | auth:AddGroupMember | arn:lakefs:auth:::group/{groupId} | PUT /auth/groups/{groupId}/members/{userId} | - | . | Remove Group Member | auth:RemoveGroupMember | arn:lakefs:auth:::group/{groupId} | DELETE /auth/groups/{groupId}/members/{userId} | - | . | List User Credentials | auth:ListCredentials | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId}/credentials | - | . | Create User Credentials | auth:CreateCredentials | arn:lakefs:auth:::user/{userId} | POST /auth/users/{userId}/credentials | - | . | Delete User Credentials | auth:DeleteCredentials | arn:lakefs:auth:::user/{userId} | DELETE /auth/users/{userId}/credentials/{accessKeyId} | - | . | Get User Credentials | auth:ReadCredentials | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId}/credentials/{accessKeyId} | - | . | List User Groups | auth:ReadUser | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId}/groups | - | . | List User Policies | auth:ReadUser | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId}/policies | - | . | Attach Policy To User | auth:AttachPolicy | arn:lakefs:auth:::user/{userId} | PUT /auth/users/{userId}/policies/{policyId} | - | . | Detach Policy From User | auth:DetachPolicy | arn:lakefs:auth:::user/{userId} | DELETE /auth/users/{userId}/policies/{policyId} | - | . | List Group Policies | auth:ReadGroup | arn:lakefs:auth:::group/{groupId} | GET /auth/groups/{groupId}/policies | - | . | Attach Policy To Group | auth:AttachPolicy | arn:lakefs:auth:::group/{groupId} | PUT /auth/groups/{groupId}/policies/{policyId} | - | . | Detach Policy From Group | auth:DetachPolicy | arn:lakefs:auth:::group/{groupId} | DELETE /auth/groups/{groupId}/policies/{policyId} | - | . | Read Storage Config | fs:ReadConfig | * | GET /config/storage | - | . | Get Garbage Collection Rules | retention:GetGarbageCollectionRules | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/gc/rules | - | . | Set Garbage Collection Rules | retention:SetGarbageCollectionRules | arn:lakefs:fs:::repository/{repositoryId} | POST /repositories/{repositoryId}/gc/rules | - | . | Prepare Garbage Collection Commits | retention:PrepareGarbageCollectionCommits | arn:lakefs:fs:::repository/{repositoryId} | POST /repositories/{repositoryId}/gc/prepare_commits | - | . Preconfigured Policies . The following Policies are created during initial setup: . FSFullAccess . Policy: . { \"statement\": [ { \"action\": [ \"fs:*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . FSReadAll . Policy: . { \"statement\": [ { \"action\": [ \"fs:List*\", \"fs:Read*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . FSReadWriteAll . Policy: . { \"statement\": [ { \"action\": [ \"fs:ListRepositories\", \"fs:ReadRepository\", \"fs:ReadCommit\", \"fs:ListBranches\", \"fs:ListObjects\", \"fs:ReadObject\", \"fs:WriteObject\", \"fs:DeleteObject\", \"fs:RevertBranch\", \"fs:ReadBranch\", \"fs:CreateBranch\", \"fs:DeleteBranch\", \"fs:CreateCommit\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . AuthFullAccess . Policy: . { \"statement\": [ { \"action\": [ \"auth:*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . AuthManageOwnCredentials . Policy: . { \"statement\": [ { \"action\": [ \"auth:CreateCredentials\", \"auth:DeleteCredentials\", \"auth:ListCredentials\", \"auth:ReadCredentials\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:auth:::user/${user}\" } ] } . RepoManagementFullAccess . Policy: . { \"statement\": [ { \"action\": [ \"ci:*\" ], \"effect\": \"allow\", \"resource\": \"*\" }, { \"action\": [ \"retention:*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . RepoManagementReadAll . Policy: . { \"statement\": [ { \"action\": [ \"ci:Read*\" ], \"effect\": \"allow\", \"resource\": \"*\" }, { \"action\": [ \"retention:Get*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . ExportSetConfiguration . Policy: . { \"statement\": [ { \"action\": [ \"fs:ExportConfig\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . Preconfigured Groups . Admins . Policies: [\"FSFullAccess\", \"AuthFullAccess\", \"RepoManagementFullAccess\", \"ExportSetConfiguration\"] . SuperUsers . Policies: [\"FSFullAccess\", \"AuthManageOwnCredentials\", \"RepoManagementReadAll\"] . Developers . Policies: [\"FSReadWriteAll\", \"AuthManageOwnCredentials\", \"RepoManagementReadAll\"] . Viewers . Policies: [\"FSReadAll\", \"AuthManageOwnCredentials\"] . ",
    "url": "/reference/authorization.html#authorization",
    "relUrl": "/reference/authorization.html#authorization"
  },"24": {
    "doc": "Authentication & Authorization",
    "title": "Authentication & Authorization",
    "content": " ",
    "url": "/reference/authorization.html",
    "relUrl": "/reference/authorization.html"
  },"25": {
    "doc": "On AWS",
    "title": "Deploy lakeFS on AWS",
    "content": "Expected deployment time: 25min . ",
    "url": "/deploy/aws.html#deploy-lakefs-on-aws",
    "relUrl": "/deploy/aws.html#deploy-lakefs-on-aws"
  },"26": {
    "doc": "On AWS",
    "title": "Table of contents",
    "content": ". | Prerequisites | Creating the Database on AWS RDS | Installation Options . | On EC2 | On ECS | On EKS | . | Load balancing | DNS on AWS Route53 | Next Steps | . ",
    "url": "/deploy/aws.html#table-of-contents",
    "relUrl": "/deploy/aws.html#table-of-contents"
  },"27": {
    "doc": "On AWS",
    "title": "Prerequisites",
    "content": "A production-suitable lakeFS installation will require three DNS records pointing at your lakeFS server. A good convention for those will be, assuming you already own the domain example.com: . | lakefs.example.com | s3.lakefs.example.com - this is the S3 Gateway Domain | *.s3.lakefs.example.com | . The second record, the S3 Gateway Domain, needs to be specified in the lakeFS configuration (see the S3_GATEWAY_DOMAIN placeholder below). This will allow lakeFS to route requests to the S3-compatible API. For more info, see Why do I need these three DNS records? . ",
    "url": "/deploy/aws.html#prerequisites",
    "relUrl": "/deploy/aws.html#prerequisites"
  },"28": {
    "doc": "On AWS",
    "title": "Creating the Database on AWS RDS",
    "content": "lakeFS requires a PostgreSQL database to synchronize actions on your repositories. We will show you how to create a database on AWS RDS, but you can use any PostgreSQL database as long as it’s accessible by your lakeFS installation. If you already have a database, take note of the connection string and skip to the next step . | Follow the official AWS documentation on how to create a PostgreSQL instance and connect to it. You may use the default PostgreSQL engine, or Aurora PostgreSQL. Make sure you’re using PostgreSQL version &gt;= 11. | Once your RDS is set up and the server is in Available state, take note of the endpoint and port. | Make sure your security group rules allow you to connect to the database instance. | . ",
    "url": "/deploy/aws.html#creating-the-database-on-aws-rds",
    "relUrl": "/deploy/aws.html#creating-the-database-on-aws-rds"
  },"29": {
    "doc": "On AWS",
    "title": "Installation Options",
    "content": "On EC2 . | Save the following configuration file as config.yaml: . --- database: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: # replace this with a randomly-generated string: secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: s3 s3: region: us-east-1 gateways: s3: # replace this with the host you will use for the lakeFS S3-compatible endpoint: domain_name: [S3_GATEWAY_DOMAIN] . | Download the binary to the EC2 instance. | Run the lakefs binary on the EC2 instance: lakefs --config config.yaml run . Note: it is preferable to run the binary as a service using systemd or your operating system’s facilities. | . On ECS . To support container-based environments like AWS ECS, lakeFS can be configured using environment variables. Here is a docker run command to demonstrate starting lakeFS using Docker: . docker run \\ --name lakefs \\ -p 8000:8000 \\ -e LAKEFS_DATABASE_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\ -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\ -e LAKEFS_BLOCKSTORE_TYPE=\"s3\" \\ -e LAKEFS_GATEWAYS_S3_DOMAIN_NAME=\"[S3_GATEWAY_DOMAIN]\" \\ treeverse/lakefs:latest run . See the reference for a complete list of environment variables. On EKS . See Kubernetes Deployment. ",
    "url": "/deploy/aws.html#installation-options",
    "relUrl": "/deploy/aws.html#installation-options"
  },"30": {
    "doc": "On AWS",
    "title": "Load balancing",
    "content": "Depending on how you chose to install lakeFS, you should have a load balancer direct requests to the lakeFS server. By default, lakeFS operates on port 8000, and exposes a /_health endpoint which you can use for health checks. Notes for using an AWS Application Load Balancer . | Your security groups should allow the load balancer to access the lakeFS server. | Create a target group with a listener for port 8000. | Setup TLS termination using the domain names you wish to use for both endpoints (e.g. s3.lakefs.example.com, *.s3.lakefs.example.com, lakefs.example.com). | Configure the health-check to use the exposed /_health URL | . ",
    "url": "/deploy/aws.html#load-balancing",
    "relUrl": "/deploy/aws.html#load-balancing"
  },"31": {
    "doc": "On AWS",
    "title": "DNS on AWS Route53",
    "content": "As mentioned above, you should create 3 DNS records for lakeFS: . | One record for the lakeFS API: lakefs.example.com | Two records for the S3-compatible API: s3.lakefs.example.com and *.s3.lakefs.example.com. | . For an AWS load balancer with Route53 DNS, create a simple record, and choose Alias to Application and Classic Load Balancer with an A record type. For other DNS providers, refer to the documentation on how to add CNAME records. ",
    "url": "/deploy/aws.html#dns-on-aws-route53",
    "relUrl": "/deploy/aws.html#dns-on-aws-route53"
  },"32": {
    "doc": "On AWS",
    "title": "Next Steps",
    "content": "Your next step is to prepare your storage. If you already have a storage bucket/container, you are ready to create your first lakeFS repository. ",
    "url": "/deploy/aws.html#next-steps",
    "relUrl": "/deploy/aws.html#next-steps"
  },"33": {
    "doc": "On AWS",
    "title": "Why do I need the three DNS records?",
    "content": "Multiple DNS records are needed to access the two different lakeFS APIs (covered in more detail in the Architecture section): . | The lakeFS OpenAPI: used by the lakectl CLI tool. Exposes git-like operations (branching, diffing, merging etc.). | An S3-compatible API: read and write your data in any tool that can communicate with S3. Examples include: AWS CLI, Boto, Presto and Spark. | . lakeFS actually exposes only one API endpoint. For every request, lakeFS checks the Host header. If the header is under the S3 gateway domain, the request is directed to the S3-compatible API. The third DNS record (*.s3.lakefs.example.com) allows for virtual-host style access. This is a way for AWS clients to specify the bucket name in the Host subdomain. ",
    "url": "/deploy/aws.html#why-do-i-need-the-three-dns-records",
    "relUrl": "/deploy/aws.html#why-do-i-need-the-three-dns-records"
  },"34": {
    "doc": "On AWS",
    "title": "On AWS",
    "content": " ",
    "url": "/deploy/aws.html",
    "relUrl": "/deploy/aws.html"
  },"35": {
    "doc": "Add Data",
    "title": "Add Data",
    "content": "In this section we’ll review how to copy files into lakeFS using the AWS CLI. | If you don’t have the AWS CLI installed, follow the instructions here. | Configure a new connection profile using the credentials we generated earlier: . aws configure --profile local # output: # AWS Access Key ID [None]: AKIAJVHTOKZWGCD2QQYQ # AWS Secret Access Key [None]: **************************************** # Default region name [None]: # Default output format [None]: . | Let’s test to see that it works. We’ll do that by calling s3 ls which should list our repositories for us: . aws --endpoint-url=http://s3.local.lakefs.io:8000 --profile local s3 ls # output: # 2021-06-15 13:43:03 example-repo . Note: We’re using s3.local.lakefs.io - a special DNS record which always resolves to localhost, subdomains included. Since S3’s API uses subdomains for bucket addressing, simply using localhost:8000 as an endpoint URL will not work. | Great, now let’s copy some files. We’ll write to the main branch. This is done by prefixing our path with the name of the branch we’d like to read/write from: . aws --endpoint-url=http://s3.local.lakefs.io:8000 --profile local s3 cp ./foo.txt s3://example-repo/main/ # output: # upload: ./foo.txt to s3://example-repo/main/foo.txt . | Back in the lakeFS UI, we should be able to see our file added to the main branch! . | . Next steps . Now that your repository contains some data, what about using the lakeFS CLI for committing, branching and viewing the history? . ",
    "url": "/quickstart/aws_cli.html",
    "relUrl": "/quickstart/aws_cli.html"
  },"36": {
    "doc": "AWS CLI",
    "title": "Using lakeFS with AWS CLI",
    "content": "The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts. We could use the file commands for S3 to access lakeFS . ",
    "url": "/integrations/aws_cli.html#using-lakefs-with-aws-cli",
    "relUrl": "/integrations/aws_cli.html#using-lakefs-with-aws-cli"
  },"37": {
    "doc": "AWS CLI",
    "title": "Table of contents",
    "content": ". | Configuration | Path convention | Usage | Examples . | List directory | Copy from lakeFS to lakeFS | Copy from lakeFS to a local path | Copy from a local path to lakeFS | Delete file | Delete directory | . | Adding an alias | . ",
    "url": "/integrations/aws_cli.html#table-of-contents",
    "relUrl": "/integrations/aws_cli.html#table-of-contents"
  },"38": {
    "doc": "AWS CLI",
    "title": "Configuration",
    "content": "We would like to configure an AWS profile for lakeFS. In order to configure the lakeFS credentials run: . aws configure --profile lakefs . we will be prompted to enter AWS Access Key ID , AWS Secret Access Key . It should look like this: . aws configure --profile lakefs # output: # AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE # AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # Default region name [None]: # Default output format [None]: . ",
    "url": "/integrations/aws_cli.html#configuration",
    "relUrl": "/integrations/aws_cli.html#configuration"
  },"39": {
    "doc": "AWS CLI",
    "title": "Path convention",
    "content": "When accessing objects in s3 we will need to use the lakeFS path convention s3://[REPOSITORY]/[BRANCH]/PATH/TO/OBJECT . ",
    "url": "/integrations/aws_cli.html#path-convention",
    "relUrl": "/integrations/aws_cli.html#path-convention"
  },"40": {
    "doc": "AWS CLI",
    "title": "Usage",
    "content": "After configuring the credentials, This is how a command should look: . aws s3 --profile lakefs \\ --endpoint-url https://s3.lakefs.example.com \\ ls s3://example-repo/main/example-directory . Where endpoint-url should be the same value configured for gateways.s3.domain_name. We could use an alias to make it shorter and more convenient. ",
    "url": "/integrations/aws_cli.html#usage",
    "relUrl": "/integrations/aws_cli.html#usage"
  },"41": {
    "doc": "AWS CLI",
    "title": "Examples",
    "content": "List directory . aws --profile lakefs \\ --endpoint-url https://s3.lakefs.example.com \\ s3 ls s3://example-repo/main/example-directory . Copy from lakeFS to lakeFS . aws --profile lakefs \\ --endpoint-url https://s3.lakefs.example.com \\ s3 cp s3://example-repo/main/example-file-1 s3://example-repo/main/example-file-2 . Copy from lakeFS to a local path . aws --profile lakefs \\ --endpoint-url https://s3.lakefs.example.com \\ s3 cp s3://example-repo/main/example-file-1 /path/to/local/file . Copy from a local path to lakeFS . aws --profile lakefs \\ --endpoint-url https://s3.lakefs.example.com \\ s3 cp /path/to/local/file s3://example-repo/main/example-file-1 . Delete file . aws --profile lakefs \\ --endpoint-url https://s3.lakefs.example.com \\ s3 rm s3://example-repo/main/example-directory/example-file . Delete directory . aws --profile lakefs \\ --endpoint-url https://s3.lakefs.example.com \\ s3 rm s3://example-repo/main/example-directory/ --recursive . ",
    "url": "/integrations/aws_cli.html#examples",
    "relUrl": "/integrations/aws_cli.html#examples"
  },"42": {
    "doc": "AWS CLI",
    "title": "Adding an alias",
    "content": "In order to make the command shorter and more convenient we can create an alias: . alias awslfs='aws --endpoint https://s3.lakefs.example.com --profile lakefs' . Now, the ls command using the alias will be: . awslfs s3 ls s3://example-repo/main/example-directory . ",
    "url": "/integrations/aws_cli.html#adding-an-alias",
    "relUrl": "/integrations/aws_cli.html#adding-an-alias"
  },"43": {
    "doc": "AWS CLI",
    "title": "AWS CLI",
    "content": " ",
    "url": "/integrations/aws_cli.html",
    "relUrl": "/integrations/aws_cli.html"
  },"44": {
    "doc": "On Azure",
    "title": "Deploy lakeFS on Azure",
    "content": "Expected deployment time: 25min . ",
    "url": "/deploy/azure.html#deploy-lakefs-on-azure",
    "relUrl": "/deploy/azure.html#deploy-lakefs-on-azure"
  },"45": {
    "doc": "On Azure",
    "title": "Table of contents",
    "content": ". | Prerequisites | Creating the Database on Azure Database | Installation Options . | On Azure VM | On Azure Container instances | On AKS | . | Load balancing | DNS | Next Steps | . ",
    "url": "/deploy/azure.html#table-of-contents",
    "relUrl": "/deploy/azure.html#table-of-contents"
  },"46": {
    "doc": "On Azure",
    "title": "Prerequisites",
    "content": "A production-suitable lakeFS installation will require three DNS records pointing at your lakeFS server. A good convention for those will be, assuming you already own the domain example.com: . | lakefs.example.com | s3.lakefs.example.com - this is the S3 Gateway Domain | *.s3.lakefs.example.com | . The second record, the S3 Gateway Domain, needs to be specified in the lakeFS configuration (see the S3_GATEWAY_DOMAIN placeholder below). This will allow lakeFS to route requests to the S3-compatible API. For more info, see Why do I need these three DNS records? . ",
    "url": "/deploy/azure.html#prerequisites",
    "relUrl": "/deploy/azure.html#prerequisites"
  },"47": {
    "doc": "On Azure",
    "title": "Creating the Database on Azure Database",
    "content": "lakeFS requires a PostgreSQL database to synchronize actions on your repositories. We will show you how to create a database on Azure Database, but you can use any PostgreSQL database as long as it’s accessible by your lakeFS installation. If you already have a database, take note of the connection string and skip to the next step . | Follow the official Azure documentation on how to create a PostgreSQL instance and connect to it. Make sure you’re using PostgreSQL version &gt;= 11. | Once your Azure Database for PostgreSQL server is set up and the server is in Available state, take note of the endpoint and username. | Make sure your Access control roles allow you to connect to the database instance. | . ",
    "url": "/deploy/azure.html#creating-the-database-on-azure-database",
    "relUrl": "/deploy/azure.html#creating-the-database-on-azure-database"
  },"48": {
    "doc": "On Azure",
    "title": "Installation Options",
    "content": "On Azure VM . | Save the following configuration file as config.yaml: . --- database: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: # replace this with a randomly-generated string: secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: azure azure: auth_method: msi # msi for active directory, access-key for access key # In case you chose to authenticate via access key unmark the following rows and insert the values from the previous step # storage_account: [your storage account] # storage_access_key: [your access key] gateways: s3: # replace this with the host you will use for the lakeFS S3-compatible endpoint: domain_name: [S3_GATEWAY_DOMAIN] . | Download the binary to the Azure Virtual Machine. | Run the lakefs binary on the machine: lakefs --config config.yaml run . Note: it is preferable to run the binary as a service using systemd or your operating system’s facilities. | To support Azure AD authentication go to Identity tab and switch Status toggle to on, then add the `Storage Blob Data Contributor’ role on the container you created. | . On Azure Container instances . To support container-based environments like Azure Container Instances, lakeFS can be configured using environment variables. Here is a docker run command to demonstrate starting lakeFS using Docker: . docker run \\ --name lakefs \\ -p 8000:8000 \\ -e LAKEFS_DATABASE_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\ -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\ -e LAKEFS_BLOCKSTORE_TYPE=\"azure\" \\ -e LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCOUNT=\"[YOUR_STORAGE_ACCOUNT]\" \\ -e LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCESS_KEY=\"[YOUR_ACCESS_KEY]\" \\ -e LAKEFS_GATEWAYS_S3_DOMAIN_NAME=\"[S3_GATEWAY_DOMAIN]\" \\ treeverse/lakefs:latest run . See the reference for a complete list of environment variables. On AKS . See Kubernetes Deployment. ",
    "url": "/deploy/azure.html#installation-options",
    "relUrl": "/deploy/azure.html#installation-options"
  },"49": {
    "doc": "On Azure",
    "title": "Load balancing",
    "content": "Depending on how you chose to install lakeFS, you should have a load balancer direct requests to the lakeFS server. By default, lakeFS operates on port 8000, and exposes a /_health endpoint which you can use for health checks. ",
    "url": "/deploy/azure.html#load-balancing",
    "relUrl": "/deploy/azure.html#load-balancing"
  },"50": {
    "doc": "On Azure",
    "title": "DNS",
    "content": "As mentioned above, you should create 3 DNS records for lakeFS: . | One record for the lakeFS API: lakefs.example.com | Two records for the S3-compatible API: s3.lakefs.example.com and *.s3.lakefs.example.com. | . Depending on your DNS provider, refer to the documentation on how to add CNAME records. ",
    "url": "/deploy/azure.html#dns",
    "relUrl": "/deploy/azure.html#dns"
  },"51": {
    "doc": "On Azure",
    "title": "Next Steps",
    "content": "Your next step is to prepare your storage. If you already have a storage bucket/container, you are ready to create your first lakeFS repository. ",
    "url": "/deploy/azure.html#next-steps",
    "relUrl": "/deploy/azure.html#next-steps"
  },"52": {
    "doc": "On Azure",
    "title": "Why do I need the three DNS records?",
    "content": "Multiple DNS records are needed to access the two different lakeFS APIs (covered in more detail in the Architecture section): . | The lakeFS OpenAPI: used by the lakectl CLI tool. Exposes git-like operations (branching, diffing, merging etc.). | An S3-compatible API: read and write your data in any tool that can communicate with S3. Examples include: AWS CLI, Boto, Presto and Spark. | . lakeFS actually exposes only one API endpoint. For every request, lakeFS checks the Host header. If the header is under the S3 gateway domain, the request is directed to the S3-compatible API. The third DNS record (*.s3.lakefs.example.com) allows for virtual-host style access. This is a way for AWS clients to specify the bucket name in the Host subdomain. ",
    "url": "/deploy/azure.html#why-do-i-need-the-three-dns-records",
    "relUrl": "/deploy/azure.html#why-do-i-need-the-three-dns-records"
  },"53": {
    "doc": "On Azure",
    "title": "On Azure",
    "content": " ",
    "url": "/deploy/azure.html",
    "relUrl": "/deploy/azure.html"
  },"54": {
    "doc": "Azure Blob Storage",
    "title": "Prepare Your Blob Storage Container",
    "content": "Create a container in Azure portal: . | From the Azure portal, Storage Accounts, choose your account, then in the container tab click + Container. | Make sure you block public access | . ",
    "url": "/setup/storage/blob.html#prepare-your-blob-storage-container",
    "relUrl": "/setup/storage/blob.html#prepare-your-blob-storage-container"
  },"55": {
    "doc": "Azure Blob Storage",
    "title": "Authenticate with Secret Key",
    "content": "In case you want to use the secret key for authentication you will need to use the account key in the configuration Go to the Access Keys tab and click on Show Keys save the values under Storage account name and Key we will need them in the installing lakeFS step . ",
    "url": "/setup/storage/blob.html#authenticate-with-secret-key",
    "relUrl": "/setup/storage/blob.html#authenticate-with-secret-key"
  },"56": {
    "doc": "Azure Blob Storage",
    "title": "Authenticate with Active Directory",
    "content": "In case you want your lakeFS Installation (we will install in the next step) to access this Container using Active Directory authentication, First go to the container you created in step 1. | Go to Access Control (IAM) | Go to the Role assignments tab | Add the Storage Blob Data Contributor role to the Installation running lakeFS. | . You are now ready to create your first lakeFS repository. ",
    "url": "/setup/storage/blob.html#authenticate-with-active-directory",
    "relUrl": "/setup/storage/blob.html#authenticate-with-active-directory"
  },"57": {
    "doc": "Azure Blob Storage",
    "title": "Azure Blob Storage",
    "content": " ",
    "url": "/setup/storage/blob.html",
    "relUrl": "/setup/storage/blob.html"
  },"58": {
    "doc": "Boto (Python)",
    "title": "Using lakeFS with Boto (Python)",
    "content": " ",
    "url": "/integrations/boto.html#using-lakefs-with-boto-python",
    "relUrl": "/integrations/boto.html#using-lakefs-with-boto-python"
  },"59": {
    "doc": "Boto (Python)",
    "title": "Table of contents",
    "content": ". | Creating a Boto client | Usage Examples . | Put Object | List Objects | Head Object | . | . ",
    "url": "/integrations/boto.html#table-of-contents",
    "relUrl": "/integrations/boto.html#table-of-contents"
  },"60": {
    "doc": "Boto (Python)",
    "title": "Creating a Boto client",
    "content": "Create a Boto3 S3 client with your lakeFS endpoint and key-pair: . import boto3 s3 = boto3.client('s3', endpoint_url='https://s3.lakefs.example.com', aws_access_key_id='AKIAIOSFODNN7EXAMPLE', aws_secret_access_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY') . ",
    "url": "/integrations/boto.html#creating-a-boto-client",
    "relUrl": "/integrations/boto.html#creating-a-boto-client"
  },"61": {
    "doc": "Boto (Python)",
    "title": "Usage Examples",
    "content": "Put Object . Use a branch name and a path to put an object in lakeFS: . with open('/local/path/to/file_0', 'rb') as f: s3.put_object(Body=f, Bucket='example-repo', Key='main/example-file.parquet') . List Objects . List branch objects starting with a prefix: . list_resp = s3.list_objects_v2(Bucket='example-repo', Prefix='main/example-prefix') for obj in list_resp['Contents']: print(obj['Key']) . Or, use a lakeFS commit ID to list objects for a specific commit: . list_resp = s3.list_objects_v2(Bucket='example-repo', Prefix='c7a632d74f/example-prefix') for obj in list_resp['Contents']: print(obj['Key']) . Head Object . Get object metadata using branch and path: . s3.head_object(Bucket='example-repo', Key='main/example-file.parquet') # output: # {'ResponseMetadata': {'RequestId': '72A9EBD1210E90FA', # 'HostId': '', # 'HTTPStatusCode': 200, # 'HTTPHeaders': {'accept-ranges': 'bytes', # 'content-length': '1024', # 'etag': '\"2398bc5880e535c61f7624ad6f138d62\"', # 'last-modified': 'Sun, 24 May 2020 10:42:24 GMT', # 'x-amz-request-id': '72A9EBD1210E90FA', # 'date': 'Sun, 24 May 2020 10:45:42 GMT'}, # 'RetryAttempts': 0}, # 'AcceptRanges': 'bytes', # 'LastModified': datetime.datetime(2020, 5, 24, 10, 42, 24, tzinfo=tzutc()), # 'ContentLength': 1024, # 'ETag': '\"2398bc5880e535c61f7624ad6f138d62\"', # 'Metadata': {}} . ",
    "url": "/integrations/boto.html#usage-examples",
    "relUrl": "/integrations/boto.html#usage-examples"
  },"62": {
    "doc": "Boto (Python)",
    "title": "Boto (Python)",
    "content": " ",
    "url": "/integrations/boto.html",
    "relUrl": "/integrations/boto.html"
  },"63": {
    "doc": "Branching Model",
    "title": "Branching Model",
    "content": "At its core, lakeFS uses a Git-like branching model. ",
    "url": "/understand/branching-model.html",
    "relUrl": "/understand/branching-model.html"
  },"64": {
    "doc": "Branching Model",
    "title": "Table of contents",
    "content": ". | Repositories | Branches | Commits | Objects | . Repositories . In lakeFS, a repository is a logical namespace used to group together objects, branches and commits. It is the equivalent of a Bucket in S3, and a repository in Git. Branches . Branches are similar in concept to Git branches. When creating a new branch in lakeFS, we are actually creating a consistent snapshot of the entire repository, which is isolated from other branches and their changes. Another way to think of branches is like a very long-lived database transaction, providing us with Snapshot Isolation. Once we’ve made the necessary changes to our data within our isolated branch, we can merge it back to the branch we branched from. This operation is atomic in lakeFS - readers will either see all our committed changes or none at all. Isolation and Atomicity are very powerful tools: it allows us to do things that are otherwise extremely hard to get right: replace data in-place, add or update multiple objects and collections as a single piece, run tests and validations before exposing data to others and more. Commits . Commits are immutable “checkpoints”, containing an entire snapshot of a repository at a given point in time. This is again very similar to commits in Git. Each commit contains metadata - who performed it, timestamp, a commit message as well as arbitrary key/value pairs we can choose to add. Using commits, we can view our Data Lake at a certain point in its history and we are guaranteed that the data we see is exactly is it was at the point of committing it. In lakeFS, different users can view different branches (or even commits, directly) at the same time on the same repository. there’s no “checkout” process that copies data around. All live branches and commits are immediately available at all times. Objects . Objects in lakeFS are very similar to those found in S3 (or other object stores, for that matter). lakeFS is agnostic to what these objects contain: Parquet, CSV, ORC and even JPEG or other forms of unstructured data. Unlike Git, lakeFS does not care about the contents of an object - if we try to merge two branches that both update the same file, it is up to the user to resolve this conflict. This is because lakeFS doesn’t assume anything about the structure of the object and so cannot try to merge both changesets into a single object (additionally, this operation makes little sense for machine generated files, and data in general). The actual data itself is not stored inside lakeFS directly, but rather stored in an underlying object store. lakeFS will manage these writes, and will store a pointer to the object in its metadata database. ",
    "url": "/understand/branching-model.html#table-of-contents",
    "relUrl": "/understand/branching-model.html#table-of-contents"
  },"65": {
    "doc": "During Deployment",
    "title": "During Deployment",
    "content": "Every day we introduce new data to the lake. And even if the code and infra doesn’t change, the data might, and those changes introduce potential quality issues. This is one of the complexities of a data product; the data we consume changes over the course of a month, a week, day, hour, or even minute-to-minute. Examples of changes to data that may occur: . | A client-side bug in the data collection of website events | A new Android version that interferes with the collecting events from your App | COVID-19 abrupt impact on consumers’ behavior, and its effect on the accuracy of ML models. | During a change to Salesforce interface, the validation requirement from a certain field had been lost | . lakeFS enable CI/CD-inspired workflows to help validate expectations and assumptions about the data before it goes live in production or lands in the data environment. Example 1: Version Control - CI/CD deployment . Continuous deployment of existing data we expect to consume, flowing from ingest-pipelines into the lake. We merge data from an ingest branch (“events-data”), which allows us to create tests using data analysis tools or data quality services (e.g. Great Expectations, Monte Carlo) to ensure reliability of the data we merge to the main branch. Since merge is atomic, no performance issue will be introduced by using lakeFS, but your main branch will only include quality data. Each merge to the main branch creates a new commit on the main branch, which serves as a new version of the data. This allows us to easily revert to previous states of the data if a newer change introduces data issues. Example 2: Test - Validate new data . Examples of common validation checks enforced in organizations: . | No user_* columns except under /private/… | Only (*.parquet | *.orc | _delta_log/*.json) files allowed | Under /production, only backward-compatible schema changes are allowed | New tables on main must be registered in our metadata repository first, with owner and SLA | . lakeFS will assist in enforcing best practices by giving you a designated branch to ingest new data (“new-data-1” in the drawing). You may run automated tests to validate predefined best practices as pre-merge hooks. If the validation passes, the new data will be automatically and atomically merged to the main branch. However, if the validation fails, you will be alerted and the new data will not be exposed to consumers. By using this branching model and implementing best practices as pre merge hooks, you ensure the main lake is never compromised. ",
    "url": "/usecases/ci.html",
    "relUrl": "/usecases/ci.html"
  },"66": {
    "doc": "Command (CLI) Reference",
    "title": "Commands (CLI) Reference",
    "content": " ",
    "url": "/reference/commands.html#commands-cli-reference",
    "relUrl": "/reference/commands.html#commands-cli-reference"
  },"67": {
    "doc": "Command (CLI) Reference",
    "title": "Table of contents",
    "content": ". | Installing the lakectl command locally . | Configuring credentials and API endpoint | lakectl | lakectl abuse | lakectl abuse create-branches | lakectl abuse help | lakectl abuse random-read | lakectl abuse random-write | lakectl actions | lakectl actions help | lakectl actions runs | lakectl actions runs describe | lakectl actions runs help | lakectl actions runs list | lakectl actions validate | lakectl auth | lakectl auth groups | lakectl auth groups create | lakectl auth groups delete | lakectl auth groups help | lakectl auth groups list | lakectl auth groups members | lakectl auth groups members add | lakectl auth groups members help | lakectl auth groups members list | lakectl auth groups members remove | lakectl auth groups policies | lakectl auth groups policies attach | lakectl auth groups policies detach | lakectl auth groups policies help | lakectl auth groups policies list | lakectl auth help | lakectl auth policies | lakectl auth policies create | lakectl auth policies delete | lakectl auth policies help | lakectl auth policies list | lakectl auth policies show | lakectl auth users | lakectl auth users create | lakectl auth users credentials | lakectl auth users credentials create | lakectl auth users credentials delete | lakectl auth users credentials help | lakectl auth users credentials list | lakectl auth users delete | lakectl auth users groups | lakectl auth users groups help | lakectl auth users groups list | lakectl auth users help | lakectl auth users list | lakectl auth users policies | lakectl auth users policies attach | lakectl auth users policies detach | lakectl auth users policies help | lakectl auth users policies list | lakectl branch | lakectl branch create | lakectl branch delete | lakectl branch help | lakectl branch list | lakectl branch reset | lakectl branch revert | lakectl branch show | lakectl cat-hook-output | lakectl cat-sst | lakectl commit | lakectl completion | lakectl config | lakectl diff | lakectl docs | lakectl fs | lakectl fs cat | lakectl fs help | lakectl fs ls | lakectl fs rm | lakectl fs stage | lakectl fs stat | lakectl fs upload | lakectl gc | lakectl gc get-config | lakectl gc help | lakectl gc set-config | lakectl help | lakectl ingest | lakectl log | lakectl merge | lakectl metastore | lakectl metastore copy | lakectl metastore copy-all | lakectl metastore create-symlink | lakectl metastore diff | lakectl metastore help | lakectl metastore import-all | lakectl refs-dump | lakectl refs-restore | lakectl repo | lakectl repo create | lakectl repo create-bare | lakectl repo delete | lakectl repo help | lakectl repo list | lakectl show | lakectl tag | lakectl tag create | lakectl tag delete | lakectl tag help | lakectl tag list | lakectl tag show | . | . ",
    "url": "/reference/commands.html#table-of-contents",
    "relUrl": "/reference/commands.html#table-of-contents"
  },"68": {
    "doc": "Command (CLI) Reference",
    "title": "Installing the lakectl command locally",
    "content": "lakectl is distributed as a single binary, with no external dependencies - and is available for MacOS, Windows and Linux. Download lakectl . Configuring credentials and API endpoint . Once you’ve installed the lakectl command, run: . lakectl config # output: # Config file /home/janedoe/.lakectl.yaml will be used # Access key ID: AKIAIOSFODNN7EXAMPLE # Secret access key: **************************************** # Server endpoint URL: http://localhost:8000/api/v1 . This will setup a $HOME/.lakectl.yaml file with the credentials and API endpoint you’ve supplied. When setting up a new installation and creating initial credentials (see Quick start), the UI will provide a link to download a preconfigured configuration file for you. lakectl . A cli tool to explore manage and work with lakeFS . Synopsis . lakeFS is data lake management solution, allowing Git-like semantics over common object stores . lakectl is a CLI tool allowing exploration and manipulation of a lakeFS environment . Options . --base-uri string base URI used for lakeFS address parse -c, --config string config file (default is $HOME/.lakectl.yaml) -h, --help help for lakectl --log-format string set logging output format --log-level string set logging level (default \"none\") --log-output string set logging output file --no-color don't use fancy output colors (default when not attached to an interactive terminal) . lakectl abuse . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Abuse a running lakeFS instance. See sub commands for more info. Options . -h, --help help for abuse . lakectl abuse create-branches . Create a lot of branches very quickly. lakectl abuse create-branches &lt;source ref uri&gt; [flags] . Options . --amount int amount of things to do (default 1000000) --branch-prefix string prefix to create branches under (default \"abuse-\") --clean-only only clean up past runs -h, --help help for create-branches --parallelism int amount of things to do in parallel (default 100) . lakectl abuse help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type abuse help [path to command] for full details. lakectl abuse help [command] [flags] . Options . -h, --help help for help . lakectl abuse random-read . Read keys from a file and generate random reads from the source ref for those keys. lakectl abuse random-read &lt;source ref uri&gt; [flags] . Options . --amount int amount of reads to do (default 1000000) --from-file string read keys from this file (\"-\" for stdin) -h, --help help for random-read --parallelism int amount of reads to do in parallel (default 100) . lakectl abuse random-write . Generate random writes to the source branch . lakectl abuse random-write &lt;source branch uri&gt; [flags] . Options . --amount int amount of writes to do (default 1000000) -h, --help help for random-write --parallelism int amount of writes to do in parallel (default 100) --prefix string prefix to create paths under (default \"abuse/\") . lakectl actions . Manage Actions commands . Options . -h, --help help for actions . lakectl actions help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type actions help [path to command] for full details. lakectl actions help [command] [flags] . Options . -h, --help help for help . lakectl actions runs . Explore runs information . Options . -h, --help help for runs . lakectl actions runs describe . Describe run results . Synopsis . Show information about the run and all the hooks that were executed as part of the run . lakectl actions runs describe [flags] . Examples . lakectl actions runs describe lakefs://&lt;repository&gt; &lt;run_id&gt; . Options . --after string show results after this value (used for pagination) --amount int number of results to return. By default, all results are returned. -h, --help help for describe . lakectl actions runs help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type runs help [path to command] for full details. lakectl actions runs help [command] [flags] . Options . -h, --help help for help . lakectl actions runs list . List runs . Synopsis . List all runs on a repository optional filter by branch or commit . lakectl actions runs list [flags] . Examples . lakectl actions runs list lakefs://&lt;repository&gt; [--branch &lt;branch&gt;] [--commit &lt;commit_id&gt;] . Options . --after string show results after this value (used for pagination) --amount int number of results to return (default 100) --branch string show results for specific branch --commit string show results for specific commit ID -h, --help help for list . lakectl actions validate . Validate action file . Synopsis . Tries to parse the input action file as lakeFS action file . lakectl actions validate [flags] . Examples . lakectl actions validate &lt;path&gt; . Options . -h, --help help for validate . lakectl auth . Manage authentication and authorization . Synopsis . manage authentication and authorization including users, groups and policies . Options . -h, --help help for auth . lakectl auth groups . Manage groups . Options . -h, --help help for groups . lakectl auth groups create . Create a group . lakectl auth groups create [flags] . Options . -h, --help help for create --id string group identifier . lakectl auth groups delete . Delete a group . lakectl auth groups delete [flags] . Options . -h, --help help for delete --id string group identifier . lakectl auth groups help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type groups help [path to command] for full details. lakectl auth groups help [command] [flags] . Options . -h, --help help for help . lakectl auth groups list . List groups . lakectl auth groups list [flags] . Options . --after string show results after this value (used for pagination) --amount int how many results to return (default 100) -h, --help help for list . lakectl auth groups members . Manage group user memberships . Options . -h, --help help for members . lakectl auth groups members add . Add a user to a group . lakectl auth groups members add [flags] . Options . -h, --help help for add --id string group identifier --user string user identifier to add to the group . lakectl auth groups members help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type members help [path to command] for full details. lakectl auth groups members help [command] [flags] . Options . -h, --help help for help . lakectl auth groups members list . List users in a group . lakectl auth groups members list [flags] . Options . --after string show results after this value (used for pagination) --amount int how many results to return (default 100) -h, --help help for list --id string group identifier . lakectl auth groups members remove . Remove a user from a group . lakectl auth groups members remove [flags] . Options . -h, --help help for remove --id string group identifier --user string user identifier to add to the group . lakectl auth groups policies . Manage group policies . Options . -h, --help help for policies . lakectl auth groups policies attach . Attach a policy to a group . lakectl auth groups policies attach [flags] . Options . -h, --help help for attach --id string user identifier --policy string policy identifier . lakectl auth groups policies detach . Detach a policy from a group . lakectl auth groups policies detach [flags] . Options . -h, --help help for detach --id string user identifier --policy string policy identifier . lakectl auth groups policies help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type policies help [path to command] for full details. lakectl auth groups policies help [command] [flags] . Options . -h, --help help for help . lakectl auth groups policies list . List policies for the given group . lakectl auth groups policies list [flags] . Options . --after string show results after this value (used for pagination) --amount int how many results to return (default 100) -h, --help help for list --id string group identifier . lakectl auth help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type auth help [path to command] for full details. lakectl auth help [command] [flags] . Options . -h, --help help for help . lakectl auth policies . Manage policies . Options . -h, --help help for policies . lakectl auth policies create . Create a policy . lakectl auth policies create [flags] . Options . -h, --help help for create --id string policy identifier --statement-document string JSON statement document path (or \"-\" for stdin) . lakectl auth policies delete . Delete a policy . lakectl auth policies delete [flags] . Options . -h, --help help for delete --id string policy identifier . lakectl auth policies help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type policies help [path to command] for full details. lakectl auth policies help [command] [flags] . Options . -h, --help help for help . lakectl auth policies list . List policies . lakectl auth policies list [flags] . Options . --after string show results after this value (used for pagination) --amount int how many results to return (default 100) -h, --help help for list . lakectl auth policies show . Show a policy . lakectl auth policies show [flags] . Options . -h, --help help for show --id string policy identifier . lakectl auth users . Manage users . Options . -h, --help help for users . lakectl auth users create . Create a user . lakectl auth users create [flags] . Options . -h, --help help for create --id string user identifier . lakectl auth users credentials . Manage user credentials . Options . -h, --help help for credentials . lakectl auth users credentials create . Create user credentials . lakectl auth users credentials create [flags] . Options . -h, --help help for create --id string user identifier (default: current user) . lakectl auth users credentials delete . Delete user credentials . lakectl auth users credentials delete [flags] . Options . --access-key-id string access key ID to delete -h, --help help for delete --id string user identifier (default: current user) . lakectl auth users credentials help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type credentials help [path to command] for full details. lakectl auth users credentials help [command] [flags] . Options . -h, --help help for help . lakectl auth users credentials list . List user credentials . lakectl auth users credentials list [flags] . Options . --after string show results after this value (used for pagination) --amount int how many results to return (default 100) -h, --help help for list --id string user identifier (default: current user) . lakectl auth users delete . Delete a user . lakectl auth users delete [flags] . Options . -h, --help help for delete --id string user identifier . lakectl auth users groups . Manage user groups . Options . -h, --help help for groups . lakectl auth users groups help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type groups help [path to command] for full details. lakectl auth users groups help [command] [flags] . Options . -h, --help help for help . lakectl auth users groups list . List groups for the given user . lakectl auth users groups list [flags] . Options . --after string show results after this value (used for pagination) --amount int how many results to return (default 100) -h, --help help for list --id string user identifier . lakectl auth users help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type users help [path to command] for full details. lakectl auth users help [command] [flags] . Options . -h, --help help for help . lakectl auth users list . List users . lakectl auth users list [flags] . Options . --after string show results after this value (used for pagination) --amount int how many results to return (default 100) -h, --help help for list . lakectl auth users policies . Manage user policies . Options . -h, --help help for policies . lakectl auth users policies attach . Attach a policy to a user . lakectl auth users policies attach [flags] . Options . -h, --help help for attach --id string user identifier --policy string policy identifier . lakectl auth users policies detach . Detach a policy from a user . lakectl auth users policies detach [flags] . Options . -h, --help help for detach --id string user identifier --policy string policy identifier . lakectl auth users policies help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type policies help [path to command] for full details. lakectl auth users policies help [command] [flags] . Options . -h, --help help for help . lakectl auth users policies list . List policies for the given user . lakectl auth users policies list [flags] . Options . --after string show results after this value (used for pagination) --amount int how many results to return (default 100) --effective list all distinct policies attached to the user, even through group memberships -h, --help help for list --id string user identifier . lakectl branch . Create and manage branches within a repository . Synopsis . Create delete and list branches within a lakeFS repository . Options . -h, --help help for branch . lakectl branch create . Create a new branch in a repository . lakectl branch create &lt;ref uri&gt; [flags] . Options . -h, --help help for create -s, --source string source branch uri . lakectl branch delete . Delete a branch in a repository, along with its uncommitted changes (CAREFUL) . lakectl branch delete &lt;branch uri&gt; [flags] . Options . -h, --help help for delete -y, --yes Automatically say yes to all confirmations . lakectl branch help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type branch help [path to command] for full details. lakectl branch help [command] [flags] . Options . -h, --help help for help . lakectl branch list . List branches in a repository . lakectl branch list &lt;repository uri&gt; [flags] . Examples . lakectl branch list lakefs://&lt;repository&gt; . Options . --after string show results after this value (used for pagination) --amount int number of results to return (default 100) -h, --help help for list . lakectl branch reset . Reset changes to specified commit, or reset uncommitted changes - all changes, or by path . Synopsis . reset changes. There are four different ways to reset changes: . | reset all uncommitted changes - reset lakefs://myrepo/main | reset uncommitted changes under specific path - reset lakefs://myrepo/main –prefix path | reset uncommitted changes for specific object - reset lakefs://myrepo/main –object path | . lakectl branch reset &lt;branch uri&gt; [flags] . Options . -h, --help help for reset --object string path to object to be reset --prefix string prefix of the objects to be reset -y, --yes Automatically say yes to all confirmations . lakectl branch revert . Given a commit, record a new commit to reverse the effect of this commit . lakectl branch revert &lt;branch uri&gt; &lt;commit ref to revert&gt; [flags] . Options . -h, --help help for revert -m, --parent-number int the parent number (starting from 1) of the mainline. The revert will reverse the change relative to the specified parent. -y, --yes Automatically say yes to all confirmations . lakectl branch show . Show branch latest commit reference . lakectl branch show &lt;branch uri&gt; [flags] . Options . -h, --help help for show . lakectl cat-hook-output . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Cat actions hook output . lakectl cat-hook-output [flags] . Examples . lakectl cat-hook-output lakefs://&lt;repository&gt; &lt;run_id&gt; &lt;run_hook_id&gt; . Options . -h, --help help for cat-hook-output . lakectl cat-sst . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Explore lakeFS .sst files . lakectl cat-sst &lt;sst-file&gt; [flags] . Options . --amount int how many records to return, or -1 for all records (default -1) -f, --file string path to an sstable file, or \"-\" for stdin -h, --help help for cat-sst . lakectl commit . Commit changes on a given branch . lakectl commit &lt;branch uri&gt; [flags] . Options . -h, --help help for commit -m, --message string commit message --meta strings key value pair in the form of key=value . lakectl completion . Generate completion script . Synopsis . To load completions: . Bash: . $ source &lt;(lakectl completion bash) . To load completions for each session, execute once: Linux: . $ lakectl completion bash &gt; /etc/bash_completion.d/lakectl . MacOS: . $ lakectl completion bash &gt; /usr/local/etc/bash_completion.d/lakectl . Zsh: . If shell completion is not already enabled in your environment you will need to enable it. You can execute the following once: . $ echo \"autoload -U compinit; compinit\" &gt;&gt; ~/.zshrc . To load completions for each session, execute once: . $ lakectl completion zsh &gt; \"${fpath[1]}/_lakectl\" . You will need to start a new shell for this setup to take effect. Fish: . $ lakectl completion fish | source . To load completions for each session, execute once: . $ lakectl completion fish &gt; ~/.config/fish/completions/lakectl.fish . lakectl completion &lt;bash|zsh|fish&gt; . Options . -h, --help help for completion . lakectl config . Create/update local lakeFS configuration . lakectl config [flags] . Options . -h, --help help for config . lakectl diff . diff between commits/hashes . Synopsis . see the list of paths added/changed/removed in a branch or between two references (could be either commit hash or branch name) . lakectl diff &lt;ref uri&gt; [other ref uri] [flags] . Options . -h, --help help for diff . lakectl docs . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. lakectl docs [outfile] [flags] . Options . -h, --help help for docs . lakectl fs . View and manipulate objects . Options . -h, --help help for fs . lakectl fs cat . Dump content of object to stdout . lakectl fs cat &lt;path uri&gt; [flags] . Options . -d, --direct read directly from backing store (faster but requires more credentials) -h, --help help for cat . lakectl fs help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type fs help [path to command] for full details. lakectl fs help [command] [flags] . Options . -h, --help help for help . lakectl fs ls . List entries under a given tree . lakectl fs ls &lt;path uri&gt; [flags] . Options . -h, --help help for ls --recursive list all objects under the specified prefix . lakectl fs rm . Delete object . lakectl fs rm &lt;path uri&gt; [flags] . Options . -h, --help help for rm . lakectl fs stage . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Stage a reference to an existing object, to be managed in lakeFS . lakectl fs stage &lt;path uri&gt; [flags] . Options . --checksum string Object MD5 checksum as a hexadecimal string -h, --help help for stage --location string fully qualified storage location (i.e. \"s3://bucket/path/to/object\") --meta strings key value pairs in the form of key=value --mtime int Object modified time (Unix Epoch in seconds). Defaults to current time --size int Object size in bytes . lakectl fs stat . View object metadata . lakectl fs stat &lt;path uri&gt; [flags] . Options . -h, --help help for stat . lakectl fs upload . Upload a local file to the specified URI . lakectl fs upload &lt;path uri&gt; [flags] . Options . -d, --direct write directly to backing store (faster but requires more credentials) -h, --help help for upload -r, --recursive recursively copy all files under local source -s, --source string local file to upload, or \"-\" for stdin . lakectl gc . Manage garbage collection configuration . Options . -h, --help help for gc . lakectl gc get-config . Show garbage collection configuration JSON . lakectl gc get-config [flags] . Examples . lakectl gc get-config &lt;repository uri&gt; . Options . -h, --help help for get-config -p, --json get rules as JSON . lakectl gc help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type gc help [path to command] for full details. lakectl gc help [command] [flags] . Options . -h, --help help for help . lakectl gc set-config . Set garbage collection configuration JSON . Synopsis . Sets the garbage collection configuration JSON. Example configuration file: { “default_retention_days”: 21, “branches”: [ { “branch_id”: “main”, “retention_days”: 28 }, { “branch_id”: “dev”, “retention_days”: 14 } ] } . lakectl gc set-config [flags] . Examples . lakectl gc set-config &lt;repository uri&gt; -f config.json . Options . -f, --filename string file containing the GC configuration -h, --help help for set-config . lakectl help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type lakectl help [path to command] for full details. lakectl help [command] [flags] . Options . -h, --help help for help . lakectl ingest . Ingest objects from an external source into a lakeFS branch (without actually copying them) . lakectl ingest --from &lt;object store URI&gt; --to &lt;lakeFS path URI&gt; [--dry-run] [flags] . Options . -C, --concurrency int max concurrent API calls to make to the lakeFS server (default 64) --dry-run only print the paths to be ingested --from string prefix to read from (e.g. \"s3://bucket/sub/path/\") -h, --help help for ingest --to string lakeFS path to load objects into (e.g. \"lakefs://repo/branch/sub/path/\") -v, --verbose print stats for each individual object staged . lakectl log . Show log of commits . Synopsis . Show log of commits for a given branch . lakectl log &lt;branch uri&gt; [flags] . Options . --after string show results after this value (used for pagination) --amount int number of results to return. By default, all results are returned. -h, --help help for log --show-meta-range-id also show meta range ID . lakectl merge . Merge &amp; commit changes from source branch into destination branch . Synopsis . Merge &amp; commit changes from source branch into destination branch . lakectl merge &lt;source ref&gt; &lt;destination ref&gt; [flags] . Options . -h, --help help for merge . lakectl metastore . Manage metastore commands . Options . -h, --help help for metastore . lakectl metastore copy . Copy or merge table . Synopsis . copy or merge table. the destination table will point to the selected branch . lakectl metastore copy [flags] . Options . --catalog-id string Glue catalog ID --dbfs-root dbfs:/ dbfs location root will replace dbfs:/ in the location before transforming --from-client-type string metastore type [hive, glue] --from-schema string source schema name --from-table string source table name -h, --help help for copy --metastore-uri string Hive metastore URI -p, --partition strings partition to copy --serde string serde to set copy to [default is to-table] --to-branch string lakeFS branch name --to-client-type string metastore type [hive, glue] --to-schema string destination schema name [default is from-branch] --to-table string destination table name [default is from-table] . lakectl metastore copy-all . Copy from one metastore to another . Synopsis . copy or merge requested tables between hive metastores. the destination tables will point to the selected branch . lakectl metastore copy-all [flags] . Options . --branch string lakeFS branch name --continue-on-error prevent copy-all from failing when a single table fails --dbfs-root dbfs:/ dbfs location root will replace dbfs:/ in the location before transforming --from-address string source metastore address --from-client-type string metastore type [hive, glue] -h, --help help for copy-all --schema-filter string filter for schemas to copy in metastore pattern (default \".*\") --table-filter string filter for tables to copy in metastore pattern (default \".*\") --to-address string destination metastore address --to-client-type string metastore type [hive, glue] . lakectl metastore create-symlink . Create symlink table and data . Synopsis . create table with symlinks, and create the symlinks in s3 in order to access from external services that could only access s3 directly (e.g athena) . lakectl metastore create-symlink [flags] . Options . --branch string lakeFS branch name --catalog-id string Glue catalog ID --from-schema string source schema name --from-table string source table name -h, --help help for create-symlink --path string path to table on lakeFS --repo string lakeFS repository name --to-schema string destination schema name --to-table string destination table name . lakectl metastore diff . Show column and partition differences between two tables . lakectl metastore diff [flags] . Options . --catalog-id string Glue catalog ID --from-address string source metastore address --from-client-type string metastore type [hive, glue] --from-schema string source schema name --from-table string source table name -h, --help help for diff --metastore-uri string Hive metastore URI --to-address string destination metastore address --to-client-type string metastore type [hive, glue] --to-schema string destination schema name --to-table string destination table name [default is from-table] . lakectl metastore help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type metastore help [path to command] for full details. lakectl metastore help [command] [flags] . Options . -h, --help help for help . lakectl metastore import-all . Import from one metastore to another . Synopsis . import requested tables between hive metastores. the destination tables will point to the selected repository and branch table with location s3://my-s3-bucket/path/to/table will be transformed to location s3://repo-param/bucket-param/path/to/table . lakectl metastore import-all [flags] . Options . --branch string lakeFS branch name --continue-on-error prevent import-all from failing when a single table fails --dbfs-root dbfs:/ dbfs location root will replace dbfs:/ in the location before transforming --from-address string source metastore address --from-client-type string metastore type [hive, glue] -h, --help help for import-all --repo string lakeFS repo name --schema-filter string filter for schemas to copy in metastore pattern (default \".*\") --table-filter string filter for tables to copy in metastore pattern (default \".*\") --to-address string destination metastore address --to-client-type string metastore type [hive, glue] . lakectl refs-dump . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Dumps refs (branches, commits, tags) to the underlying object store . lakectl refs-dump &lt;repository uri&gt; [flags] . Options . -h, --help help for refs-dump . lakectl refs-restore . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Restores refs (branches, commits, tags) from the underlying object store to a bare repository . Synopsis . restores refs (branches, commits, tags) from the underlying object store to a bare repository. This command is expected to run on a bare repository (i.e. one created with ‘lakectl repo create-bare’). Since a bare repo is expected, in case of transient failure, delete the repository and recreate it as bare and retry. lakectl refs-restore &lt;repository uri&gt; [flags] . Examples . aws s3 cp s3://bucket/_lakefs/refs_manifest.json - | lakectl refs-load lakefs://my-bare-repository --manifest - . Options . -h, --help help for refs-restore --manifest refs-dump path to a refs manifest json file (as generated by refs-dump). Alternatively, use \"-\" to read from stdin . lakectl repo . Manage and explore repos . Options . -h, --help help for repo . lakectl repo create . Create a new repository . lakectl repo create &lt;repository uri&gt; &lt;storage namespace&gt; [flags] . Options . -d, --default-branch string the default branch of this repository (default \"main\") -h, --help help for create . lakectl repo create-bare . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Create a new repository with no initial branch or commit . lakectl repo create-bare &lt;repository uri&gt; &lt;storage namespace&gt; [flags] . Options . -d, --default-branch string the default branch name of this repository (will not be created) (default \"main\") -h, --help help for create-bare . lakectl repo delete . Delete existing repository . lakectl repo delete &lt;repository uri&gt; [flags] . Options . -h, --help help for delete -y, --yes Automatically say yes to all confirmations . lakectl repo help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type repo help [path to command] for full details. lakectl repo help [command] [flags] . Options . -h, --help help for help . lakectl repo list . List repositories . lakectl repo list [flags] . Options . --after string show results after this value (used for pagination) --amount int number of results to return (default 100) -h, --help help for list . lakectl show . See detailed information about an entity by ID (commit, user, etc) . lakectl show &lt;repository uri&gt; [flags] . Options . --commit string commit ID to show -h, --help help for show --show-meta-range-id when showing commits, also show meta range ID . lakectl tag . Create and manage tags within a repository . Synopsis . Create delete and list tags within a lakeFS repository . Options . -h, --help help for tag . lakectl tag create . Create a new tag in a repository . lakectl tag create &lt;tag uri&gt; &lt;commit ref&gt; [flags] . Options . -f, --force override the tag if it exists -h, --help help for create . lakectl tag delete . Delete a tag from a repository . lakectl tag delete &lt;tag uri&gt; [flags] . Options . -h, --help help for delete . lakectl tag help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type tag help [path to command] for full details. lakectl tag help [command] [flags] . Options . -h, --help help for help . lakectl tag list . List tags in a repository . lakectl tag list &lt;repository uri&gt; [flags] . Examples . lakectl tag list lakefs://&lt;repository&gt; . Options . --after string show results after this value (used for pagination) --amount int number of results to return (default 100) -h, --help help for list . lakectl tag show . Show tag’s commit reference . lakectl tag show &lt;tag uri&gt; [flags] . Options . -h, --help help for show . ",
    "url": "/reference/commands.html#installing-the-lakectl-command-locally",
    "relUrl": "/reference/commands.html#installing-the-lakectl-command-locally"
  },"69": {
    "doc": "Command (CLI) Reference",
    "title": "Command (CLI) Reference",
    "content": " ",
    "url": "/reference/commands.html",
    "relUrl": "/reference/commands.html"
  },"70": {
    "doc": "Configuration Reference",
    "title": "Configuration Reference",
    "content": " ",
    "url": "/reference/configuration.html",
    "relUrl": "/reference/configuration.html"
  },"71": {
    "doc": "Configuration Reference",
    "title": "Table of contents",
    "content": ". | Reference | Using Environment Variables | Example: Local Development | Example: AWS Deployment | Example: Google Storage | Example: MinIO | Example: Azure blob storage | . Configuring lakeFS is done using a yaml configuration file. This reference uses . to denote the nesting of values. ",
    "url": "/reference/configuration.html#table-of-contents",
    "relUrl": "/reference/configuration.html#table-of-contents"
  },"72": {
    "doc": "Configuration Reference",
    "title": "Reference",
    "content": ". | logging.format (one of [\"json\", \"text\"] : \"text\") - Format to output log message in | logging.level (one of [\"DEBUG\", \"INFO\", \"WARN\", \"ERROR\", \"NONE\"] : \"DEBUG\") - Logging level to output | logging.output (string : \"-\") - Path name to write logs to. \"-\" means Standard Output | database.connection_string (string : \"postgres://localhost:5432/postgres?sslmode=disable\") - PostgreSQL connection string to use | database.max_open_connections (int : 25) - Maximum number of open connections to the database | database.max_idle_connections (int : 25) - Sets the maximum number of connections in the idle connection pool | database.connection_max_lifetime (duration : 5m) - Sets the maximum amount of time a connection may be reused | listen_address (string : \"0.0.0.0:8000\") - A &lt;host&gt;:&lt;port&gt; structured string representing the address to listen on | auth.cache.enabled (bool : true) - Whether to cache access credentials and user policies in-memory. Can greatly improve throughput when enabled. | auth.cache.size (int : 1024) - How many items to store in the auth cache. Systems with a very high user count should use a larger value at the expense of ~1kb of memory per cached user. | auth.cache.ttl (time duration : \"20s\") - How long to store an item in the auth cache. Using a higher value reduces load on the database, but will cause changes longer to take effect for cached users. | auth.cache.jitter (time duration : \"3s\") - A random amount of time between 0 and this value is added to each item’s TTL. This is done to avoid a large bulk of keys expiring at once and overwhelming the database. | auth.encrypt.secret_key (string : required) - A random (cryptographically safe) generated string that is used for encryption and HMAC signing . Note: It is best to keep this somewhere safe such as KMS or Hashicorp Vault, and provide it to the system at run time . | blockstore.type (one of [\"local\", \"s3\", \"gs\", \"azure\", \"mem\"]: \"mem\") - Block adapter to use. This controls where the underlying data will be stored | blockstore.local.path (string: \"~/lakefs/data\") - When using the local Block Adapter, which directory to store files in | blockstore.gs.credentials_file (string : ) - If specified will be used as a file path of the JSON file that contains your Google service account key | blockstore.gs.credentials_json (string : ) - If specified will be used as JSON string that contains your Google service account key (when credentials_file is not set) | blockstore.azure.storage_account (string : ) - If specified, will be used as the Azure storage account | blockstore.azure.storage_access_key (string : ) - If specified, will be used as the Azure storage access key | blockstore.azure.auth_method (one of [\"msi\", \"access-key\"]: \"access-key\" ) - Authentication method to use (msi is used for Azure AD authentication). | blockstore.s3.region (string : \"us-east-1\") - When using the S3 block adapter, AWS region to use | blockstore.s3.profile (string : ) - If specified, will be used as a named credentials profile | blockstore.s3.credentials_file (string : ) - If specified, will be used as a credentials file | blockstore.s3.credentials.access_key_id (string : ) - If specified, will be used as a static set of credential | blockstore.s3.credentials.secret_access_key (string : ) - If specified, will be used as a static set of credential | blockstore.s3.credentials.session_token (string : ) - If specified, will be used as a static session token | blockstore.s3.endpoint (string : ) - If specified, custom endpoint for the AWS S3 API (https://s3_compatible_service_endpoint:port) | blockstore.s3.force_path_style (boolean : false) - When true, use path-style S3 URLs (https:/// instead of https://.) . | blockstore.s3.streaming_chunk_size (int : 1048576) - Object chunk size to buffer before streaming to blockstore (use a lower value for less reliable networks). Minimum is 8192. | blockstore.s3.streaming_chunk_timeout (time duration : \"60s\") - Per object chunk timeout for blockstore streaming operations (use a larger value for less reliable networks). | committed.local_cache - an object describing the local (on-disk) cache of metadata from permanent storage: . | committed.local_cache.size_bytes (int : 1073741824) - bytes for local cache to use on disk. The cache may use more storage for short periods of time. | committed.local_cache.dir (string, ~/lakefs/local_tier) - directory to store local cache. | committed.local_cache.range_proportion (float : 0.9) - proportion of local cache to use for storing ranges (leaves of committed metadata storage). | committed.local_cache.range.open_readers (int : 500) - maximal number of unused open SSTable readers to keep for ranges. | committed.local_cache.range.num_shards (int : 30) - sharding factor for open SSTable readers for ranges. Should be at least sqrt(committed.local_cache.range.open_readers). | committed.local_cache.metarange_proportion (float : 0.1) - proportion of local cache to use for storing metaranges (roots of committed metadata storage). | committed.local_cache.metarange.open_readers (int : 50) - maximal number of unused open SSTable readers to keep for metaranges. | committed.local_cache.metarange.num_shards (int : 10) - sharding factor for open SSTable readers for metaranges. Should be at least sqrt(committed.local_cache.metarange.open_readers). | . | committed.block_storage_prefix (string : _lakefs) - Prefix for metadata file storage in each repository’s storage namespace | committed.permanent.min_range_size_bytes (int : 0) - Smallest allowable range in metadata. Increase to somewhat reduce random access time on committed metadata, at the cost of increased committed metadata storage cost. | committed.permanent.max_range_size_bytes (int : 20971520) - Largest allowable range in metadata. Should be close to the size at which fetching from remote storage becomes linear. | committed.permanent.range_raggedness_entries (int : 50_000) - Average number of object pointers to store in each range (subject to min_range_size_bytes and max_range_size_bytes). | committed.sstable.memory.cache_size_bytes (int : 200_000_000) - maximal size of in-memory cache used for each SSTable reader. | gateways.s3.domain_name (string : \"s3.local.lakefs.io\") - a FQDN representing the S3 endpoint used by S3 clients to call this server (*.s3.local.lakefs.io always resolves to 127.0.0.1, useful for local development | gateways.s3.region (string : \"us-east-1\") - AWS region we’re pretending to be. Should match the region configuration used in AWS SDK clients | gateways.s3.fallback_url (string) - If specified, requests with a non-existing repository will be forwarded to this url. This can be useful for using lakeFS side-by-side with S3, with the URL pointing at an S3Proxy instance. | stats.enabled (boolean : true) - Whether or not to periodically collect anonymous usage statistics | . ",
    "url": "/reference/configuration.html#reference",
    "relUrl": "/reference/configuration.html#reference"
  },"73": {
    "doc": "Configuration Reference",
    "title": "Using Environment Variables",
    "content": "All configuration variables can be set or overridden using environment variables. To set an environment variable, prepend LAKEFS_ to its name, convert it to upper case, and replace . with _: . For example, logging.format becomes LAKEFS_LOGGING_FORMAT, blockstore.s3.region becomes LAKEFS_BLOCKSTORE_S3_REGION, etc. ",
    "url": "/reference/configuration.html#using-environment-variables",
    "relUrl": "/reference/configuration.html#using-environment-variables"
  },"74": {
    "doc": "Configuration Reference",
    "title": "Example: Local Development",
    "content": "--- listen_address: \"0.0.0.0:8000\" database: connection_string: \"postgres://localhost:5432/postgres?sslmode=disable\" logging: format: text level: DEBUG output: \"-\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc09e90b6641\" blockstore: type: local local: path: \"~/lakefs/dev/data\" gateways: s3: domain_name: s3.local.lakefs.io region: us-east-1 . ",
    "url": "/reference/configuration.html#example-local-development",
    "relUrl": "/reference/configuration.html#example-local-development"
  },"75": {
    "doc": "Configuration Reference",
    "title": "Example: AWS Deployment",
    "content": "--- logging: format: json level: WARN output: \"-\" database: connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: s3 s3: region: us-east-1 credentials_file: /secrets/aws/credentials profile: default gateways: s3: domain_name: s3.my-company.com region: us-east-1 . ",
    "url": "/reference/configuration.html#example-aws-deployment",
    "relUrl": "/reference/configuration.html#example-aws-deployment"
  },"76": {
    "doc": "Configuration Reference",
    "title": "Example: Google Storage",
    "content": "--- logging: format: json level: WARN output: \"-\" database: connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: gs gs: credentials_file: /secrets/lakefs-service-account.json gateways: s3: domain_name: s3.my-company.com region: us-east-1 . ",
    "url": "/reference/configuration.html#example-google-storage",
    "relUrl": "/reference/configuration.html#example-google-storage"
  },"77": {
    "doc": "Configuration Reference",
    "title": "Example: MinIO",
    "content": "--- logging: format: json level: WARN output: \"-\" database: connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: s3 s3: region: us-east-1 force_path_style: true endpoint: http://localhost:9000 credentials: access_key_id: minioadmin secret_access_key: minioadmin gateways: s3: domain_name: s3.my-company.com region: us-east-1 . ",
    "url": "/reference/configuration.html#example-minio",
    "relUrl": "/reference/configuration.html#example-minio"
  },"78": {
    "doc": "Configuration Reference",
    "title": "Example: Azure blob storage",
    "content": "--- logging: format: json level: WARN output: \"-\" database: connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: azure azure: auth_method: access-key storage_account: exampleStorageAcount storage_access_key: ExampleAcessKeyMD7nkPOWgV7d4BUjzLw== gateways: s3: domain_name: s3.my-company.com region: us-east-1 . ",
    "url": "/reference/configuration.html#example-azure-blob-storage",
    "relUrl": "/reference/configuration.html#example-azure-blob-storage"
  },"79": {
    "doc": "Contributing",
    "title": "Contributing to lakeFS",
    "content": "Thank you for your interest in contributing to our project. Whether it’s a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.. If you don’t know where to start, please join our community on Slack and ask us. We will help you get started! . ",
    "url": "/contributing.html#contributing-to-lakefs",
    "relUrl": "/contributing.html#contributing-to-lakefs"
  },"80": {
    "doc": "Contributing",
    "title": "Ground Rules",
    "content": "Before you get started, we ask that you: . | Check out the code of conduct. | Sign the lakeFS CLA when making your first pull request (individual / corporate) | Submit any security issues directly to security@treeverse.io | . ",
    "url": "/contributing.html#ground-rules",
    "relUrl": "/contributing.html#ground-rules"
  },"81": {
    "doc": "Contributing",
    "title": "Getting Started",
    "content": "Want to report a bug or request a feature? Please open an issue . Working on your first Pull Request? You can learn how from this free series, How to Contribute to an Open Source Project on GitHub. ",
    "url": "/contributing.html#getting-started",
    "relUrl": "/contributing.html#getting-started"
  },"82": {
    "doc": "Contributing",
    "title": "Setting up an Environment",
    "content": "This section was tested on macOS and Linux (Fedora 32, Ubuntu 20.04) - Your mileage may vary . Our Go release workflow holds the Go and Node.js versions we currently use under go-version and node-version compatibly. The Java workflows use Maven 3.8.1 (but any recent version of Maven should work). | Install the required dependencies for your OS: . | Git | GNU make (probably best to install from your OS package manager such as apt or brew) | Docker | Go | Node.js &amp; npm | Maven to build and test Spark client codes. | Optional - PostgreSQL 11 (useful for running and debugging locally) | . | Install statik: . go get github.com/rakyll/statik . Make sure (go env GOPATH)/bin is in your $PATH (or at least, that the statik binary is). | Clone the repository from https://github.com/treeverse/lakeFS (gives you read-only access to the repository. To contribute, see the next section). | Build the project: . make build . | Make sure tests are passing: . make test . | . ",
    "url": "/contributing.html#setting-up-an-environment",
    "relUrl": "/contributing.html#setting-up-an-environment"
  },"83": {
    "doc": "Contributing",
    "title": "Before creating a pull request",
    "content": ". | Review this document in full | Make sure there’s an open issue on GitHub that this pull request addresses, and that it isn’t labeled x/wontfix | Fork the lakeFS repository | If you’re adding new functionality, create a new branch named feature/&lt;DESCRIPTIVE NAME&gt; | If you’re fixing a bug, create a new branch named fix/&lt;DESCRIPTIVE NAME&gt;-&lt;ISSUE NUMBER&gt; | . ",
    "url": "/contributing.html#before-creating-a-pull-request",
    "relUrl": "/contributing.html#before-creating-a-pull-request"
  },"84": {
    "doc": "Contributing",
    "title": "Creating a pull request",
    "content": "Once you’ve made the necessary changes to the code, make sure tests pass: . make test . Check linting rules are passing: . make checks-validator . lakeFS uses go fmt as a style guide for Go code. ",
    "url": "/contributing.html#creating-a-pull-request",
    "relUrl": "/contributing.html#creating-a-pull-request"
  },"85": {
    "doc": "Contributing",
    "title": "After submitting your pull request",
    "content": "After submitting your pull request, GitHub Actions will automatically run tests on your changes and make sure that your updated code builds and runs on Go 1.16.2. Check back shortly after submitting your pull request to make sure that your code passes these checks. If any of the checks come back with a red X, then do your best to address the errors. ",
    "url": "/contributing.html#after-submitting-your-pull-request",
    "relUrl": "/contributing.html#after-submitting-your-pull-request"
  },"86": {
    "doc": "Contributing",
    "title": "Documentation",
    "content": "Documentation of features and changes in behaviour should be included in the pull-request. You can create separate pull requests for documentation changes only. Documentation site customizations should be performed in accordance with the Just The Docs Customization guide, which is applied during the site creation process. ",
    "url": "/contributing.html#documentation",
    "relUrl": "/contributing.html#documentation"
  },"87": {
    "doc": "Contributing",
    "title": "Contributing",
    "content": " ",
    "url": "/contributing.html",
    "relUrl": "/contributing.html"
  },"88": {
    "doc": "Create a Repository",
    "title": "Create a Repository",
    "content": " ",
    "url": "/setup/create-repo.html",
    "relUrl": "/setup/create-repo.html"
  },"89": {
    "doc": "Create a Repository",
    "title": "Create the first user",
    "content": "Once we have lakeFS configured and running, open https://&lt;OPENAPI_SERVER_ENDPOINT&gt;/setup (e.g. https://lakefs.example.com). Note: If you already have lakeFS credentials, skip to step 2 and login. | Follow the steps to create an initial administrator user. Save the credentials you’ve received somewhere safe, you won’t be able to see them again! . | Follow the link and go to the login screen . | Use the credentials from step #1 to login as an administrator . | . ",
    "url": "/setup/create-repo.html#create-the-first-user",
    "relUrl": "/setup/create-repo.html#create-the-first-user"
  },"90": {
    "doc": "Create a Repository",
    "title": "Create the repository",
    "content": ". | Click Create Repository . Under Storage Namespace, be sure to set the path to the bucket you’ve configured in a previous step. | . ",
    "url": "/setup/create-repo.html#create-the-repository",
    "relUrl": "/setup/create-repo.html#create-the-repository"
  },"91": {
    "doc": "Create a Repository",
    "title": "Next steps",
    "content": "After creating a repo, you can import your existing data into it. lakeFS offers an Import API to bring your data without copying it. Alternatively, if you wish to copy existing data from an S3 bucket to lakeFS, use DistCp or Rclone. Check out the usage guides under Integrations for other options. ",
    "url": "/setup/create-repo.html#next-steps",
    "relUrl": "/setup/create-repo.html#next-steps"
  },"92": {
    "doc": "In Development",
    "title": "In Development",
    "content": "As part of our routine work with data we develop new code, improve and upgrade old code, upgrade infrastructures, and test new technologies. lakeFS enables a safe development environment on your data lake without the need to copy or mock data, work on the pipelines or involve DevOps. Creating a branch provides you an isolated environment with a snapshot of your repository (any part of your data lake you chose to manage on lakeFS). While working on your own branch in isolation, all other data users will be looking at the repository’s main branch. They can’t see your changes, and you don’t see changes to main done after you created the branch. No worries, no data duplication is done, it’s all metadata management behind the scenes. Let’s look at 3 examples of a development environment and their branching models. Example 1: Upgrading Spark and using Reset action . You installed the latest version of Apache Spark. As a first step you’ll test your Spark jobs to see that the upgrade doesn’t have any undesired side effects. For this purpose, you may create a branch (testing-spark-3.0) which will only be used to test the Spark upgrade, and discarded later. Jobs may run smoothly (the theoretical possibility exists!), or they may fail halfway through, leaving you with some intermediate partitions, data and metadata. In this case, you can simply reset the branch to its original state, without worrying about the intermediate results of your last experiment, and perform another (hopefully successful) test in an isolated branch. Reset actions are atomic and immediate, so no manual cleanup is required. Once testing is completed, and you have achieved the desired result, you can delete this experimental branch, and all data not used on any other branch will be deleted with it. Creating a testing branch: . lakectl branch create \\ lakefs://example-repo/testing-spark-3 \\ --source lakefs://example-repo/main # output: # created branch 'testing-spark-3', pointing to commit ID: '~79RU9aUsQ9GLnU' . Resetting changes to a branch: . lakectl branch reset lakefs://example-repo/testing-spark-3 # are you sure you want to reset all uncommitted changes?: y█ . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. Example 2: Collaborate &amp; Compare - Which option is better? . Easily compare by testing which one performs better on your data set. Examples may be: . | Different computation tools, e.g Spark vs. Presto | Different compression algorithms | Different Spark configurations | Different code versions of an ETL | . Run each experiment on its own independent branch, while the main remains untouched. Once both experiments are done, create a comparison query (using Hive or Presto or any other tool of your choice) to compare data characteristics, performance or any other metric you see fit. With lakeFS you don’t need to worry about creating data paths for the experiments, copying data, and remembering to delete it. It’s substantially easier to avoid errors and maintain a clean lake after. Reading from and comparing branches using Spark: . val dfExperiment1 = sc.read.parquet(\"s3a://example-repo/experiment-1/events/by-date\") val dfExperiment2 = sc.read.parquet(\"s3a://example-repo/experiment-2/events/by-date\") dfExperiment1.groupBy(\"...\").count() dfExperiment2.groupBy(\"...\").count() // now we can compare the properties of the data itself . ",
    "url": "/usecases/data-devenv.html",
    "relUrl": "/usecases/data-devenv.html"
  },"93": {
    "doc": "Data Model",
    "title": "Data Model",
    "content": " ",
    "url": "/understand/data-model.html",
    "relUrl": "/understand/data-model.html"
  },"94": {
    "doc": "Data Model",
    "title": "Table of contents",
    "content": ". | Overview | SSTable File Format (“Graveler File”) | Constructing a consistent view of the keyspace (i.e., a commit) | Representing references and uncommitted data | . ",
    "url": "/understand/data-model.html#table-of-contents",
    "relUrl": "/understand/data-model.html#table-of-contents"
  },"95": {
    "doc": "Data Model",
    "title": "Overview",
    "content": "Since commits in lakeFS are immutable, they are easy to store on an immutable object store. Older commits are rarely accessed, while newer commits are accessed very frequently, a tiered storage approach can work very well - The object store is the source of truth, while local disk and even RAM can be used to cache the more frequently accessed ones. Since they are immutable - once cached, we only need to evict them when space is running out. There’s no complex invalidation that needs to happen. In terms of storage format, commits are be stored as SSTables, compatible with RocksDB. SSTables were chosen as a storage format for 3 major reasons: . | Extremely high read throughput on modern hardware: using commits representing a 200m object repository (modeled after the S3 inventory of one of our design partners), we were able to achieve close to 500k random GetObject calls / second. This provides a very high throughput/cost ratio, probably as high as can be achieved on public clouds. | Being a known storage format means it’s relatively easy to generate and consume. Storing it in the object store makes it accessible to data engineering tools for analysis and distributed computation, effectively reducing the silo effect of storing it in an operational database. | The SSTable format supports delta encoding for keys which makes them very space efficient for data lakes where many keys share the same common prefixes. | . Each lakeFS commit is represented as a set of contiguous, non-overlapping SSTables that make up the entire keyspace of a repository at that commit. ",
    "url": "/understand/data-model.html#overview",
    "relUrl": "/understand/data-model.html#overview"
  },"96": {
    "doc": "Data Model",
    "title": "SSTable File Format (“Graveler File”)",
    "content": "lakeFS metadata is encoded into a format called “Graveler” - a standardized way to encode content-addressable key value pairs. This is what a Graveler file looks like: . Each Key/Value pair (“ValueRecord”) is constructed of a key, an identity and a value. A simple identity could be, for example, a sha256 hash of the value’s bytes, but it could be any sequence of bytes that uniquely identifies the value. As far as Graveler is concerned, two ValueRecords are considered identical, if their key and identity fields are equal. A Graveler file itself is content-addressable, i.e. similarly to Git, the name of the file is its identity. File identity is calculated based on the identity of the ValueRecords the file contains: . valueRecordID = h(h(valueRecord.key) || h(valueRecord.Identity)) fileID = h(valueRecordID1 + … + valueRecordIDN) . ",
    "url": "/understand/data-model.html#sstable-file-format-graveler-file",
    "relUrl": "/understand/data-model.html#sstable-file-format-graveler-file"
  },"97": {
    "doc": "Data Model",
    "title": "Constructing a consistent view of the keyspace (i.e., a commit)",
    "content": "We have 2 additional requirements for the storage format: . | Be space and time efficient when creating a commit - assuming a commit changes a single object out of a billion, we don’t want to write a full snapshot of the entire repository. Ideally, we’ll be able to reuse some data files that haven’t changed to make the commit operations (in both space and time) proportional to the size of the difference as opposed to the total size of the repository. | Allow an efficient diff between commits which runs in time proportional to the size of their difference and not their absolute sizes. | . To support these requirements, we decided to essentially build a 2-layer Merkle tree composed of a set of leaf nodes (“Range”) addressed by their content address, and a “Meta Range”, which is a special range containing all ranges, thus representing an entire consistent view of the keyspace: . Assuming commit B is derived from commit A, and only changed files in range e-f, it can reuse all ranges except for SSTable #N (the one containing the modified range of keys), which will be recreated with a new hash, representing the state as exists after applying commit B’s changes. This will in turn, also create a new Metarange since its hash is now changed as well (as it is derived from the hash of all contained ranges). Assuming most commits usually change related objects (i.e. that are likely to share some common prefix), the reuse ratio could be very high. We tested this assumption using S3 inventory from 2 design partners - we partitioned the keyspace to an arbitrary number of simulated blocks and measured their change over time. We saw a daily change rate of about 5-20%. Given the size of the repositories, it is safe to assume that a single day would translate into multiple commits. At a modest 20 commits per day, a commit is expected to reuse &gt;= 99% of the previous commit blocks, so acceptable in terms of write amplification generated on commit. On the object store, ranges are stored in the following hierarchy: . &lt;lakefs root&gt; _lakefs/ &lt;range hash1&gt; &lt;range hash2&gt; &lt;range hashN&gt; ... &lt;metarange hash1&gt; &lt;metarange hash2&gt; &lt;metarange hashN&gt; ... &lt;data object hash1&gt; &lt;data object hash2&gt; &lt;data object hashN&gt; ... Note: this relatively flat structure could be modified in the future: looking at the diagram above, it imposes no real limitations on the depth of the tree. A tree could easily be made recursive by having Meta Ranges point to other Meta Ranges - and still provide all the same characteristics. For simplicity, we decided to start with a fixed 2-level hierarchy. ",
    "url": "/understand/data-model.html#constructing-a-consistent-view-of-the-keyspace-ie-a-commit",
    "relUrl": "/understand/data-model.html#constructing-a-consistent-view-of-the-keyspace-ie-a-commit"
  },"98": {
    "doc": "Data Model",
    "title": "Representing references and uncommitted data",
    "content": "Unlike committed data which is immutable, uncommitted (or “staged”) data experiences frequent random writes and is very mutable in nature. This is also true for “refs” - in particular, branches, which are simply pointers to an underlying commit, are modified frequently: on every commit or merge operation. Both these types of data are not only mutable, but also require strong consistency guarantees while also being fault tolerant. If we can’t access the current pointer of the main branch, a big portion of the system is essentially down. Luckily, this is also much smaller data, compared to the committed dataset. References and uncommitted data are currently stored on PostgreSQL for its strong consistency and transactional guarantees. In the future we plan on eliminating the need for an RDBMS by embedding Raft to replicate these writes across a cluster of machines, with the data itself being stored in RocksDB. To make operations easier, the replicated RocksDB database will be periodically snapshotted to the underlying object store. For extremely large installations ( &gt;= millions of read/write operations per second), it will be possible to utilize multi-Raft to shard references across a wider fleet of machines. ",
    "url": "/understand/data-model.html#representing-references-and-uncommitted-data",
    "relUrl": "/understand/data-model.html#representing-references-and-uncommitted-data"
  },"99": {
    "doc": "Databricks",
    "title": "Using lakeFS with Databricks",
    "content": "Databricks is an Apache Spark-based analytics platform. ",
    "url": "/integrations/databricks.html#using-lakefs-with-databricks",
    "relUrl": "/integrations/databricks.html#using-lakefs-with-databricks"
  },"100": {
    "doc": "Databricks",
    "title": "Table of contents",
    "content": ". | Configuration . | When running lakeFS inside your VPC . | Using multi-cluster writes | . | . | Reading Data | Writing Data | Case Study: SimilarWeb | . ",
    "url": "/integrations/databricks.html#table-of-contents",
    "relUrl": "/integrations/databricks.html#table-of-contents"
  },"101": {
    "doc": "Databricks",
    "title": "Configuration",
    "content": "For Databricks to work with lakeFS, set the S3 Hadoop configuration to the lakeFS endpoint and credentials: . | In databricks, go to your cluster configuration page. | Click Edit. | Expand Advanced Options | Under the Spark tab, add the following configurations, replacing &lt;repo-name&gt; with your lakeFS repository name. Also replace the credentials and endpoint with those of your lakeFS installation. | . spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.access.key AKIAIOSFODNN7EXAMPLE spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.secret.key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.endpoint https://s3.lakefs.example.com . When using DeltaLake tables, the following is also needed in some versions of Databricks: . spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.aws.credentials.provider shaded.databricks.org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.session.token lakefs . For more information, see the documentation from Databricks. When running lakeFS inside your VPC . When lakeFS runs inside your private network, your Databricks cluster needs to be able to access it. This can be done by setting up a VPC peering between the two VPCs (the one where lakeFS runs, and the one where Databricks runs). For this to work on DeltaLake tables, you would also have to disable multi-cluster writes with: . spark.databricks.delta.multiClusterWrites.enabled false . Using multi-cluster writes . When using multi-cluster writes, Databricks overrides Delta’s s3-commit action. The new action tries to contact lakeFS from servers on Databricks own AWS account, which of course will not be able to access your private network. So, if you must use multi-cluster writes, your will have to allow access from Databricks’ AWS account to lakeFS. We are researching for the best ways to achieve that, and will update here soon. ",
    "url": "/integrations/databricks.html#configuration",
    "relUrl": "/integrations/databricks.html#configuration"
  },"102": {
    "doc": "Databricks",
    "title": "Reading Data",
    "content": "In order for us to access objects in lakeFS we will need to use the lakeFS path conventions: s3a://[REPOSITORY]/[BRANCH]/PATH/TO/OBJECT . Here is an example for reading a parquet file from lakeFS to a Spark DataFrame: . val repo = \"example-repo\" val branch = \"main\" val dataPath = s\"s3a://${repo}/${branch}/example-path/example-file.parquet\" val df = spark.read.parquet(dataPath) . You can now use this DataFrame like you would normally do. ",
    "url": "/integrations/databricks.html#reading-data",
    "relUrl": "/integrations/databricks.html#reading-data"
  },"103": {
    "doc": "Databricks",
    "title": "Writing Data",
    "content": "Now simply write your results back to a lakeFS path: . df.write .partitionBy(\"example-column\") .parquet(s\"s3a://${repo}/${branch}/output-path/\") . The data is now created in lakeFS as new changes in your branch. You can now commit these changes, or revert them. ",
    "url": "/integrations/databricks.html#writing-data",
    "relUrl": "/integrations/databricks.html#writing-data"
  },"104": {
    "doc": "Databricks",
    "title": "Case Study: SimilarWeb",
    "content": "See how SimilarWeb integrated lakeFS with DataBricks. ",
    "url": "/integrations/databricks.html#case-study-similarweb",
    "relUrl": "/integrations/databricks.html#case-study-similarweb"
  },"105": {
    "doc": "Databricks",
    "title": "Databricks",
    "content": " ",
    "url": "/integrations/databricks.html",
    "relUrl": "/integrations/databricks.html"
  },"106": {
    "doc": "Delta Lake",
    "title": "Using lakeFS with Delta Lake",
    "content": "Delta Lake is an open file format designed to improve performance and provide transactional guarantees to data lake tables. lakeFS is format-agnostic, so you can save data in Delta format within a lakeFS repository to get the benefits of both technologies. Specifically: . | ACID operations can now span across many Delta tables. | CI/CD hooks can validate Delta table contents, schema, or even referential integrity. | lakeFS supports zero-copy branching for quick experimentation with full isolation. | . ",
    "url": "/integrations/delta.html#using-lakefs-with-delta-lake",
    "relUrl": "/integrations/delta.html#using-lakefs-with-delta-lake"
  },"107": {
    "doc": "Delta Lake",
    "title": "Table of contents",
    "content": ". | Configuration | Limitations | Read more | . ",
    "url": "/integrations/delta.html#table-of-contents",
    "relUrl": "/integrations/delta.html#table-of-contents"
  },"108": {
    "doc": "Delta Lake",
    "title": "Configuration",
    "content": "Most commonly Delta tables are interacted with in a Spark environment given the native integration between Delta Lake and Spark. To configure a Spark environment to read from and write to a Delta table within a lakeFS repository, we need to set the proper credentials and endpoint in the S3 Hadoop configuration, like we do with any Spark script. sc.hadoopConfiguration.set(\"spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.access.key\", \"AKIAIOSFODNN7EXAMPLE\") sc.hadoopConfiguration.set(\"spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\") sc.hadoopConfiguration.set(\"spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.endpoint\", \"https://s3.lakefs.example.com\") . Once set, you can now interact with Delta tables using regular Spark path URIs. Make sure you include the lakeFS repository and branch name: . data.write.format(\"delta\").save(\"s3a://&lt;repo-name&gt;/&lt;branch-name&gt;/path/to/delta-table\") . Note: If using the Databricks Analytics Platform, see the integration guide for configuring a Databricks cluster to use lakeFS. ",
    "url": "/integrations/delta.html#configuration",
    "relUrl": "/integrations/delta.html#configuration"
  },"109": {
    "doc": "Delta Lake",
    "title": "Limitations",
    "content": "The Delta log is an auto-generated sequence of text files used to keep track of transactions on a Delta table sequentially. Writing to one Delta table from multiple lakeFS branches is possible, but note that it will result in conflicts if later attempting to merge one branch into the other. For that reason, production workflows should ideally write to a single lakeFS branch that could then be safely merged into main. ",
    "url": "/integrations/delta.html#limitations",
    "relUrl": "/integrations/delta.html#limitations"
  },"110": {
    "doc": "Delta Lake",
    "title": "Read more",
    "content": "See this post on the lakeFS blog that shows how to guarantee data quality in a Delta table by utilizing lakeFS branches. ",
    "url": "/integrations/delta.html#read-more",
    "relUrl": "/integrations/delta.html#read-more"
  },"111": {
    "doc": "Delta Lake",
    "title": "Delta Lake",
    "content": " ",
    "url": "/integrations/delta.html",
    "relUrl": "/integrations/delta.html"
  },"112": {
    "doc": "Copying Data with DistCp",
    "title": "Copying Data to/from lakeFS with DistCp",
    "content": "Apache Hadoop DistCp (distributed copy) is a tool used for large inter/intra-cluster copying. You can easily use it with your lakeFS repositories. ",
    "url": "/integrations/distcp.html#copying-data-tofrom-lakefs-with-distcp",
    "relUrl": "/integrations/distcp.html#copying-data-tofrom-lakefs-with-distcp"
  },"113": {
    "doc": "Copying Data with DistCp",
    "title": "Table of contents",
    "content": ". | Copying from lakeFS to lakeFS | Copying between S3 and lakeFS . | From S3 to lakeFs | From lakeFS to S3 | . | . Note In the following examples we set AWS credentials on the command line, for clarity. In production, these properties should be set using one of Hadoop’s standard ways of Authenticating with S3. ",
    "url": "/integrations/distcp.html#table-of-contents",
    "relUrl": "/integrations/distcp.html#table-of-contents"
  },"114": {
    "doc": "Copying Data with DistCp",
    "title": "Copying from lakeFS to lakeFS",
    "content": "You can use DistCP to copy between two different lakeFS repositories. Replace the access key pair with your lakeFS access key pair: . hadoop distcp \\ -Dfs.s3a.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\ -Dfs.s3a.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\ -Dfs.s3a.endpoint=\"https://s3.lakefs.example.com\" \\ \"s3a://example-repo-1/main/example-file.parquet\" \\ \"s3a://example-repo-2/main/example-file.parquet\" . val workDir = s”s3a://${repo}/${branch}/collection/shows” val dataPath = s”$workDir/title.basics.parquet” . ",
    "url": "/integrations/distcp.html#copying-from-lakefs-to-lakefs",
    "relUrl": "/integrations/distcp.html#copying-from-lakefs-to-lakefs"
  },"115": {
    "doc": "Copying Data with DistCp",
    "title": "Copying between S3 and lakeFS",
    "content": "In order to copy between an S3 bucket and lakeFS repository, use Hadoop’s per-bucket configuration. In the following examples, replace the first access key pair with your lakeFS key pair, and the second one with your AWS IAM key pair: . From S3 to lakeFs . hadoop distcp \\ -Dfs.s3a.bucket.example-repo.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\ -Dfs.s3a.bucket.example-repo.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\ -Dfs.s3a.bucket.example-repo.endpoint=\"https://s3.lakefs.example.com\" \\ -Dfs.s3a.bucket.example-bucket.access.key=\"AKIAIOSFODNN3EXAMPLE\" \\ -Dfs.s3a.bucket.example-bucket.secret.key=\"wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY\" \\ \"s3a://example-bucket/example-file.parquet\" \\ \"s3a://example-repo/main/example-file.parquet\" . From lakeFS to S3 . hadoop distcp \\ -Dfs.s3a.bucket.example-repo.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\ -Dfs.s3a.bucket.example-repo.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\ -Dfs.s3a.bucket.example-repo.endpoint=\"https://s3.lakefs.example.com\" \\ -Dfs.s3a.bucket.example-bucket.access.key=\"AKIAIOSFODNN3EXAMPLE\" \\ -Dfs.s3a.bucket.example-bucket.secret.key=\"wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY\" \\ \"s3a://example-repo/main/myfile\" \\ \"s3a://example-bucket/myfile\" . ",
    "url": "/integrations/distcp.html#copying-between-s3-and-lakefs",
    "relUrl": "/integrations/distcp.html#copying-between-s3-and-lakefs"
  },"116": {
    "doc": "Copying Data with DistCp",
    "title": "Copying Data with DistCp",
    "content": " ",
    "url": "/integrations/distcp.html",
    "relUrl": "/integrations/distcp.html"
  },"117": {
    "doc": "With Docker",
    "title": "Deploy lakeFS on Docker",
    "content": " ",
    "url": "/deploy/docker.html#deploy-lakefs-on-docker",
    "relUrl": "/deploy/docker.html#deploy-lakefs-on-docker"
  },"118": {
    "doc": "With Docker",
    "title": "Database",
    "content": "lakeFS requires a PostgreSQL database to synchronize actions on your repositories. This section assumes you already have a PostgreSQL database accessible from where you intend to install lakeFS. Instructions for creating the database can be found on the deployment instructions for AWS, Azure and GCP. ",
    "url": "/deploy/docker.html#database",
    "relUrl": "/deploy/docker.html#database"
  },"119": {
    "doc": "With Docker",
    "title": "Table of contents",
    "content": ". | Prerequisites | Installing on Docker | Load balancing | DNS | Next Steps | . ",
    "url": "/deploy/docker.html#table-of-contents",
    "relUrl": "/deploy/docker.html#table-of-contents"
  },"120": {
    "doc": "With Docker",
    "title": "Prerequisites",
    "content": "A production-suitable lakeFS installation will require three DNS records pointing at your lakeFS server. A good convention for those will be, assuming you already own the domain example.com: . | lakefs.example.com | s3.lakefs.example.com - this is the S3 Gateway Domain | *.s3.lakefs.example.com | . The second record, the S3 Gateway Domain, needs to be specified in the lakeFS configuration (see the S3_GATEWAY_DOMAIN placeholder below). This will allow lakeFS to route requests to the S3-compatible API. For more info, see Why do I need these three DNS records? . ",
    "url": "/deploy/docker.html#prerequisites",
    "relUrl": "/deploy/docker.html#prerequisites"
  },"121": {
    "doc": "With Docker",
    "title": "Installing on Docker",
    "content": "To deploy using Docker, create a yaml configuration file. Here is a minimal example, but you can see the reference for the full list of configurations. | AWS | Google Cloud | Microsoft Azure | . database: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: s3 gateways: s3: domain_name: \"[S3_GATEWAY_DOMAIN]\" . database: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: gs # Uncomment the following lines to give lakeFS access to your buckets using a service account: # gs: # credentials_json: [YOUR SERVICE ACCOUNT JSON STRING] gateways: s3: domain_name: \"[S3_GATEWAY_DOMAIN]\" . database: connection_string: \"postgres://user:pass@&lt;AZURE_POSTGRES_SERVER_NAME&gt;...\" auth: encrypt: secret_key: \"&lt;RANDOM_GENERATED_STRING&gt;\" blockstore: type: azure azure: auth_method: msi # msi for active directory, access-key for access key # In case you chose to authenticate via access key replace unmark the following rows and insert the values from the previous step # storage_account: &lt;your storage account&gt; # storage_access_key: &lt;your access key&gt; gateways: s3: domain_name: s3.lakefs.example.com . Save the configuration file locally as lakefs-config.yaml and run the following command: . docker run \\ --name lakefs \\ -p 8000:8000 \\ -v $(pwd)/lakefs-config.yaml:/home/lakefs/.lakefs.yaml \\ treeverse/lakefs:latest run . ",
    "url": "/deploy/docker.html#installing-on-docker",
    "relUrl": "/deploy/docker.html#installing-on-docker"
  },"122": {
    "doc": "With Docker",
    "title": "Load balancing",
    "content": "You should have a load balancer direct requests to the lakeFS server. By default, lakeFS operates on port 8000, and exposes a /_health endpoint which you can use for health checks. ",
    "url": "/deploy/docker.html#load-balancing",
    "relUrl": "/deploy/docker.html#load-balancing"
  },"123": {
    "doc": "With Docker",
    "title": "DNS",
    "content": "As mentioned above, you should create 3 DNS records for lakeFS: . | One record for the lakeFS API: lakefs.example.com | Two records for the S3-compatible API: s3.lakefs.example.com and *.s3.lakefs.example.com. | . All records should point to your Load Balancer, preferably with a short TTL value. ",
    "url": "/deploy/docker.html#dns",
    "relUrl": "/deploy/docker.html#dns"
  },"124": {
    "doc": "With Docker",
    "title": "Next Steps",
    "content": "Your next step is to prepare your storage. If you already have a storage bucket/container, you are ready to create your first lakeFS repository. ",
    "url": "/deploy/docker.html#next-steps",
    "relUrl": "/deploy/docker.html#next-steps"
  },"125": {
    "doc": "With Docker",
    "title": "Why do I need the three DNS records?",
    "content": "Multiple DNS records are needed to access the two different lakeFS APIs (covered in more detail in the Architecture section): . | The lakeFS OpenAPI: used by the lakectl CLI tool. Exposes git-like operations (branching, diffing, merging etc.). | An S3-compatible API: read and write your data in any tool that can communicate with S3. Examples include: AWS CLI, Boto, Presto and Spark. | . lakeFS actually exposes only one API endpoint. For every request, lakeFS checks the Host header. If the header is under the S3 gateway domain, the request is directed to the S3-compatible API. The third DNS record (*.s3.lakefs.example.com) allows for virtual-host style access. This is a way for AWS clients to specify the bucket name in the Host subdomain. ",
    "url": "/deploy/docker.html#why-do-i-need-the-three-dns-records",
    "relUrl": "/deploy/docker.html#why-do-i-need-the-three-dns-records"
  },"126": {
    "doc": "With Docker",
    "title": "With Docker",
    "content": " ",
    "url": "/deploy/docker.html",
    "relUrl": "/deploy/docker.html"
  },"127": {
    "doc": "Dremio",
    "title": "Using lakeFS with Dremio",
    "content": "Dremio is a next-generation data lake engine that liberates your data with live, interactive queries directly on cloud data lake storage, including S3 and lakeFS. ",
    "url": "/integrations/dremio.html#using-lakefs-with-dremio",
    "relUrl": "/integrations/dremio.html#using-lakefs-with-dremio"
  },"128": {
    "doc": "Dremio",
    "title": "Configuration",
    "content": "Starting from 3.2.3, Dremio supports Minio as an experimental S3-compatible plugin. Similarly, we can connect lakeFS with Dremio. Suppose you already have both lakeFS and Dremio deployed, and want to utilize Dremio to query your data in the lakeFS repositories. You can follow below steps to configure on Dremio UI: . | click Add Data Lake. | Under File Stores, choose Amazon S3. | Under Advanced Options, check Enable compatibility mode (experimental). | Under Advanced Options &gt; Connection Properties, add fs.s3a.path.style.access and set the value to true. | Under Advanced Options &gt; Connection Properties, add fs.s3a.endpoint and set lakeFS S3 endpoint to the value. | Under the General tab, specify the access_key_id and secret_access_key provided by lakeFS server. | Click Save, and now you should be able to browse lakeFS repositories on Dremio. | . ",
    "url": "/integrations/dremio.html#configuration",
    "relUrl": "/integrations/dremio.html#configuration"
  },"129": {
    "doc": "Dremio",
    "title": "Dremio",
    "content": " ",
    "url": "/integrations/dremio.html",
    "relUrl": "/integrations/dremio.html"
  },"130": {
    "doc": "EMR",
    "title": "Using lakeFS with EMR",
    "content": "Amazon EMR is a managed cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark. ",
    "url": "/integrations/emr.html#using-lakefs-with-emr",
    "relUrl": "/integrations/emr.html#using-lakefs-with-emr"
  },"131": {
    "doc": "EMR",
    "title": "Configuration",
    "content": "In order to configure Spark on EMR to work with lakeFS we will set the lakeFS credentials and endpoint in the appropriate fields. The exact configuration keys depends on the application running in EMR, but their format is of the form: . lakeFS endpoint: *.fs.s3a.endpoint . lakeFS access key: *.fs.s3a.access.key . lakeFS secret key: *.fs.s3a.secret.key . EMR will encourage users to use s3:// with Spark as it will use EMR’s proprietary driver. Users need to use s3a:// for this guide to work. The Spark job reads and writes will be directed to the lakeFS instance, using the s3 gateway. There are 2 options for configuring an EMR cluster to work with lakeFS: . | When you create a cluster - All steps will use the cluster configuration. No specific configuration needed when adding a step. | Configuring on each step - cluster is created with the default s3 configuration. Each step using lakeFS should pass the appropriate config params. | . ",
    "url": "/integrations/emr.html#configuration",
    "relUrl": "/integrations/emr.html#configuration"
  },"132": {
    "doc": "EMR",
    "title": "Configuration on cluster creation",
    "content": "Use the below configuration when creating the cluster. You may delete any app configuration which is not suitable for your use-case. [ { \"Classification\": \"presto-connector-hive\", \"Properties\": { \"hive.s3.aws-access-key\": \"AKIAIOSFODNN7EXAMPLE\", \"hive.s3.aws-secret-key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"hive.s3.endpoint\": \"https://s3.lakefs.example.com\", \"hive.s3-file-system-type\": \"PRESTO\" } }, { \"Classification\": \"hive-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://s3.lakefs.example.com\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://s3.lakefs.example.com\" } }, { \"Classification\": \"hdfs-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://s3.lakefs.example.com\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://s3.lakefs.example.com\" } }, { \"Classification\": \"core-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://s3.lakefs.example.com\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://s3.lakefs.example.com\" } }, { \"Classification\": \"emrfs-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://s3.lakefs.example.com\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://s3.lakefs.example.com\" } }, { \"Classification\": \"mapred-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://s3.lakefs.example.com\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://s3.lakefs.example.com\" } }, { \"Classification\": \"spark-defaults\", \"Properties\": { \"spark.sql.catalogImplementation\": \"hive\" } } ] . ",
    "url": "/integrations/emr.html#configuration-on-cluster-creation",
    "relUrl": "/integrations/emr.html#configuration-on-cluster-creation"
  },"133": {
    "doc": "EMR",
    "title": "Configuration on adding a step",
    "content": "When a cluster was created without the above configuration, you can still use lakeFS when adding a step. For example, when creating a Spark job: . aws emr add-steps --cluster-id j-197B3AEGQ9XE4 \\ --steps=\"Type=Spark,Name=SparkApplication,ActionOnFailure=CONTINUE, \\ Args=[--conf,spark.hadoop.fs.s3a.access.key=AKIAIOSFODNN7EXAMPLE, \\ --conf,spark.hadoop.fs.s3a.secret.key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY, \\ --conf,spark.hadoop.fs.s3a.endpoint=https://s3.lakefs.example.com, \\ s3a://&lt;lakefs-repo&gt;/&lt;lakefs-branch&gt;/path/to/jar]\" . The Spark context in the running job will already be initialized to use the provided lakeFS configuration. There’s no need to repeat the configuration steps mentioned in Using lakeFS with Spark . ",
    "url": "/integrations/emr.html#configuration-on-adding-a-step",
    "relUrl": "/integrations/emr.html#configuration-on-adding-a-step"
  },"134": {
    "doc": "EMR",
    "title": "EMR",
    "content": " ",
    "url": "/integrations/emr.html",
    "relUrl": "/integrations/emr.html"
  },"135": {
    "doc": "Exporting Data",
    "title": "Exporting Data",
    "content": "The export operation copies all data from a given lakeFS commit to a designated object store location. For instance, the contents lakefs://example/main might be exported on s3://company-bucket/example/latest. Clients entirely unaware of lakeFS could use that base URL to access latest files on main. Clients aware of lakeFS can continue to use the lakeFS S3 endpoint to access repository files on s3://example/main, as well as other versions and uncommitted versions. Possible use-cases: . | External consumers of data don’t have access to your lakeFS installation. | Some data pipelines in the organization are not fully migrated to lakeFS. | You want to experiment with lakeFS as a side-by-side installation first. | Create copies of your data lake in other regions (taking into account read pricing). | . ",
    "url": "/reference/export.html",
    "relUrl": "/reference/export.html"
  },"136": {
    "doc": "Exporting Data",
    "title": "Table of contents",
    "content": ". | How to use . | Using spark-submit | Using custom code (notebook/spark) | . | Success/Failure Indications | Export Rounds (Spark success files) | Example | . ",
    "url": "/reference/export.html#table-of-contents",
    "relUrl": "/reference/export.html#table-of-contents"
  },"137": {
    "doc": "Exporting Data",
    "title": "How to use",
    "content": "Using spark-submit . You can use the export main in 3 different modes: . | Export all objects from branch example-branch on example-repo repository to s3 location s3://example-bucket/prefix/: .... example-repo s3://example-bucket/prefix/ --branch=example-branch . | Export all objects from a commit c805e49bafb841a0875f49cd555b397340bbd9b8 on example-repo repository to s3 location s3://example-bucket/prefix/: .... example-repo s3://example-bucket/prefix/ --commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8 . | Export only the diff between branch example-branch and commit c805e49bafb841a0875f49cd555b397340bbd9b8 on example-repo repository to s3 location s3://example-bucket/prefix/: .... example-repo s3://example-bucket/prefix/ --branch=example-branch --prev_commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8 . | . The complete spark-submit command would look like: . spark-submit --conf spark.hadoop.lakefs.api.url=https://&lt;LAKEFS_ENDPOINT&gt;/api/v1 \\ --conf spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY_ID&gt; \\ --conf spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_ACCESS_KEY&gt; \\ --packages io.lakefs:lakefs-spark-client-301_2.12:0.1.0 \\ --class io.treeverse.clients.Main export-app example-repo s3://example-bucket/prefix \\ --branch=example-branch . The command assumes the spark cluster has permissions to write to s3://example-bucket/prefix. Otherwise, add spark.hadoop.fs.s3a.access.key and spark.hadoop.fs.s3a.secret.key with the proper credentials. Using custom code (notebook/spark) . Set up lakeFS Spark metadata client with the endpoint and credentials as instructed in the previous page. The client exposes the Exporter object with 3 export options: . | Export all objects at the HEAD of a given branch. Does not include files that were added to that branch, but were not committed. | . exportAllFromBranch(branch: String) . | Export ALL objects from a commit: | . exportAllFromCommit(commitID: String) . | Export just the diff between a commit and the HEAD of a branch. This is the ideal option for continuous exports of a branch, as it will change only the files that have been changed since the previous commit. exportFrom(branch: String, prevCommitID: String) . | . ",
    "url": "/reference/export.html#how-to-use",
    "relUrl": "/reference/export.html#how-to-use"
  },"138": {
    "doc": "Exporting Data",
    "title": "Success/Failure Indications",
    "content": "When the Spark export operation ends, an additional status file will be added to the root object storage destination. If all files were exported successfully the file path will be of form: EXPORT_&lt;commitID&gt;_&lt;ISO-8601-time-UTC&gt;_SUCCESS. For failures: the form will beEXPORT_&lt;commitID&gt;_&lt;ISO-8601-time-UTC&gt;_FAILURE, and the file will include a log of the failed files operations. ",
    "url": "/reference/export.html#successfailure-indications",
    "relUrl": "/reference/export.html#successfailure-indications"
  },"139": {
    "doc": "Exporting Data",
    "title": "Export Rounds (Spark success files)",
    "content": "Some files should be exported before others, e.g. a Spark _SUCCESS file exported before other files under the same prefix might send the wrong indication. The export operation may contain several rounds within the same export. A failing round will stop the export of all the files of the next rounds. By default, lakeFS will use the SparkFilter and have 2 rounds for each export. The first round will export any non Spark _SUCCESS files. Second round will export all Spark’s _SUCCESS files. You may override the default behaviour by passing a custom filter to the Exporter. ",
    "url": "/reference/export.html#export-rounds-spark-success-files",
    "relUrl": "/reference/export.html#export-rounds-spark-success-files"
  },"140": {
    "doc": "Exporting Data",
    "title": "Example",
    "content": ". | First configure the Exporter instance: . import io.treeverse.clients.{ApiClient, Exporter} import org.apache.spark.sql.SparkSession val endpoint = \"http://&lt;LAKEFS_ENDPOINT&gt;/api/v1\" val accessKey = \"&lt;LAKEFS_ACCESS_KEY_ID&gt;\" val secretKey = \"&lt;LAKEFS_SECRET_ACCESS_KEY&gt;\" val repo = \"example-repo\" val spark = SparkSession.builder().appName(\"I can export\").master(\"local\").getOrCreate() val sc = spark.sparkContext sc.hadoopConfiguration.set(\"lakefs.api.url\", endpoint) sc.hadoopConfiguration.set(\"lakefs.api.access_key\", accessKey) sc.hadoopConfiguration.set(\"lakefs.api.secret_key\", secretKey) // Add any required spark context configuration for s3 val rootLocation = \"s3://company-bucket/example/latest\" val apiClient = new ApiClient(endpoint, accessKey, secretKey) val exporter = new Exporter(spark, apiClient, repo, rootLocation) . | Now you can export all objects from main branch to s3://company-bucket/example/latest: . val branch = \"main\" exporter.exportAllFromBranch(branch) . | Assuming a previous successful export on commit f3c450d8cd0e84ac67e7bc1c5dcde9bef82d8ba7, you can alternatively export just the difference between main branch and the commit: . val branch = \"main\" val commit = \"f3c450d8cd0e84ac67e7bc1c5dcde9bef82d8ba7\" exporter.exportFrom(branch, commit) . | . ",
    "url": "/reference/export.html#example",
    "relUrl": "/reference/export.html#example"
  },"141": {
    "doc": "FAQ",
    "title": "FAQ",
    "content": "1. Is lakeFS open source? . lakeFS is completely free and open source and licensed under the Apache 2.0 License. We maintain a public product roadmap and Slack channel for open discussions. 2. How does lakeFS data versioning work? . lakeFS uses a copy-on-write mechanism to avoid data duplication. For example, creating a new branch is a metadata-only operation: no objects are actually copied. Only when an object changes does lakeFS create another version of the data in the storage. For more information, see Data Model. 3. How do I get support for my lakeFS installation? . We are extremely responsive on our slack channel, and we make sure to prioritize and with the community the issues most urgent for it. For SLA based support, please contact us at support@treeverse.io. 4. Do you collect data from your active installations? . We collect anonymous usage statistics in order to understand the patterns of use and to detect product gaps we may have so we can fix them. This is completely optional and may be turned off by setting stats.enabled to false. See the configuration reference for more details. 5. How is lakeFS different from Delta Lake / Hudi / Iceberg? . Delta Lake, Hudi and Iceberg all define dedicated, structured data formats that allow deletes and upserts. lakeFS is format-agnostic and enables consistent cross-collection versioning of your data using git-like operations. Read our blog for a more detailed comparison. 6. What inspired the lakeFS logo? . The Axolotl – a species of salamander, also known as the Mexican Lake Monster or the Peter Pan of the animal kingdom. It’s a magical creature, living in a lake, just like us :-). copyright . ",
    "url": "/faq.html",
    "relUrl": "/faq.html"
  },"142": {
    "doc": "Garbage Collection",
    "title": "Garbage Collection",
    "content": "By default, lakeFS keeps all your objects forever. This allows you to travel back in time to previous versions of your data. However, sometimes you may want to hard-delete your objects, namely delete them from the underlying storage. Reasons for this include cost-reduction and privacy policies. Garbage collection rules in lakeFS define for how long to retain objects after they have been deleted (see more information below). After running a GC job, objects that have been deleted prior to the retention period are hard-deleted. The GC job does not remove any commits: you will still be able to use commits containing hard-deleted objects, but trying to read these objects from lakeFS will result in a 410 Gone HTTP status. ",
    "url": "/reference/garbage-collection.html",
    "relUrl": "/reference/garbage-collection.html"
  },"143": {
    "doc": "Garbage Collection",
    "title": "Understanding Garbage Collection",
    "content": "For every branch, the GC job retains deleted objects for the number of days defined for the branch. In the absence of a branch-specific rule, the default rule for the repository is used. If an object is present in more than one branch ancestry, it is retained according to the rule with the largest number of days between those branches. That is, it is hard-deleted only after the retention period has ended for all relevant branches. Example GC rules for a repository: . { \"default_retention_days\": 21, \"branches\": [ {\"branch_id\": \"main\", \"retention_days\": 28}, {\"branch_id\": \"dev\", \"retention_days\": 7} ] } . In the above example, objects are retained for 21 days after deletion by default. However, if they are present in the branch main, they are retained for 28 days. Objects present in the dev branch (but not in any other branch), are retained for 7 days after they are deleted. ",
    "url": "/reference/garbage-collection.html#understanding-garbage-collection",
    "relUrl": "/reference/garbage-collection.html#understanding-garbage-collection"
  },"144": {
    "doc": "Garbage Collection",
    "title": "Configuring GC rules",
    "content": "Use the lakectl CLI to define the GC rules: . cat &lt;&lt;EOT &gt;&gt; example_repo_gc_rules.json { \"default_retention_days\": 21, \"branches\": [ {\"branch_id\": \"main\", \"retention_days\": 28}, {\"branch_id\": \"dev\", \"retention_days\": 7} ] } EOT lakectl gc set-config lakefs://example-repo -f example_repo_gc_rules.json . ",
    "url": "/reference/garbage-collection.html#configuring-gc-rules",
    "relUrl": "/reference/garbage-collection.html#configuring-gc-rules"
  },"145": {
    "doc": "Garbage Collection",
    "title": "Running the GC job",
    "content": "spark-submit --class io.treeverse.clients.GarbageCollector \\ -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1 \\ -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\ -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\ -c spark.hadoop.fs.s3a.access.key=&lt;S3_ACCESS_KEY&gt; \\ -c spark.hadoop.fs.s3a.secret.key=&lt;S3_SECRET_KEY&gt; \\ --packages io.lakefs:lakefs-spark-client-301_2.12:0.5.0 \\ example-repo us-east-1 . ",
    "url": "/reference/garbage-collection.html#running-the-gc-job",
    "relUrl": "/reference/garbage-collection.html#running-the-gc-job"
  },"146": {
    "doc": "Garbage Collection",
    "title": "Considerations",
    "content": ". | In order for an object to be hard-deleted, it must be deleted from all branches. You should remove stale branches to prevent them from retaining old objects. For example, consider a branch that has been merged to main and has become stale. An object which is later deleted from main will always be present in the stale branch, preventing it from being hard-deleted. | lakeFS will never delete objects outside your repository’s storage namespace. In particular, objects that were imported using lakefs import or lakectl ingest will not be affected by GC jobs. | In cases where deleted objects are brought back to life while a GC job is running, said objects may or may not be deleted. Such actions include: . | Reverting a commit in which a file was deleted. | Branching out from an old commit. | Expanding the retention period of a branch. | Creating a branch from an existing branch, where the new branch has a longer retention period. | . | . ",
    "url": "/reference/garbage-collection.html#considerations",
    "relUrl": "/reference/garbage-collection.html#considerations"
  },"147": {
    "doc": "On GCP",
    "title": "Deploy lakeFS on GCP",
    "content": "Expected deployment time: 25min . ",
    "url": "/deploy/gcp.html#deploy-lakefs-on-gcp",
    "relUrl": "/deploy/gcp.html#deploy-lakefs-on-gcp"
  },"148": {
    "doc": "On GCP",
    "title": "Table of contents",
    "content": ". | Prerequisites | Creating the Database on GCP SQL | Installation Options . | On Google Compute Engine | On Google Cloud Run | On GKE | . | Load balancing | DNS | Next Steps | . ",
    "url": "/deploy/gcp.html#table-of-contents",
    "relUrl": "/deploy/gcp.html#table-of-contents"
  },"149": {
    "doc": "On GCP",
    "title": "Prerequisites",
    "content": "A production-suitable lakeFS installation will require three DNS records pointing at your lakeFS server. A good convention for those will be, assuming you already own the domain example.com: . | lakefs.example.com | s3.lakefs.example.com - this is the S3 Gateway Domain | *.s3.lakefs.example.com | . The second record, the S3 Gateway Domain, needs to be specified in the lakeFS configuration (see the S3_GATEWAY_DOMAIN placeholder below). This will allow lakeFS to route requests to the S3-compatible API. For more info, see Why do I need these three DNS records? . ",
    "url": "/deploy/gcp.html#prerequisites",
    "relUrl": "/deploy/gcp.html#prerequisites"
  },"150": {
    "doc": "On GCP",
    "title": "Creating the Database on GCP SQL",
    "content": "lakeFS requires a PostgreSQL database to synchronize actions on your repositories. We will show you how to create a database on Google Cloud SQL, but you can use any PostgreSQL database as long as it’s accessible by your lakeFS installation. If you already have a database, take note of the connection string and skip to the next step . | Follow the official Google documentation on how to create a PostgreSQL instance. Make sure you’re using PostgreSQL version &gt;= 11. | On the Users tab in the console, create a user. The lakeFS installation will use it to connect to your database. | Choose the method by which lakeFS will connect to your database. Google recommends using the SQL Auth Proxy. | . Depending on the chosen lakeFS installation method, you will need to make sure lakeFS can access your database. For example, if you install lakeFS on GKE, you need to deploy the SQL Auth Proxy from this Helm chart, or as a sidecar container in your lakeFS pod. ",
    "url": "/deploy/gcp.html#creating-the-database-on-gcp-sql",
    "relUrl": "/deploy/gcp.html#creating-the-database-on-gcp-sql"
  },"151": {
    "doc": "On GCP",
    "title": "Installation Options",
    "content": "On Google Compute Engine . | Save the following configuration file as config.yaml: . --- database: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: # replace this with a randomly-generated string: secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: gs # Uncomment the following lines to give lakeFS access to your buckets using a service account: # gs: # credentials_json: [YOUR SERVICE ACCOUNT JSON STRING] gateways: s3: # replace this with the host you will use for the lakeFS S3-compatible endpoint: domain_name: [S3_GATEWAY_DOMAIN] . | Download the binary to the GCE instance. | Run the lakefs binary on the GCE machine: lakefs --config config.yaml run . Note: it is preferable to run the binary as a service using systemd or your operating system’s facilities. | . On Google Cloud Run . To support container-based environments like Google Cloud Run, lakeFS can be configured using environment variables. Here is a docker run command to demonstrate starting lakeFS using Docker: . docker run \\ --name lakefs \\ -p 8000:8000 \\ -e LAKEFS_DATABASE_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\ -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\ -e LAKEFS_BLOCKSTORE_TYPE=\"gs\" \\ -e LAKEFS_GATEWAYS_S3_DOMAIN_NAME=\"[S3_GATEWAY_DOMAIN]\" \\ treeverse/lakefs:latest run . See the reference for a complete list of environment variables. On GKE . See Kubernetes Deployment. ",
    "url": "/deploy/gcp.html#installation-options",
    "relUrl": "/deploy/gcp.html#installation-options"
  },"152": {
    "doc": "On GCP",
    "title": "Load balancing",
    "content": "Depending on how you chose to install lakeFS, you should have a load balancer direct requests to the lakeFS server. By default, lakeFS operates on port 8000, and exposes a /_health endpoint which you can use for health checks. ",
    "url": "/deploy/gcp.html#load-balancing",
    "relUrl": "/deploy/gcp.html#load-balancing"
  },"153": {
    "doc": "On GCP",
    "title": "DNS",
    "content": "As mentioned above, you should create 3 DNS records for lakeFS: . | One record for the lakeFS API: lakefs.example.com | Two records for the S3-compatible API: s3.lakefs.example.com and *.s3.lakefs.example.com. | . Depending on your DNS provider, refer to the documentation on how to add CNAME records. ",
    "url": "/deploy/gcp.html#dns",
    "relUrl": "/deploy/gcp.html#dns"
  },"154": {
    "doc": "On GCP",
    "title": "Next Steps",
    "content": "Your next step is to prepare your storage. If you already have a storage bucket/container, you are ready to create your first lakeFS repository. ",
    "url": "/deploy/gcp.html#next-steps",
    "relUrl": "/deploy/gcp.html#next-steps"
  },"155": {
    "doc": "On GCP",
    "title": "Why do I need the three DNS records?",
    "content": "Multiple DNS records are needed to access the two different lakeFS APIs (covered in more detail in the Architecture section): . | The lakeFS OpenAPI: used by the lakectl CLI tool. Exposes git-like operations (branching, diffing, merging etc.). | An S3-compatible API: read and write your data in any tool that can communicate with S3. Examples include: AWS CLI, Boto, Presto and Spark. | . lakeFS actually exposes only one API endpoint. For every request, lakeFS checks the Host header. If the header is under the S3 gateway domain, the request is directed to the S3-compatible API. The third DNS record (*.s3.lakefs.example.com) allows for virtual-host style access. This is a way for AWS clients to specify the bucket name in the Host subdomain. ",
    "url": "/deploy/gcp.html#why-do-i-need-the-three-dns-records",
    "relUrl": "/deploy/gcp.html#why-do-i-need-the-three-dns-records"
  },"156": {
    "doc": "On GCP",
    "title": "On GCP",
    "content": " ",
    "url": "/deploy/gcp.html",
    "relUrl": "/deploy/gcp.html"
  },"157": {
    "doc": "Google Cloud Storage",
    "title": "Prepare Your GCS Bucket",
    "content": ". | On the Google Cloud Storage console, click Create Bucket. Follow the instructions. | On the Permissions tab, add the service account you intend to use lakeFS with. Give it a role that allows reading and writing to the bucket, e.g. Storage Object Creator. | . You are now ready to create your first lakeFS repository. ",
    "url": "/setup/storage/gcs.html#prepare-your-gcs-bucket",
    "relUrl": "/setup/storage/gcs.html#prepare-your-gcs-bucket"
  },"158": {
    "doc": "Google Cloud Storage",
    "title": "Google Cloud Storage",
    "content": " ",
    "url": "/setup/storage/gcs.html",
    "relUrl": "/setup/storage/gcs.html"
  },"159": {
    "doc": "Glue ETL",
    "title": "Using lakeFS with Glue ETL",
    "content": "AWS Glue is a fully managed extract, transform, and load (ETL) service. With AWS Glue ETL you can run your ETL jobs as soon as new data becomes available in Amazon S3 by invoking your AWS Glue ETL jobs from an AWS Lambda function. ",
    "url": "/integrations/glue_etl.html#using-lakefs-with-glue-etl",
    "relUrl": "/integrations/glue_etl.html#using-lakefs-with-glue-etl"
  },"160": {
    "doc": "Glue ETL",
    "title": "Configuration",
    "content": "Since Glue ETL is essentially running Spark jobs, to configure Glue ETL to work with lakeFS, you should apply the lakeFS Spark configuration to your Glue ETL script. ",
    "url": "/integrations/glue_etl.html#configuration",
    "relUrl": "/integrations/glue_etl.html#configuration"
  },"161": {
    "doc": "Glue ETL",
    "title": "Glue ETL",
    "content": " ",
    "url": "/integrations/glue_etl.html",
    "relUrl": "/integrations/glue_etl.html"
  },"162": {
    "doc": "Glue / Hive metastore",
    "title": "Table of contents",
    "content": ". | Glue / Hive Metastore Intro | Managing Tables With lakeFS Branches . | Motivation | Configurations | Suggested Model | Commands . | Copy | Diff | . | Athena with lakeFS branches | . | . ",
    "url": "/integrations/glue_hive_metastore.html#table-of-contents",
    "relUrl": "/integrations/glue_hive_metastore.html#table-of-contents"
  },"163": {
    "doc": "Glue / Hive metastore",
    "title": "Glue / Hive Metastore Intro",
    "content": "This part contains a brief explanation about how Glue/Hive metastore work with lakeFS . Glue and Hive Metastore stores metadata related to Hive and other services (such as Spark and Trino). They contain metadata such as the location of the table, information about columns, partitions and many more. ",
    "url": "/integrations/glue_hive_metastore.html#glue--hive-metastore-intro",
    "relUrl": "/integrations/glue_hive_metastore.html#glue--hive-metastore-intro"
  },"164": {
    "doc": "Glue / Hive metastore",
    "title": "Without lakeFS",
    "content": "In order to query the table my_table, Spark will: . | Request the metadata from Hive metastore (steps 1,2) | Use the location from the metadata to access the data in S3 (steps 3,4). | . ",
    "url": "/integrations/glue_hive_metastore.html#without-lakefs",
    "relUrl": "/integrations/glue_hive_metastore.html#without-lakefs"
  },"165": {
    "doc": "Glue / Hive metastore",
    "title": "With lakeFS",
    "content": "When using lakeFS, the flow stays exactly the same. Note that the location of the table my_table now contains the branch s3://example/main/path/to/table . ",
    "url": "/integrations/glue_hive_metastore.html#with-lakefs",
    "relUrl": "/integrations/glue_hive_metastore.html#with-lakefs"
  },"166": {
    "doc": "Glue / Hive metastore",
    "title": "Managing Tables With lakeFS Branches",
    "content": " ",
    "url": "/integrations/glue_hive_metastore.html#managing-tables-with-lakefs-branches",
    "relUrl": "/integrations/glue_hive_metastore.html#managing-tables-with-lakefs-branches"
  },"167": {
    "doc": "Glue / Hive metastore",
    "title": "Motivation",
    "content": "When creating a table in Glue/Hive metastore (using a client such as Spark, Hive, Presto), we specify the table location. Consider the table my_table which was created with the location s3://example/main/path/to/table. Assume we created a new branch called DEV with main as the source branch. The data from s3://example/main/path/to/table is now accessible in s3://example/DEV/path/to/table. The metadata is not managed in lakeFS, meaning we don’t have any table pointing to s3://example/DEV/path/to/table. To address this, lakeFS introduces lakectl metastore commands. The case above could be handled using the copy command: it can create a copy of my_table with data located in s3://example/DEV/path/to/table. Note that this is a fast, metadata-only operation. ",
    "url": "/integrations/glue_hive_metastore.html#motivation",
    "relUrl": "/integrations/glue_hive_metastore.html#motivation"
  },"168": {
    "doc": "Glue / Hive metastore",
    "title": "Configurations",
    "content": "The lakectl metastore commands could run on Glue or Hive metastore. Add the following to the lakectl configuration file (by default ~/.lakectl.yaml): . Hive . metastore: type: hive hive: uri: thrift://hive-metastore:9083 . Glue . metastore: type: glue glue: catalog-id: 123456789012 region: us-east-1 profile: default # optional, implies using a credentials file credentials: access_key_id: AKIAIOSFODNN7EXAMPLE secret_access_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY . Notice: It’s recommended to set type and catalog-id/metastore-uri in the lakectl configuration file. ",
    "url": "/integrations/glue_hive_metastore.html#configurations",
    "relUrl": "/integrations/glue_hive_metastore.html#configurations"
  },"169": {
    "doc": "Glue / Hive metastore",
    "title": "Suggested Model",
    "content": "For simplicity, we recommend creating a schema for each branch, this way you can use the same table name across different schemas. For example: after creating branch example_branch also create a schema named example_branch. For a table named my_table under the schema main, create a new table by the same name under the schema example_branch. You now have two my_tables, one in the main schema and one, in the branch schema. ",
    "url": "/integrations/glue_hive_metastore.html#suggested-model",
    "relUrl": "/integrations/glue_hive_metastore.html#suggested-model"
  },"170": {
    "doc": "Glue / Hive metastore",
    "title": "Commands",
    "content": "Metastore tools support three commands: copy, diff and create-symlink. copy and diff could work both on Glue and on Hive. create-symlink works only on Glue. Notice: If to-schema or to-table are not specified, the destination branch and source table names will be used as per the suggested model. Notice: Metastore commands can only run on tables located in lakeFS, you should not use tables that are not located in lakeFS. Copy . The copy command creates a copy of a table pointing to the defined branch. In case the destination table already exists, the command will only merge the changes. Example: . Suppose we created the table inventory on branch main on schema default. CREATE EXTERNAL TABLE `inventory`( `inv_item_sk` int, `inv_warehouse_sk` int, `inv_quantity_on_hand` int) PARTITIONED BY ( `inv_date_sk` int) STORED AS ORC LOCATION 's3a://my_repo/main/path/to/table'; . We create a new lakeFS branch example_branch: . lakectl branch create lakefs://my_repo/example_branch --source lakefs://my_repo/main . The data from s3://my_repo/main/path/to/table is now accessible in s3://my_repo/DEV/path/to/table. In order to query the data in s3://my_repo/DEV/path/to/table we would like to create a copy of the table inventory in schema example_branch pointing to the new branch. lakectl metastore copy --from-schema default --from-table inventory --to-schema example_branch --to-table inventory --to-branch example_branch . After running this command, query the table example_branch.inventory to get the data from s3://my_repo/DEV/path/to/table . Copy Partition . After adding a partition to the branch table, we may want to copy the partition to the main table. For example, for the new partition 2020-08-01, run the following in order to copy the partition to the main table: . lakectl metastore copy --type hive --from-schema example_branch --from-table inventory --to-schema default --to-table inventory --to-branch main -p 2020-08-01 . For a table partitioned by more than one column, specify the partition flag for every column. For example for the partition (year='2020',month='08',day='01'): . lakectl metastore copy --from-schema example_branch --from-table branch_inventory --to-schema default --to-branch main -p 2020 -p 08 -p 01 . Diff . Provides a 2-way diff between two tables. Shows added+ , removed- and changed~ partitions and columns. Example: . Suppose that we made some changes on the copied table inventory on schema example_branch and we want to view the changes before merging back to inventory on schema default. Hive: . lakectl metastore diff --type hive --address thrift://hive-metastore:9083 --from-schema example_branch --from-table branch --to-schema default --to-table inventory . The output will be something like: . Columns are identical Partitions - 2020-07-04 + 2020-07-05 + 2020-07-06 ~ 2020-07-08 . ",
    "url": "/integrations/glue_hive_metastore.html#commands",
    "relUrl": "/integrations/glue_hive_metastore.html#commands"
  },"171": {
    "doc": "Glue / Hive metastore",
    "title": "Athena with lakeFS branches",
    "content": "Athena doesn’t support configuring the endpoint-uri. to use S3-compatible services like lakeFS. Hence, Athena can’t access lakeFS, and can only be used with AWS S3 as the storage. In order to enable accessing partitioned data we could use the create-symlink command. create-symlink receives a source table, destination table and the location of the table and does two actions: . | Creates partitioned directories with symlink files in the underlying S3 bucket. | Creates a table in Glue catalog with symlink format type and location pointing to the created symlinks. | . Notice: create-symlink source table must point to a location in lakeFS. Example: . Let’s assume we have the table inventory in Glue. The table is pointing to repo example-repo branch main and the data is located at path/to/table/in/lakeFS . We want to query the table using Athena. To do this, we run the command: . lakectl metastore create-symlink --address 123456789012 --branch main --from-schema default --from-table branch_inventory --to-schema default --to-table sym_inventory --repo example-repository --path path/to/table/in/lakeFS . We can now use Amazon Athena to query the created table sym_inventory. ",
    "url": "/integrations/glue_hive_metastore.html#athena-with-lakefs-branches",
    "relUrl": "/integrations/glue_hive_metastore.html#athena-with-lakefs-branches"
  },"172": {
    "doc": "Glue / Hive metastore",
    "title": "Glue / Hive metastore",
    "content": " ",
    "url": "/integrations/glue_hive_metastore.html",
    "relUrl": "/integrations/glue_hive_metastore.html"
  },"173": {
    "doc": "Hive",
    "title": "Using lakeFS with Hive",
    "content": "The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive. ",
    "url": "/integrations/hive.html#using-lakefs-with-hive",
    "relUrl": "/integrations/hive.html#using-lakefs-with-hive"
  },"174": {
    "doc": "Hive",
    "title": "Table of contents",
    "content": ". | Configuration | Examples . | Example with schema | Example with external table | . | . ",
    "url": "/integrations/hive.html#table-of-contents",
    "relUrl": "/integrations/hive.html#table-of-contents"
  },"175": {
    "doc": "Hive",
    "title": "Configuration",
    "content": "In order to configure hive to work with lakeFS we will set the lakeFS credentials in the corresponding S3 credential fields. lakeFS endpoint: fs.s3a.endpoint . lakeFS access key: fs.s3a.access.key . lakeFS secret key: fs.s3a.secret.key . Note In the following examples we set AWS credentials at runtime, for clarity. In production, these properties should be set using one of Hadoop’s standard ways of Authenticating with S3. For example, we could add the configurations to the file hdfs-site.xml: . &lt;configuration&gt; ... &lt;property&gt; &lt;name&gt;fs.s3a.secret.key&lt;/name&gt; &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.access.key&lt;/name&gt; &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;https://s3.lakefs.example.com&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . ",
    "url": "/integrations/hive.html#configuration",
    "relUrl": "/integrations/hive.html#configuration"
  },"176": {
    "doc": "Hive",
    "title": "Examples",
    "content": "Example with schema . CREATE SCHEMA example LOCATION 's3a://example/main/' ; CREATE TABLE example.request_logs ( request_time timestamp, url string, ip string, user_agent string ); . Example with external table . CREATE EXTERNAL TABLE request_logs ( request_time timestamp, url string, ip string, user_agent string ) LOCATION 's3a://example/main/request_logs' ; . ",
    "url": "/integrations/hive.html#examples",
    "relUrl": "/integrations/hive.html#examples"
  },"177": {
    "doc": "Hive",
    "title": "Hive",
    "content": " ",
    "url": "/integrations/hive.html",
    "relUrl": "/integrations/hive.html"
  },"178": {
    "doc": "Hooks",
    "title": "Configurable Hooks",
    "content": " ",
    "url": "/setup/hooks.html#configurable-hooks",
    "relUrl": "/setup/hooks.html#configurable-hooks"
  },"179": {
    "doc": "Hooks",
    "title": "Table of contents",
    "content": ". | Example use-cases | Terminology | Uploading Action files | Runs API &amp; CLI | Hook types . | Webhooks . | Action file Webhook properties | Request body schema | . | Airflow Hooks . | Action file Airflow hook properties | Hook Record in configuration field | . | . | Experimentation | . Like other version control systems, lakeFS allows the configuration of Actions to trigger when predefined events occur. Supported Events: . | pre_commit - Action runs when the commit occurs, before the commit is finalized. | post_commit - Action runs after the commit is finalized. | pre_merge - Action runs when the merge occurs, before the merge is finalized. | post_merge - Action runs after the merge is finalized. | . lakeFS Actions are handled per repository and cannot be shared between repositories. Failure of any Hook under any Action of a pre_* event will result in aborting the lakeFS operation that is taking place. On the contrary, Hook failures under any Action of a post_* event will not affect the same operation. Hooks are managed by Action files that are written to a prefix in the lakeFS repository. This allows configuration-as-code inside lakeFS, where Action files are declarative and written in YAML. ",
    "url": "/setup/hooks.html#table-of-contents",
    "relUrl": "/setup/hooks.html#table-of-contents"
  },"180": {
    "doc": "Hooks",
    "title": "Example use-cases",
    "content": ". | Format Validator: A webhook that checks new files to ensure they are of a set of allowed data format. | Schema Validator: A webhook that reads new Parquet and ORC files to ensure they don’t contain a block list of column names (or name prefixes). This is useful when we want to avoid accidental PII exposure. | . For more examples and configuration samples, check out lakeFS-hooks example repo. ",
    "url": "/setup/hooks.html#example-use-cases",
    "relUrl": "/setup/hooks.html#example-use-cases"
  },"181": {
    "doc": "Hooks",
    "title": "Terminology",
    "content": "Action . An Action is a list of Hooks with the same trigger configuration, i.e. an event will trigger all Hooks under an Action, or none at all. The Hooks under an Action are ordered and so is their execution. A Hook will only be executed if all previous Hooks that were triggered with it, had passed. Hook . A Hook is the basic building block of an Action. Failure of a single Hook will stop the execution of the containing Action and fail the Run. Action file . Schema of the Action file: . | Property | Description | Data Type | Required | Default Value | . | name | Identify the Action file | String | false | If missing, filename is used instead | . | on | List of events that will trigger the hooks | List | true |   | . | on.branches | Glob pattern list of branches that triggers the hooks | List | false | If empty, Action runs on all branches | . | hooks | List of hooks to be executed | List | true |   | . | hook.id | ID of the hook, must be unique within the Action | String | true |   | . | hook.type | Type of the hook (currently only webhook supported | String | true |   | . | hook.properties | Hook’s specific configuration | Dictionary | true |   | . Example: . name: Good files check description: set of checks to verify that branch is good on: pre-commit: pre-merge: branches: - main hooks: - id: no_temp type: webhook description: checking no temporary files found properties: url: \"https://your.domain.io/webhook?notmp=true?t=1za2PbkZK1bd4prMuTDr6BeEQwWYcX2R\" - id: no_freeze type: webhook description: check production is not in dev freeze properties: url: \"https://your.domain.io/webhook?nofreeze=true?t=1za2PbkZK1bd4prMuTDr6BeEQwWYcX2R\" . Note: lakeFS will validate action files only when an Event occurred. Use lakectl actions validate &lt;path&gt; to validate your action files locally. Run . A Run is an instantiation of the repository’s Action files when the triggering event occurs. For example, if our repository contains a pre-commit hook, every commit would generate a Run for that specific commit. lakeFS will fetch, parse and filter the repository Action files and start to execute the Hooks under each Action. All executed Hooks (each with hook_run_id) exists in the context of that Run (run_id). ",
    "url": "/setup/hooks.html#terminology",
    "relUrl": "/setup/hooks.html#terminology"
  },"182": {
    "doc": "Hooks",
    "title": "Uploading Action files",
    "content": "Action files should be uploaded with the prefix _lakefs_actions/ to the lakeFS repository. When an actionable event (see Supported Events above) takes place, lakeFS will read all files with prefix _lakefs_actions/ in the repository branch where the action occurred. A failure to parse an Action file will result with a failing Run. For example, lakeFS will search and execute all matching Action files with the prefix lakefs://repo1/feature-1/_lakefs_actions/ on: . | Commit to feature-1 branch on repo1 repository. | Merge to main branch from feature-1 branch on repo1 repository. | . ",
    "url": "/setup/hooks.html#uploading-action-files",
    "relUrl": "/setup/hooks.html#uploading-action-files"
  },"183": {
    "doc": "Hooks",
    "title": "Runs API &amp; CLI",
    "content": "OpenAPI endpoint and lakectl expose the results of Runs execution per repository, branch, commit and specific Action. The endpoint also allows to download the execution log of any executed Hook under each Run for observability. Result Files . There are 2 types of files that are stored in the metadata section of lakeFS repository with each Run: . | _lakefs/actions/log/&lt;runID&gt;/&lt;hookRunID&gt;.log - Execution log of the specific Hook run. | _lakefs/actions/log/&lt;runID&gt;/run.manifest - Manifest with all Hooks execution for the run with their results and additional metadata. | . Note: Metadata section of a lakeFS repository is where lakeFS keeps its metadata, like commits and metaranges. Metadata files stored in the metadata section aren’t accessible like user stored files. ",
    "url": "/setup/hooks.html#runs-api--cli",
    "relUrl": "/setup/hooks.html#runs-api--cli"
  },"184": {
    "doc": "Hooks",
    "title": "Hook types",
    "content": "Currently, there are two types of Hooks that are supported by lakeFS: Webhook and Airflow. Webhooks . A Webhook is a Hook type that sends an HTTP POST request to the configured URL. Any non 2XX response by the responding endpoint will fail the Hook, cancel the execution of the following Hooks under the same Action. For pre_* hooks, the triggering operation (commit/merge) will also be aborted. Warning: You should not use pre_* webhooks for long-running tasks, since they block the performed operation. Moreover, the branch is locked during the execution of pre_* hooks, so the webhook server cannot perform any write operations (like uploading or commits) on the branch. Action file Webhook properties . | Property | Description | Data Type | Required | Default Value | . | url | The URL address of the request | String | true |   | . | timeout | Time to wait for response before failing the hook | String (golang’s Duration representation) | false | 1m | . | query_params | List of query params that will be added to the request | Dictionary(String:String or String:List(String) | false |   | . Example: ... hooks: - id: prevent_user_columns type: webhook description: Ensure no user_* columns under public/ properties: url: \"http://&lt;host:port&gt;/webhooks/schema\" timeout: 1m30s query_params: disallow: [\"user_\", \"private_\"] prefix: public/ ... Request body schema . Upon execution, a webhook will send a request containing a JSON object with the following fields: . | Field | Description | Type | Example | . | EventType | Type of the event that triggered the Action | string | pre_commit | . | EventTime | Time of the event that triggered the Action (RFC3339) | string | 2006-01-02T15:04:05Z07:00 | . | ActionName | Containing Hook Action’s Name | string |   | . | HookID | ID of the Hook | string |   | . | RepositoryID | ID of the Repository | string |   | . | BranchID | ID of the Branch | string |   | . | SourceRef | Reference to the source that triggered the event (source Branch for commit or merge) | string |   | . | CommitMessage | The message for the commit (or merge) that is taking place | string |   | . | Committer | Name of the committer | string |   | . | CommitMetadata | The metadata for the commit that is taking place | string |   | . Example: . { \"event_type\": \"pre-merge\", \"event_time\": \"2021-02-28T14:03:31Z\", \"action_name\": \"test action\", \"hook_id\": \"prevent_user_columns\", \"repository_id\": \"repo1\", \"branch_id\": \"feature-1\", \"source_ref\": \"feature-1\", \"commit_message\": \"merge commit message\", \"committer\": \"committer\", \"commit_metadata\": { \"key\": \"value\" } } . Airflow Hooks . Airflow Hook triggers a DAG run in an Airflow installation using Airflow’s REST API. The hook run succeeds if the DAG was triggered, and fails otherwise. Action file Airflow hook properties . | Property | Description | Data Type | Example | Required | . | url | The URL of the Airflow instance | String | “http://localhost:8080” | true | . | dag_id | The DAG to trigger | String | “example_dag” | true | . | username | The name of the Airflow user performing the request | String | “admin” | true | . | password | The password of the Airflow user performing the request | String | “admin” | true | . | dag_conf | DAG run configuration that will be passed as is | JSON |   | false | . Example: ... hooks: - id: trigger_my_dag type: airflow description: Trigger an example_dag properties: url: \"http://localhost:8000\" dag_id: \"example_dag\" username: \"admin\" password: \"admin\" dag_conf: some: \"additional_conf\" ... Hook Record in configuration field . lakeFS will add an entry to the Airflow request configuration property (conf) with the event that triggered the action. The key of the record will be lakeFS_event and the value will match the one described here . ",
    "url": "/setup/hooks.html#hook-types",
    "relUrl": "/setup/hooks.html#hook-types"
  },"185": {
    "doc": "Hooks",
    "title": "Experimentation",
    "content": "It’s sometimes easier to start experimenting with lakeFS webhooks, even before you have a running server to receive the calls. There are a couple of online tools that can intercept and display the webhook requests, one of them is Svix. | Go to play.svix.com and copy the URL address supplied by Svix. It should look like https://api.relay.svix.com/api/v1/play/receive/&lt;Random_Gen_String&gt;/ . | Upload the following action file to lakeFS under the path _lakefs_actions/test.yaml in the default branch: . name: Sending everything to Svix description: Experimenting with webhooks on: pre-commit: branches: pre-merge: branches: post-commit: branches: post-merge: branches: hooks: - id: svix type: webhook properties: url: \"https://api.relay.svix.com/api/v1/play/receive/&lt;Random_Gen_String&gt;/\" . by using: . lakectl fs upload lakefs://example-repo/main/_lakefs_action/test.yaml -s path/to/action/file . or the UI. | Commit that file to the branch. lakectl commit lakefs://example-repo/main -m 'added webhook action file' . | Every time you commit or merge to a branch, the relevant pre_* and post_* requests will be available in the Svix endpoint you provided. You can also check the Actions tab in the lakeFS UI for more details. | . ",
    "url": "/setup/hooks.html#experimentation",
    "relUrl": "/setup/hooks.html#experimentation"
  },"186": {
    "doc": "Hooks",
    "title": "Hooks",
    "content": " ",
    "url": "/setup/hooks.html",
    "relUrl": "/setup/hooks.html"
  },"187": {
    "doc": "Import data into lakeFS",
    "title": "Import data into lakeFS",
    "content": " ",
    "url": "/setup/import.html#import-data-into-lakefs",
    "relUrl": "/setup/import.html#import-data-into-lakefs"
  },"188": {
    "doc": "Import data into lakeFS",
    "title": "Table of contents",
    "content": ". | Copying using external tools | Importing data from an object store without actually copying it . | Ingesting from S3 | Ingesting from Azure Blob storage | Ingesting from Google Cloud storage | . | Very large buckets: Using lakeFS S3 inventory import tool . | Prerequisites | Usage | Gradual Import | Limitations | . | . ",
    "url": "/setup/import.html#table-of-contents",
    "relUrl": "/setup/import.html#table-of-contents"
  },"189": {
    "doc": "Import data into lakeFS",
    "title": "Copying using external tools",
    "content": "In order to import existing data to lakeFS, you may choose to copy it using S3 CLI or using tools like Apache DistCp. This is the most straightforward way, and we recommend it if it’s applicable for you. ",
    "url": "/setup/import.html#copying-using-external-tools",
    "relUrl": "/setup/import.html#copying-using-external-tools"
  },"190": {
    "doc": "Import data into lakeFS",
    "title": "Importing data from an object store without actually copying it",
    "content": "For cases where copying data is not feasible, the lakectl command supports ingesting objects from a source object store without actually copying the data itself. This is done by listing the source bucket (and optional prefix), and creating pointers to the returned objects in lakeFS. By doing this, it’s possible to take even large sets of objects, and have them appear as objects in a lakeFS branch, as if they were written directly to it. For this to work, we’d need to ensure 2 things first: . | The user calling lakectl ingest must have permissions to list the object at the source object store | The lakeFS installation must have read permissions to the objects being ingested | . Ingesting from S3 . lakectl ingest \\ --from s3://bucket/optional/prefix/ \\ --to lakefs://my-repo/ingest-branch/optional/path/ . The lakectl ingest command will attempt to use the current user’s existing credentials and will respect instance profiles, environment variables and credential files in the same way that the AWS cli does . Ingesting from Azure Blob storage . export AZURE_STORAGE_ACCOUNT=\"storageAccountName\" export AZURE_STORAGE_ACCESS_KEY=\"EXAMPLEroozoo2gaec9fooTieWah6Oshai5Sheofievohthapob0aidee5Shaekahw7loo1aishoonuuquahr3==\" lakectl ingest \\ --from https://storageAccountName.blob.core.windows.net/container/optional/prefix/ \\ --to lakefs://my-repo/ingest-branch/optional/path/ . The lakectl ingest command currently supports storage accounts configured through environment variables as shown above. Note: Currently lakectl import supports the http:// and https:// schemes for Azure storage URIs. wasb, abfs or adls are currently not supported. Ingesting from Google Cloud storage . export GOOGLE_APPLICATION_CREDENTIALS=\"$HOME/.gcs_credentials.json\" # Optional, will fallback to the default configured credentials lakectl ingest \\ --from gs://bucket/optional/prefix/ \\ --to lakefs://my-repo/ingest-branch/optional/path/ . The lakectl ingest command currently supports the standard GOOGLE_APPLICATION_CREDENTIALS environment variable as described in Google Cloud’s documentation. ",
    "url": "/setup/import.html#importing-data-from-an-object-store-without-actually-copying-it",
    "relUrl": "/setup/import.html#importing-data-from-an-object-store-without-actually-copying-it"
  },"191": {
    "doc": "Import data into lakeFS",
    "title": "Very large buckets: Using lakeFS S3 inventory import tool",
    "content": "Importing a very large amount of objects (&gt; ~250M) might take some time using lakectl ingest as described above, since it has to paginate through all the objects in the source using API calls. For S3, we provide a utility as part of the lakefs binary, called lakefs import. The lakeFS import tool will use the S3 Inventory feature to create lakeFS metadata. The imported metadata will be committed to a special branch, called import-from-inventory. You should not make any changes or commit anything to branch import-from-inventory: it will be operated on only by lakeFS. After importing, you will be able to merge this branch into your main branch. How it works . The imported data is not copied to the repository’s dedicated bucket. Rather, it will be read directly from your existing bucket when you access it through lakeFS. Files created or replaced through lakeFS will then be stored in the repository’s dedicated bucket. It is important to note that due to the deduplication feature of lakeFS, data will be read from your original bucket even when accessing it through other branches. In a sense, your original bucket becomes an initial snapshot of your data. Note: lakeFS will never make any changes to the import source bucket. Prerequisites . | Your bucket should have S3 Inventory enabled. | The inventory should be in Parquet or ORC format. | The inventory must contain (at least) the size, last-modified-at, and e-tag columns. | The S3 credentials you provided to lakeFS should have GetObject permissions on the source bucket and on the bucket where the inventory is stored. | If you want to use the tool for gradual import, you should not delete the data for the most recently imported inventory, until a more recent inventory is successfully imported. | . Usage . Import is performed by the lakefs import command. Assuming your manifest.json is at s3://example-bucket/path/to/inventory/YYYY-MM-DDT00-00Z/manifest.json, and your lakeFS configuration yaml is at config.yaml (see notes below), run the following command to start the import: . lakefs import lakefs://example-repo -m s3://example-bucket/path/to/inventory/YYYY-MM-DDT00-00Z/manifest.json --config config.yaml . You will see the progress of your import as it is performed. After the import is finished, a summary will be printed along with suggestions for commands to access your data. Added or changed objects: 565000 Deleted objects: 0 Commit ref: cf349ded0a0e65e20bd3b25ea8d9b656c2870b7f1f32f60eb1d90ca5873b6c03 Import to branch import-from-inventory finished successfully. To list imported objects, run: $ lakectl fs ls lakefs://example-repo/cf349ded0a0e65e20bd3b25ea8d9b656c2870b7f1f32f60eb1d90ca5873b6c03/ To merge the changes to your main branch, run: $ lakectl merge lakefs://example-repo/import-from-inventory lakefs://goo/main . Merging imported data to the main branch . As previously mentioned, the above command imports data to the dedicated import-from-inventory branch. By adding the --with-merge flag to the import command, this branch will be automatically merged to your main branch immediately after the import. lakefs import --with-merge lakefs://example-repo -m s3://example-bucket/path/to/inventory/YYYY-MM-DDT00-00Z/manifest.json --config config.yaml . Notes . | Perform the import from a machine with access to your database, and on the same region of your destination bucket. | You can download the lakefs binary from here. Make sure you choose one compatible with your installation of lakeFS. | Use a configuration file like the one used to start your lakeFS installation. This will be used to access your database. An example can be found here. | . Warning: the import-from-inventory branch should only be used by lakeFS. You should not make any operations on it. Gradual Import . Once you switch to using the lakeFS S3-compatible endpoint in all places, you can stop making changes to your original bucket. However, if your operation still requires that you work on the original bucket, you can repeat using the import API with up-to-date inventories every day, until you complete the onboarding process. You can specify only the prefixes that require import. lakeFS will merge those prefixes with the previous imported inventory. For example, a prefixes-file that contains only the prefix new/data/. The new commit to import-from-inventory branch will include all objects from the HEAD of that branch, except for objects with prefix new/data/ that is imported from the inventory. Limitations . Note that lakeFS cannot manage your metadata if you make changes to data in the original bucket. The following table describes the results of making changes in the original bucket, without importing it to lakeFS: . | Object action in the original bucket | ListObjects result in lakeFS | GetObject result in lakeFS | . | Create | Object not visible | Object not accessible | . | Overwrite | Object visible with outdated metadata | Updated object accessible | . | Delete | Object visible | Object not accessible | . ",
    "url": "/setup/import.html#very-large-buckets-using-lakefs-s3-inventory-import-tool",
    "relUrl": "/setup/import.html#very-large-buckets-using-lakefs-s3-inventory-import-tool"
  },"192": {
    "doc": "Import data into lakeFS",
    "title": "Import data into lakeFS",
    "content": "This page describes importing from versions &gt;= v0.24.0. For ealier versions, see mvcc import . ",
    "url": "/setup/import.html",
    "relUrl": "/setup/import.html"
  },"193": {
    "doc": "Using lakeFS",
    "title": "Using lakeFS",
    "content": " ",
    "url": "/usecases/",
    "relUrl": "/usecases/"
  },"194": {
    "doc": "Prepare Your Storage",
    "title": "Prepare Your Storage",
    "content": "A production installation of lakeFS will usually use your cloud provider’s object storage as the underlying storage layer. You can choose to create a new bucket/container (recommended), or use an existing one with a path prefix. The path under the existing bucket/container should be empty. If you already have a bucket/container configured, you are ready to create your first lakeFS repository. Choose your storage provider to configure your storage: . ",
    "url": "/setup/storage/",
    "relUrl": "/setup/storage/"
  },"195": {
    "doc": "Setup lakeFS",
    "title": "Setup lakeFS",
    "content": "Once lakeFS is installed, a few simple steps are required to start working. The first step is to create a bucket/container in your underlying storage. If you already have one, you are ready to create your first lakeFS repository. ",
    "url": "/setup/",
    "relUrl": "/setup/"
  },"196": {
    "doc": "Slack",
    "title": "Slack",
    "content": " ",
    "url": "/slack/",
    "relUrl": "/slack/"
  },"197": {
    "doc": "Understanding lakeFS",
    "title": "Understanding lakeFS",
    "content": "This section includes all the details about the lakeFS open source project. ",
    "url": "/understand/",
    "relUrl": "/understand/"
  },"198": {
    "doc": "Quickstart",
    "title": "Quickstart",
    "content": "This section will guide you through setting up your lakeFS environment locally. Note: The quickstart section is for learning purposes. The installations below will not persist your data. Instead, it will spin-up a database in a docker container, which will be discarded later. For a production suitable deployment, see Deploying on AWS. ",
    "url": "/quickstart/",
    "relUrl": "/quickstart/"
  },"199": {
    "doc": "Reference",
    "title": "Reference",
    "content": " ",
    "url": "/reference/",
    "relUrl": "/reference/"
  },"200": {
    "doc": "Integrations",
    "title": "Integrations",
    "content": " ",
    "url": "/integrations/",
    "relUrl": "/integrations/"
  },"201": {
    "doc": "Deploy lakeFS",
    "title": "Deploy lakeFS",
    "content": "This page contains a collection of practical step-by-step instructions to help you set up lakeFS on your preferred cloud environemnt. If you just want to try out lakeFS locally, see Quickstart. ",
    "url": "/deploy/",
    "relUrl": "/deploy/"
  },"202": {
    "doc": "What is lakeFS",
    "title": "What is lakeFS",
    "content": "lakeFS is an open source platform that delivers resilience and manageability to object-storage based data lakes. With lakeFS you can build repeatable, atomic and versioned data lake operations - from complex ETL jobs to data science and analytics. lakeFS supports AWS S3, Azure Blob Storage and Google Cloud Storage (GCS) as its underlying storage service. It is API compatible with S3 and works seamlessly with all modern data frameworks such as Spark, Hive, AWS Athena, Presto, etc. ",
    "url": "/",
    "relUrl": "/"
  },"203": {
    "doc": "What is lakeFS",
    "title": "Why you need lakeFS and what it can do",
    "content": "lakeFS provides a Git-like branching and committing model that scales to exabytes of data by utilizing S3, GCS, or Azure Blob for storage. This branching model makes your data lake ACID compliant by allowing changes to happen in isolated branches that can be created, merged and rolled back atomically and instantly. Since lakeFS is compatible with the S3 API, all popular applications will work without modification, by simply adding the branch name to the object path: . ",
    "url": "/#why-you-need-lakefs-and-what-it-can-do",
    "relUrl": "/#why-you-need-lakefs-and-what-it-can-do"
  },"204": {
    "doc": "What is lakeFS",
    "title": "Use-cases:",
    "content": "lakeFS enhances processing workflows at each step of the data lifecycle: . In Development . | Experiment - try new tools, upgrade versions, and evaluate code changes in isolation. By creating a branch of the data you get an isolated snapshot to run experiments over, while others are not exposed. Compare between branches with different experiments or to the main branch of the repository to understand a change’s impact. | Debug - checkout specific commits in a repository’s commit history to materialize consistent, historical versions of your data. See the exact state of your data at the point-in-time of an error to understand its root cause. | Collaborate - avoid managing data access at the two extremes of either 1) treating your data lake like a shared folder or 2) creating multiple copies of the data to safely collaborate. Instead, leverage isolated branches managed by metadata (not copies of files) to work in parallel. | . Learn more . During Deployment . | Version Control - deploy data safely with CI/CD workflows borrowed from software engineering best practices. Ingest new data onto an isolated branch, perform data validations, then add to production through a merge operation. | Test - define pre-merge and pre-commit hooks to run tests that enforce schema and validate properties of the data to catch issues before they reach production. | . Learn more . In Production . | Roll Back - recover from errors by instantly reverting data to a former, consistent snapshot of the data lake. Choose any commit in a repository’s commit history to revert in one atomic action. | Troubleshoot - investigate production errors by starting with a snapshot of the inputs to the failed process. Spend less time re-creating the state of datasets at the time of failure, and more time finding the solution. | Cross-collection Consistency - provide consumers multiple synchronized collections of data in one atomic, revertable action. Using branches, writers provide consistency guarantees across different logical collections - merging to the main branch only after all relevant datasets have been created or updated successfully. | . Learn more . ",
    "url": "/#use-cases",
    "relUrl": "/#use-cases"
  },"205": {
    "doc": "What is lakeFS",
    "title": "Downloads",
    "content": "Binary Releases . Binary packages are available for Linux/macOS/Windows on GitHub Releases . Docker Images . Official Docker images are available at https://hub.docker.com/r/treeverse/lakefs . ",
    "url": "/#downloads",
    "relUrl": "/#downloads"
  },"206": {
    "doc": "What is lakeFS",
    "title": "Next steps",
    "content": "Read about the branching model of lakeFS or run it locally and see how it works for yourself! . Check out the Quick Start Guide . ",
    "url": "/#next-steps",
    "relUrl": "/#next-steps"
  },"207": {
    "doc": "Install lakeFS",
    "title": "Install lakeFS",
    "content": "Note: The quickstart section is for learning purposes. The installations below will not persist your data. Instead, it will spin-up a database in a docker container, which will be discarded later. For a production suitable deployment, see Deploying on AWS. ",
    "url": "/quickstart/installing.html",
    "relUrl": "/quickstart/installing.html"
  },"208": {
    "doc": "Install lakeFS",
    "title": "Using docker-compose",
    "content": "Other quickstart methods can be found here. To run a local lakeFS instance using Docker Compose: . | Ensure you have Docker &amp; Docker Compose installed on your computer, and that compose version is 1.25.04 or higher. For more information, please see this issue. | Run the following command in your terminal: . curl https://compose.lakefs.io | docker-compose -f - up . | Check your installation by opening http://127.0.0.1:8000/setup in your web browser. | . Next steps . Now that your lakeFS is running, try creating a repository. ",
    "url": "/quickstart/installing.html#using-docker-compose",
    "relUrl": "/quickstart/installing.html#using-docker-compose"
  },"209": {
    "doc": "With Kubernetes",
    "title": "Deploy lakeFS on Kubernetes",
    "content": " ",
    "url": "/deploy/k8s.html#deploy-lakefs-on-kubernetes",
    "relUrl": "/deploy/k8s.html#deploy-lakefs-on-kubernetes"
  },"210": {
    "doc": "With Kubernetes",
    "title": "Database",
    "content": "lakeFS requires a PostgreSQL database to synchronize actions on your repositories. This section assumes you already have a PostgreSQL database accessible from your Kubernetes cluster. Instructions for creating the database can be found on the deployment instructions for AWS, Azure and GCP. ",
    "url": "/deploy/k8s.html#database",
    "relUrl": "/deploy/k8s.html#database"
  },"211": {
    "doc": "With Kubernetes",
    "title": "Table of contents",
    "content": ". | Prerequisites | Installing on Kubernetes | Load balancing | DNS | Next Steps | . ",
    "url": "/deploy/k8s.html#table-of-contents",
    "relUrl": "/deploy/k8s.html#table-of-contents"
  },"212": {
    "doc": "With Kubernetes",
    "title": "Prerequisites",
    "content": "A production-suitable lakeFS installation will require three DNS records pointing at your lakeFS server. A good convention for those will be, assuming you already own the domain example.com: . | lakefs.example.com | s3.lakefs.example.com - this is the S3 Gateway Domain | *.s3.lakefs.example.com | . The second record, the S3 Gateway Domain, needs to be specified in the lakeFS configuration (see the S3_GATEWAY_DOMAIN placeholder below). This will allow lakeFS to route requests to the S3-compatible API. For more info, see Why do I need these three DNS records? . ",
    "url": "/deploy/k8s.html#prerequisites",
    "relUrl": "/deploy/k8s.html#prerequisites"
  },"213": {
    "doc": "With Kubernetes",
    "title": "Installing on Kubernetes",
    "content": "lakeFS can be easily installed on Kubernetes using a Helm chart. To install lakeFS with Helm: . | Copy the Helm values file relevant to your storage provider: . | S3 | GCS | Azure Blob | . secrets: # replace DATABASE_CONNECTION_STRING with the connection string of the database you created in a previous step. # e.g. postgres://postgres:myPassword@my-lakefs-db.rds.amazonaws.com:5432/lakefs databaseConnectionString: [DATABASE_CONNECTION_STRING] # replace this with a randomly-generated string authEncryptSecretKey: [ENCRYPTION_SECRET_KEY] lakefsConfig: | blockstore: type: s3 s3: region: us-east-1 gateways: s3: # replace this with the host you will use for the lakeFS S3-compatible endpoint: domain_name: [S3_GATEWAY_DOMAIN] . secrets: # replace DATABASE_CONNECTION_STRING with the connection string of the database you created in a previous step. # e.g.: postgres://postgres:myPassword@localhost/postgres:5432 databaseConnectionString: [DATABASE_CONNECTION_STRING] # replace this with a randomly-generated string authEncryptSecretKey: [ENCRYPTION_SECRET_KEY] lakefsConfig: | blockstore: type: gs # Uncomment the following lines to give lakeFS access to your buckets using a service account: # gs: # credentials_json: [YOUR SERVICE ACCOUNT JSON STRING] gateways: s3: # replace this with the host you will use for the lakeFS S3-compatible endpoint: domain_name: [S3_GATEWAY_DOMAIN] . Notes for running lakeFS on GKE . | To connect to your database, you need to use one of the ways of connecting GKE to Cloud SQL. | To give lakeFS access to your bucket, you can start the cluster in storage-rw mode. Alternatively, you can use a service account JSON string by uncommenting the gs.credentials_json property in the following yaml. | . secrets: # replace this with the connection string of the database you created in a previous step: databaseConnectionString: [DATABASE_CONNECTION_STRING] # replace this with a randomly-generated string authEncryptSecretKey: [ENCRYPTION_SECRET_KEY] lakefsConfig: | blockstore: type: azure azure: auth_method: msi # msi for active directory, access-key for access key # In case you chose to authenticate via access key unmark the following rows and insert the values from the previous step # storage_account: [your storage account] # storage_access_key: [your access key] gateways: s3: # replace this with the host you will use for the lakeFS S3-compatible endpoint: domain_name: s3.lakefs.example.com . | Fill in the missing values and save the file as conf-values.yaml. For more configuration options, see our Helm chart README. The lakefsConfig parameter is the lakeFS configuration documented here, but without sensitive information. Sensitive information like databaseConnectionString is given through separate parameters, and the chart will inject them into Kubernetes secrets. | In the directory where you created conf-values.yaml, run the following commands: . # Add the lakeFS repository helm repo add lakefs https://charts.lakefs.io # Deploy lakeFS helm install example-lakefs lakefs/lakefs -f conf-values.yaml . example-lakefs is the Helm Release name. | . You should give your Kubernetes nodes access to all buckets/containers you intend to use lakeFS with. If you can’t provide such access, lakeFS can be configured to use an AWS key-pair, an Azure access key, or a Google Cloud credentials file to authenticate (part of the lakefsConfig YAML below). ",
    "url": "/deploy/k8s.html#installing-on-kubernetes",
    "relUrl": "/deploy/k8s.html#installing-on-kubernetes"
  },"214": {
    "doc": "With Kubernetes",
    "title": "Load balancing",
    "content": "You should have a load balancer direct requests to the lakeFS server. Options to do so include a Kubernetes Service of type LoadBalancer, or a Kubernetes Ingress. By default, lakeFS operates on port 8000, and exposes a /_health endpoint which you can use for health checks. ",
    "url": "/deploy/k8s.html#load-balancing",
    "relUrl": "/deploy/k8s.html#load-balancing"
  },"215": {
    "doc": "With Kubernetes",
    "title": "DNS",
    "content": "As mentioned above, you should create 3 DNS records for lakeFS: . | One record for the lakeFS API: lakefs.example.com | Two records for the S3-compatible API: s3.lakefs.example.com and *.s3.lakefs.example.com. | . All records should point to your Load Balancer, preferably with a short TTL value. ",
    "url": "/deploy/k8s.html#dns",
    "relUrl": "/deploy/k8s.html#dns"
  },"216": {
    "doc": "With Kubernetes",
    "title": "Next Steps",
    "content": "Your next step is to prepare your storage. If you already have a storage bucket/container, you are ready to create your first lakeFS repository. ",
    "url": "/deploy/k8s.html#next-steps",
    "relUrl": "/deploy/k8s.html#next-steps"
  },"217": {
    "doc": "With Kubernetes",
    "title": "Why do I need the three DNS records?",
    "content": "Multiple DNS records are needed to access the two different lakeFS APIs (covered in more detail in the Architecture section): . | The lakeFS OpenAPI: used by the lakectl CLI tool. Exposes git-like operations (branching, diffing, merging etc.). | An S3-compatible API: read and write your data in any tool that can communicate with S3. Examples include: AWS CLI, Boto, Presto and Spark. | . lakeFS actually exposes only one API endpoint. For every request, lakeFS checks the Host header. If the header is under the S3 gateway domain, the request is directed to the S3-compatible API. The third DNS record (*.s3.lakefs.example.com) allows for virtual-host style access. This is a way for AWS clients to specify the bucket name in the Host subdomain. ",
    "url": "/deploy/k8s.html#why-do-i-need-the-three-dns-records",
    "relUrl": "/deploy/k8s.html#why-do-i-need-the-three-dns-records"
  },"218": {
    "doc": "With Kubernetes",
    "title": "With Kubernetes",
    "content": " ",
    "url": "/deploy/k8s.html",
    "relUrl": "/deploy/k8s.html"
  },"219": {
    "doc": "Kafka",
    "title": "Using lakeFS with Kafka",
    "content": "Apache Kafka provides a unified, high-throughput, low-latency platform for handling real-time data feeds. Different distributions of Kafka have different methods for exporting data to s3, called Kafka Sink Connectors. Most commonly used for S3 is Confluent’s S3 Sink Connector. Add the following to connector.properties file for lakeFS support: . # Your lakeFS repository s3.bucket.name=example-repo # Your lakeFS S3 endpoint and credentials store.url=https://s3.lakefs.example.com aws.access.key.id=AKIAIOSFODNN7EXAMPLE aws.secret.access.key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # main being the branch we want to write to topics.dir=main/topics . ",
    "url": "/integrations/kakfa.html#using-lakefs-with-kafka",
    "relUrl": "/integrations/kakfa.html#using-lakefs-with-kafka"
  },"220": {
    "doc": "Kafka",
    "title": "Kafka",
    "content": " ",
    "url": "/integrations/kakfa.html",
    "relUrl": "/integrations/kakfa.html"
  },"221": {
    "doc": "Kubeflow",
    "title": "Using lakeFS with Kubeflow pipelines",
    "content": "Kubeflow is a project dedicated to making deployments of ML workflows on Kubernetes simple, portable and scalable. A Kubeflow pipeline is a portable and scalable definition of an ML workflow composed of steps. Each step in the pipeline is an instance of a component represented as an instance of ContainerOp. ",
    "url": "/integrations/kubeflow.html#using-lakefs-with-kubeflow-pipelines",
    "relUrl": "/integrations/kubeflow.html#using-lakefs-with-kubeflow-pipelines"
  },"222": {
    "doc": "Kubeflow",
    "title": "Table of contents",
    "content": ". | Add pipeline steps for lakeFS operations . | Function-based ContainerOps | Non-function-based ContainerOps | . | Add the lakeFS steps to your pipeline | . ",
    "url": "/integrations/kubeflow.html#table-of-contents",
    "relUrl": "/integrations/kubeflow.html#table-of-contents"
  },"223": {
    "doc": "Kubeflow",
    "title": "Add pipeline steps for lakeFS operations",
    "content": "To integrate lakeFS onto your Kubeflow pipeline, we will need to create Kubeflow components that perform lakeFS operations. Currently, there are two methods to create lakeFS ContainerOps: . | Implement a function-based ContainerOp that uses lakeFS’s Python API to invoke lakeFS operations. | Implement a ContainerOp that uses the lakectl CLI docker image to invoke lakeFS operations. | . Function-based ContainerOps . To implement a function-based component that invokes lakeFS operations, you should use the Python OpenAPI client lakeFS has. See the example below that demonstrates how to make the client’s package available to your ContainerOp. Example operations . Create new branch: A function-based ContainerOp that creates a branch called example-branch based on the main branch of example-repo. from kfp import components def create_branch(repo_name, branch_name, source_branch): import lakefs_client from lakefs_client import models from lakefs_client.client import LakeFSClient # lakeFS credentials and endpoint configuration = lakefs_client.Configuration() configuration.username = 'AKIAIOSFODNN7EXAMPLE' configuration.password = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' configuration.host = 'https://lakefs.example.com' client = LakeFSClient(configuration) client.branches.create_branch(repository=repo_name, branch_creation=models.BranchCreation(name=branch_name, source=source_branch)) # Convert the function to a lakeFS pipeline step. create_branch_op = components.func_to_container_op( func=create_branch, packages_to_install=['lakefs_client==&lt;lakeFS version&gt;']) # Type in the lakeFS version you are using . You can invoke any lakeFS operation supported by lakeFS OpenAPI, for example, you could implement a commit and merge function-based ContainerOps. Check out the full API reference. Non-function-based ContainerOps . To implement a non-function based ContainerOp, you should use the treeverse/lakectl docker image. With this image you can run lakeFS CLI commands to execute the desired lakeFS operation. For lakectl to work with Kubeflow, you will need to pass your lakeFS configurations as environment variables named: . | LAKECTL_CREDENTIALS_ACCESS_KEY_ID: AKIAIOSFODNN7EXAMPLE | LAKECTL_SECRET_ACCESS_KEY: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY | LAKECTL_SERVER_ENDPOINT_URL: https://lakefs.example.com | . Example operations . | Commit changes to a branch: A ContainerOp that commits uncommitted changes to example-branch on example-repo. from kubernetes.client.models import V1EnvVar def commit_op(): return dsl.ContainerOp( name='commit', image='treeverse/lakectl', arguments=['commit', 'lakefs://example-repo/example-branch', '-m', 'commit message']).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_ACCESS_KEY_ID',value='AKIAIOSFODNN7EXAMPLE')).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY',value='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')).add_env_variable(V1EnvVar(name='LAKECTL_SERVER_ENDPOINT_URL',value='https://lakefs.example.com')) . | Merge two lakeFS branches: A ContainerOp that merges example-branch into the main branch of example-repo. def merge_op(): return dsl.ContainerOp( name='merge', image='treeverse/lakectl', arguments=['merge', 'lakefs://example-repo/example-branch', 'lakefs://example-repo/main']).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_ACCESS_KEY_ID',value='AKIAIOSFODNN7EXAMPLE')).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY',value='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')).add_env_variable(V1EnvVar(name='LAKECTL_SERVER_ENDPOINT_URL',value='https://lakefs.example.com')) . | . You can invoke any lakeFS operation supported by lakectl by implementing it as a ContainerOp. Check out the complete CLI reference for the list of supported operations. Note The lakeFS Kubeflow integration that uses lakectl is supported on lakeFS version &gt;= v0.43.0. ",
    "url": "/integrations/kubeflow.html#add-pipeline-steps-for-lakefs-operations",
    "relUrl": "/integrations/kubeflow.html#add-pipeline-steps-for-lakefs-operations"
  },"224": {
    "doc": "Kubeflow",
    "title": "Add the lakeFS steps to your pipeline",
    "content": "Add the steps created on the previous step to your pipeline before compiling it. Example pipeline . A pipeline that implements a simple ETL, that has steps for branch creation and commits. def lakectl_pipeline(): create_branch_task = create_branch_op('example-repo', 'example-branch', 'main') # A function-based component extract_task = example_extract_op() commit_task = commit_op() transform_task = example_transform_op() commit_task = commit_op() load_task = example_load_op() . Note It is recommended to store credentials as kubernetes secrets and pass them as environment variables to Kubeflow operations using V1EnvVarSource. ",
    "url": "/integrations/kubeflow.html#add-the-lakefs-steps-to-your-pipeline",
    "relUrl": "/integrations/kubeflow.html#add-the-lakefs-steps-to-your-pipeline"
  },"225": {
    "doc": "Kubeflow",
    "title": "Kubeflow",
    "content": " ",
    "url": "/integrations/kubeflow.html",
    "relUrl": "/integrations/kubeflow.html"
  },"226": {
    "doc": "Install lakeFS CLI",
    "title": "Install lakeFS CLI",
    "content": "lakeFS comes with its own native CLI client. You can see the complete command reference here. The CLI is a great way to get started with lakeFS since it is a complete implementation of the lakeFS API. Here’s how to get started with the CLI: . | Download the CLI binary: . Download lakectl . | It’s recommended that you place it somewhere in your PATH (this is OS dependant but for *NIX systems , /usr/local/bin is usually a safe bet). | configure the CLI to use the credentials you’ve created earlier: . lakectl config # output: # Config file /home/janedoe/.lakectl.yaml will be used # Access key ID: AKIAJVHTOKZWGCD2QQYQ # Secret access key: **************************************** # Server endpoint URL: http://localhost:8000/api/v1 . | Now that we’ve configured it, let’s run a few sample commands: . lakectl branch list lakefs://example-repo # output: # +----------+------------------------------------------------------------------+ # | REF NAME | COMMIT ID | # +----------+------------------------------------------------------------------+ # | main | a91f56a7e11be1348fc405053e5234e4af7d6da01ed02f3d9a8ba7b1f71499c8 | # +----------+------------------------------------------------------------------+ lakectl commit lakefs://example-repo/main -m 'added our first file!' # output: # Commit for branch \"main\" done. # # ID: 901f7b21e1508e761642b142aea0ccf28451675199655381f65101ea230ebb87 # Timestamp: 2021-06-15 13:48:37 +0300 IDT # Parents: a91f56a7e11be1348fc405053e5234e4af7d6da01ed02f3d9a8ba7b1f71499c8 lakectl log lakefs://example-repo/main # output: # commit 901f7b21e1508e761642b142aea0ccf28451675199655381f65101ea230ebb87 # Author: Example User &lt;user@example.com&gt; # Date: 2021-06-15 13:48:37 +0300 IDT added our first file! . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. | . Next steps . Once you’re ready to test lakeFS with a real workflow, it’s time to deploy lakeFS to AWS. ",
    "url": "/quickstart/lakefs_cli.html",
    "relUrl": "/quickstart/lakefs_cli.html"
  },"227": {
    "doc": "Licensing",
    "title": "Licensing",
    "content": "lakeFS is an open source project under the Apache 2.0 license. The project was created and is supported by Treeverse, a commercial company, founded by engineers passionate about providing solutions to the evolving world of data engineering. Why did we choose to open the source of our core capabilities? . We believe in bottom up adoption of technologies. We believe collaborative communities have the power to bring the best solutions to the community. We believe small organizations should be able to use cutting edge technologies for free, so they can innovate in their domain and compete with well established companies that can afford paying for technology. As a commercial organization, we intend to use an open core model. What is our commitment to open source? . We define lakeFS, our open source project, as the data lake management platform that provides Git-like operations over an object-storage, that can be utilized by any applications accessing the data lake. In other words, the following will remain part of lakeFS: . | The Versioning capabilities | ACID Guarantees | Git-Like interface for the versioning operations | Support for public object store APIs | Integration to publicly available applications accessing an object storage | Clear CLI, API and GUI interfaces | . We also commit that lakeFS will be scalable in throughput and performance, and will support multi region architecture. We are deeply committed to our community of engineers who use and contribute to the project. We are and will continue to be highly responsive and shape lakeFS together to provide the data lake management capabilities we are all looking for. What will be included in the commercial layer? . Well, it will take a while to figure out, and currently everything we do is open source, but in the future, we will provide a commercial offering according to users’ feedback. For example, SLA based support was already requested by some of our design partners. Apparently some organizations feel comfortable with being early adopters of technology if they have the guarantee that when an issue arises someone is committed contractually to resolve it within a given SLA. If this is your preference, we are here for you. In addition, in the future, we may choose to provide usability features over the core capabilities, enterprise features revolving around user management and security, and a SaaS offering. ",
    "url": "/understand/licensing.html",
    "relUrl": "/understand/licensing.html"
  },"228": {
    "doc": "MapReduce",
    "title": "Using lakeFS with MapReduce",
    "content": " ",
    "url": "/integrations/mapreduce.html#using-lakefs-with-mapreduce",
    "relUrl": "/integrations/mapreduce.html#using-lakefs-with-mapreduce"
  },"229": {
    "doc": "MapReduce",
    "title": "MapReduce",
    "content": " ",
    "url": "/integrations/mapreduce.html",
    "relUrl": "/integrations/mapreduce.html"
  },"230": {
    "doc": "MinIO",
    "title": "Using lakeFS with MinIO",
    "content": "MinIO is a high performance, distributed object storage system. You can use lakeFS to add git-like capabilities over it. For learning purposes, it is recommended to follow our step-by-step guide on how to deploy lakeFS locally over MinIO. If you already know how to install lakeFS, and want to configure it to use MinIO as the underlying storage, your lakeFS configuration should contain the following: . blockstore: type: s3 s3: force_path_style: true endpoint: http://&lt;minio_endpoint&gt;:9000 credentials: access_key_id: &lt;minio_access_key&gt; secret_access_key: &lt;minio_secret_key&gt; . The full example can be found here. Note that lakeFS can also be configured using environment variables. ",
    "url": "/integrations/minio.html#using-lakefs-with-minio",
    "relUrl": "/integrations/minio.html#using-lakefs-with-minio"
  },"231": {
    "doc": "MinIO",
    "title": "MinIO",
    "content": " ",
    "url": "/integrations/minio.html",
    "relUrl": "/integrations/minio.html"
  },"232": {
    "doc": "Monitoring using Prometheus",
    "title": "Monitoring using Prometheus",
    "content": " ",
    "url": "/reference/monitor.html",
    "relUrl": "/reference/monitor.html"
  },"233": {
    "doc": "Monitoring using Prometheus",
    "title": "Table of contents",
    "content": ". | Example prometheus.yml | Metrics exposed by lakeFS | Example queries . | 99th percentile of API request latencies | 50th percentile of S3-compatible API latencies | Number of errors in outgoing S3 requests | Number of open connections to the database | Example Grafana dashboard | . | . ",
    "url": "/reference/monitor.html#table-of-contents",
    "relUrl": "/reference/monitor.html#table-of-contents"
  },"234": {
    "doc": "Monitoring using Prometheus",
    "title": "Example prometheus.yml",
    "content": "lakeFS exposes metrics through the same port used by the lakeFS service, using the standard /metrics path. An example prometheus.yml could look like this: . scrape_configs: - job_name: lakeFS scrape_interval: 10s metrics_path: /metrics static_configs: - targets: - lakefs.example.com:8000 . ",
    "url": "/reference/monitor.html#example-prometheusyml",
    "relUrl": "/reference/monitor.html#example-prometheusyml"
  },"235": {
    "doc": "Monitoring using Prometheus",
    "title": "Metrics exposed by lakeFS",
    "content": "By default, Prometheus exports metrics with OS process information like memory and CPU. It also includes Go-specific metrics like details about GC and number of goroutines. You can learn about these default metrics in this post. In addition, lakeFS exposes the following metrics to help monitor your deployment: . | Name in Prometheus | Description | Labels | . | api_requests_total | lakeFS API requests (counter) | code: http statusmethod: http method | . | api_request_duration_seconds | Durations of lakeFS API requests (histogram) | operation: name of API operationcode: http status | . | gateway_request_duration_seconds | lakeFS S3-compatible endpoint request (histogram) | operation: name of gateway operationcode: http status | . | s3_operation_duration_seconds | Outgoing S3 operations (histogram) | operation: operation nameerror: “true” if error, “false” otherwise | . | gs_operation_duration_seconds | Outgoing Google Storage operations (histogram) | operation: operation nameerror: “true” if error, “false” otherwise | . | azure_operation_duration_seconds | Outgoing Azure storage operations (histogram) | operation: operation nameerror: “true” if error, “false” otherwise | . | go_sql_stats_* | Go DB stats metrics have this prefix.dlmiddlecote/sqlstats is used to expose them. |   | . ",
    "url": "/reference/monitor.html#metrics-exposed-by-lakefs",
    "relUrl": "/reference/monitor.html#metrics-exposed-by-lakefs"
  },"236": {
    "doc": "Monitoring using Prometheus",
    "title": "Example queries",
    "content": "Note: when using Prometheus functions like rate or increase, results are extrapolated and may not be exact. 99th percentile of API request latencies . sum by (operation)(histogram_quantile(0.99, rate(api_request_duration_seconds_bucket[1m]))) . 50th percentile of S3-compatible API latencies . sum by (operation)(histogram_quantile(0.5, rate(gateway_request_duration_seconds_bucket[1m]))) . Number of errors in outgoing S3 requests . sum by (operation) (increase(s3_operation_duration_seconds_count{error=\"true\"}[1m])) . Number of open connections to the database . go_sql_stats_connections_open . Example Grafana dashboard . ",
    "url": "/reference/monitor.html#example-queries",
    "relUrl": "/reference/monitor.html#example-queries"
  },"237": {
    "doc": "More Quickstart Options",
    "title": "More Quickstart Options",
    "content": "Note: The quickstart section is for learning purposes. The installations below will not persist your data. Instead, it will spin-up a database in a docker container, which will be discarded later. For a production suitable deployment, see Deploying on AWS. ",
    "url": "/quickstart/more_quickstart_options.html",
    "relUrl": "/quickstart/more_quickstart_options.html"
  },"238": {
    "doc": "More Quickstart Options",
    "title": "Table of Contents",
    "content": ". | Docker on Windows | On Kubernetes with Helm | Using the Binary | . ",
    "url": "/quickstart/more_quickstart_options.html#table-of-contents",
    "relUrl": "/quickstart/more_quickstart_options.html#table-of-contents"
  },"239": {
    "doc": "More Quickstart Options",
    "title": "Docker on Windows",
    "content": "To run a local lakeFS instance using Docker Compose: . | Ensure you have Docker installed on your computer, and that compose version is 1.25.04 or higher. For more information, please see this issue. | Run the following command in your terminal: . Invoke-WebRequest https://compose.lakefs.io | Select-Object -ExpandProperty Content | docker-compose -f - up . | Check your installation by opening http://127.0.0.1:8000/setup in your web browser. | . ",
    "url": "/quickstart/more_quickstart_options.html#docker-on-windows",
    "relUrl": "/quickstart/more_quickstart_options.html#docker-on-windows"
  },"240": {
    "doc": "More Quickstart Options",
    "title": "On Kubernetes with Helm",
    "content": "You can install lakeFS on a Kubernetes cluster with the following commands: . # Add the lakeFS Helm repository helm repo add lakefs https://charts.lakefs.io # Deploy lakeFS with helm release \"my-lakefs\" helm install my-lakefs lakefs/lakefs . ",
    "url": "/quickstart/more_quickstart_options.html#on-kubernetes-with-helm",
    "relUrl": "/quickstart/more_quickstart_options.html#on-kubernetes-with-helm"
  },"241": {
    "doc": "More Quickstart Options",
    "title": "Using the Binary",
    "content": "Alternatively, you may opt to run the lakefs binary directly on your computer. | Download the lakeFS binary for your operating system: . Download lakefs . | Install and configure PostgreSQL . | Create a configuration file: . --- database: connection_string: \"postgres://localhost:5432/postgres?sslmode=disable\" blockstore: type: \"local\" local: path: \"~/lakefs_data\" auth: encrypt: secret_key: \"a random string that should be kept secret\" gateways: s3: domain_name: s3.local.lakefs.io:8000 . | Create a local directory to store objects: . mkdir ~/lakefs_data . | Run the server: ./lakefs --config /path/to/config.yaml run . | . ",
    "url": "/quickstart/more_quickstart_options.html#using-the-binary",
    "relUrl": "/quickstart/more_quickstart_options.html#using-the-binary"
  },"242": {
    "doc": "Object Model",
    "title": "Commands (CLI) Reference",
    "content": " ",
    "url": "/reference/object-model.html#commands-cli-reference",
    "relUrl": "/reference/object-model.html#commands-cli-reference"
  },"243": {
    "doc": "Object Model",
    "title": "Table of contents",
    "content": ". | Introduction | lakeFS the object store | lakeFS the version control system . | Repository | Commits | Identifying commits . | Tags | Branches | Ref expressions | . | History | Three way merge | . | Concepts unique to lakeFS . | lakefs protocol URIs | . | . ",
    "url": "/reference/object-model.html#table-of-contents",
    "relUrl": "/reference/object-model.html#table-of-contents"
  },"244": {
    "doc": "Object Model",
    "title": "Introduction",
    "content": "lakeFS blends concepts from object stores such as S3 with concepts from Git. This reference defines the common concepts of lakeFS. Every concept appearing in italic text is its definition. ",
    "url": "/reference/object-model.html#introduction",
    "relUrl": "/reference/object-model.html#introduction"
  },"245": {
    "doc": "Object Model",
    "title": "lakeFS the object store",
    "content": "lakeFS is an object store, and borrows concepts from S3. An object store links objects to paths. An object holds: . | Some contents, with unlimited size and format. | Some metadata, including . | size in bytes | the creation time, a timestamp with seconds resolution | a checksum string which uniquely identifies the contents | some user metadata, a small map of strings to strings. | . | . Similarly to many object stores, lakeFS objects are immutable and never rewritten. They can be entirely replaced or deleted, but not modified. A path is a readable string, typically decoded as UTF-8. lakeFS maps paths to their objects according to specific rules. lakeFS paths use the lakefs protocol, described below. ",
    "url": "/reference/object-model.html#lakefs-the-object-store",
    "relUrl": "/reference/object-model.html#lakefs-the-object-store"
  },"246": {
    "doc": "Object Model",
    "title": "lakeFS the version control system",
    "content": "lakeFS borrows its concepts for version control from Git. Repository . A repository is a collection of objects with common history tracking. lakeFS manages versions of the repository, identified by their commits. A commit is a collection of object metadata and data, including especially all paths and the object contents and metadata at that commit. Commits have their own commit metadata, which includes a textual comment and additional user metadata. Commits . Commits are organized into a history using their parent commits. Every repository has exactly one initial commit with no parents. Note that a Git repository may have multiple initial commits. A commit with more than one parent is a merge commit. Currently lakeFS only supports merge commits with two parents. Identifying commits . A commit is identified by its commit ID, a digest of all contents of the commit. Commit IDs are by nature long, so a unique prefix may be used to abbreviate them (but note that short prefixes can become non-unique as the repository grows, so prefer to avoid abbreviating when storing commits for the long term). A commit may also be identified by using a textual definition, called a ref. Examples of refs include tags, branches, and expressions. The state of the repository at any commit is always readable. Tags . A tag is an immutable pointer to a single commit. Tags have readable names. Because tags are commits, a repository can be read from any tag. Example tags: . | v2.3 to mark a release | dev:jane-before-v2.3-merge to mark Jane’s private temporary point. | . Branches . A branch is a mutable pointer to a commit and its staging area. Repositories are readable from any branch, but they are also writable to a branch. The staging area associated with a branch is mutable storage where objects can be created, updated or deleted. These objects are readable when reading from the branch. To create a commit from a branch, all files from the staging area are merged into the contents of the current branch, creating the new set of objects. The parent of the commit is the previous branch tip, and the new branch tip is set to this new commit. Example branches: . | main, the trunk | staging, maybe ahead of main | dev:joe-bugfix-1234 for Joe to fix issue 1234. | . Ref expressions . lakeFS also supports expressions for creating a ref. These are similar to revisions in Git; indeed all ~ and ^ examples at the end of that section will work unchanged in lakeFS. | A branch or a tag are ref expressions. | If &lt;ref&gt; is a ref expression, then: . | &lt;ref&gt;^ is a ref expression referring to its first parent. | &lt;ref&gt;^N is a ref expression referring to its N’th parent; in particular &lt;ref&gt;^1 is the same as &lt;ref&gt;^. | &lt;ref&gt;~ is a ref expression referring to its first parent; in particular &lt;ref&gt;~ is the same as &lt;ref&gt;^ and &lt;ref&gt;~. | &lt;ref&gt;~N is a ref expression referring to its N’th parent, always traversing to the first parent. So &lt;ref&gt;~N is the same as &lt;ref&gt;^^...^ with N consecutive carets ^. | . | . History . The history of the branch is the list of commits from the branch tip through the first parent of each commit. Histories go back in time. The other way to create a commit is to merge an existing commit onto a branch. To merge a source commit into a branch, lakeFS finds the best common ancestor of that source commit and the branch tip, called the “base”. Then it performs a 3-way merge. The “best” ancestor is exactly that defined in the documentation for git-merge-base. The result of a merge is a new commit, with the destination as the first parent and the source as the second. Thus the previous tip of the merge destination is part of the history of the merged object. Three way merge . To merge a merge source (a commit) into a merge destination (another commit), lakeFS first finds the merge base, the nearest common parent of the two commits. It can now perform a three-way merge, by examining the presence and identity of files in each commit. In the table below, “A”, “B” and “C” are possible file contents, “X” is a missing file, and “conflict” (which only appears as a result) is a merge failure. | In base | In source | In destination | Result | Comment | . | A | A | A | A | Unchanged file | . | A | B | B | B | Files changed on both sides in same way | . | A | B | C | conflict | Files changed on both sides differently | . | A | A | B | B | File changed only on one branch | . | A | B | A | B | File changed only on one branch | . | A | X | X | X | Files deleted on both sides | . | A | B | X | conflict | File changed on one side, deleted on the other | . | A | X | B | conflict | File changed on one side, deleted on the other | . | A | A | X | X | File deleted on one side | . | A | X | A | X | File deleted on one side | . As a format-agnostic system, lakeFS currently merges by complete files. Format-specific and other user-defined merge strategies for handling conflicts are on the roadmap. ",
    "url": "/reference/object-model.html#lakefs-the-version-control-system",
    "relUrl": "/reference/object-model.html#lakefs-the-version-control-system"
  },"247": {
    "doc": "Object Model",
    "title": "Concepts unique to lakeFS",
    "content": "Underlying storage is the area on some other object store that lakeFS uses to store object contents and some of its metadata. We sometimes refer to underlying storage as physical. The path used to store the contents of an object is then termed a physical path. The object itself on underlying storage is never modified, except to remove it entirely during some cleanups. A lot of what lakeFS does is to manage how lakeFS paths translate to physical paths on the object store. This mapping is generally not straightforward. Importantly (and unlike many object stores), lakeFS may map multiple paths to the same object on backing storage, and always does this for objects that are unchanged across versions. lakefs protocol URIs . lakeFS uses a specific format for path URIs. The URI lakefs://&lt;REPO&gt;/&lt;REF&gt;/&lt;KEY&gt; is a path to objects in the given repo and ref expression under key. This is used both for path prefixes and for full paths. In similar fashion, lakefs://&lt;REPO&gt;/&lt;REF&gt; identifies the repository at a ref expression, and lakefs://&lt;REPO&gt; identifes a repo. ",
    "url": "/reference/object-model.html#concepts-unique-to-lakefs",
    "relUrl": "/reference/object-model.html#concepts-unique-to-lakefs"
  },"248": {
    "doc": "Object Model",
    "title": "Object Model",
    "content": " ",
    "url": "/reference/object-model.html",
    "relUrl": "/reference/object-model.html"
  },"249": {
    "doc": "Migrating away from lakeFS",
    "title": "Migrating away from lakeFS",
    "content": " ",
    "url": "/reference/offboarding.html",
    "relUrl": "/reference/offboarding.html"
  },"250": {
    "doc": "Migrating away from lakeFS",
    "title": "Copying data from a lakeFS repository to an S3 bucket",
    "content": "The simplest way to migrate away from lakeFS is to copy data from a lakeFS repository to an S3 bucket (or any other object store). For smaller repositories, this could be done using the AWS cli or rclone. For larger repositories, running distcp with lakeFS as the source is also an option. ",
    "url": "/reference/offboarding.html#copying-data-from-a-lakefs-repository-to-an-s3-bucket",
    "relUrl": "/reference/offboarding.html#copying-data-from-a-lakefs-repository-to-an-s3-bucket"
  },"251": {
    "doc": "Presto/Trino",
    "title": "Using lakeFS with Presto/Trino",
    "content": "Presto and Trino are a distributed SQL query engine designed to query large data sets distributed over one or more heterogeneous data sources. ",
    "url": "/integrations/presto_trino.html#using-lakefs-with-prestotrino",
    "relUrl": "/integrations/presto_trino.html#using-lakefs-with-prestotrino"
  },"252": {
    "doc": "Presto/Trino",
    "title": "Table of contents",
    "content": ". | Using lakeFS with Presto/Trino . | | Configuration . | Configure Hive connector | Configure Hive | . | Examples . | Example with schema | Example with External table | Example of copying a table with metastore tools: | . | . | . Querying data in lakeFS from Presto/Trino is the same as querying data in S3 from Presto/Trino. It is done using the Presto Hive connector or Trino Hive connector. Note In the following examples we set AWS credentials at runtime, for clarity. In production, these properties should be set using one of Hadoop’s standard ways of Authenticating with S3. ",
    "url": "/integrations/presto_trino.html#table-of-contents",
    "relUrl": "/integrations/presto_trino.html#table-of-contents"
  },"253": {
    "doc": "Presto/Trino",
    "title": "Configuration",
    "content": "Configure Hive connector . Create /etc/catalog/hive.properties with the following contents to mount the hive-hadoop2 connector as the hive catalog, replacing example.net:9083 with the correct host and port for your Hive metastore Thrift service: . connector.name=hive-hadoop2 hive.metastore.uri=thrift://example.net:9083 . Add to /etc/catalog/hive.properties the lakeFS configurations in the corresponding S3 configuration properties: . hive.s3.aws-access-key=AKIAIOSFODNN7EXAMPLE hive.s3.aws-secret-key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY hive.s3.endpoint=https://s3.lakefs.example.com . Configure Hive . Presto/Trino uses Hive metastore service (HMS), or a compatible implementation of the Hive metastore, such as AWS Glue Data Catalog to write data to S3. In case you are using Hive metastore, you will need to configure Hive as well. In file hive-site.xml add to the configuration: . &lt;configuration&gt; ... &lt;property&gt; &lt;name&gt;fs.s3a.access.key&lt;/name&gt; &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.secret.key&lt;/name&gt; &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;https://s3.lakefs.example.com&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . ",
    "url": "/integrations/presto_trino.html#configuration",
    "relUrl": "/integrations/presto_trino.html#configuration"
  },"254": {
    "doc": "Presto/Trino",
    "title": "Examples",
    "content": "Here are some examples based on examples from the Presto Hive connector examples and Trino Hive connector examples . Example with schema . Create a new schema named main that will store tables in a lakeFS repository named example branch: master: . CREATE SCHEMA main WITH (location = 's3a://example/main') . Create a new Hive table named page_views in the web schema that is stored using the ORC file format, partitioned by date and country, and bucketed by user into 50 buckets (note that Hive requires the partition columns to be the last columns in the table): . CREATE TABLE main.page_views ( view_time timestamp, user_id bigint, page_url varchar, ds date, country varchar ) WITH ( format = 'ORC', partitioned_by = ARRAY['ds', 'country'], bucketed_by = ARRAY['user_id'], bucket_count = 50 ) . Example with External table . Create an external Hive table named request_logs that points at existing data in lakeFS: . CREATE TABLE main.request_logs ( request_time timestamp, url varchar, ip varchar, user_agent varchar ) WITH ( format = 'TEXTFILE', external_location = 's3a://example/main/data/logs/' ) . Example of copying a table with metastore tools: . Copy the created table page_views on schema main to schema example_branch with location s3a://example/example_branch/page_views/ . lakectl metastore copy --from-schema main --from-table page_views --to-branch example_branch . ",
    "url": "/integrations/presto_trino.html#examples",
    "relUrl": "/integrations/presto_trino.html#examples"
  },"255": {
    "doc": "Presto/Trino",
    "title": "Presto/Trino",
    "content": " ",
    "url": "/integrations/presto_trino.html",
    "relUrl": "/integrations/presto_trino.html"
  },"256": {
    "doc": "In Production",
    "title": "In Production",
    "content": "Errors with data in production inevitably occur. When they do, they best thing we can do is remove the erroneous data, understand why the issue happened, and deploy changes that prevent it from occurring again. Example 1: RollBack! - Data ingested from a Kafka stream . If you introduce a new code version to production and discover it has a critical bug, you can simply roll back to the previous version. But you also need to roll back the results of running it. lakeFS gives you the power to rollback your data if you introduced low quality data. The rollback is an atomic action that prevents the data consumers from receiving low quality data until the issue is resolved. As previously mentioned, with lakeFS the recommended branching schema is to ingest data to a dedicated branch. When streaming data, we can decide to merge the incoming data to main at a given time interval or checkpoint, depending on how we chose to write it from Kafka. You can run quality tests for each merge (as presented in Example 1). Alas, tests are not perfect and we might still introduce low quality data at some point. In such a case, we can rollback main to the last known high quality commit, since our commits for streaming will include the metadata of the Kafka offset. Rolling back a branch to a previous commit using the CLI . lakectl branch reset lakefs://example-repo/stream-1 --commit ~79RU9aUsQ9GLnU . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. Example 2: Troubleshoot - Reproduce a bug in production . You upgraded spark and deployed changes in production. A few days or weeks later, you identify a data quality issue, a performance degradation, or an increase to your infra costs. Something that requires investigation and fixing (aka, a bug). lakeFS allows you to open a branch of your lake from the specific merge/commit that introduced the changes to production. Using the metadata saved on the merge/commit you can reproduce all aspects of the environment, then reproduce the issue on the branch and debug it. Meanwhile, you can revert the main to a previous point in time, or keep it as is, depending on the use case . Reading from a historic version (a previous commit) using Spark . // represents the data as existed at commit \"~79RU9aUsQ9GLnU\": spark.read.parquet(\"s3://example-repo/~79RU9aUsQ9GLnU/events/by-date\") . Example 3: Cross collection consistency . We often need consistency between different data collections. A few examples may be: . | To join different collections in order to create a unified view of an account, a user or another entity we measure. | To introduce the same data in different formats | To introduce the same data with a different leading index or sorting due to performance considerations | . lakeFS will help ensure you introduce only consistent data to your consumers by exposing the new collections and their join in one atomic action to main. Once you consumed the collections on a different branch, and only when both are synchronized, we calculated the join and merged to main. In this example you can see two data sets (Sales data and Marketing data) consumed each to its own independent branch, and after the write of both data sets is completed, they are merged to a different branch (leads branch) where the join ETL runs and creates a joined collection by account. The joined table is then merged to main. The same logic can apply if the data is ingested in streaming, using standard formats, or formats that allow upsert/delete such as Apache Hudi, Delta Lake or Iceberg. ",
    "url": "/usecases/production.html",
    "relUrl": "/usecases/production.html"
  },"257": {
    "doc": "In Production",
    "title": "Case Study: Windward",
    "content": "See how Windward is using lakeFS’ isolation and atomic commits to achieve consistency on top of S3. ",
    "url": "/usecases/production.html#case-study-windward",
    "relUrl": "/usecases/production.html#case-study-windward"
  },"258": {
    "doc": "Python",
    "title": "Calling the lakeFS API from Python",
    "content": "The lakeFS API is OpenAPI 3.0 compliant, allowing the generation of clients from multiple languages or directly accessed by any HTTP client. For Python, this example uses lakeFS’s python package. The lakefs-client pacakge was created by OpenAPI Generator using our OpenAPI definition served by a lakeFS server. ",
    "url": "/integrations/python.html#calling-the-lakefs-api-from-python",
    "relUrl": "/integrations/python.html#calling-the-lakefs-api-from-python"
  },"259": {
    "doc": "Python",
    "title": "Table of contents",
    "content": ". | Install lakeFS Python Client API | Working with the Client API | Using the generated client . | Creating a repository | Creating a branch, uploading files, committing changes | Merging changes from a branch into main | . | Full API reference | . ",
    "url": "/integrations/python.html#table-of-contents",
    "relUrl": "/integrations/python.html#table-of-contents"
  },"260": {
    "doc": "Python",
    "title": "Install lakeFS Python Client API",
    "content": "Install the Python client using pip: . pip install 'lakefs_client==&lt;lakeFS version&gt;' . The package is available from version &gt;= 0.34.0. ",
    "url": "/integrations/python.html#install-lakefs-python-client-api",
    "relUrl": "/integrations/python.html#install-lakefs-python-client-api"
  },"261": {
    "doc": "Python",
    "title": "Working with the Client API",
    "content": "How to instantiate a client: . import lakefs_client from lakefs_client import models from lakefs_client.client import LakeFSClient # lakeFS credentials and endpoint configuration = lakefs_client.Configuration() configuration.username = 'AKIAIOSFODNN7EXAMPLE' configuration.password = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' configuration.host = 'http://localhost:8000' client = LakeFSClient(configuration) . ",
    "url": "/integrations/python.html#working-with-the-client-api",
    "relUrl": "/integrations/python.html#working-with-the-client-api"
  },"262": {
    "doc": "Python",
    "title": "Using the generated client",
    "content": "Now that we have a client object, we can use it to interact with the API. Creating a repository . repo = models.RepositoryCreation(name='example-repo', storage_namespace='s3://storage-bucket/repos/example-repo', default_branch='main') client.repositories.create_repository(repo) # output: # {'creation_date': 1617532175, # 'default_branch': 'main', # 'id': 'example-repo', # 'storage_namespace': 's3://storage-bucket/repos/example-repo'} . Creating a branch, uploading files, committing changes . List repository branches: . client.branches.list_branches('example-repo') # output: # [{'commit_id': 'cdd673a4c5f42d33acdf3505ecce08e4d839775485990d231507f586ebe97656', 'id': 'main'}] . Create a new branch: . client.branches.create_branch(repository='example-repo', branch_creation=models.BranchCreation(name='experiment-aggregations1', source='main')) # output: # 'cdd673a4c5f42d33acdf3505ecce08e4d839775485990d231507f586ebe97656' . Let’s list again, to see our newly created branch: . client.branches.list_branches('example-repo').results # output: # [{'commit_id': 'cdd673a4c5f42d33acdf3505ecce08e4d839775485990d231507f586ebe97656', 'id': 'experiment-aggregations1'}, {'commit_id': 'cdd673a4c5f42d33acdf3505ecce08e4d839775485990d231507f586ebe97656', 'id': 'main'}] . Great. Now, let’s upload a file into our new branch: . with open('file.csv', 'rb') as f: client.objects.upload_object(repository='example-repo', branch='experiment-aggregations1', path='path/to/file.csv', content=f) # output: # {'checksum': '0d3b39380e2500a0f60fb3c09796fdba', # 'mtime': 1617534834, # 'path': 'path/to/file.csv', # 'path_type': 'object', # 'physical_address': 'local://example-repo/1865650a296c42e28183ad08e9b068a3', # 'size_bytes': 18} . Diffing a single branch will show all uncommitted changes on that branch: . client.branches.diff_branch(repository='example-repo', branch='experiment-aggregations1').results # output: # [{'path': 'path/to/file.csv', 'path_type': 'object', 'type': 'added'}] . As expected, our change appears here. Let’s commit it, and attach some arbitrary metadata: . client.commits.commit( repository='example-repo', branch='experiment-aggregations1', commit_creation=models.CommitCreation(message='Added a CSV file!', metadata={'using': 'python_api'})) # output: # {'committer': 'barak', # 'creation_date': 1617535120, # 'id': 'e80899a5709509c2daf797c69a6118be14733099f5928c14d6b65c9ac2ac841b', # 'message': 'Added a CSV file!', # 'meta_range_id': '', # 'metadata': {'using': 'python_api'}, # 'parents': ['cdd673a4c5f42d33acdf3505ecce08e4d839775485990d231507f586ebe97656']} . Diffing again, this time there should be no uncommitted files: . client.branches.diff_branch(repository='example-repo', branch='experiment-aggregations1').results # output: # [] . Merging changes from a branch into main . Let’s diff between our branch and the main branch: . client.refs.diff_refs(repository='example-repo', left_ref='experiment-aggregations1', right_ref='main').results # output: # [{'path': 'path/to/file.csv', 'path_type': 'object', 'type': 'added'}] . Looks like we have a change. Let’s merge it: . client.refs.merge_into_branch(repository='example-repo', source_ref='experiment-aggregations1', destination_branch='main') # output: # {'reference': 'd0414a3311a8c1cef1ef355d6aca40db72abe545e216648fe853e25db788fa2e', # 'summary': {'added': 1, 'changed': 0, 'conflict': 0, 'removed': 0}} . Let’s diff again - there should be no changes as all changes are on our main branch already: . client.refs.diff_refs(repository='example-repo', left_ref='experiment-aggregations1', right_ref='main').results # output: # [] . ",
    "url": "/integrations/python.html#using-the-generated-client",
    "relUrl": "/integrations/python.html#using-the-generated-client"
  },"263": {
    "doc": "Python",
    "title": "Full API reference",
    "content": "For a full reference of the lakeFS API, see lakeFS API . ",
    "url": "/integrations/python.html#full-api-reference",
    "relUrl": "/integrations/python.html#full-api-reference"
  },"264": {
    "doc": "Python",
    "title": "Python",
    "content": " ",
    "url": "/integrations/python.html",
    "relUrl": "/integrations/python.html"
  },"265": {
    "doc": "Copying data with Rclone",
    "title": "Copying data with rclone",
    "content": "Rclone is a command line program to sync files and directories between cloud providers. To use it with lakeFS, just create an Rclone remote as describe below, and then use it as you would any other Rclone remote. ",
    "url": "/integrations/rclone.html#copying-data-with-rclone",
    "relUrl": "/integrations/rclone.html#copying-data-with-rclone"
  },"266": {
    "doc": "Copying data with Rclone",
    "title": "Table of contents",
    "content": ". | Creating a remote for lakeFS in Rclone . | Option 1: add an entry in your Rclone configuration file | Option 2: use Rclone interactive config command | . | Examples . | Syncing your data from S3 to lakeFS | Syncing a local directory to lakeFS | . | . ",
    "url": "/integrations/rclone.html#table-of-contents",
    "relUrl": "/integrations/rclone.html#table-of-contents"
  },"267": {
    "doc": "Copying data with Rclone",
    "title": "Creating a remote for lakeFS in Rclone",
    "content": "To add the remote to Rclone, choose one of the following options: . Option 1: add an entry in your Rclone configuration file . | Find the path to your Rclone configuration file and copy it for the next step. rclone config file # output: # Configuration file is stored at: # /home/myuser/.config/rclone/rclone.conf . | If your lakeFS access key is already set in an AWS profile or environment variables, just run the following command, replacing the endpoint property with your lakeFS endpoint: . cat &lt;&lt;EOT &gt;&gt; /home/myuser/.config/rclone/rclone.conf # output: # [lakefs] # type = s3 # provider = AWS # endpoint = https://s3.lakefs.example.com # # EOT . | Otherwise, also include your lakeFS access key pair in the Rclone configuration file: . cat &lt;&lt;EOT &gt;&gt; /home/myuser/.config/rclone/rclone.conf # output: # [lakefs] # type = s3 # provider = AWS # env_auth = false # access_key_id = AKIAIOSFODNN7EXAMPLE # secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # endpoint = https://s3.lakefs.example.com # EOT . | . Option 2: use Rclone interactive config command . Run this command and follow the instructions: . rclone config . Choose AWS S3 as your type of storage, and enter your lakeFS endpoint as your S3 endpoint. You will have to choose whether you use your environment for authentication (recommended), or to enter the lakeFS access key pair into the Rclone configuration. ",
    "url": "/integrations/rclone.html#creating-a-remote-for-lakefs-in-rclone",
    "relUrl": "/integrations/rclone.html#creating-a-remote-for-lakefs-in-rclone"
  },"268": {
    "doc": "Copying data with Rclone",
    "title": "Examples",
    "content": "Syncing your data from S3 to lakeFS . rclone sync mys3remote://mybucket/path/ lakefs:example-repo/main/path . Syncing a local directory to lakeFS . rclone sync /home/myuser/path/ lakefs:example-repo/main/path . ",
    "url": "/integrations/rclone.html#examples",
    "relUrl": "/integrations/rclone.html#examples"
  },"269": {
    "doc": "Copying data with Rclone",
    "title": "Copying data with Rclone",
    "content": " ",
    "url": "/integrations/rclone.html",
    "relUrl": "/integrations/rclone.html"
  },"270": {
    "doc": "Create a Repository",
    "title": "Create a Repository",
    "content": " ",
    "url": "/quickstart/repository.html",
    "relUrl": "/quickstart/repository.html"
  },"271": {
    "doc": "Create a Repository",
    "title": "Create the first user",
    "content": "Once you have a running lakeFS instance, we’ll need to set up an initial admin user in order to log in to the UI and make our first steps in lakeFS! In this guide, we’re going to run an initial setup and then create a new repository. Once we have a repository created, we can start copying and modifying objects, commit and reset changes - and even communicate with this repository from Spark, Presto/Trino or other S3-compatible tools using our S3 Gateway API. | Open http://127.0.0.1:8000/setup in your web browser to set up an initial admin user, used to login and send API requests. | Follow the steps to create an initial administrator user. Save the credentials you’ve received somewhere safe, you won’t be able to see them again! . | Follow the link and go to the login screen . | . ",
    "url": "/quickstart/repository.html#create-the-first-user",
    "relUrl": "/quickstart/repository.html#create-the-first-user"
  },"272": {
    "doc": "Create a Repository",
    "title": "Create the repository",
    "content": ". | Use the credentials from step #2 to login as an administrator | Click Create Repository . A repository is lakeFS’s basic namespace, akin to S3’s Bucket (read more about the data model here). Since we’re using the local block adapter, the value used for Storage Namespace should be a static local://. For a deployment that uses S3 as a block adapter, this would be a bucket name with an optional prefix, e.g. s3://example-bucket/prefix. Notice that lakeFS will only manage the data written under that prefix. All actions on the managed data must go through lakeFS endpoint. Data written directly to the bucket under different paths will be accessible in the same way it was before. | . Next steps . You just created your first lakeFS repository! Go to Copy Files for adding data to your new repository. ",
    "url": "/quickstart/repository.html#create-the-repository",
    "relUrl": "/quickstart/repository.html#create-the-repository"
  },"273": {
    "doc": "Roadmap",
    "title": "Roadmap",
    "content": " ",
    "url": "/understand/roadmap.html",
    "relUrl": "/understand/roadmap.html"
  },"274": {
    "doc": "Roadmap",
    "title": "Table of contents",
    "content": ". | Use Case: Development . | Ephemeral branches with a TTL | . | Use Case: Deployment . | Repo linking | Git-lakeFS integration | Protected Branches | Native Metastore Integration High Priority | Hooks: usability improvements High Priority . | Extended hook types beyond webhooks | Expose merged snapshot to pre-merge hooks | Create and edit hooks directly in the UI: | Post-commit, Post-merge hooks | One-click reusable hook installation | . | . | Use Case: Production . | Webhook Alerting | . | Architecture &amp; Operations . | Decouple ref-store from PostgreSQL | Snowflake Support Requires Discussion | Metadata operations security and access model High Priority | Native Spark OutputCommitter | . | . ",
    "url": "/understand/roadmap.html#table-of-contents",
    "relUrl": "/understand/roadmap.html#table-of-contents"
  },"275": {
    "doc": "Roadmap",
    "title": "Use Case: Development",
    "content": "Ephemeral branches with a TTL . Throwaway development or experimentation branches that live for a pre-configured amount of time, and are cleaned up afterwards. This is especially useful when running automated tests or when experimenting with new technologies, code or algorithms. We want to see what the outcome looks like, but don’t really need the output to live much longer than the duration of the experiment. Track and discuss on GitHub . ",
    "url": "/understand/roadmap.html#use-case-development",
    "relUrl": "/understand/roadmap.html#use-case-development"
  },"276": {
    "doc": "Roadmap",
    "title": "Use Case: Deployment",
    "content": "Repo linking . The ability to explicitly depend on data residing in another repository. While it is possible to state these cross-links by sticking them in the report’s commit metadata, we think a more explicit and structured approach would be valuable. Stating our dependencies in something that resembles a pom.xml or go.mod file would allow us to support better CI and CD integrations that ensure reproducibility without vendoring or copying data. Track and discuss on GitHub . Git-lakeFS integration . The ability to connect Git commits with lakeFS commits. Especially useful for reproducibility: By looking at a set of changes to the data, be able to reference (or ever run) the job that produced it. Track and discuss on GitHub . Protected Branches . A way to ensure certain branches (i.e. main) are only merged to, and are not being directly written to. In combination with Webhook Support, this allows users to provide a set of quality guarantees about a given branch (i.e., reading from main ensures schema never breaks and all partitions are complete and tested) . Track and discuss on GitHub . Native Metastore Integration High Priority . Create a robust connection between a Hive Metastore and lakeFS. Ideally, metastore representations of tables managed in lakeFS should be versioned in the same way. This will allow users to move across different branches or commits for both data and metadata, so that querying from a certain commit will always produce the same results. Additionally, for CD use cases, it will allow a merge operation to introduce Hive table changes (schema evolution, partition addition/removal) atomically alongside the change to the data itself - as well as track those changes with the same set of commits - a lakeFS diff will show both metadata and data changes. Track and discuss on GitHub . Hooks: usability improvements High Priority . While hooks are an immensely useful tool that provides strong guarantees to data consumers, we want to make them more useful but also easier to implement: . Extended hook types beyond webhooks . While webhooks are easy to understand, they can be challenging in terms of operations: they require a running server listening for requests, network access and authentication information need to be applied between lakeFS and the hook server and network timeouts might interfere with long running hooks. To help reduce this burden, we plan on adding more hook types that are easier to manage - running a command line script, executing a docker image or calling out to an external orchestration/scheduling system such as Airflow or even Kubernetes. Track and discuss on GitHub . Expose merged snapshot to pre-merge hooks . pre-merge hooks are a great place to introduce data validation checks. However, currently lakeFS exposes the source branch, the destination branch and the diff between them. In many cases, the desired input is actually the merged view of both branches. By having a referencable commit ID that could be passed to e.g. Spark, users will be able to directly feed the merged view into a dataframe, a testing framework, etc. Track and discuss on GitHub . Create and edit hooks directly in the UI: . Hooks require a YAML configuration file to describe the required functionality and triggers. Instead of having to edit them somewhere else and then carefully uploading to the correct path for them to take effect, we want to allow creating and editing hooks with a single click in the UI . Track and discuss on GitHub . Post-commit, Post-merge hooks . For validation, testing and governance, pre-commit and pre-merge hooks provide strong guarantees about what we can or cannot expose to downstream consumers. However, in some cases we wish to run or notify another system when a commit or a merge occurs. The most common example would be registering changes in external systems: register new partitions in Hive Metastore, update last update date on data discovery tools, generate manifest files for e.g. AWS Athena and more. Track and discuss on GitHub . One-click reusable hook installation . Expanding on the abilities above (executing hooks locally as a command line or docker container - and the ability to create hooks in the UI), lakeFS can expose a set of pre-defined hooks that could be installed in a single click through the UI and provide useful functionality (schema validation, metastore updates, format enforcement, Symlink generation - and more, etc). Track and discuss on GitHub . ",
    "url": "/understand/roadmap.html#use-case-deployment",
    "relUrl": "/understand/roadmap.html#use-case-deployment"
  },"277": {
    "doc": "Roadmap",
    "title": "Use Case: Production",
    "content": "Webhook Alerting . Support integration into existing alerting systems that trigger in the event a webhook returns a failure. This is useful for example when a data quality test fails, so new data is not merged into main due to a quality issue, so will alert the owning team. Track and discuss on GitHub . ",
    "url": "/understand/roadmap.html#use-case-production",
    "relUrl": "/understand/roadmap.html#use-case-production"
  },"278": {
    "doc": "Roadmap",
    "title": "Architecture &amp; Operations",
    "content": "TL;DR - After receiving feedback on early versions of lakeFS, project “lakeFS on the Rocks” represents a set of changes to the architecture and data model of lakeFS. The main motivators are simplicity, reduced barriers of entry, scalability - and the added benefit of having lakeFS adhere more closely to Git in semantics and UX. There are 3 big shifts in design: . | Using the underlying object store as a source of truth for all committed data. We do this by storing commits as RocksDB SSTable files, where each commit is a “snapshot” of a given repository, split across multiple SSTable files that could be reused across commits. | Expose metadata operations as part of the OpenAPI gateway to allow other client types (e.g., Hadoop FileSystem) except for the S3 gateway interface | Implement a pluggable ref-store that allows storing references not (only) on PostgreSQL | . Decouple ref-store from PostgreSQL . Currently lakeFS requires a PostgreSQL database. Internally, it is used to store references (branches, tags, etc) other metadata such as user management. Making this store a pluggable component would allow the following: . | Simpler quickstart using only an object store: allow running lakeFS without any dependencies. This ref-store will use the underlying object store to also store the references. For S3 (or any object store that doesn’t support any native transaction/compare-and-swap semantics) this will be available only when running in single-instance mode. This is still beneficial for running lakeFS in POC or development mode, removing the need to run and connect multiple Docker containers. | Flexible production setup: A PostgreSQL option will still be available, but additional implementations will also be possible: running lakeFS as a Raft consensus group, using an other RDBMS types such as MySQL &amp;emdash; or using managed services such as DynamoDB that lakeFS will be able to manage itself | Easier scalability: Scaling RDBMS for very high throughput while keeping it predictable in performance for different loads and access patterns has a very high operational cost. | . This release will mark the completion of project “lakeFS on the Rocks” . Track and discuss on GitHub . Snowflake Support Requires Discussion . TBD - We don’t yet have concrete plans on how to handle Snowflake (and potentially other Data Warehouse/Database sources). If you’d like to have data in Snowflake managed by lakeFS, with full branching/merging/CI/CD capabilities, please contact us! . Contact us, we’d love to talk about it! . Metadata operations security and access model High Priority . Reduce the operational overhead of managing access control: Currently operators working with both lakeFS and the native object store are required to manage a similar set of access controls for both. Moving to a federated access control model using the object store’s native access control facilities (e.g. IAM) will help reduce this overhead. This requires more discovery around the different use cases to help design something coherent. If you’re using lakeFS and have strong opinions about access control, please reach out on Slack. Track and discuss on GitHub . Native Spark OutputCommitter . Provide a Spark OutputCommitter that actually… commits. This allows creating atomic Spark writes that are automatically tracked in lakeFS as commits. Each job will use its native job ID as (part of) a branch name for isolation, with the Output Committer doing a commit and merge operation to the requested branch on success. This has several benefits: . | Performance: This committer does metadata operations only, and doesn’t rely on copying data | Atomicity: A commit in lakeFS is guaranteed to either succeed or fail, but will not leave any intermediate state on failure. | Allows incorporating simple hooks into the spark job: users can define a webhook to happen before such a merge is completed successfully | Traceability: Attaching metadata to each commit means we get quite a lot of information on where data is coming from, how it’s generated, etc. This allows building reproducible pipelines in an easier way. | . Track and discuss on GitHub . ",
    "url": "/understand/roadmap.html#architecture--operations",
    "relUrl": "/understand/roadmap.html#architecture--operations"
  },"279": {
    "doc": "AWS S3",
    "title": "Prepare Your AWS S3 Bucket",
    "content": ". | From the S3 Administration console, choose Create Bucket. | Make sure you: . | Block public access | Disable Object Locking | . | lakeFS requires permissions to interact with your bucket. Following is a minimal bucket policy. To add it, go to the Permissions tab, and paste it as : . { \"Id\": \"Policy1590051531320\", \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Stmt1590051522178\", \"Action\": [ \"s3:GetObject\", \"s3:GetObjectVersion\", \"s3:PutObject\", \"s3:AbortMultipartUpload\", \"s3:ListMultipartUploadParts\", \"s3:GetBucketVersioning\", \"s3:ListBucket\", \"s3:GetBucketLocation\", \"s3:ListBucketMultipartUploads\", \"s3:ListBucketVersions\" ], \"Effect\": \"Allow\", \"Resource\": [\"arn:aws:s3:::&lt;BUCKET_NAME&gt;\", \"arn:aws:s3:::&lt;BUCKET_NAME_WITH_PATH_PREFIX&gt;/*\"], \"Principal\": { \"AWS\": [\"arn:aws:iam::&lt;ACCOUNT_ID&gt;:role/&lt;IAM_ROLE&gt;\"] } } ] } . Replace &lt;ACCOUNT_ID&gt;, &lt;BUCKET_NAME&gt; and &lt;IAM_ROLE&gt; with values relevant to your environment. IAM_ROLE should be the role assumed by your lakeFS installation. Alternatively, if you use an AWS user’s key-pair to authenticate lakeFS to AWS, change the policy’s Principal to be the user: . \"Principal\": { \"AWS\": [\"arn:aws:iam::&lt;ACCOUNT_ID&gt;:user/&lt;IAM_USER&gt;\"] } . | . You are now ready to create your first lakeFS repository. ",
    "url": "/setup/storage/s3.html#prepare-your-aws-s3-bucket",
    "relUrl": "/setup/storage/s3.html#prepare-your-aws-s3-bucket"
  },"280": {
    "doc": "AWS S3",
    "title": "AWS S3",
    "content": " ",
    "url": "/setup/storage/s3.html",
    "relUrl": "/setup/storage/s3.html"
  },"281": {
    "doc": "S3 Supported API",
    "title": "S3 Supported API",
    "content": "The S3 Gateway emulates a subset of the API exposed by S3. This subset includes all API endpoints relevant to data systems. for more information, see architecture . lakeFS supports the following API operations: . | Identity and authorization . | SIGv2 | SIGv4 | . | Bucket operations: . | HEAD bucket | . | Object operations: . | DeleteObject | DeleteObjects | GetObject . | Support for caching headers, ETag | Support for range requests | No support for SSE | No support for SelectObject operations | . | HeadObject | PutObject . | Support multi-part uploads | No support for storage classes | No object level tagging | . | CopyObject | . | Object Listing: . | ListObjects | ListObjectsV2 | Delimiter support (for \"/\" only) | . | Multipart Uploads: . | AbortMultipartUpload | CompleteMultipartUpload | CreateMultipartUpload | ListParts | Upload Part | UploadPartCopy | . | . ",
    "url": "/reference/s3.html",
    "relUrl": "/reference/s3.html"
  },"282": {
    "doc": "SageMaker",
    "title": "Using lakeFS with SageMaker",
    "content": "Amazon SageMaker helps prepare, build, train and deploy ML models quickly by bringing together a broad set of capabilities purpose-built for ML. ",
    "url": "/integrations/sagemaker.html#using-lakefs-with-sagemaker",
    "relUrl": "/integrations/sagemaker.html#using-lakefs-with-sagemaker"
  },"283": {
    "doc": "SageMaker",
    "title": "Table of contents",
    "content": ". | Initializing session and client | Usage Examples . | Upload train and test data | Download objects | . | . ",
    "url": "/integrations/sagemaker.html#table-of-contents",
    "relUrl": "/integrations/sagemaker.html#table-of-contents"
  },"284": {
    "doc": "SageMaker",
    "title": "Initializing session and client",
    "content": "Initialize a Sagemaker session and an s3 client with lakeFS as the endpoint: . import sagemaker import boto3 endpoint_url = '&lt;LAKEFS_S3_GATEWAY_ENDPOINT&gt;' aws_access_key_id = '&lt;LAKEFS_ACCESS_KEY_ID&gt;' aws_secret_access_key = '&lt;LAKEFS_SECRET_ACCESS_KEY&gt;' repo = 'example-repo' sm = boto3.client('sagemaker', endpoint_url=endpoint_url, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key) s3_resource = boto3.resource('s3', endpoint_url=endpoint_url, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key) session = sagemaker.Session(boto3.Session(), sagemaker_client=sm, default_bucket=repo) session.s3_resource = s3_resource . ",
    "url": "/integrations/sagemaker.html#initializing-session-and-client",
    "relUrl": "/integrations/sagemaker.html#initializing-session-and-client"
  },"285": {
    "doc": "SageMaker",
    "title": "Usage Examples",
    "content": "Upload train and test data . Let’s use the created session for uploading data to the ‘main’ branch: . prefix = \"/prefix-within-branch\" branch = 'main' train_file = 'train_data.csv'; train_data.to_csv(train_file, index=False, header=True) train_data_s3_path = session.upload_data(path=train_file, key_prefix=branch + prefix + \"/train\") test_file = 'test_data.csv'; test_data_no_target.to_csv(test_file, index=False, header=False) test_data_s3_path = session.upload_data(path=test_file, key_prefix=branch + prefix + \"/test\") . Download objects . We can use the integration with lakeFS to download a portion of the data we see fit: . repo = 'example-repo' prefix = \"/prefix-to-download\" branch = 'main' localpath = './' + branch session.download_data(path=localpath, bucket=repo, key_prefix = branch + prefix) . Note: Advanced AWS SageMaker features, like Autopilot jobs, are encapsulated and don’t have the option to override the S3 endpoint. However, it is possible to export the required inputs from lakeFS to S3. If you’re using SageMaker features that aren’t supported by lakeFS, we’d love to hear from you. ",
    "url": "/integrations/sagemaker.html#usage-examples",
    "relUrl": "/integrations/sagemaker.html#usage-examples"
  },"286": {
    "doc": "SageMaker",
    "title": "SageMaker",
    "content": " ",
    "url": "/integrations/sagemaker.html",
    "relUrl": "/integrations/sagemaker.html"
  },"287": {
    "doc": "Sizing Guide",
    "title": "Sizing guide",
    "content": " ",
    "url": "/understand/sizing-guide.html#sizing-guide",
    "relUrl": "/understand/sizing-guide.html#sizing-guide"
  },"288": {
    "doc": "Sizing Guide",
    "title": "Table of contents",
    "content": ". | System Requirements . | Operating Systems and ISA | Memory and CPU requirements | Network | Disk | PostgreSQL database . | Storage | RAM and shared_buffers | CPU | . | . | Scaling factors . | Understanding latency and throughput considerations | . | Benchmarks . | Random reads | Random Writes | Branch creation | . | Important metrics | Reference architectures . | Reference Architecture: Data Science/Research environment | Reference Architecture: Automated Production Pipelines | . | . ",
    "url": "/understand/sizing-guide.html#table-of-contents",
    "relUrl": "/understand/sizing-guide.html#table-of-contents"
  },"289": {
    "doc": "Sizing Guide",
    "title": "System Requirements",
    "content": "Operating Systems and ISA . lakeFS can run on MacOS and Linux (Windows binaries are available but not rigorously tested - we don’t recommend deploying lakeFS to production on Windows). x86_64 and arm64 architectures are supported for both MacOS and Linux. Memory and CPU requirements . lakeFS servers require a minimum of 512mb of RAM and 1 CPU core. For high throughput, additional CPUs help scale requests across different cores. “Expensive” operations such as large diff or commit operations can take advantage of multiple cores. Network . If using the data APIs such as the S3 Gateway, lakeFS will require enough network bandwidth to support the planned concurrent network upload/download operations. For most cloud providers, more powerful machines (i.e. more expensive and usually with more CPU cores) also provide increased network bandwidth. If using only the metadata APIs (for example, only using the Hadoop/Spark clients), network bandwidth is minimal, at roughly 1Kb per request. Disk . lakeFS greatly benefits from fast local disks. A lakeFS instance doesn’t require any strong durability guarantees from the underlying storage, as the disk is only ever used as a local caching layer for lakeFS metadata, and not for long-term storage. lakeFS is designed to work with ephemeral disks - these are usually based on NVMe and are tied to the machine’s lifecycle. Using ephemeral disks lakeFS can provide a very high throughput/cost ratio, probably the best that could be achieved on a public cloud, so we recommend those. A local cache of at least 512 MiB should be provided. For large installations (managing &gt;100 concurrently active branches, with &gt;100M objects per commit), we recommend allocating at least 10 GiB - since it’s a caching layer over a relatively slow storage (the object store), see Important metrics below to understand how to size this: it should be big enough to hold all commit metadata for actively referenced commits. PostgreSQL database . lakeFS uses a PostgreSQL instance to manage branch references, authentication and authorization information and to keep track of currently uncommitted data across branches. Storage . The dataset stored in PostgreSQL is relatively modest, as most metadata is pushed down into the object store. Required storage is mostly a factor of the amount of uncommitted writes across all branches at any given point in time: in the range of 150 MiB per every 100,000 uncommitted writes. We recommend starting at 10 GiB for a production deployment, as it will likely be more than enough. RAM and shared_buffers . Since the data size is small, it is recommended to provide enough memory to hold the vast majority of that data in RAM: Ideally configure shared_buffers of your PostgreSQL instances to be large enough to contain the currently active dataset. Pick a database instance with enough RAM to accommodate this buffer size, at roughly x4 the size given for shared_buffers (so for example, if an installation has ~500,000 uncommitted writes at any given time, it would require about 750 MiB of shared_buffers, that would require about 3 GiB of RAM). Cloud providers will save you the need to tune this parameter. It will be set to a fixed percentage the chosen instance’s available RAM (25% on AWS RDS, 30% on Google Cloud SQL). CPU . PostgreSQL CPU cores help scale concurrent requests. 1 CPU core for every 5,000 requests/second is ideal. ",
    "url": "/understand/sizing-guide.html#system-requirements",
    "relUrl": "/understand/sizing-guide.html#system-requirements"
  },"290": {
    "doc": "Sizing Guide",
    "title": "Scaling factors",
    "content": "Scaling lakeFS, like most data systems, moves across 2 axes: throughput of requests (amount per given timeframe), and latency (time to complete a single request). Understanding latency and throughput considerations . Most lakeFS operations are designed to be very low in latency. Assuming a well tuned local disk cache (see Storage above), most critical path operations (writing objects, requesting objects, deleting objects) are designed to complete in &lt;25ms at p90. Listing objects obviously requires accessing more data, but should always be on-par with what the underlying object store can provide, and in most cases, it is actually faster. At the worst case for directory listing with 1,000 common prefixes returned, expect a latency of 75ms at p90. Managing branches (creating them, listing them and deleting them) are all constant-time operations, generally taking &lt;30ms at p90. Committing and merging can take longer, as they are proportional to the amount of changes introduced. This is what makes lakeFS optimal for large Data Lakes - the amount of changes introduced per commit usually stays relatively stable, while the entire data set usually grows over time. This means lakeFS will provide predictable performance: committing 100 changes will take roughly the same amount of time whether the resulting commit contains 500 or 500 million objects. See Data Model for more information. Scaling throughput depends very much on the amount of CPU cores available to lakeFS. In many cases it is easier to scale lakeFS across a fleet of smaller cloud instances (or containers) than it is to scale up with machines that have many cores, and in fact, lakeFS works well in both cases. Most critical path operations scale very well across machines. ",
    "url": "/understand/sizing-guide.html#scaling-factors",
    "relUrl": "/understand/sizing-guide.html#scaling-factors"
  },"291": {
    "doc": "Sizing Guide",
    "title": "Benchmarks",
    "content": "All benchmarks below were measured using 2 x c5ad.4xlarge instances on AWS us-east-1. Similar results can be achieved on Google Cloud using a c2-standard-16 machine type, with an attached local SSD. On Azure, a Standard_F16s_v2 virtual machine can be used. The PostgreSQL instance that was used is a db.m6g.2xlarge (8 vCPUs, 32 GB RAM). Equivalent machines on Google Cloud or Azure should yield similar results. The example repository we tested against contains the metadata of a large lakeFS installation, where each commit contains ~180,000,000 objects (representing ~7.5 Petabytes of data). All tests are reproducible using the lakectl abuse command, so do use it to properly size and tune your setup. All tests are accompanied by the relevant lakectl abuse command that generated them. Random reads . This test generates random read requests to lakeFS, in a given commit. Paths are requested randomly from a file containing a set of preconfigured (and existing) paths. command executed: . lakectl abuse random-read \\ --from-file randomly_selected_paths.txt \\ --amount 500000 \\ --parallelism 128 \\ lakefs://example-repo/&lt;commit hash&gt; . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and commit hash. Result Histogram (raw): . Histogram (ms): 1 0 2 0 5 37945 7 179727 10 296964 15 399682 25 477502 50 499625 75 499998 100 499998 250 500000 350 500000 500 500000 750 500000 1000 500000 5000 500000 min 3 max 222 total 500000 . So 50% of all requests took &lt;10ms, while 99.9% of them took &lt;50ms . throughput: . Average throughput during the experiment was 10851.69 requests/second . Random Writes . This test generates random write requests to a given lakeFS branch. All paths are pre-generated and do not overwrite each other (as overwrites are relatively rare in a Data Lake setup). command executed: . lakectl abuse random-write \\ --amount 500000 \\ --parallelism 64 \\ lakefs://example-repo/main . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. Result Histogram (raw): . Histogram (ms): 1 0 2 0 5 30715 7 219647 10 455807 15 498144 25 499535 50 499742 75 499784 100 499802 250 500000 350 500000 500 500000 750 500000 1000 500000 5000 500000 min 3 max 233 total 500000 . So 50% of all requests took &lt;10ms, while 99.9% of them took &lt;25ms . throughput: . Average throughput during the experiment was 7595.46 requests/second . Branch creation . This test creates branches from a given reference . command executed: . lakectl abuse create-branches \\ --amount 500000 \\ --branch-prefix \"benchmark-\" \\ --parallelism 256 \\ lakefs://example-repo/&lt;commit hash&gt; . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and commit hash. Result Histogram (raw): . Histogram (ms): 1 0 2 1 5 5901 7 39835 10 135863 15 270201 25 399895 50 484932 75 497180 100 499303 250 499996 350 500000 500 500000 750 500000 1000 500000 5000 500000 min 2 max 304 total 500000 . So 50% of all requests took &lt;15ms, while 99.9% of them took &lt;100ms . throughput: . Average throughput during the experiment was 7069.03 requests/second . ",
    "url": "/understand/sizing-guide.html#benchmarks",
    "relUrl": "/understand/sizing-guide.html#benchmarks"
  },"292": {
    "doc": "Sizing Guide",
    "title": "Important metrics",
    "content": "lakeFS exposes metrics using the Prometheus protocol. Every lakeFS instance exposes a /metrics endpoint that could be used to extract them. Here are a few notable metrics to keep track of when sizing lakeFS: . api_requests_total - Tracks throughput of API requests over time . api_request_duration_seconds - Histogram of latency per operation type . gateway_request_duration_seconds - Histogram of latency per S3 Gateway operation . go_sql_stats_* - Important client-side metrics collected from the PostgreSQL driver. See The full reference here. ",
    "url": "/understand/sizing-guide.html#important-metrics",
    "relUrl": "/understand/sizing-guide.html#important-metrics"
  },"293": {
    "doc": "Sizing Guide",
    "title": "Reference architectures",
    "content": "Below are a few example architectures for lakeFS deployment . Reference Architecture: Data Science/Research environment . Use case: Manage Machine learning or algorithms development. Use lakeFS branches to achieve both isolation and reproducibility of experiments. Data being managed by lakeFS is both structured, tabular data; as well as unstructured sensor and image data used for training. Assuming a team of 20-50 researchers, with a dataset size of 500 TiB across 20M objects. Environment: lakeFS will be deployed on Kubernetes managed by AWS EKS with PostgreSQL on AWS RDS Aurora . Sizing: Since most of the work is done by humans (vs automated pipelines), most experiments tend to be small in scale, reading and writing 10s to 1000s of objects. The expected amount of branches active in parallel is relatively low, around 1-2 per user, each representing a small amount of uncommitted changes at any given point in time. Let’s assume 5,000 uncommitted writes per branch = ~500k. To support the expected throughput, a single moderate lakeFS instance should be more than enough, since requests per second would be on the order of 10s to 100s. For high availability, we’ll deploy 2 pods with 1 CPU core and 1 GiB of RAM each.. Since the PostgreSQL instance is expected to hold a very small dataset (at 500k, expected dataset size is 150MiB (for 100k records) * 5 = 750MiB). To ensure we have enough RAM to hold this, we’ll need 3 GiB of RAM, so a very moderate Aurora instance db.t3.large (2 vCPUs, 8 GB RAM) will be more than enough. An equivalent database instance on GCP or Azure should give similar results. Reference Architecture: Automated Production Pipelines . Use case: Manage multiple concurrent data pipelines using Apache Spark and Airflow. Airflow DAGs start by creating a branch for isolation and for CI/CD. Data being managed by lakeFS is structured, tabular data. Total dataset size is 10 PiB, spanning across 500M objects. Expected throughput is 10k reads/second + 2k writes per second across 100 concurrent branches. Environment: lakeFS will be deployed on Kubernetes managed by AWS EKS with PostgreSQL on AWS RDS . Sizing: Data pipelines tend to be bursty in nature: reading in a lot of objects concurrently, doing some calculation or aggregation and then writing many objects concurrently. The expected amount of branches active in parallel is high, with many Airflow DAGs running per day, each representing a moderate amount of uncommitted changes at any given point in time. Let’s assume 1,000 uncommitted writes/branch * 2500 branches = ~2.5M records. To support the expected throughput, looking the benchmarking numbers above, we’re doing roughly 625 requests/core, so 24 cores should cover our peak traffic. We can deploy 6 * 4 CPU core pods. The PostgreSQL instance (at 500k, expected dataset size is 150MiB (for 100k records) * 25 = 3750 MiB). To ensure we have enough RAM to hold this, we’ll need at least 15 GiB of RAM, so we’ll go with a db.r5.xlarge (4 vCPUs, 32GB RAM) Aurora instance. An equivalent database instance on GCP or Azure should give similar results. ",
    "url": "/understand/sizing-guide.html#reference-architectures",
    "relUrl": "/understand/sizing-guide.html#reference-architectures"
  },"294": {
    "doc": "Sizing Guide",
    "title": "Sizing Guide",
    "content": " ",
    "url": "/understand/sizing-guide.html",
    "relUrl": "/understand/sizing-guide.html"
  },"295": {
    "doc": "Spark Client",
    "title": "lakeFS Spark Client",
    "content": "Utilize the power of Spark to interact with the metadata on lakeFS. Possible use-cases include: . | Create a DataFrame for listing the objects in a specific commit or branch. | Compute changes between two commits. | Export your data for consumption outside lakeFS. | Bulk operations on underlying storage. | . ",
    "url": "/reference/spark-client.html#lakefs-spark-client",
    "relUrl": "/reference/spark-client.html#lakefs-spark-client"
  },"296": {
    "doc": "Spark Client",
    "title": "Getting Started",
    "content": "Start Spark Shell / PySpark with the --packages flag: . Spark 3.0.1: . spark-shell --packages io.lakefs:lakefs-spark-client-301_2.12:0.1.0 . Spark 2.4.7: . spark-shell --packages io.lakefs:lakefs-spark-client-247_2.11:0.1.0 . Alternatively, the Jars are publicly available on S3: . | Spark 3.0.1: s3://treeverse-clients-us-east/lakefs-spark-client-301_2.12/0.1.0/lakefs-spark-client-301_2.12-0.1.0.jar . | Spark 2.4.7: s3://treeverse-clients-us-east/lakefs-spark-client-247_2.11/0.1.0/lakefs-spark-client-247_2.11-0.1.0.jar . | . ",
    "url": "/reference/spark-client.html#getting-started",
    "relUrl": "/reference/spark-client.html#getting-started"
  },"297": {
    "doc": "Spark Client",
    "title": "Configuration",
    "content": ". | To read metadata from lakeFS, the client should be configured with your lakeFS endpoint and credentials, using the following Hadoop configurations: . | Configuration | Description | . | spark.hadoop.lakefs.api.url | lakeFS API endpoint, e.g: http://lakefs.example.com/api/v1 | . | spark.hadoop.lakefs.api.access_key | The access key to use for fetching metadata from lakeFS | . | spark.hadoop.lakefs.api.secret_key | Corresponding lakeFS secret key | . | The client will also directly interact with your storage using Hadoop FileSystem. Therefore, your Spark session must be able to access the underlying storage of your lakeFS repository. There are various ways to do this, but for a non-production environment you can use the following Hadoop configurations: . | Configuration | Description | . | spark.hadoop.fs.s3a.access.key | Access key to use for accessing underlying storage on S3 | . | spark.hadoop.fs.s3a.secret.key | Corresponding secret key to use with S3 access key | . | . ",
    "url": "/reference/spark-client.html#configuration",
    "relUrl": "/reference/spark-client.html#configuration"
  },"298": {
    "doc": "Spark Client",
    "title": "Examples",
    "content": ". | Get a DataFrame for listing all objects in a commit: . import io.treeverse.clients.LakeFSContext val commitID = \"a1b2c3d4\" val df = LakeFSContext.newDF(spark, \"example-repo\", commitID) df.show /* output example: +------------+--------------------+--------------------+-------------------+----+ | key | address| etag| last_modified|size| +------------+--------------------+--------------------+-------------------+----+ | file_1 |791457df80a0465a8...|7b90878a7c9be5a27...|2021-03-05 11:23:30| 36| file_2 |e15be8f6e2a74c329...|95bee987e9504e2c3...|2021-03-05 11:45:25| 36| file_3 |f6089c25029240578...|32e2f296cb3867d57...|2021-03-07 13:43:19| 36| file_4 |bef38ef97883445c8...|e920efe2bc220ffbb...|2021-03-07 13:43:11| 13| +------------+--------------------+--------------------+-------------------+----+ */ . | Run SQL queries on your metadata: . df.createOrReplaceTempView(\"files\") spark.sql(\"SELECT DATE(last_modified), COUNT(*) FROM files GROUP BY 1 ORDER BY 1\") /* output example: +----------+--------+ | dt|count(1)| +----------+--------+ |2021-03-05| 2|2021-03-07| 2| +----------+--------+ */ . | . ",
    "url": "/reference/spark-client.html#examples",
    "relUrl": "/reference/spark-client.html#examples"
  },"299": {
    "doc": "Spark Client",
    "title": "Spark Client",
    "content": " ",
    "url": "/reference/spark-client.html",
    "relUrl": "/reference/spark-client.html"
  },"300": {
    "doc": "Spark",
    "title": "Using lakeFS with Spark",
    "content": "Apache Spark is a unified analytics engine for big data processing, with built-in modules for streaming, SQL, machine learning and graph processing. ",
    "url": "/integrations/spark.html#using-lakefs-with-spark",
    "relUrl": "/integrations/spark.html#using-lakefs-with-spark"
  },"301": {
    "doc": "Spark",
    "title": "Table of contents",
    "content": ". | Two-tiered Spark support | Access lakeFS using the S3A gateway . | Configuration . | Per-bucket configuration | . | Reading Data | Writing Data | . | Access lakeFS using the lakeFS-specific Hadoop FileSystem . | Configuration . | Load the FileSystem JARs | Configure the lakeFS FileSystem and the underlying S3A FileSystem | Per-bucket and per-repo configuration | . | Reading Data | Writing Data | . | Case Study: SimilarWeb | . Note In all following examples we set AWS and lakeFS credentials at runtime, for clarity. In production, properties defining AWS credentials should be set using one of Hadoop’s standard ways of authenticating with S3. Similarly, properties defining lakeFS credentials should be configured in secure site files, not on the command line or inlined in code where they might be exposed. ",
    "url": "/integrations/spark.html#table-of-contents",
    "relUrl": "/integrations/spark.html#table-of-contents"
  },"302": {
    "doc": "Spark",
    "title": "Two-tiered Spark support",
    "content": "lakeFS support in Spark has two tiers: . | Access lakeFS using the S3A gateway. | Access lakeFS using the lakeFS-specific Hadoop FileSystem. | . Using the S3A gateway is easier to configure and may be more suitable for legacy or small-scale applications. Using the lakeFS FileSystem requires somewhat more complex configuration, but offers greatly increased performance. ",
    "url": "/integrations/spark.html#two-tiered-spark-support",
    "relUrl": "/integrations/spark.html#two-tiered-spark-support"
  },"303": {
    "doc": "Spark",
    "title": "Access lakeFS using the S3A gateway",
    "content": "To use this mode you configure the Spark application to use S3A using the S3-compatible endpoint which the lakeFS server provides. Accordingly all data flows through the lakeFS server. Accessing data in lakeFS from Spark is the same as accessing S3 data from Spark. The only changes we need to consider are: . | Setting the configurations to access lakeFS. | Accessing objects using the lakeFS S3 path convention. | . Configuration . In order to configure Spark to work with lakeFS, we set S3 Hadoop configuration to the lakeFS endpoint and credentials: . | Hadoop Configuration | Value | . | fs.s3a.access.key | Set to the lakeFS access key | . | fs.s3a.secret.key | Set to the lakeFS secret key | . | fs.s3a.endpoint | Set to the lakeFS S3-compatible API endpoint | . Here is how to do it: . | CLI | Scala | XML Configuration | . spark-shell --conf spark.hadoop.fs.s3a.access.key='AKIAlakefs12345EXAMPLE' \\ --conf spark.hadoop.fs.s3a.secret.key='abc/lakefs/1234567bPxRfiCYEXAMPLEKEY' \\ --conf spark.hadoop.fs.s3a.endpoint='https://s3.lakefs.example.com' ... spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", \"AKIAlakefs12345EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.endpoint\", \"https://s3.lakefs.example.com\") . Add these into a configuration file, e.g. $SPARK_HOME/conf/hdfs-site.xml: . &lt;?xml version=\"1.0\"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.s3a.access.key&lt;/name&gt; &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.secret.key&lt;/name&gt; &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;https://s3.lakefs.example.com&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . Per-bucket configuration . The above configuration will use lakeFS as the sole S3 endpoint. To use lakeFS in parallel with S3, you can configure Spark to use lakeFS only for specific bucket names. For example, to configure only example-repo to use lakeFS, set the following configurations: . | CLI | Scala | XML Configuration | . spark-shell --conf spark.hadoop.fs.s3a.bucket.example-repo.access.key='AKIAlakefs12345EXAMPLE' \\ --conf spark.hadoop.fs.s3a.bucket.example-repo.secret.key='abc/lakefs/1234567bPxRfiCYEXAMPLEKEY' \\ --conf spark.hadoop.fs.s3a.bucket.example-repo.endpoint='https://s3.lakefs.example.com' . spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.access.key\", \"AKIAlakefs12345EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.endpoint\", \"https://s3.lakefs.example.com\") . Add these into a configuration file, e.g. $SPARK_HOME/conf/hdfs-site.xml: . &lt;?xml version=\"1.0\"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.s3a.bucket.example-repo.access.key&lt;/name&gt; &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.bucket.example-repo.secret.key&lt;/name&gt; &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.bucket.example-repo.endpoint&lt;/name&gt; &lt;value&gt;https://s3.lakefs.example.com&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . With this configuration set , reading s3a paths with example-repo as the bucket will use lakeFS, while all other buckets will use AWS S3. Reading Data . In order for us to access objects in lakeFS we will need to use the lakeFS S3 gateway path conventions: . s3a://[REPOSITORY]/[BRANCH]/PATH/TO/OBJECT . Here is an example for reading a parquet file from lakeFS to a Spark DataFrame: . val repo = \"example-repo\" val branch = \"main\" val dataPath = s\"s3a://${repo}/${branch}/example-path/example-file.parquet\" val df = spark.read.parquet(dataPath) . You can now use this DataFrame like you would normally do. Writing Data . Now simply write your results back to a lakeFS path: . df.write.partitionBy(\"example-column\").parquet(s\"s3a://${repo}/${branch}/output-path/\") . The data is now created in lakeFS as new changes in your branch. You can now commit these changes, or revert them. ",
    "url": "/integrations/spark.html#access-lakefs-using-the-s3a-gateway",
    "relUrl": "/integrations/spark.html#access-lakefs-using-the-s3a-gateway"
  },"304": {
    "doc": "Spark",
    "title": "Access lakeFS using the lakeFS-specific Hadoop FileSystem",
    "content": "To use this mode you configure the Spark application to perform metadata operations on the lakeFS server, and all data operations directly through the same underlying object store that lakeFS uses. The lakeFS FileSystem currently supports using only the S3A Hadoop FileSystem for data access. In this mode the Spark application will directly read and write from the underlying object store, significantly increasing application scalability and performance by reducing the load on the lakeFS server. Accessing data in lakeFS from Spark is the same as accessing S3 data from Spark. The only changes we need to perform are: . | Configure Spark to access lakeFS for metadata and S3 or a compatible underlying object store to access data. | Use lakefs://repo/ref/path/to/data URIs to read and write data on lakeFS, rather than s3a://... URIs. | . Configuration . In order to configure Spark to work using the lakeFS Hadoop FileSystem, you will need to load the filesystem JARs and then configure both that FileSystem and the underlying data access FileSystem. Load the FileSystem JARs . Add the package io.lakefs:hadoop-lakefs-assembly:&lt;VERSION&gt; to your Spark job. Right now this is version 0.1.0, so add: . --packages io.lakefs:hadoop-lakefs-assembly:0.1.0 . to your Spark commandlines. Configure the lakeFS FileSystem and the underlying S3A FileSystem . Add Hadoop configuration to the underlying storage and additionally to lakeFS credentials. When using this mode, do not set the S3A endpoint URL to point at lakeFS – it should point at the underlying storage. | Hadoop Configuration | Value | . | fs.s3a.access.key | Set to the AWS S3 access key | . | fs.s3a.secret.key | Set to the AWS S3 secret key | . | fs.s3a.endpoint | Set to the AWS S3-compatible endpoint | . | fs.lakefs.impl | io.lakefs.LakeFSFileSystem | . | fs.lakefs.access.key | Set to the lakeFS access key | . | fs.lakefs.secret.key | Set to the lakeFS secret key | . | fs.lakefs.endpoint | Set to the lakeFS API URL | . When using AWS S3 itself, the default configuration works with us-east-1, so you may still need to configure fs.s3a.endpoint. Amazon provides these S3 endpoints you can use. Note: If not running on AWS, all s3a configuration properties are required! Unlike when using the S3 gateway, when using the lakeFS-specific Hadoop FileSystem you configure s3a to access the S3 underlying object storage, and lakefs to access the lakeFS server. When running on AWS you do not need to configure credentials if the instance profile has sufficient permissions. Here is how to do it: . | CLI | Scala | XML Configuration | . spark-shell --conf spark.hadoop.fs.s3a.access.key='AKIAIOSFODNN7EXAMPLE' \\ --conf spark.hadoop.fs.s3a.secret.key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' \\ --conf spark.hadoop.fs.s3a.endpoint='https://s3.eu-central-1.amazonaws.com' \\ --conf spark.hadoop.fs.lakefs.impl=io.lakefs.LakeFSFileSystem \\ --conf spark.hadoop.fs.lakefs.access.key=AKIAlakefs12345EXAMPLE \\ --conf spark.hadoop.fs.lakefs.secret.key=abc/lakefs/1234567bPxRfiCYEXAMPLEKEY \\ --conf spark.hadoop.fs.lakefs.endpoint=https://lakefs.example.com/api/v1 \\ --packages io.lakefs:hadoop-lakefs-assembly:0.1.0 ... Ensure you load the lakeFS FileSystem into Spark by running it with --packages or --jars, and then run: . spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", \"AKIAIOSFODNN7EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.endpoint\", \"https://s3.eu-central-1.amazonaws.com\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.access.key\", \"AKIAlakefs12345EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.endpoint\", \"https://lakefs.example.com/api/v1\") . Ensure you load the lakeFS FileSystem into Spark by running it with --packages or --jars, and then add these into a configuration file, e.g. $SPARK_HOME/conf/hdfs-site.xml: . &lt;?xml version=\"1.0\"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.s3a.access.key&lt;/name&gt; &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.secret.key&lt;/name&gt; &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;https://s3.eu-central-1.amazonaws.com&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.impl&lt;/name&gt; &lt;value&gt;io.lakefs.LakeFSFileSystem&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.access.key&lt;/name&gt; &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.secret.key&lt;/name&gt; &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.endpoint&lt;/name&gt; &lt;value&gt;https://lakefs.example.com/api/v1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . Per-bucket and per-repo configuration . As above, S3 allows for per-bucket configuration. You can use this if: . | You need to use S3A directly to access data in an S3 outside of lakeFS, and | different credentials are required to access data inside that bucket. | . Refer to the Hadoop AWS guide on Configuring different S3 buckets with Per-Bucket Configuration. There is no need for per-repo configurations in lakeFS when all repositories are on the same lakeFS server. If you need to access repositories that are on multiple lakeFS servers, configure multiple prefixes. For instance, you might configure both fs.lakefs.impl and fs.lakefs2.impl to be io.lakefs.LakeFSFileSystem, place separate endpoints and credentials under fs.lakefs.* and fs.lakefs2.*, and access the two servers using lakefs://... and lakefs2://... URLs. Reading Data . In order for us to access objects in lakeFS we will need to use the lakeFS path conventions: . lakefs://[REPOSITORY]/[BRANCH]/PATH/TO/OBJECT . Here is an example for reading a parquet file from lakeFS to a Spark DataFrame: . val repo = \"example-repo\" val branch = \"main\" val dataPath = s\"lakefs://${repo}/${branch}/example-path/example-file.parquet\" val df = spark.read.parquet(dataPath) . You can now use this DataFrame like you would normally do. Writing Data . Now simply write your results back to a lakeFS path: . df.write.partitionBy(\"example-column\").parquet(s\"lakefs://${repo}/${branch}/output-path/\") . The data is now created in lakeFS as new changes in your branch. You can now commit these changes, or revert them. ",
    "url": "/integrations/spark.html#access-lakefs-using-the-lakefs-specific-hadoop-filesystem",
    "relUrl": "/integrations/spark.html#access-lakefs-using-the-lakefs-specific-hadoop-filesystem"
  },"305": {
    "doc": "Spark",
    "title": "Case Study: SimilarWeb",
    "content": "See how SimilarWeb is using lakeFS with Spark to manage algorithm changes in data pipelines. ",
    "url": "/integrations/spark.html#case-study-similarweb",
    "relUrl": "/integrations/spark.html#case-study-similarweb"
  },"306": {
    "doc": "Spark",
    "title": "Spark",
    "content": " ",
    "url": "/integrations/spark.html",
    "relUrl": "/integrations/spark.html"
  },"307": {
    "doc": "Try without installing",
    "title": "Try lakeFS without installing",
    "content": " ",
    "url": "/quickstart/try.html#try-lakefs-without-installing",
    "relUrl": "/quickstart/try.html#try-lakefs-without-installing"
  },"308": {
    "doc": "Try without installing",
    "title": "Katacoda Tutorial",
    "content": "Learn how to use lakeFS using the CLI and an interactive Spark shell - all from your browser, without installing anything. In the tutorial we cover: . | Basic lakectl command line usage | How to read, write, list and delete objects from lakeFS using the lakectl command | Read from, and write to lakeFS using its S3 API interface using Spark | Diff, commit and merge the changes created by Spark | Track commit history to understand changes to your data over time | . The web based environment provides a full working lakeFS and Spark environment, so feel free to explore it on your own. Start Katacoda Tutorial Now . ",
    "url": "/quickstart/try.html#katacoda-tutorial",
    "relUrl": "/quickstart/try.html#katacoda-tutorial"
  },"309": {
    "doc": "Try without installing",
    "title": "Next Steps",
    "content": "After getting acquainted with lakeFS, you can easily get a local lakeFS environment working on your computer. ",
    "url": "/quickstart/try.html#next-steps",
    "relUrl": "/quickstart/try.html#next-steps"
  },"310": {
    "doc": "Try without installing",
    "title": "Try without installing",
    "content": " ",
    "url": "/quickstart/try.html",
    "relUrl": "/quickstart/try.html"
  },"311": {
    "doc": "Upgrade lakeFS",
    "title": "Upgrading lakeFS",
    "content": "Upgrading lakeFS from a previous version usually just requires re-deploying with the latest image (or downloading the latest version, if you’re using the binary). There are cases where the database will require a migration - check whether the release you are upgrading to requires that. ",
    "url": "/reference/upgrade.html#upgrading-lakefs",
    "relUrl": "/reference/upgrade.html#upgrading-lakefs"
  },"312": {
    "doc": "Upgrade lakeFS",
    "title": "When DB migrations are required",
    "content": "lakeFS 0.30.0 or greater . In case a migration is required, first stop the running lakeFS service. Using the lakefs binary for the new version, run the following: . lakefs migrate up . Deploy (or run) the new version of lakeFS. Note that an older version of lakeFS cannot run on a migrated database. Prior to lakeFS 0.30.0 . Note: with lakeFS &lt; 0.30.0, you should first upgrade to 0.30.0 following this guide. Then, proceed to upgrade to the newest version. Starting version 0.30.0, lakeFS handles your committed metadata in a new way, which is more robust and has better performance. To move your existing data, you will need to run the following upgrade commands. Verify lakeFS version == 0.30.0 (can skip if using Docker) . lakefs --version . Migrate data from previous format: . lakefs migrate db . Or migrate using Docker image: . docker run --rm -it -e LAKEFS_DATABASE_CONNECTION_STRING=&lt;database connection string&gt; treeverse/lakefs:rocks-migrate migrate db . Once migrated, it is possible to now use more recent lakeFS versions. Please refer to their release notes for more information on ugrading and usage). If you want to start over, discarding your existing data, you need to explicitly state this in your lakeFS configuration file. To do so, add the following to your configuration (relevant only for 0.30.0): . cataloger: type: rocks . ",
    "url": "/reference/upgrade.html#when-db-migrations-are-required",
    "relUrl": "/reference/upgrade.html#when-db-migrations-are-required"
  },"313": {
    "doc": "Upgrade lakeFS",
    "title": "Upgrade lakeFS",
    "content": " ",
    "url": "/reference/upgrade.html",
    "relUrl": "/reference/upgrade.html"
  }
}
