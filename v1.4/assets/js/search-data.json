{"0": {
    "doc": "Page not found",
    "title": "Page not found",
    "content": " ",
    "url": "/v1.4/404.html",
    
    "relUrl": "/404.html"
  },"1": {
    "doc": "Page not found",
    "title": "We can’t seem to find the page you’re looking for.",
    "content": "Error code: 404 . Back to Docs . ",
    "url": "/v1.4/404.html#we-cant-seem-to-find-the-page-youre-looking-for",
    
    "relUrl": "/404.html#we-cant-seem-to-find-the-page-youre-looking-for"
  },"2": {
    "doc": "Access Control Lists (ACLs)",
    "title": "Access Control Lists (ACLs)",
    "content": "ACLs were introduced in their current form in v0.98 of lakeFS as part of changes to the security model in lakeFS. They are an alternative to the more granular control that role-based access control provides. ",
    "url": "/v1.4/reference/security/access-control-lists.html",
    
    "relUrl": "/reference/security/access-control-lists.html"
  },"3": {
    "doc": "Access Control Lists (ACLs)",
    "title": "Table of contents",
    "content": ". | ACLs | Pluggable Authentication and Authorization | Previous versions of ACL in lakeFS | . ",
    "url": "/v1.4/reference/security/access-control-lists.html#table-of-contents",
    
    "relUrl": "/reference/security/access-control-lists.html#table-of-contents"
  },"4": {
    "doc": "Access Control Lists (ACLs)",
    "title": "ACLs",
    "content": "You can attach Permissions and scope them to groups in the Groups page. There are 4 default groups, named after the 4 permissions. Each group is global (applies for all repositories). | Group ID | Allows | . | Read | Read operations, creating access keys | . | Write | Allows all data read and write operations. | . | Super | Allows all operations except auth. | . | Admin | Allows all operations. | . ",
    "url": "/v1.4/reference/security/access-control-lists.html#acls",
    
    "relUrl": "/reference/security/access-control-lists.html#acls"
  },"5": {
    "doc": "Access Control Lists (ACLs)",
    "title": "Pluggable Authentication and Authorization",
    "content": "Authorization and authentication is pluggable in lakeFS. If lakeFS is attached to a remote authentication server (or you are using lakeFS Cloud) then the role-based access control user interface can be used. If you are using ACL then the lakeFS configuration element auth.ui_config.RBAC should be set to simplified. ",
    "url": "/v1.4/reference/security/access-control-lists.html#pluggable-authentication-and-authorization",
    
    "relUrl": "/reference/security/access-control-lists.html#pluggable-authentication-and-authorization"
  },"6": {
    "doc": "Access Control Lists (ACLs)",
    "title": "Previous versions of ACL in lakeFS",
    "content": "Here’s a comparison of the current ACL model against the behavior prior to the changes introduced in v0.98. | Permission | Allows | Previous Group Name | Previous Policy Names and Actions | . | Read | Read operations, creating access keys. | Viewers | FSReadAll [fs:List, fs:Read] | . | Write | Allows all data read and write operations. | Developers | FSReadWriteAll [fs:ListRepositories, fs:ReadRepository, fs:ReadCommit, fs:ListBranches, fs:ListTags, fs:ListObjects, fs:ReadObject, fs:WriteObject, fs:DeleteObject, fs:RevertBranch, fs:ReadBranch, fs:ReadTag, fs:CreateBranch, fs:CreateTag, fs:DeleteBranch, fs:DeleteTag, fs:CreateCommit] RepoManagementReadAll [ci:Read, retention:Get, branches:Get*, fs:ReadConfig] | . | Super | Allows all operations except auth. | SuperUsers (with changes) | FSFullAccess [fs:] RepoManagementReadAll [ci:Read, retention:Get, branches:Get, fs:ReadConfig] | . | Admin | Allows all operations. | Admins | AuthFullAccess [auth:] FSFullAccess [fs:] RepoManagementFullAccess [ci:, retention:, branches:*, fs:ReadConfig] | . Migrating from the previous version of ACLs . Upgrading the lakeFS version will require migrating to the new ACL authorization model. In order to run the migration run: . lakefs migrate up . The command will run the migration to ACL. The migration process might adjust the current authorization policies to fit ACL, in that case the command will not make any changes, only print warnings. In case of warnings to apply the migration, re-run with the --force flag . The upgrade will ensure that the 4 default groups exist, and modify existing groups to fit into the new ACLs model: . | When creating the 4 default global groups: if another group exists and has the desired name, upgrading will rename it by appending “.orig”. So after upgrading the 4 default global groups exist, with these known names. | For any group, upgrading configured policies follows these rules, possibly increasing access: . | Any “Deny” rules are stripped, and a warning printed. | “Manage own credentials” is added. | If any actions outside of “fs:” and manage own credentials are allowed, the group becomes an Admin group, a warning is printed, and no further changes apply. | The upgrade script unifies repositories: If a resource applies to a set of repositories, permissions are unified to all repositories. | The upgrade script unifies actions: it selects the least permission of Read, Write, Super that contains all of the allowed actions. | . | . The upgrade will detach every directly attached policy from users . Note that moving to ACL from RBAC may only be performed once and will lose some configuration. The upgrade script will detail the changes made by the transition. For any question or concern during the upgrade, don’t hesitate to get in touch with us through Slack or email. ",
    "url": "/v1.4/reference/security/access-control-lists.html#previous-versions-of-acl-in-lakefs",
    
    "relUrl": "/reference/security/access-control-lists.html#previous-versions-of-acl-in-lakefs"
  },"7": {
    "doc": "6️⃣ Using Actions and Hooks in lakeFS",
    "title": "Actions and Hooks in lakeFS",
    "content": "When we interact with lakeFS it can be useful to have certain checks performed at stages along the way. Let’s see how actions in lakeFS can be of benefit here. We’re going to enforce a rule that when a commit is made to any branch that begins with etl: . | the commit message must not be blank | there must be job_name and version metadata | the version must be numeric | . To do this we’ll create an action. In lakeFS, an action specifies one or more events that will trigger it, and references one or more hooks to run when triggered. Actions are YAML files written to lakeFS under the _lakefs_actions/ folder of the lakeFS repository. Hooks can be either a Lua script that lakeFS will execute itself, an external web hook, or an Airflow DAG. In this example, we’re using a Lua hook. ",
    "url": "/v1.4/quickstart/actions-and-hooks.html#actions-and-hooks-in-lakefs",
    
    "relUrl": "/quickstart/actions-and-hooks.html#actions-and-hooks-in-lakefs"
  },"8": {
    "doc": "6️⃣ Using Actions and Hooks in lakeFS",
    "title": "Configuring the Action",
    "content": ". | In lakeFS create a new branch called add_action. You can do this through the UI or with lakectl: . docker exec lakefs \\ lakectl branch create \\ lakefs://quickstart/add_action \\ --source lakefs://quickstart/main . | Open up your favorite text editor (or emacs), and paste the following YAML: . name: Check Commit Message and Metadata on: pre-commit: branches: - etl** hooks: - id: check_metadata type: lua properties: script: | commit_message=action.commit.message if commit_message and #commit_message&gt;0 then print(\"✅ The commit message exists and is not empty: \" .. commit_message) else error(\"\\n\\n❌ A commit message must be provided\") end job_name=action.commit.metadata[\"job_name\"] if job_name == nil then error(\"\\n❌ Commit metadata must include job_name\") else print(\"✅ Commit metadata includes job_name: \" .. job_name) end version=action.commit.metadata[\"version\"] if version == nil then error(\"\\n❌ Commit metadata must include version\") else print(\"✅ Commit metadata includes version: \" .. version) if tonumber(version) then print(\"✅ Commit metadata version is numeric\") else error(\"\\n❌ Version metadata must be numeric: \" .. version) end end . | Save this file as /tmp/check_commit_metadata.yml . | You can save it elsewhere, but make sure you change the path below when uploading | . | Upload the check_commit_metadata.yml file to the add_action branch under _lakefs_actions/. As above, you can use the UI (make sure you select the correct branch when you do), or with lakectl: . docker exec lakefs \\ lakectl fs upload \\ lakefs://quickstart/add_action/_lakefs_actions/check_commit_metadata.yml \\ --source /tmp/check_commit_metadata.yml . | Go to the Uncommitted Changes tab in the UI, and make sure that you see the new file in the path shown: . Click Commit Changes and enter a suitable message to commit this new file to the branch. | Now we’ll merge this new branch into main. From the Compare tab in the UI compare the main branch with add_action and click Merge . | . ",
    "url": "/v1.4/quickstart/actions-and-hooks.html#configuring-the-action",
    
    "relUrl": "/quickstart/actions-and-hooks.html#configuring-the-action"
  },"9": {
    "doc": "6️⃣ Using Actions and Hooks in lakeFS",
    "title": "Testing the Action",
    "content": "Let’s remind ourselves what the rules are that the action is going to enforce. When a commit is made to any branch that begins with etl: . | the commit message must not be blank | there must be job_name and version metadata | the version must be numeric | . We’ll start by creating a branch that’s going to match the etl pattern, and then go ahead and commit a change and see how the action works. | Create a new branch (see above instructions on how to do this if necessary) called etl_20230504. Make sure you use main as the source branch. In your new branch you should see the action that you created and merged above: . | To simulate an ETL job we’ll use the built-in DuckDB editor to run some SQL and write the result back to the lakeFS branch. Open the lakes.parquet file on the etl_20230504 branch from the Objects tab. Replace the SQL statement with the following: . COPY ( WITH src AS ( SELECT lake_name, country, depth_m, RANK() OVER ( ORDER BY depth_m DESC) AS lake_rank FROM READ_PARQUET('lakefs://quickstart/etl_20230504/lakes.parquet')) SELECT * FROM SRC WHERE lake_rank &lt;= 10 ) TO 'lakefs://quickstart/etl_20230504/top10_lakes.parquet' . | Head to the Uncommitted Changes tab in the UI and notice that there is now a file called top10_lakes.parquet waiting to be committed. Now we’re ready to start trying out the commit rules, and seeing what happens if we violate them. | Click on Commit Changes, leave the Commit message blank, and click Commit Changes to confirm. Note that the commit fails because the hook did not succeed . pre-commit hook aborted . with the output from the hook’s code displayed . ❌ A commit message must be provided . | Do the same as the previous step, but provide a message this time: . The commit still fails as we need to include metadata too, which is what the error tells us . ❌ Commit metadata must include job_name . | Repeat the Commit Changes dialog and use the Add Metadata field to add the required metadata: . We’re almost there, but this still fails (as it should), since the version is not entirely numeric but includes v and ß: . ❌ Version metadata must be numeric: v1.00ß . Repeat the commit attempt specify the version as 1.00 this time, and rejoice as the commit succeeds . | . You can view the history of all action runs from the Action tab: . ",
    "url": "/v1.4/quickstart/actions-and-hooks.html#testing-the-action",
    
    "relUrl": "/quickstart/actions-and-hooks.html#testing-the-action"
  },"10": {
    "doc": "6️⃣ Using Actions and Hooks in lakeFS",
    "title": "Bonus Challenge",
    "content": "And so with that, this quickstart for lakeFS draws to a close. If you’re simply having too much fun to stop then here’s an exercise for you. Implement the requirement from the beginning of this quickstart correctly, such that you write denmark-lakes.parquet in the respective branch and successfully merge it back into main. Look up how to list the contents of the main branch and verify that it looks like this: . object 2023-03-21 17:33:51 +0000 UTC 20.9 kB denmark-lakes.parquet object 2023-03-21 14:45:38 +0000 UTC 916.4 kB lakes.parquet . ",
    "url": "/v1.4/quickstart/actions-and-hooks.html#bonus-challenge",
    
    "relUrl": "/quickstart/actions-and-hooks.html#bonus-challenge"
  },"11": {
    "doc": "6️⃣ Using Actions and Hooks in lakeFS",
    "title": "Finishing Up",
    "content": "Once you’ve finished the quickstart, shut down your local environment with the following command: . docker stop lakefs . ",
    "url": "/v1.4/quickstart/actions-and-hooks.html#finishing-up",
    
    "relUrl": "/quickstart/actions-and-hooks.html#finishing-up"
  },"12": {
    "doc": "6️⃣ Using Actions and Hooks in lakeFS",
    "title": "6️⃣ Using Actions and Hooks in lakeFS",
    "content": " ",
    "url": "/v1.4/quickstart/actions-and-hooks.html",
    
    "relUrl": "/quickstart/actions-and-hooks.html"
  },"13": {
    "doc": "Airbyte",
    "title": "Airbyte",
    "content": "Airbyte is an open-source platform for syncing data from applications, APIs, and databases to warehouses, lakes, and other destinations. You can use Airbyte’s connectors to get your data pipelines to consolidate many input sources. The integration between Airbyte and lakeFS brings resilience and manageability when you use Airbyte connectors to sync data to your S3 buckets by leveraging lakeFS branches and atomic commits and merges. ",
    "url": "/v1.4/integrations/airbyte.html",
    
    "relUrl": "/integrations/airbyte.html"
  },"14": {
    "doc": "Airbyte",
    "title": "Use cases",
    "content": "You can take advantage of lakeFS consistency guarantees and Data Lifecycle Management when ingesting data to S3 using lakeFS: . | Consolidate many data sources to a single branch and expose them to consumers simultaneously when merging to the main branch. | Test incoming data for breaking schema changes using lakeFS hooks. | Prevent consumers from reading partial data from connectors which failed half-way through sync. | Experiment with ingested data on a branch before exposing it. | . ",
    "url": "/v1.4/integrations/airbyte.html#use-cases",
    
    "relUrl": "/integrations/airbyte.html#use-cases"
  },"15": {
    "doc": "Airbyte",
    "title": "S3 Connector",
    "content": "lakeFS exposes an S3 Gateway that enables applications to communicate with lakeFS the same way they would with Amazon S3. You can use Airbyte’s S3 Connector to upload data to lakeFS. Note . If using Airbyte OSS, please ensure you are using S3 destination connector version 0.3.17 or higher. Previous connector versions are not supported. Configuring lakeFS using the connector . Set the following parameters when creating a new Destination of type S3: . | Name | Value | Example | . | Endpoint | The lakeFS S3 gateway URL | https://cute-axolotol.lakefs-demo.io | . | S3 Bucket Name | The lakeFS repository where the data will be written | example-repo | . | S3 Bucket Path | The branch and the path where the data will be written | main/data/from/airbyte Where main is the branch name, and data/from/airbyte is the path under the branch. | . | S3 Bucket Region | Not applicable to lakeFS, use us-east-1 | us-east-1 | . | S3 Key ID | The lakeFS access key id used to authenticate to lakeFS. | AKIAlakefs12345EXAMPLE | . | S3 Access Key | The lakeFS secret access key used to authenticate to lakeFS. | abc/lakefs/1234567bPxRfiCYEXAMPLEKEY | . The UI configuration will look as follows: . ",
    "url": "/v1.4/integrations/airbyte.html#s3-connector",
    
    "relUrl": "/integrations/airbyte.html#s3-connector"
  },"16": {
    "doc": "Apache Airflow",
    "title": "Using lakeFS with Apache Airflow",
    "content": "Apache Airflow is a platform that allows users to programmatically author, schedule, and monitor workflows. To run Airflow with lakeFS, you need to follow a few steps. ",
    "url": "/v1.4/integrations/airflow.html#using-lakefs-with-apache-airflow",
    
    "relUrl": "/integrations/airflow.html#using-lakefs-with-apache-airflow"
  },"17": {
    "doc": "Apache Airflow",
    "title": "Create a lakeFS connection on Airflow",
    "content": "To access the lakeFS server and authenticate with it, create a new Airflow Connection of type HTTP and add it to your DAG. You can do that using the Airflow UI or the CLI. Here’s an example Airflow command that does just that: . airflow connections add conn_lakefs --conn-type=HTTP --conn-host=http://&lt;LAKEFS_ENDPOINT&gt; \\ --conn-extra='{\"access_key_id\":\"&lt;LAKEFS_ACCESS_KEY_ID&gt;\",\"secret_access_key\":\"&lt;LAKEFS_SECRET_ACCESS_KEY&gt;\"}' . ",
    "url": "/v1.4/integrations/airflow.html#create-a-lakefs-connection-on-airflow",
    
    "relUrl": "/integrations/airflow.html#create-a-lakefs-connection-on-airflow"
  },"18": {
    "doc": "Apache Airflow",
    "title": "Install the lakeFS Airflow package",
    "content": "You can use pip to install the package . pip install airflow-provider-lakefs . ",
    "url": "/v1.4/integrations/airflow.html#install-the-lakefs-airflow-package",
    
    "relUrl": "/integrations/airflow.html#install-the-lakefs-airflow-package"
  },"19": {
    "doc": "Apache Airflow",
    "title": "Use the package",
    "content": "Operators . The package exposes several operations to interact with a lakeFS server: . | CreateBranchOperator creates a new lakeFS branch from the source branch (main by default). task_create_branch = CreateBranchOperator( task_id='create_branch', repo='example-repo', branch='example-branch', source_branch='main' ) . | CommitOperator commits uncommitted changes to a branch. task_commit = CommitOperator( task_id='commit', repo='example-repo', branch='example-branch', msg='committing to lakeFS using airflow!', metadata={'committed_from\": \"airflow-operator'} ) . | MergeOperator merges 2 lakeFS branches. task_merge = MergeOperator( task_id='merge_branches', source_ref='example-branch', destination_branch='main', msg='merging job outputs', metadata={'committer': 'airflow-operator'} ) . | . Sensors . Sensors are also available that allow synchronizing a running DAG with external operations: . | CommitSensor waits until a commit has been applied to the branch . task_sense_commit = CommitSensor( repo='example-repo', branch='example-branch', task_id='sense_commit' ) . | FileSensor waits until a given file is present on a branch. task_sense_file = FileSensor( task_id='sense_file', repo='example-repo', branch='example-branch', path=\"file/to/sense\" ) . | . Example . This example DAG in the airflow-provider-lakeFS repository shows how to use all of these. Performing other operations . Sometimes an operator might not be supported by airflow-provider-lakeFS yet. You can access lakeFS directly by using: . | SimpleHttpOperator to send API requests to lakeFS. | BashOperator with lakectl commands. For example, deleting a branch using BashOperator: commit_extract = BashOperator( task_id='delete_branch', bash_command='lakectl branch delete lakefs://example-repo/example-branch', dag=dag, ) . | . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. ",
    "url": "/v1.4/integrations/airflow.html#use-the-package",
    
    "relUrl": "/integrations/airflow.html#use-the-package"
  },"20": {
    "doc": "Apache Airflow",
    "title": "Apache Airflow",
    "content": " ",
    "url": "/v1.4/integrations/airflow.html",
    
    "relUrl": "/integrations/airflow.html"
  },"21": {
    "doc": "Airflow Hooks",
    "title": "Airflow Hooks",
    "content": " ",
    "url": "/v1.4/howto/hooks/airflow.html",
    
    "relUrl": "/howto/hooks/airflow.html"
  },"22": {
    "doc": "Airflow Hooks",
    "title": "Table of contents",
    "content": ". | Action file Airflow hook properties | Hook Record in configuration field | . Airflow Hook triggers a DAG run in an Airflow installation using Airflow’s REST API. The hook run succeeds if the DAG was triggered, and fails otherwise. ",
    "url": "/v1.4/howto/hooks/airflow.html#table-of-contents",
    
    "relUrl": "/howto/hooks/airflow.html#table-of-contents"
  },"23": {
    "doc": "Airflow Hooks",
    "title": "Action file Airflow hook properties",
    "content": "See the Action configuration for overall configuration schema and details. | Property | Description | Data Type | Example | Required | Environment Variables Supported | . | url | The URL of the Airflow instance | String | http://localhost:8080 | yes | no | . | dag_id | The DAG to trigger | String | example_dag | yes | no | . | username | The name of the Airflow user performing the request | String | admin | yes | no | . | password | The password of the Airflow user performing the request | String | admin | yes | yes | . | dag_conf | DAG run configuration that will be passed as is | JSON |   | no | no | . | wait_for_dag | Wait for DAG run to complete and reflect state (default: false) | Boolean |   | no | no | . | timeout | Time to wait for the DAG run to complete (default: 1m) | String (golang’s Duration representation) |   | no | no | . Example: ... hooks: - id: trigger_my_dag type: airflow description: Trigger an example_dag properties: url: \"http://localhost:8000\" dag_id: \"example_dag\" username: \"admin\" password: \"{{ ENV.AIRFLOW_SECRET }}\" dag_conf: some: \"additional_conf\" ... ",
    "url": "/v1.4/howto/hooks/airflow.html#action-file-airflow-hook-properties",
    
    "relUrl": "/howto/hooks/airflow.html#action-file-airflow-hook-properties"
  },"24": {
    "doc": "Airflow Hooks",
    "title": "Hook Record in configuration field",
    "content": "lakeFS will add an entry to the Airflow request configuration property (conf) with the event that triggered the action. The key of the record will be lakeFS_event and the value will match the one described here . ",
    "url": "/v1.4/howto/hooks/airflow.html#hook-record-in-configuration-field",
    
    "relUrl": "/howto/hooks/airflow.html#hook-record-in-configuration-field"
  },"25": {
    "doc": "lakeFS API",
    "title": "lakeFS API",
    "content": "| ",
    "url": "/v1.4/reference/api.html",
    
    "relUrl": "/reference/api.html"
  },"26": {
    "doc": "Architecture",
    "title": "lakeFS Architecture",
    "content": "lakeFS is distributed as a single binary encapsulating several logical services. The server itself is stateless, meaning you can easily add more instances to handle a bigger load. ",
    "url": "/v1.4/understand/architecture.html#lakefs-architecture",
    
    "relUrl": "/understand/architecture.html#lakefs-architecture"
  },"27": {
    "doc": "Architecture",
    "title": "Table of contents",
    "content": ". | Object Storage | Metadata Storage | Load Balancing | lakeFS Components . | S3 Gateway | OpenAPI Server | Storage Adapter | Graveler | Authentication &amp; Authorization Service | Hooks Engine | UI | . | Applications | lakeFS Clients . | OpenAPI Generated SDKs | lakectl | Spark Metadata Client | lakeFS Hadoop FileSystem | . | . Object Storage . lakeFS stores data in object stores. Those supported include: . | AWS S3 | Google Cloud Storage | Azure Blob Storage | MinIO | Ceph | . Metadata Storage . In additional a Key Value storage is used for storing metadata, with supported databases including PostgreSQL, DynamoDB, and CosmosDB Instructions of how to deploy such database on AWS can be found here. Additional information on the data format can be found in Versioning internals and Internal database structure . Load Balancing . Accessing lakeFS is done using HTTP. lakeFS exposes a frontend UI, an OpenAPI server, as well as an S3-compatible service (see S3 Gateway below). lakeFS uses a single port that serves all three endpoints, so for most use cases a single load balancer pointing to lakeFS server(s) would do. ",
    "url": "/v1.4/understand/architecture.html#table-of-contents",
    
    "relUrl": "/understand/architecture.html#table-of-contents"
  },"28": {
    "doc": "Architecture",
    "title": "lakeFS Components",
    "content": "S3 Gateway . The S3 Gateway is the layer in lakeFS responsible for the compatibility with S3. It implements a compatible subset of the S3 API to ensure most data systems can use lakeFS as a drop-in replacement for S3. See the S3 API Reference section for information on supported API operations. OpenAPI Server . The Swagger (OpenAPI) server exposes the full set of lakeFS operations (see Reference). This includes basic CRUD operations against repositories and objects, as well as versioning related operations such as branching, merging, committing, and reverting changes to data. Storage Adapter . The Storage Adapter is an abstraction layer for communicating with any underlying object store. Its implementations allow compatibility with many types of underlying storage such as S3, GCS, Azure Blob Storage, or non-production usages such as the local storage adapter. See the roadmap for information on the future plans for storage compatibility. Graveler . The Graveler handles lakeFS versioning by translating lakeFS addresses to the actual stored objects. To learn about the data model used to store lakeFS metadata, see the versioning internals page. Authentication &amp; Authorization Service . The Auth service handles the creation, management, and validation of user credentials and RBAC policies. The credential scheme, along with the request signing logic, are compatible with AWS IAM (both SIGv2 and SIGv4). Currently, the Auth service manages its own database of users and credentials and doesn’t use IAM in any way. Hooks Engine . The Hooks Engine enables CI/CD for data by triggering user defined Actions that will run during commit/merge. UI . The UI layer is a simple browser-based client that uses the OpenAPI server. It allows management, exploration, and data access to repositories, branches, commits and objects in the system. ",
    "url": "/v1.4/understand/architecture.html#lakefs-components",
    
    "relUrl": "/understand/architecture.html#lakefs-components"
  },"29": {
    "doc": "Architecture",
    "title": "Applications",
    "content": "As a rule of thumb, lakeFS supports any S3-compatible application. This means that many common data applications work with lakeFS out-of-the-box. Check out our integrations to learn more. ",
    "url": "/v1.4/understand/architecture.html#applications",
    
    "relUrl": "/understand/architecture.html#applications"
  },"30": {
    "doc": "Architecture",
    "title": "lakeFS Clients",
    "content": "Some data applications benefit from deeper integrations with lakeFS to support different use cases or enhanced functionality provided by lakeFS clients. OpenAPI Generated SDKs . OpenAPI specification can be used to generate lakeFS clients for many programming languages. For example, the Python lakefs-client or the Java client are published with every new lakeFS release. lakectl . lakectl is a CLI tool that enables lakeFS operations using the lakeFS API from your preferred terminal. Spark Metadata Client . The lakeFS Spark Metadata Client makes it easy to perform operations related to lakeFS metadata, at scale. Examples include garbage collection or exporting data from lakeFS. lakeFS Hadoop FileSystem . Thanks to the S3 Gateway, it’s possible to interact with lakeFS using Hadoop’s S3AFIleSystem, but due to limitations of the S3 API, doing so requires reading and writing data objects through the lakeFS server. Using lakeFSFileSystem increases Spark ETL jobs performance by executing the metadata operations on the lakeFS server, and all data operations directly through the same underlying object store that lakeFS uses. ",
    "url": "/v1.4/understand/architecture.html#lakefs-clients",
    
    "relUrl": "/understand/architecture.html#lakefs-clients"
  },"31": {
    "doc": "Architecture",
    "title": "Architecture",
    "content": " ",
    "url": "/v1.4/understand/architecture.html",
    
    "relUrl": "/understand/architecture.html"
  },"32": {
    "doc": "Amazon Athena",
    "title": "Using lakeFS with Amazon Athena",
    "content": "Deprecated Feature: Having heard the feedback from the community, we are planning to replace the below manual steps with an automated process. You can read more about it here. Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Amazon Athena works directly above S3 and can’t access lakeFS. Tables created using Athena aren’t readable by lakeFS. However, tables stored in lakeFS (that were created with glue/hive) can be queried by Athena. To support querying data from lakeFS with Amazon Athena, we will use create-symlink, one of the metastore commands in lakectl. create-symlink receives a source table, destination table, and the table location. It performs two actions: . | It creates partitioned directories with symlink files in the underlying S3 bucket. | It creates a table in Glue catalog with symlink format type and location pointing to the created symlinks. | . Note .lakectl.yaml file should be configured with the proper hive/glue credentials. For more information . create-symlink receives a table in glue or hive pointing to lakeFS and creates a copy of the table in glue. The table data will use the SymlinkTextInputFormat, which will point to the lakeFS repository storage namespace. You will be able to query your data with Athena without copying any data. However, the symlinks table will only show the data that existed during the copy. If the table changed in lakeFS, you need to run create-symlink again for your changed to be reflected in Athena. Example: . Let’s assume that some time ago, we created a hive table my_table that is stored in lakeFS repo example under branch main, using the command: . CREATE EXTERNAL TABLE `my_table`( `id` bigint, `key` string ) PARTITIONED BY (YEAR INT, MONTH INT) LOCATION 's3://example/main/my_table'; WITH (format = 'PARQUET', external_location 's3a://example/main/my_table' ); . The repository example has the S3 storage space s3://my-bucket/my-repo-prefix/. After inserting some data into it, the object structure under lakefs://example/main/my_table looks as follows: . To query that table with Athena, you need to use the create-symlink command as follows: . lakectl metastore create-symlink \\ --repo example \\ --branch main \\ --path my_table \\ --from-client-type hive \\ --from-schema default \\ --from-table my_table \\ --to-schema default \\ --to-table my_table . The command will generate two notable outputs: . | For each partition, the command will create a symlink file: | . aws s3 ls s3://my-bucket/my-repo-prefix/my_table/ --recursive 2021-11-23 17:46:29 0 my-repo-prefix/my_table/symlinks/example/main/my_table/year=2021/month=11/symlink.txt 2021-11-23 17:46:29 60 my-repo-prefix/my_table/symlinks/example/main/my_table/year=2021/month=12/symlink.txt 2021-11-23 17:46:30 60 my-repo-prefix/my_table/symlinks/example/main/my_table/year=2022/month=1/symlink.txt . An example content of a symlink file, where each line represents a single object of the specific partition: . s3://my-bucket/my-repo-prefix/5bdc62da516944b49889770d98274227 s3://my-bucket/my-repo-prefix/64262fbf3d6347a79ead641d2b2baee6 s3://my-bucket/my-repo-prefix/64486c8de6484de69f12d7d26804c93e s3://my-bucket/my-repo-prefix/b0165d5c5b13473d8a0f460eece9eb26 . | A glue table pointing to the symlink directories structure: | . aws glue get-table --name my_table --database-name default { \"Table\": { \"Name\": \"my_table\", \"DatabaseName\": \"default\", \"Owner\": \"anonymous\", \"CreateTime\": \"2021-11-23T17:46:30+02:00\", \"UpdateTime\": \"2021-11-23T17:46:30+02:00\", \"LastAccessTime\": \"1970-01-01T02:00:00+02:00\", \"Retention\": 0, \"StorageDescriptor\": { \"Columns\": [ { \"Name\": \"id\", \"Type\": \"bigint\", \"Comment\": \"\" }, { \"Name\": \"key\", \"Type\": \"string\", \"Comment\": \"\" } ], \"Location\": \"s3://my-bucket/my-repo-prefix/symlinks/example/main/my_table\", \"InputFormat\": \"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat\", \"OutputFormat\": \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\", \"Compressed\": false, \"NumberOfBuckets\": -1, \"SerdeInfo\": { \"Name\": \"default\", \"SerializationLibrary\": \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\", \"Parameters\": { \"serialization.format\": \"1\" } }, \"StoredAsSubDirectories\": false }, \"PartitionKeys\": [ { \"Name\": \"year\", \"Type\": \"int\", \"Comment\": \"\" }, { \"Name\": \"month\", \"Type\": \"int\", \"Comment\": \"\" } ], \"ViewOriginalText\": \"\", \"ViewExpandedText\": \"\", \"TableType\": \"EXTERNAL_TABLE\", \"Parameters\": { \"EXTERNAL\": \"TRUE\", \"bucketing_version\": \"2\", \"transient_lastDdlTime\": \"1637681750\" }, \"CreatedBy\": \"arn:aws:iam::************:user/********\", \"IsRegisteredWithLakeFormation\": false, \"CatalogId\": \"*********\" } } . You can now safely use Athena to query my_table. ",
    "url": "/v1.4/integrations/athena.html#using-lakefs-with-amazon-athena",
    
    "relUrl": "/integrations/athena.html#using-lakefs-with-amazon-athena"
  },"33": {
    "doc": "Amazon Athena",
    "title": "Amazon Athena",
    "content": " ",
    "url": "/v1.4/integrations/athena.html",
    
    "relUrl": "/integrations/athena.html"
  },"34": {
    "doc": "Auditing",
    "title": "Auditing",
    "content": "lakeFS Cloud . Auditing is only available for lakeFS Cloud. The lakeFS audit log allows you to view all relevant user action information in a clear and organized table, including when the action was performed, by whom, and what it was they did. This can be useful for several purposes, including: . | Compliance - Audit logs can be used to show what data users accessed, as well as any changes they made to user management. | Troubleshooting - If something changes on your underlying object store that you weren’t expecting, such as a big file suddenly breaking into thousands of smaller files, you can use the audit log to find out what action led to this change. | . ",
    "url": "/v1.4/reference/auditing.html",
    
    "relUrl": "/reference/auditing.html"
  },"35": {
    "doc": "Auditing",
    "title": "Audit Fields",
    "content": "The audit log includes the following fields: . | Time - Time of action | User - Name of the user who performed the action | Region - Region of the lakeFS installation where the action was performed | Status - Status code returned for the action . | 2xx - Successful | 3xx - Redirection | 4xx - Client error | 5xx - Server error | . | Action - Specific lakeFS action (such as Login, Commit, ListRepositories, CreateUser, etc…) | Resource - Full URL of the command (e.g for a commit on branch main we would see the action Commit and the resource /api/v1/repositories/e2e-monitoring/branches/main/commits) | . ",
    "url": "/v1.4/reference/auditing.html#audit-fields",
    
    "relUrl": "/reference/auditing.html#audit-fields"
  },"36": {
    "doc": "Auditing",
    "title": "Filtering",
    "content": "Filtering is available using the filter bar. The filter bar works with a simple query language. The table fields can be filtered by the following operators . | User - =,!= | Region - =,!= | Status(Number) - &lt;,&gt;,&lt;=,&gt;=,=,!= | Action - =,!= | Resource - =,!= | . ",
    "url": "/v1.4/reference/auditing.html#filtering",
    
    "relUrl": "/reference/auditing.html#filtering"
  },"37": {
    "doc": "Authentication",
    "title": "Authentication",
    "content": " ",
    "url": "/v1.4/reference/security/authentication.html",
    
    "relUrl": "/reference/security/authentication.html"
  },"38": {
    "doc": "Authentication",
    "title": "Table of contents",
    "content": ". | Authentication . | User Authentication | API Server Authentication | S3 Gateway Authentication | . | OIDC support . | Configuring lakeFS server for OIDC | . | User permissions . | Using a different claim name | . | . ",
    "url": "/v1.4/reference/security/authentication.html#table-of-contents",
    
    "relUrl": "/reference/security/authentication.html#table-of-contents"
  },"39": {
    "doc": "Authentication",
    "title": "Authentication",
    "content": "User Authentication . lakeFS authenticates users from a built-in authentication database. Built-in database . The built-in authentication database is always present and active. You can use the Web UI at Administration / Users to create users. Users have an access key AKIA... and an associated secret access key. These credentials are valid for logging into the Web UI or authenticating programmatic requests to the API Server or the S3 Gateway. Remote Authenticator Service . lakeFS server supports external authentication, the feature can be configured by providing an HTTP endpoint to an external authentication service. This integration can be especially useful if you already have an existing authentication system in place, as it allows you to reuse that system instead of maintaining a new one. To configure a Remote Authenticator see the configuration fields. API Server Authentication . Authenticating against the API server is done using a key-pair, passed via Basic Access Authentication. All HTTP requests must carry an Authorization header with the following structure: . Authorization: Basic &lt;base64 encoded access_key_id:secret_access_key&gt; . For example, assuming my access_key_id is my_access_key_id and my secret_access_key is my_secret_access_key, we’d send the following header with every request: . Authorization: Basic bXlfYWNjZXNzX2tleV9pZDpteV9hY2Nlc3Nfc2VjcmV0X2tleQ== . S3 Gateway Authentication . To provide API compatibility with Amazon S3, authentication with the S3 Gateway supports both SIGv2 and SIGv4. Clients such as the AWS SDK that implement these authentication methods should work without modification. See this example for authenticating with the AWS CLI. ",
    "url": "/v1.4/reference/security/authentication.html",
    
    "relUrl": "/reference/security/authentication.html"
  },"40": {
    "doc": "Authentication",
    "title": "OIDC support",
    "content": "Note This feature is deprecated. For single sign-on with lakeFS, try lakeFS Cloud . OpenID Connect (OIDC) is a simple identity layer on top of the OAuth 2.0 protocol. You can configure lakeFS to enable OIDC to manage your lakeFS users externally. Essentially, once configured, this enables you the benefit of OpenID connect, such as a single sign-on (SSO), etc. Configuring lakeFS server for OIDC . To support OIDC, add the following to your lakeFS configuration: . auth: oidc: enabled: true client_id: example-client-id client_secret: exampleSecretValue callback_base_url: https://lakefs.example.com # The scheme, domain (and port) of your lakeFS installation url: https://my-account.oidc-provider-example.com default_initial_groups: [\"Developers\"] friendly_name_claim_name: name # Optional: use the value from this claim as the user's display name . Your login page will now include a link to sign in using the OIDC provider. When a user first logs in through the provider, a corresponding user is created in lakeFS. Notes . | As always, you may choose to provide these configurations using environment variables. | You may already have other configuration values under the auth key, so make sure you combine them correctly. | . ",
    "url": "/v1.4/reference/security/authentication.html#oidc-support",
    
    "relUrl": "/reference/security/authentication.html#oidc-support"
  },"41": {
    "doc": "Authentication",
    "title": "User permissions",
    "content": "Authorization is managed via lakeFS groups and policies. By default, an externally managed user is assigned to the lakeFS groups configured in the default_initial_groups property above. For a user to be assigned to other groups, add the initial_groups claim to their ID token claims. The claim should contain a comma-separated list of group names. Once the user has been created, you can manage their permissions from the Administration pages in the lakeFS UI or using lakectl. Using a different claim name . To supply the initial groups using another claim from your ID token, you can use the auth.oidc.initial_groups_claim_name lakeFS configuration. For example, to take the initial groups from the roles claim, add: . auth: oidc: # ... Other OIDC configurations initial_groups_claim_name: roles . ",
    "url": "/v1.4/reference/security/authentication.html#user-permissions",
    
    "relUrl": "/reference/security/authentication.html#user-permissions"
  },"42": {
    "doc": "AWS",
    "title": "Deploy lakeFS on AWS",
    "content": "The instructions given here are for a self-managed deployment of lakeFS on AWS. For a hosted lakeFS service with guaranteed SLAs, try lakeFS Cloud . When you deploy lakeFS on AWS these are the options available to use: . This guide walks you through the options available and how to configure them, finishing with configuring and running lakeFS itself and creating your first repository. ",
    "url": "/v1.4/howto/deploy/aws.html#deploy-lakefs-on-aws",
    
    "relUrl": "/howto/deploy/aws.html#deploy-lakefs-on-aws"
  },"43": {
    "doc": "AWS",
    "title": "Table of contents",
    "content": ". | Grant lakeFS permissions to DynamoDB | Run the lakeFS server | Prepare your S3 bucket | Create the admin user | Create your first repository | . ⏰ Expected deployment time: 25 min . ",
    "url": "/v1.4/howto/deploy/aws.html#table-of-contents",
    
    "relUrl": "/howto/deploy/aws.html#table-of-contents"
  },"44": {
    "doc": "AWS",
    "title": "Grant lakeFS permissions to DynamoDB",
    "content": "By default, lakeFS will create the required DynamoDB table if it does not already exist. You’ll have to give the IAM role used by lakeFS the following permissions: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"ListAndDescribe\", \"Effect\": \"Allow\", \"Action\": [ \"dynamodb:List*\", \"dynamodb:DescribeReservedCapacity*\", \"dynamodb:DescribeLimits\", \"dynamodb:DescribeTimeToLive\" ], \"Resource\": \"*\" }, { \"Sid\": \"kvstore\", \"Effect\": \"Allow\", \"Action\": [ \"dynamodb:BatchGet*\", \"dynamodb:DescribeTable\", \"dynamodb:Get*\", \"dynamodb:Query\", \"dynamodb:Scan\", \"dynamodb:BatchWrite*\", \"dynamodb:CreateTable\", \"dynamodb:Delete*\", \"dynamodb:Update*\", \"dynamodb:PutItem\" ], \"Resource\": \"arn:aws:dynamodb:*:*:table/kvstore\" } ] } . 💡 You can also use lakeFS with PostgreSQL instead of DynamoDB! See the configuration reference for more information. ",
    "url": "/v1.4/howto/deploy/aws.html#grant-lakefs-permissions-to-dynamodb",
    
    "relUrl": "/howto/deploy/aws.html#grant-lakefs-permissions-to-dynamodb"
  },"45": {
    "doc": "AWS",
    "title": "Run the lakeFS server",
    "content": ". | EC2 | EKS | . Connect to your EC2 instance using SSH: . | Create a config.yaml on your EC2 instance, with the following parameters: . --- database: type: \"dynamodb\" auth: encrypt: # replace this with a randomly-generated string. Make sure to keep it safe! secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: s3 . | Download the binary to run on the EC2 instance. | Run the lakefs binary on the EC2 instance: . lakefs --config config.yaml run . | . Note: It’s preferable to run the binary as a service using systemd or your operating system’s facilities. Advanced: Deploying lakeFS behind an AWS Application Load Balancer . | Your security groups should allow the load balancer to access the lakeFS server. | Create a target group with a listener for port 8000. | Setup TLS termination using the domain names you wish to use (e.g., lakefs.example.com and potentially s3.lakefs.example.com, *.s3.lakefs.example.com if using virtual-host addressing). | Configure the health-check to use the exposed /_health URL | . You can install lakeFS on Kubernetes using a Helm chart. To install lakeFS with Helm: . | Copy the Helm values file relevant for S3: . secrets: # replace this with a randomly-generated string authEncryptSecretKey: [ENCRYPTION_SECRET_KEY] lakefsConfig: | database: type: dynamodb blockstore: type: s3 . | Fill in the missing values and save the file as conf-values.yaml. For more configuration options, see our Helm chart README. The lakefsConfig parameter is the lakeFS configuration documented here but without sensitive information. Sensitive information like databaseConnectionString is given through separate parameters, and the chart will inject it into Kubernetes secrets. | In the directory where you created conf-values.yaml, run the following commands: . # Add the lakeFS repository helm repo add lakefs https://charts.lakefs.io # Deploy lakeFS helm install my-lakefs lakefs/lakefs -f conf-values.yaml . my-lakefs is the Helm Release name. | . ⚠️ Make sure the Kubernetes nodes have access to all buckets/containers with which you intend to use with lakeFS. If you can’t provide such access, configure lakeFS with an AWS key-pair. Load balancing . To configure a load balancer to direct requests to the lakeFS servers you can use the LoadBalancer Service type or a Kubernetes Ingress. By default, lakeFS operates on port 8000 and exposes a /_health endpoint that you can use for health checks. 💡 The NGINX Ingress Controller by default limits the client body size to 1 MiB. Some clients use bigger chunks to upload objects - for example, multipart upload to lakeFS using the S3 Gateway or a simple PUT request using the OpenAPI Server. Checkout Nginx documentation for increasing the limit, or an example of Nginx configuration with MinIO. ",
    "url": "/v1.4/howto/deploy/aws.html#run-the-lakefs-server",
    
    "relUrl": "/howto/deploy/aws.html#run-the-lakefs-server"
  },"46": {
    "doc": "AWS",
    "title": "Prepare your S3 bucket",
    "content": ". | Take note of the bucket name you want to use with lakeFS | Use the following as your bucket policy, filling in the placeholders: . | Standard Permissions | Standard Permissions (with s3express) | Minimal Permissions (Advanced) | . { \"Id\": \"lakeFSPolicy\", \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"lakeFSObjects\", \"Action\": [ \"s3:GetObject\", \"s3:PutObject\", \"s3:AbortMultipartUpload\", \"s3:ListMultipartUploadParts\" ], \"Effect\": \"Allow\", \"Resource\": [\"arn:aws:s3:::[BUCKET_NAME_AND_PREFIX]/*\"], \"Principal\": { \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"] } }, { \"Sid\": \"lakeFSBucket\", \"Action\": [ \"s3:ListBucket\", \"s3:GetBucketLocation\", \"s3:ListBucketMultipartUploads\" ], \"Effect\": \"Allow\", \"Resource\": [\"arn:aws:s3:::[BUCKET]\"], \"Principal\": { \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"] } } ] } . | Replace [BUCKET_NAME], [ACCOUNT_ID] and [IAM_ROLE] with values relevant to your environment. | [BUCKET_NAME_AND_PREFIX] can be the bucket name. If you want to minimize the bucket policy permissions, use the bucket name together with a prefix (e.g. example-bucket/a/b/c). This way, lakeFS will be able to create repositories only under this specific path (see: Storage Namespace). | lakeFS will try to assume the role [IAM_ROLE]. | . To use an S3 Express One Zone directory bucket, use the following policy. Note the lakeFSDirectoryBucket statement which is specifically required for using a directory bucket. { \"Id\": \"lakeFSPolicy\", \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"lakeFSObjects\", \"Action\": [ \"s3:GetObject\", \"s3:PutObject\", \"s3:AbortMultipartUpload\", \"s3:ListMultipartUploadParts\" ], \"Effect\": \"Allow\", \"Resource\": [\"arn:aws:s3:::[BUCKET_NAME_AND_PREFIX]/*\"], \"Principal\": { \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"] } }, { \"Sid\": \"lakeFSBucket\", \"Action\": [ \"s3:ListBucket\", \"s3:GetBucketLocation\", \"s3:ListBucketMultipartUploads\" ], \"Effect\": \"Allow\", \"Resource\": [\"arn:aws:s3:::[BUCKET]\"], \"Principal\": { \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"] } }, { \"Sid\": \"lakeFSDirectoryBucket\", \"Action\": [ \"s3express:CreateSession\" ], \"Effect\": \"Allow\", \"Resource\": \"arn:aws:s3express:[REGION]:[ACCOUNT_ID]:bucket/[BUCKET_NAME]\" } ] } . | Replace [BUCKET_NAME], [ACCOUNT_ID] and [IAM_ROLE] with values relevant to your environment. | [BUCKET_NAME_AND_PREFIX] can be the bucket name. If you want to minimize the bucket policy permissions, use the bucket name together with a prefix (e.g. example-bucket/a/b/c). This way, lakeFS will be able to create repositories only under this specific path (see: Storage Namespace). | lakeFS will try to assume the role [IAM_ROLE]. | . If required lakeFS can operate without accessing the data itself, this permission section is useful if you are using presigned URLs mode or the lakeFS Hadoop FileSystem Spark integration. Since this FileSystem performs many operations directly on the storage, lakeFS requires less permissive permissions, resulting in increased security. lakeFS always requires permissions to access the _lakefs prefix under your storage namespace, in which metadata is stored (learn more). By setting this policy without presign mode you’ll be able to perform only metadata operations through lakeFS, meaning that you’ll not be able to use lakeFS to upload or download objects. Specifically you won’t be able to: . | Upload objects using the lakeFS GUI (Works with presign mode) | Upload objects through Spark using the S3 gateway | Run lakectl fs commands (unless using presign mode with --pre-sign flag) | Use Actions and Hooks | . { \"Id\": \"[POLICY_ID]\", \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"lakeFSObjects\", \"Action\": [ \"s3:GetObject\", \"s3:PutObject\" ], \"Effect\": \"Allow\", \"Resource\": [ \"arn:aws:s3:::[STORAGE_NAMESPACE]/_lakefs/*\" ], \"Principal\": { \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"] } }, { \"Sid\": \"lakeFSBucket\", \"Action\": [ \"s3:ListBucket\", \"s3:GetBucketLocation\" ], \"Effect\": \"Allow\", \"Resource\": [\"arn:aws:s3:::[BUCKET]\"], \"Principal\": { \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"] } } ] } . We can use presigned URLs mode without allowing access to the data from the lakeFS server directly. We can achieve this by using condition keys such as aws:referer, aws:SourceVpc and aws:SourceIp. For example, assume the following scenario: . | lakeFS is deployed outside the company (i.e lakeFS cloud or other VPC not vpc-123) | We don’t want lakeFS to be able to access the data, so we use presign URL, we still need lakeFS role to be able to sign the URL. | We want to allow access from the internal company VPC: vpc-123. | . { \"Sid\": \"allowLakeFSRoleFromCompanyOnly\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\" }, \"Action\": [ \"s3:GetObject\", \"s3:PutObject\", ], \"Resource\": [ \"arn:aws:s3:::[BUCKET]/*\", ], \"Condition\": { \"StringEquals\": { \"aws:SourceVpc\": \"vpc-123\" } } } . | . Alternative: use an AWS user . lakeFS can authenticate with your AWS account using an AWS user, using an access key and secret. To allow this, change the policy’s Principal accordingly: . \"Principal\": { \"AWS\": [\"arn:aws:iam::&lt;ACCOUNT_ID&gt;:user/&lt;IAM_USER&gt;\"] } . ",
    "url": "/v1.4/howto/deploy/aws.html#prepare-your-s3-bucket",
    
    "relUrl": "/howto/deploy/aws.html#prepare-your-s3-bucket"
  },"47": {
    "doc": "AWS",
    "title": "Create the admin user",
    "content": "When you first open the lakeFS UI, you will be asked to create an initial admin user. | Open http://&lt;lakefs-host&gt;/ in your browser. If you haven’t set up a load balancer, this will likely be http://&lt;instance ip address&gt;:8000/ | On first use, you’ll be redirected to the setup page: . | Follow the steps to create an initial administrator user. Save the credentials you’ve received somewhere safe, you won’t be able to see them again! . | Follow the link and go to the login screen. Use the credentials from the previous step to log in. | . ",
    "url": "/v1.4/howto/deploy/aws.html#create-the-admin-user",
    
    "relUrl": "/howto/deploy/aws.html#create-the-admin-user"
  },"48": {
    "doc": "AWS",
    "title": "Create your first repository",
    "content": ". | Use the credentials from the previous step to log in | Click Create Repository and choose Blank Repository. | Under Storage Namespace, enter a path to your desired location on the object store. This is where data written to this repository will be stored. | Click Create Repository | You should now have a configured repository, ready to use! . | . Congratulations! Your environment is now ready 🤩 . ",
    "url": "/v1.4/howto/deploy/aws.html#create-your-first-repository",
    
    "relUrl": "/howto/deploy/aws.html#create-your-first-repository"
  },"49": {
    "doc": "AWS",
    "title": "AWS",
    "content": " ",
    "url": "/v1.4/howto/deploy/aws.html",
    
    "relUrl": "/howto/deploy/aws.html"
  },"50": {
    "doc": "AWS CLI",
    "title": "Using lakeFS with AWS CLI",
    "content": "lakeFS exposes an S3-compatible API, so you can use the AWS S3 CLI to interact with objects in your repositories. ",
    "url": "/v1.4/integrations/aws_cli.html#using-lakefs-with-aws-cli",
    
    "relUrl": "/integrations/aws_cli.html#using-lakefs-with-aws-cli"
  },"51": {
    "doc": "AWS CLI",
    "title": "Table of contents",
    "content": ". | Configuration | Path convention | Usage | Examples . | List directory | Copy from lakeFS to lakeFS | Copy from lakeFS to a local path | Copy from a local path to lakeFS | Delete file | Delete directory | . | Adding an alias | . ",
    "url": "/v1.4/integrations/aws_cli.html#table-of-contents",
    
    "relUrl": "/integrations/aws_cli.html#table-of-contents"
  },"52": {
    "doc": "AWS CLI",
    "title": "Configuration",
    "content": "You would like to configure an AWS profile for lakeFS. To configure the lakeFS credentials, run: . aws configure --profile lakefs . You will be prompted to enter the AWS Access Key ID and the AWS Secret Access Key. It should look like this: . aws configure --profile lakefs # output: # AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE # AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # Default region name [None]: # Default output format [None]: . ",
    "url": "/v1.4/integrations/aws_cli.html#configuration",
    
    "relUrl": "/integrations/aws_cli.html#configuration"
  },"53": {
    "doc": "AWS CLI",
    "title": "Path convention",
    "content": "When accessing objects in S3, you will need to use the lakeFS path convention: s3://[REPOSITORY]/[BRANCH]/PATH/TO/OBJECT . ",
    "url": "/v1.4/integrations/aws_cli.html#path-convention",
    
    "relUrl": "/integrations/aws_cli.html#path-convention"
  },"54": {
    "doc": "AWS CLI",
    "title": "Usage",
    "content": "After configuring the credentials, this is what a command should look: . aws s3 --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ ls s3://example-repo/main/example-directory . You can use an alias to make it shorter and more convenient. ",
    "url": "/v1.4/integrations/aws_cli.html#usage",
    
    "relUrl": "/integrations/aws_cli.html#usage"
  },"55": {
    "doc": "AWS CLI",
    "title": "Examples",
    "content": "List directory . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 ls s3://example-repo/main/example-directory . Copy from lakeFS to lakeFS . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 cp s3://example-repo/main/example-file-1 s3://example-repo/main/example-file-2 . Copy from lakeFS to a local path . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 cp s3://example-repo/main/example-file-1 /path/to/local/file . Copy from a local path to lakeFS . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 cp /path/to/local/file s3://example-repo/main/example-file-1 . Delete file . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 rm s3://example-repo/main/example-directory/example-file . Delete directory . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 rm s3://example-repo/main/example-directory/ --recursive . ",
    "url": "/v1.4/integrations/aws_cli.html#examples",
    
    "relUrl": "/integrations/aws_cli.html#examples"
  },"56": {
    "doc": "AWS CLI",
    "title": "Adding an alias",
    "content": "To make the command shorter and more convenient, you can create an alias: . alias awslfs='aws --endpoint https://lakefs.example.com --profile lakefs' . Now, the ls command using the alias will be as follows: . awslfs s3 ls s3://example-repo/main/example-directory . ",
    "url": "/v1.4/integrations/aws_cli.html#adding-an-alias",
    
    "relUrl": "/integrations/aws_cli.html#adding-an-alias"
  },"57": {
    "doc": "AWS CLI",
    "title": "AWS CLI",
    "content": " ",
    "url": "/v1.4/integrations/aws_cli.html",
    
    "relUrl": "/integrations/aws_cli.html"
  },"58": {
    "doc": "Azure",
    "title": "Deploy lakeFS on Azure",
    "content": "The instructions given here are for a self-managed deployment of lakeFS on Azure. For a hosted lakeFS service with guaranteed SLAs, try lakeFS Cloud . When you deploy lakeFS on Azure these are the options available to use: . This guide walks you through the options available and how to configure them, finishing with configuring and running lakeFS itself and creating your first repository. ",
    "url": "/v1.4/howto/deploy/azure.html#deploy-lakefs-on-azure",
    
    "relUrl": "/howto/deploy/azure.html#deploy-lakefs-on-azure"
  },"59": {
    "doc": "Azure",
    "title": "Table of contents",
    "content": ". | 1. Object Storage | 2. Authentication Method | 3. K/V Store | 4. Run the lakeFS server | Create the admin user | Create your first repository | . ⏰ Expected deployment time: 25 min . ",
    "url": "/v1.4/howto/deploy/azure.html#table-of-contents",
    
    "relUrl": "/howto/deploy/azure.html#table-of-contents"
  },"60": {
    "doc": "Azure",
    "title": "1. Object Storage",
    "content": "lakeFS supports the following Azure Storage types: . | Azure Blob Storage | Azure Data Lake Storage Gen2 (HNS) | . Data Lake Storage Gen1 is not supported. ",
    "url": "/v1.4/howto/deploy/azure.html#1-object-storage",
    
    "relUrl": "/howto/deploy/azure.html#1-object-storage"
  },"61": {
    "doc": "Azure",
    "title": "2. Authentication Method",
    "content": "lakeFS supports two ways to authenticate with Azure. | Identity Based Authentication (recommended) | Storage Account Credentials | . lakeFS uses environment variables to determine credentials to use for authentication. The following authentication methods are supported: . | Managed Service Identity (MSI) | Service Principal RBAC | Azure CLI | . For deployments inside the Azure ecosystem it is recommended to use a managed identity. More information on authentication methods and environment variables can be found here . How to Create Service Principal for Resource Group . It is recommended to create a resource group that consists of all the resources lakeFS should have access to. Using a resource group will allow dynamic removal/addition of services from the group, effectively providing/preventing access for lakeFS to these resources without requiring any changes in configuration in lakeFS or providing lakeFS with any additional credentials. The minimal role required for the service principal is “Storage Blob Data Contributor” . The following Azure CLI command creates a service principal for a resource group called “lakeFS” with permission to access (read/write/delete) Blob Storage resources in the resource group and with an expiry of 5 years . az ad sp create-for-rbac \\ --role \"Storage Blob Data Contributor\" \\ --scopes /subscriptions/XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX/resourceGroups/lakeFS --years 5 Creating 'Storage Blob Data Contributor' role assignment under scope '/subscriptions/947382ea-681a-4541-99ab-b718960c6289/resourceGroups/lakeFS' The output includes credentials that you must protect. Be sure that you do not include these credentials in your code or check the credentials into your source control. For more information, see https://aka.ms/azadsp-cli { \"appId\": \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\", \"displayName\": \"azure-cli-2023-01-30-06-18-30\", \"password\": \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\", \"tenant\": \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\" } . The command output should be used to populate the following environment variables: . AZURE_CLIENT_ID = $appId AZURE_TENANT_ID = $tenant AZURE_CLIENT_SECRET = $password . Note: Service Principal credentials have an expiry date and lakeFS will lose access to resources unless credentials are renewed on time. Note: It is possible to provide both account based credentials and environment variables to lakeFS. In that case - lakeFS will use the account credentials for any access to data located in the given account, and will try to use the identity credentials for any data located outside the given account. Storage account credentials can be set directly in the lakeFS configuration using the following parameters: . | blockstore.azure.storage_account | blockstore.azure.storage_access_key | . Limitations . Please note that using this authentication method limits lakeFS to the scope of the given storage account. Specifically, the following operations will not work: . | Import of data from different storage accounts | Copy/Read/Write of data that was imported from a different storage account | Create pre-signed URL for data that was imported from a different storage account | . ",
    "url": "/v1.4/howto/deploy/azure.html#2-authentication-method",
    
    "relUrl": "/howto/deploy/azure.html#2-authentication-method"
  },"62": {
    "doc": "Azure",
    "title": "3. K/V Store",
    "content": "lakeFS stores metadata in a database for its versioning engine. This is done via a Key-Value interface that can be implemented on any DB engine and lakeFS comes with several built-in driver implementations (You can read more about it here). The database used doesn’t have to be a dedicated K/V database. | CosmosDB (Beta) | PostgreSQL | . CosmosDB is a managed database service provided by Azure. lakeFS supports CosmosDB For NoSQL as a database backend. | Follow the official Azure documentation on how to create a CosmosDB account for NoSQL and connect to it. | Once your CosmosDB account is set up, you can create a Database for lakeFS. For lakeFS ACID guarantees, make sure to select the Bounded staleness consistency, for single region deployments. | Create a new container in the database and select type partitionKey as the Partition key (case sensitive). | Pass the endpoint, database name and container name to lakeFS as described in the configuration guide. You can either pass the CosmosDB’s account read-write key to lakeFS, or use a managed identity to authenticate to CosmosDB, as described earlier. | . A note on CosmosDB capacity modes: lakeFS usage of CosmosDB is still in its early days and has not been battle tested. Both capacity modes, Provisioned and Serverless, has been tested for some workloads and passed with flying colors. The Provisioned mode was configured with 400-4000 RU/s. Below we show you how to create a database on Azure Database, but you can use any PostgreSQL database as long as it’s accessible by your lakeFS installation. If you already have a database, take note of the connection string and skip to the next step . | Follow the official Azure documentation on how to create a PostgreSQL instance and connect to it. Make sure that you’re using PostgreSQL version &gt;= 11. | Once your Azure Database for PostgreSQL server is set up and the server is in the Available state, take note of the endpoint and username. | Make sure your Access control roles allow you to connect to the database instance. | . ",
    "url": "/v1.4/howto/deploy/azure.html#3-kv-store",
    
    "relUrl": "/howto/deploy/azure.html#3-kv-store"
  },"63": {
    "doc": "Azure",
    "title": "4. Run the lakeFS server",
    "content": "Now that you’ve chosen and configured object storage, a K/V store, and authentication—you’re ready to configure and run lakeFS. There are three different ways you can run lakeFS: . | Azure VM | Docker | Azure Kubernetes Service (AKS) | . Connect to your VM instance using SSH: . | Create a config.yaml on your VM, with the following parameters: . --- database: type: \"postgres\" postgres: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: # replace this with a randomly-generated string. Make sure to keep it safe! secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: azure azure: . | Download the binary to run on the VM. | Run the lakefs binary: . lakefs --config config.yaml run . | . Note: It’s preferable to run the binary as a service using systemd or your operating system’s facilities. To support container-based environments, you can configure lakeFS using environment variables. Here is a docker run command to demonstrate starting lakeFS using Docker: . docker run \\ --name lakefs \\ -p 8000:8000 \\ -e LAKEFS_DATABASE_TYPE=\"postgres\" \\ -e LAKEFS_DATABASE_POSTGRES_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\ -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\ -e LAKEFS_BLOCKSTORE_TYPE=\"azure\" \\ -e LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCOUNT=\"[YOUR_STORAGE_ACCOUNT]\" \\ -e LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCESS_KEY=\"[YOUR_ACCESS_KEY]\" \\ treeverse/lakefs:latest run . See the reference for a complete list of environment variables. You can install lakeFS on Kubernetes using a Helm chart. To install lakeFS with Helm: . | Copy the Helm values file relevant for Azure Blob: . secrets: # replace this with the connection string of the database you created in a previous step: databaseConnectionString: [DATABASE_CONNECTION_STRING] # replace this with a randomly-generated string authEncryptSecretKey: [ENCRYPTION_SECRET_KEY] lakefsConfig: | blockstore: type: azure azure: # If you chose to authenticate via access key, unmark the following rows and insert the values from the previous step # storage_account: [your storage account] # storage_access_key: [your access key] . | Fill in the missing values and save the file as conf-values.yaml. For more configuration options, see our Helm chart README. The lakefsConfig parameter is the lakeFS configuration documented here but without sensitive information. Sensitive information like databaseConnectionString is given through separate parameters, and the chart will inject it into Kubernetes secrets. | In the directory where you created conf-values.yaml, run the following commands: . # Add the lakeFS repository helm repo add lakefs https://charts.lakefs.io # Deploy lakeFS helm install my-lakefs lakefs/lakefs -f conf-values.yaml . my-lakefs is the Helm Release name. | . Load balancing . To configure a load balancer to direct requests to the lakeFS servers you can use the LoadBalancer Service type or a Kubernetes Ingress. By default, lakeFS operates on port 8000 and exposes a /_health endpoint that you can use for health checks. 💡 The NGINX Ingress Controller by default limits the client body size to 1 MiB. Some clients use bigger chunks to upload objects - for example, multipart upload to lakeFS using the S3-compatible Gateway or a simple PUT request using the OpenAPI Server. Check out Nginx documentation for increasing the limit, or an example of Nginx configuration with MinIO. ",
    "url": "/v1.4/howto/deploy/azure.html#4-run-the-lakefs-server",
    
    "relUrl": "/howto/deploy/azure.html#4-run-the-lakefs-server"
  },"64": {
    "doc": "Azure",
    "title": "Create the admin user",
    "content": "When you first open the lakeFS UI, you will be asked to create an initial admin user. | Open http://&lt;lakefs-host&gt;/ in your browser. If you haven’t set up a load balancer, this will likely be http://&lt;instance ip address&gt;:8000/ | On first use, you’ll be redirected to the setup page: . | Follow the steps to create an initial administrator user. Save the credentials you’ve received somewhere safe, you won’t be able to see them again! . | Follow the link and go to the login screen. Use the credentials from the previous step to log in. | . ",
    "url": "/v1.4/howto/deploy/azure.html#create-the-admin-user",
    
    "relUrl": "/howto/deploy/azure.html#create-the-admin-user"
  },"65": {
    "doc": "Azure",
    "title": "Create your first repository",
    "content": ". | Use the credentials from the previous step to log in | Click Create Repository and choose Blank Repository. | Under Storage Namespace, enter a path to your desired location on the object store. This is where data written to this repository will be stored. | Click Create Repository | You should now have a configured repository, ready to use! . | . Congratulations! Your environment is now ready 🤩 . ",
    "url": "/v1.4/howto/deploy/azure.html#create-your-first-repository",
    
    "relUrl": "/howto/deploy/azure.html#create-your-first-repository"
  },"66": {
    "doc": "Azure",
    "title": "Azure",
    "content": " ",
    "url": "/v1.4/howto/deploy/azure.html",
    
    "relUrl": "/howto/deploy/azure.html"
  },"67": {
    "doc": "3️⃣ Create a branch",
    "title": "Create a Branch",
    "content": "lakeFS uses branches in a similar way to Git. It’s a great way to isolate changes until, or if, we are ready to re-integrate them. lakeFS uses a copy-on-write technique which means that it’s very efficient to create branches of your data. Having seen the lakes data in the previous step we’re now going to create a new dataset to hold data only for lakes in Denmark. Why? Well, because :) . The first thing we’ll do is create a branch for us to do this development against. We’ll use the lakectl tool to create the branch, which we first need to configure with our credentials. In a new terminal window run the following: . docker exec -it lakefs lakectl config . Follow the prompts to enter the credentials that you got in the first step. Leave the Server endpoint URL as http://127.0.0.1:8000. Now that lakectl is configured, we can use it to create the branch. Run the following: . docker exec lakefs \\ lakectl branch create \\ lakefs://quickstart/denmark-lakes \\ --source lakefs://quickstart/main . You should get a confirmation message like this: . Source ref: lakefs://quickstart/main created branch 'denmark-lakes' 3384cd7cdc4a2cd5eb6249b52f0a709b49081668bb1574ce8f1ef2d956646816 . ",
    "url": "/v1.4/quickstart/branch.html#create-a-branch",
    
    "relUrl": "/quickstart/branch.html#create-a-branch"
  },"68": {
    "doc": "3️⃣ Create a branch",
    "title": "Transforming the Data",
    "content": "Now we’ll make a change to the data. lakeFS has several native clients, as well as an S3-compatible endpoint. This means that anything that can use S3 will work with lakeFS. Pretty neat. We’re going to use DuckDB which is embedded within the web interface of lakeFS. From the lakeFS Objects page select the lakes.parquet file to open the DuckDB editor: . To start with, we’ll load the lakes data into a DuckDB table so that we can manipulate it. Replace the previous text in the DuckDB editor with this: . CREATE OR REPLACE TABLE lakes AS SELECT * FROM READ_PARQUET('lakefs://quickstart/denmark-lakes/lakes.parquet'); . You’ll see a row count of 100,000 to confirm that the DuckDB table has been created. Just to check that it’s the same data that we saw before we’ll run the same query. Note that we are querying a DuckDB table (lakes), rather than using a function to query a parquet file directly. SELECT country, COUNT(*) FROM lakes GROUP BY country ORDER BY COUNT(*) DESC LIMIT 5; . Making a Change to the Data . Now we can change our table, which was loaded from the original lakes.parquet, to remove all rows not for Denmark: . DELETE FROM lakes WHERE Country != 'Denmark'; . We can verify that it’s worked by reissuing the same query as before: . SELECT country, COUNT(*) FROM lakes GROUP BY country ORDER BY COUNT(*) DESC LIMIT 5; . ",
    "url": "/v1.4/quickstart/branch.html#transforming-the-data",
    
    "relUrl": "/quickstart/branch.html#transforming-the-data"
  },"69": {
    "doc": "3️⃣ Create a branch",
    "title": "Write the Data back to lakeFS",
    "content": "The changes so far have only been to DuckDB’s copy of the data. Let’s now push it back to lakeFS. Note the path is different this time as we’re writing it to the denmark-lakes branch, not main: . COPY lakes TO 'lakefs://quickstart/denmark-lakes/lakes.parquet'; . ",
    "url": "/v1.4/quickstart/branch.html#write-the-data-back-to-lakefs",
    
    "relUrl": "/quickstart/branch.html#write-the-data-back-to-lakefs"
  },"70": {
    "doc": "3️⃣ Create a branch",
    "title": "Verify that the Data’s Changed on the Branch",
    "content": "Let’s just confirm for ourselves that the parquet file itself has the new data. We’ll drop the lakes table just to be sure, and then query the parquet file directly: . DROP TABLE lakes; SELECT country, COUNT(*) FROM READ_PARQUET('lakefs://quickstart/denmark-lakes/lakes.parquet') GROUP BY country ORDER BY COUNT(*) DESC LIMIT 5; . ",
    "url": "/v1.4/quickstart/branch.html#verify-that-the-datas-changed-on-the-branch",
    
    "relUrl": "/quickstart/branch.html#verify-that-the-datas-changed-on-the-branch"
  },"71": {
    "doc": "3️⃣ Create a branch",
    "title": "What about the data in main?",
    "content": "So we’ve changed the data in our denmark-lakes branch, deleting swathes of the dataset. What’s this done to our original data in the main branch? Absolutely nothing! See for yourself by running the same query as above, but against the main branch: . SELECT country, COUNT(*) FROM READ_PARQUET('lakefs://quickstart/main/lakes.parquet') GROUP BY country ORDER BY COUNT(*) DESC LIMIT 5; . In the next step we’ll see how to commit our changes and merge our branch back into main. ",
    "url": "/v1.4/quickstart/branch.html#what-about-the-data-in-main",
    
    "relUrl": "/quickstart/branch.html#what-about-the-data-in-main"
  },"72": {
    "doc": "3️⃣ Create a branch",
    "title": "3️⃣ Create a branch",
    "content": " ",
    "url": "/v1.4/quickstart/branch.html",
    
    "relUrl": "/quickstart/branch.html"
  },"73": {
    "doc": "Callouts",
    "title": "Callouts in lakeFS Documentation",
    "content": "These are part of the just-the-docs theme, and use the Block Inline Attribute List (IAL) feature of Kramdown. ",
    "url": "/v1.4/project/docs/callouts.html#callouts-in-lakefs-documentation",
    
    "relUrl": "/project/docs/callouts.html#callouts-in-lakefs-documentation"
  },"74": {
    "doc": "Callouts",
    "title": "Syntax",
    "content": "Whilst the lakeFS docs typically use this syntax with the code after the block: . This is a note {: .note } . This is a note . The just-the-docs documentation puts the code before the block, which makes more sense to the human eye when reading the Markdown too. {: .note } This is a note . This is a note . However, they render the same, as can be see above. ",
    "url": "/v1.4/project/docs/callouts.html#syntax",
    
    "relUrl": "/project/docs/callouts.html#syntax"
  },"75": {
    "doc": "Callouts",
    "title": "Configuration",
    "content": "Callouts are configured in the _config.yml file, with an identifier (such as note), a colour, and optionally a title. callouts: note: color: blue warning: title: ⚠️ Warning color: red tip: color: green fubar: color: purple . Here’s what the above callouts look like; notice that only warning includes a title: . {: .note } This is a note . This is a note . {: .warning } This is a warning . This is a warning . {: .fubar } This is illustrating that the callout names are completely discretionary and not tied to their meaning . This is illustrating that the callout names are completely discretionary and not tied to their meaning . ",
    "url": "/v1.4/project/docs/callouts.html#configuration",
    
    "relUrl": "/project/docs/callouts.html#configuration"
  },"76": {
    "doc": "Callouts",
    "title": "Custom titles",
    "content": "You can specify custom titles per block by appending -title to the block name and putting the custom title in the first line of the quoted block: . {: .tip-title } &gt; lakeFS Cloud &gt; &gt; For a fully-managed lakeFS solution check out https://lakefs.cloud/ today . lakeFS Cloud . For a fully-managed lakeFS solution check out https://lakefs.cloud/ today . You can also remove the title of a callout that includes one by default (such as warning) above: . {: .warning-title } &gt; Don't press the red button . Don’t press the red button . Or change the default title: . {: .warning-title } &gt; Here Be Dragons 🐲 &gt; &gt; ALLES TURISTEN UND NONTEKNISCHEN LOOKENSPEEPERS! &lt;br/&gt; &gt; DAS KOMPUTERMASCHINE IST NICHT FÜR DER GEFINGERPOKEN UND MITTENGRABEN! ODERWISE IST EASY TO SCHNAPPEN DER SPRINGENWERK, BLOWENFUSEN UND POPPENCORKEN MIT SPITZENSPARKEN. Here Be Dragons 🐲 . ALLES TURISTEN UND NONTEKNISCHEN LOOKENSPEEPERS! DAS KOMPUTERMASCHINE IST NICHT FÜR DER GEFINGERPOKEN UND MITTENGRABEN! ODERWISE IST EASY TO SCHNAPPEN DER SPRINGENWERK, BLOWENFUSEN UND POPPENCORKEN MIT SPITZENSPARKEN. ",
    "url": "/v1.4/project/docs/callouts.html#custom-titles",
    
    "relUrl": "/project/docs/callouts.html#custom-titles"
  },"77": {
    "doc": "Callouts",
    "title": "Multi-line Blocks",
    "content": "Callouts support multiple lines and/or paragraphs of content: . {: .note } &gt; A multi-paragraph block &gt; &gt; with &lt;br/&gt; &gt; linebreaks . A multi-paragraph block . with linebreaks . ",
    "url": "/v1.4/project/docs/callouts.html#multi-line-blocks",
    
    "relUrl": "/project/docs/callouts.html#multi-line-blocks"
  },"78": {
    "doc": "Callouts",
    "title": "Reference",
    "content": ". | Just the Docs: Callouts overview | Just the Docs: Configuration for Callouts | Kramdown: Block attributes Quick Reference | Kramdown: Syntax Guide for Block Inline Attribute List (IAL) | . ",
    "url": "/v1.4/project/docs/callouts.html#reference",
    
    "relUrl": "/project/docs/callouts.html#reference"
  },"79": {
    "doc": "Callouts",
    "title": "Callouts",
    "content": " ",
    "url": "/v1.4/project/docs/callouts.html",
    
    "relUrl": "/project/docs/callouts.html"
  },"80": {
    "doc": "Data Catalogs Export",
    "title": "Data Catalogs Export",
    "content": " ",
    "url": "/v1.4/howto/catalog_exports.html",
    
    "relUrl": "/howto/catalog_exports.html"
  },"81": {
    "doc": "Data Catalogs Export",
    "title": "Table of contents",
    "content": ". | About Data Catalogs Export | How it works . | Table Decleration | Catalog Exporters | Flow | . | . ",
    "url": "/v1.4/howto/catalog_exports.html#table-of-contents",
    
    "relUrl": "/howto/catalog_exports.html#table-of-contents"
  },"82": {
    "doc": "Data Catalogs Export",
    "title": "About Data Catalogs Export",
    "content": "Data Catalog Export is all about integrating query engines (like Spark, AWS Athena, Presto, etc.) with lakeFS. Data Catalogs (such as Hive Metastore or AWS Glue) store metadata for services (such as Spark, Trino and Athena). They contain metadata such as the location of the table, information about columns, partitions and much more. With Data Catalog Exports, one can leverage the versioning capabilities of lakeFS in external data warehouses and query engines to access tables with branches and commits. At the end of this guide, you will be able to query lakeFS data from Athena, Trino and other catalog-dependent tools: . USE main; USE my_branch; -- any branch USE v101; -- or tag SELECT * FROM users INNER JOIN events ON users.id = events.user_id; -- SQL stays the same, branch or tag exist as schema . ",
    "url": "/v1.4/howto/catalog_exports.html#about-data-catalogs-export",
    
    "relUrl": "/howto/catalog_exports.html#about-data-catalogs-export"
  },"83": {
    "doc": "Data Catalogs Export",
    "title": "How it works",
    "content": "Several well known formats exist today let you export existing tables in lakeFS into a “native” object store representation which does not require copying the data outside of lakeFS. These are metadata representations and can be applied automatically through hooks. Table Decleration . After creating a lakeFS repository, configure tables as table descriptor objects on the repository on the path _lakefs_tables/TABLE.yaml. Note: the Glue exporter can currently only export tables of type: hive. We expect to add more. Hive tables . Hive metadata server tables are essentially just a set of objects that share a prefix, with no table metadata stored on the object store. You need to configure prefix, partitions, and schema. name: animals type: hive path: path/to/animals/ partition_columns: ['year'] schema: type: struct fields: - name: year type: integer nullable: false metadata: {} - name: page type: string nullable: false metadata: {} - name: site type: string nullable: true metadata: comment: a comment about this column . Useful types recognized by Hive include integer, long, short, string, double, float, date, and timestamp. Catalog Exporters . Exporters are code packages accessible through Lua integration. Each exporter is exposed as a Lua function under the package namespace lakefs/catalogexport. Call them from hooks to connect lakeFS tables to various catalogs. Currently supported exporters . | Exporter | Description | Notes | . | Symlink exporter | Writes metadata for the table using Hive’s SymlinkTextInputFormat |   | . | AWS Glue Catalog (+ Athena) exporter | Creates a table in Glue using Hive’s format and updates the location to symlink files (reuses Symlink Exporter). | See a step-by-step guide on how to integrate with Glue Exporter | . | Delta Lake table exporter | Export Delta Lake tables from lakeFS to an external storage |   | . | Unity Catalog exporter | The Unity Catalog exporter serves the purpose of registering a Delta Lake table in Unity Catalog. It operates in conjunction with the Delta Lake exporter. In this workflow, the Delta Lake exporter is utilized to export a Delta Lake table from lakeFS. Subsequently, the obtained result is passed to the Unity Catalog exporter to facilitate its registration within Unity Catalog. | See a step-by-step guide on how to integrate with Unity Catalog Exporter&lt;/br&gt;Currently, only AWS S3 storage is supported | . Running an Exporter . Exporters are meant to run as Lua hooks. Configure the actions trigger by using events and branches. Of course, you can add additional custom filtering logic to the Lua script if needed. The default table name when exported is ${repository_id}_${_lakefs_tables/TABLE.md(name field)}_${ref_name}_${short_commit}. Example of an action that will be triggered when a post-commit event happens in the export_table branch. name: Glue Table Exporter description: export my table to glue on: post-commit: branches: [\"export_table\"] hooks: - id: my_exporter type: lua properties: # exporter script location script_path: \"scripts/my_export_script.lua\" args: # table descriptor table_source: '_lakefs_tables/my_table.yaml' . Tip: Actions can be extended to customize any desired behavior, for example validating branch names since they are part of the table name: . # _lakefs_actions/validate_branch_name.yaml name: validate-lower-case-branches on: pre-create-branch: hooks: - id: check_branch_id type: lua properties: script: | regexp = require(\"regexp\") if not regexp.match(\"^[a-z0-9\\\\_\\\\-]+$\", action.branch_id) then error(\"branches must be lower case, invalid branch ID: \" .. action.branch_id) end . Flow . The following diagram demonstrates what happens when a lakeFS Action triggers runs a lua hook that calls an exporter. sequenceDiagram note over Lua Hook: lakeFS Action trigger. &lt;br&gt; Pass Context for the export. Lua Hook-&gt;&gt;Exporter: export request note over Table Registry: _lakefs_tables/TABLE.yaml Exporter-&gt;&gt;Table Registry: Get table descriptor Table Registry-&gt;&gt;Exporter: Parse table structure Exporter-&gt;&gt;Object Store: materialize an exported table Exporter-&gt;&gt;Catalog: register object store location Query Engine--&gt;Catalog: Query Query Engine--&gt;Object Store: Query . ",
    "url": "/v1.4/howto/catalog_exports.html#how-it-works",
    
    "relUrl": "/howto/catalog_exports.html#how-it-works"
  },"84": {
    "doc": "During Deployment",
    "title": "During Deployment",
    "content": "Every day we introduce new data to the lake. And even if the code and infra doesn’t change, the data might, and those changes introduce potential quality issues. This is one of the complexities of a data product; the data we consume changes over the course of a month, a week, day, hour, or even minute-to-minute. Examples of changes to data that may occur: . | A client-side bug in the data collection of website events | A new Android version that interferes with the collecting events from your App | COVID-19 abrupt impact on consumers’ behavior, and its effect on the accuracy of ML models. | During a change to Salesforce interface, the validation requirement from a certain field had been lost | . lakeFS enable CI/CD-inspired workflows to help validate expectations and assumptions about the data before it goes live in production or lands in the data environment. Example 1: Data update safety . Continuous deployment of existing data we expect to consume, flowing from ingest-pipelines into the lake. We merge data from an ingest branch (“events-data”), which allows us to create tests using data analysis tools or data quality services (e.g. Great Expectations, Monte Carlo) to ensure reliability of the data we merge to the main branch. Since merge is atomic, no performance issue will be introduced by using lakeFS, but your main branch will only include quality data. Each merge to the main branch creates a new commit on the main branch, which serves as a new version of the data. This allows us to easily revert to previous states of the data if a newer change introduces data issues. Example 2: Test - Validate new data . Examples of common validation checks enforced in organizations: . | No user_* columns except under /private/… | Only (*.parquet | *.orc | _delta_log/*.json) files allowed | Under /production, only backward-compatible schema changes are allowed | New tables on main must be registered in our metadata repository first, with owner and SLA | . lakeFS will assist in enforcing best practices by giving you a designated branch to ingest new data (“new-data-1” in the drawing). You may run automated tests to validate predefined best practices as pre-merge hooks. If the validation passes, the new data will be automatically and atomically merged to the main branch. However, if the validation fails, you will be alerted and the new data will not be exposed to consumers. By using this branching model and implementing best practices as pre merge hooks, you ensure the main lake is never compromised. ",
    "url": "/v1.4/understand/data_lifecycle_management/ci.html",
    
    "relUrl": "/understand/data_lifecycle_management/ci.html"
  },"85": {
    "doc": "CI/CD for Data Lakes",
    "title": "CI/CD for Data",
    "content": " ",
    "url": "/v1.4/understand/use_cases/cicd_for_data.html#cicd-for-data",
    
    "relUrl": "/understand/use_cases/cicd_for_data.html#cicd-for-data"
  },"86": {
    "doc": "CI/CD for Data Lakes",
    "title": "Table of contents",
    "content": ". | Why do I need CI/CD? | How do I implement CI/CD for data with lakeFS? | Using hooks as data quality gates | Implementing CI/CD pipeline with lakeFS - Demo | Resources | . ",
    "url": "/v1.4/understand/use_cases/cicd_for_data.html#table-of-contents",
    
    "relUrl": "/understand/use_cases/cicd_for_data.html#table-of-contents"
  },"87": {
    "doc": "CI/CD for Data Lakes",
    "title": "Why do I need CI/CD?",
    "content": "Data pipelines feed processed data from data lakes to downstream consumers like business dashboards and machine learning models. As more and more organizations rely on data to enable business critical decisions, data reliability and trust are of paramount concern. Thus, it’s important to ensure that production data adheres to the data governance policies of businesses. These data governance requirements can be as simple as a file format validation, schema check, or an exhaustive PII(Personally Identifiable Information) data removal from all of organization’s data. Thus, to ensure the quality and reliability at each stage of the data lifecycle, data quality gates need to be implemented. That is, we need to run Continuous Integration(CI) tests on the data, and only if data governance requirements are met can the data can be promoted to production for business use. Everytime there is an update to production data, the best practice would be to run CI tests and then promote(deploy) the data to production. ",
    "url": "/v1.4/understand/use_cases/cicd_for_data.html#why-do-i-need-cicd",
    
    "relUrl": "/understand/use_cases/cicd_for_data.html#why-do-i-need-cicd"
  },"88": {
    "doc": "CI/CD for Data Lakes",
    "title": "How do I implement CI/CD for data with lakeFS?",
    "content": "lakeFS makes implementing CI/CD pipelines for data simpler. lakeFS provides a feature called hooks that allow automation of checks and validations of data on lakeFS branches. These checks can be triggered by certain data operations like committing, merging, etc. Functionally, lakeFS hooks are similar to Git Hooks. lakeFS hooks are run remotely on a server, and they are guaranteed to run when the appropriate event is triggered. Here are some examples of the hooks lakeFS supports: . | pre-merge | pre-commit | post-merge | post-commit | pre-create-branch | post-create-branch | . and so on. By leveraging the pre-commit and pre-merge hooks with lakeFS, you can implement CI/CD pipelines on your data lakes. Specific trigger rules, quality checks and the branch on which the rules are to be applied are declared in actions.yaml file. When a specific event (say, pre-merge) occurs, lakeFS runs all the validations declared in actions.yaml file. If validations error out, the merge event is blocked. Here is a sample actions.yaml file that has pre-merge hook configured to allow only parquet and delta lake file formats on main branch. name: ParquetOnlyInProduction description: This webhook ensures that only parquet files are written under production/ on: pre-merge: branches: - main hooks: - id: production_format_validator type: webhook description: Validate file formats properties: url: \"http://lakefs-hooks:5001/webhooks/format\" query_params: allow: [\"parquet\", \"delta_lake\"] prefix: analytics/ . ",
    "url": "/v1.4/understand/use_cases/cicd_for_data.html#how-do-i-implement-cicd-for-data-with-lakefs",
    
    "relUrl": "/understand/use_cases/cicd_for_data.html#how-do-i-implement-cicd-for-data-with-lakefs"
  },"89": {
    "doc": "CI/CD for Data Lakes",
    "title": "Using hooks as data quality gates",
    "content": "Hooks are run on a remote server that can serve http requests from lakeFS server. lakeFS supports two types of hooks. | webhooks (run remotely on a web server. e.g.: flask server in python) | airflow hooks (a dag of complex data quality checks/tasks that can be run on airflow server) | . In this tutorial, we will show how to use webhooks (python flask webserver) to implement quality gates on your data branches. Specifically, how to configure hooks to allow only parquet and delta lake format files in the main branch. The tutorial provides a lakeFS environment, python flask server, a Jupyter notebook and sample data sets to demonstrate the integration of lakeFS hooks with Apache Spark and Python. It runs on Docker Compose. To understand how hooks work and how to configure hooks in your production system, refer to the documentation: Hooks. Follow the steps below to try out CI/CD for data lakes. ",
    "url": "/v1.4/understand/use_cases/cicd_for_data.html#using-hooks-as-data-quality-gates",
    
    "relUrl": "/understand/use_cases/cicd_for_data.html#using-hooks-as-data-quality-gates"
  },"90": {
    "doc": "CI/CD for Data Lakes",
    "title": "Implementing CI/CD pipeline with lakeFS - Demo",
    "content": "The sample below provides a lakeFS environment, a Jupyter notebook, and a server on which for the lakeFS webhooks to run. Prerequisites &amp; Setup . Before we get started, make sure Docker is installed on your machine. | Start by cloning the lakeFS samples Git repository: . git clone https://github.com/treeverse/lakeFS-samples.git . cd lakeFS-samples . | Run following commands to start the components: . git submodule init git submodule update docker compose up . | . Open the local Jupyter Notebook and go to the hooks-demo.ipynb notebook. ",
    "url": "/v1.4/understand/use_cases/cicd_for_data.html#implementing-cicd-pipeline-with-lakefs---demo",
    
    "relUrl": "/understand/use_cases/cicd_for_data.html#implementing-cicd-pipeline-with-lakefs---demo"
  },"91": {
    "doc": "CI/CD for Data Lakes",
    "title": "Resources",
    "content": "To explore different checks and validations on your data, refer to pre-built hooks config by the lakeFS team. To understand the comprehensive list of hooks supported by lakeFS, refer to the documentation. ",
    "url": "/v1.4/understand/use_cases/cicd_for_data.html#resources",
    
    "relUrl": "/understand/use_cases/cicd_for_data.html#resources"
  },"92": {
    "doc": "CI/CD for Data Lakes",
    "title": "CI/CD for Data Lakes",
    "content": " ",
    "url": "/v1.4/understand/use_cases/cicd_for_data.html",
    
    "relUrl": "/understand/use_cases/cicd_for_data.html"
  },"93": {
    "doc": "lakectl (lakeFS command-line tool)",
    "title": "lakectl (lakeFS command-line tool)",
    "content": " ",
    "url": "/v1.4/reference/cli.html#lakectl-lakefs-command-line-tool",
    
    "relUrl": "/reference/cli.html#lakectl-lakefs-command-line-tool"
  },"94": {
    "doc": "lakectl (lakeFS command-line tool)",
    "title": "Table of contents",
    "content": ". | Installing lakectl locally | Running lakectl from Docker | Command Reference | . ",
    "url": "/v1.4/reference/cli.html#table-of-contents",
    
    "relUrl": "/reference/cli.html#table-of-contents"
  },"95": {
    "doc": "lakectl (lakeFS command-line tool)",
    "title": "Installing lakectl locally",
    "content": "lakectl is available for Linux, macOS, and Windows. You can also run it using Docker. Download lakectl . Or using Homebrew for Linux/macOS: . brew tap treeverse/lakefs brew install lakefs . Configuring credentials and API endpoint . Once you’ve installed the lakectl command, run: . lakectl config # output: # Config file /home/janedoe/.lakectl.yaml will be used # Access key ID: AKIAIOSFODNN7EXAMPLE # Secret access key: **************************************** # Server endpoint URL: http://localhost:8000 . This will setup a $HOME/.lakectl.yaml file with the credentials and API endpoint you’ve supplied. When setting up a new installation and creating initial credentials (see Quickstart), the UI will provide a link to download a preconfigured configuration file for you. lakectl configuration items can each be controlled by an environment variable. The variable name will have a prefix of LAKECTL_, followed by the name of the configuration, replacing every ‘.’ with a ‘_’. Example: LAKECTL_SERVER_ENDPOINT_URL controls server.endpoint_url. ",
    "url": "/v1.4/reference/cli.html#installing-lakectl-locally",
    
    "relUrl": "/reference/cli.html#installing-lakectl-locally"
  },"96": {
    "doc": "lakectl (lakeFS command-line tool)",
    "title": "Running lakectl from Docker",
    "content": "If you’d rather run lakectl from a Docker container you can do so by passing configuration elements as environment variables. Here is an example: . docker run --rm --pull always \\ -e LAKECTL_CREDENTIALS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE \\ -e LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY=xxxxx -e LAKECTL_SERVER_ENDPOINT_URL=https://host.us-east-2.lakefscloud.io/ \\ --entrypoint lakectl treeverse/lakefs \\ repo list . Bear in mind that if you are running lakeFS itself locally you will need to account for this in your networking configuration of the Docker container. That is to say, localhost to a Docker container is itself, not the host machine on which it is running. ",
    "url": "/v1.4/reference/cli.html#running-lakectl-from-docker",
    
    "relUrl": "/reference/cli.html#running-lakectl-from-docker"
  },"97": {
    "doc": "lakectl (lakeFS command-line tool)",
    "title": "Command Reference",
    "content": "lakectl . A cli tool to explore manage and work with lakeFS . Synopsis . lakectl is a CLI tool allowing exploration and manipulation of a lakeFS environment . lakectl [flags] . Options . --base-uri string base URI used for lakeFS address parse -c, --config string config file (default is $HOME/.lakectl.yaml) -h, --help help for lakectl --log-format string set logging output format --log-level string set logging level (default \"none\") --log-output strings set logging output(s) --no-color don't use fancy output colors (default value can be set by NO_COLOR environment variable) --verbose run in verbose mode -v, --version version for lakectl . note: The base-uri option can be controlled with the LAKECTL_BASE_URI environment variable. Example usage . $ export LAKECTL_BASE_URI=\"lakefs://my-repo/my-branch\" # Once set, use relative lakefs uri's: $ lakectl fs ls /path . lakectl abuse . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Abuse a running lakeFS instance. See sub commands for more info. Options . -h, --help help for abuse . lakectl abuse commit . Commits to the source branch repeatedly . lakectl abuse commit &lt;branch URI&gt; [flags] . Options . --amount int amount of commits to do (default 100) --gap duration duration to wait between commits (default 2s) -h, --help help for commit . lakectl abuse create-branches . Create a lot of branches very quickly. lakectl abuse create-branches &lt;source ref URI&gt; [flags] . Options . --amount int amount of things to do (default 1000000) --branch-prefix string prefix to create branches under (default \"abuse-\") --clean-only only clean up past runs -h, --help help for create-branches --parallelism int amount of things to do in parallel (default 100) . lakectl abuse help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type abuse help [path to command] for full details. lakectl abuse help [command] [flags] . Options . -h, --help help for help . lakectl abuse link-same-object . Link the same object in parallel. lakectl abuse link-same-object &lt;branch URI&gt; [flags] . Options . --amount int amount of link object to do (default 1000000) -h, --help help for link-same-object --key string key used for the test (default \"linked-object\") --parallelism int amount of link object to do in parallel (default 100) . lakectl abuse list . List from the source ref . lakectl abuse list &lt;source ref URI&gt; [flags] . Options . --amount int amount of lists to do (default 1000000) -h, --help help for list --parallelism int amount of lists to do in parallel (default 100) --prefix string prefix to list under (default \"abuse/\") . lakectl abuse random-read . Read keys from a file and generate random reads from the source ref for those keys. lakectl abuse random-read &lt;source ref URI&gt; [flags] . Options . --amount int amount of reads to do (default 1000000) --from-file string read keys from this file (\"-\" for stdin) -h, --help help for random-read --parallelism int amount of reads to do in parallel (default 100) . lakectl abuse random-write . Generate random writes to the source branch . lakectl abuse random-write &lt;branch URI&gt; [flags] . Options . --amount int amount of writes to do (default 1000000) -h, --help help for random-write --parallelism int amount of writes to do in parallel (default 100) --prefix string prefix to create paths under (default \"abuse/\") . lakectl actions . Manage Actions commands . Options . -h, --help help for actions . lakectl actions help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type actions help [path to command] for full details. lakectl actions help [command] [flags] . Options . -h, --help help for help . lakectl actions runs . Explore runs information . Options . -h, --help help for runs . lakectl actions runs describe . Describe run results . Synopsis . Show information about the run and all the hooks that were executed as part of the run . lakectl actions runs describe &lt;repository URI&gt; &lt;run_id&gt; [flags] . Examples . lakectl actions runs describe lakefs://my-repo 20230719152411arS0z6I . Options . --after string show results after this value (used for pagination) --amount int number of results to return. By default, all results are returned. -h, --help help for describe . lakectl actions runs help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type runs help [path to command] for full details. lakectl actions runs help [command] [flags] . Options . -h, --help help for help . lakectl actions runs list . List runs . Synopsis . List all runs on a repository optional filter by branch or commit . lakectl actions runs list &lt;repository URI&gt; [--branch &lt;branch&gt;] [--commit &lt;commit_id&gt;] [flags] . Examples . lakectl actions runs list lakefs://my-repo --branch my-branch --commit 600dc0ffee . Options . --after string show results after this value (used for pagination) --amount int number of results to return (default 100) --branch string show results for specific branch --commit string show results for specific commit ID -h, --help help for list . lakectl actions validate . Validate action file . Synopsis . Tries to parse the input action file as lakeFS action file . lakectl actions validate [flags] . Examples . lakectl actions validate path/to/my/file . Options . -h, --help help for validate . lakectl annotate . List entries under a given path, annotating each with the latest modifying commit . lakectl annotate &lt;path URI&gt; [flags] . Options . --first-parent follow only the first parent commit upon seeing a merge commit -h, --help help for annotate -r, --recursive recursively annotate all entries under a given path or prefix . lakectl auth . Manage authentication and authorization . Synopsis . manage authentication and authorization including users, groups and ACLs . Options . -h, --help help for auth . lakectl auth groups . Manage groups . Options . -h, --help help for groups . lakectl auth groups acl . Manage ACLs . Synopsis . manage ACLs of groups . Options . -h, --help help for acl . lakectl auth groups acl get . Get ACL of group . lakectl auth groups acl get [flags] . Options . -h, --help help for get --id string Group identifier . lakectl auth groups acl help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type acl help [path to command] for full details. lakectl auth groups acl help [command] [flags] . Options . -h, --help help for help . lakectl auth groups acl set . Set ACL of group . Synopsis . Set ACL of group. permission will be attached to all repositories. lakectl auth groups acl set [flags] . Options . -h, --help help for set --id string Group identifier --permission string Permission, typically one of \"Read\", \"Write\", \"Super\" or \"Admin\" . lakectl auth groups create . Create a group . lakectl auth groups create [flags] . Options . -h, --help help for create --id string Group identifier . lakectl auth groups delete . Delete a group . lakectl auth groups delete [flags] . Options . -h, --help help for delete --id string Group identifier . lakectl auth groups help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type groups help [path to command] for full details. lakectl auth groups help [command] [flags] . Options . -h, --help help for help . lakectl auth groups list . List groups . lakectl auth groups list [flags] . Options . --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth groups members . Manage group user memberships . Options . -h, --help help for members . lakectl auth groups members add . Add a user to a group . lakectl auth groups members add [flags] . Options . -h, --help help for add --id string Group identifier --user string Username (email for password-based users, default: current user) . lakectl auth groups members help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type members help [path to command] for full details. lakectl auth groups members help [command] [flags] . Options . -h, --help help for help . lakectl auth groups members list . List users in a group . lakectl auth groups members list [flags] . Options . --id string Group identifier --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth groups members remove . Remove a user from a group . lakectl auth groups members remove [flags] . Options . -h, --help help for remove --id string Group identifier --user string Username (email for password-based users, default: current user) . lakectl auth groups policies . Manage group policies . Synopsis . Manage group policies. Requires an external authorization server with matching support. Options . -h, --help help for policies . lakectl auth groups policies attach . Attach a policy to a group . lakectl auth groups policies attach [flags] . Options . -h, --help help for attach --id string User identifier --policy string Policy identifier . lakectl auth groups policies detach . Detach a policy from a group . lakectl auth groups policies detach [flags] . Options . -h, --help help for detach --id string User identifier --policy string Policy identifier . lakectl auth groups policies help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type policies help [path to command] for full details. lakectl auth groups policies help [command] [flags] . Options . -h, --help help for help . lakectl auth groups policies list . List policies for the given group . lakectl auth groups policies list [flags] . Options . --id string Group identifier --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type auth help [path to command] for full details. lakectl auth help [command] [flags] . Options . -h, --help help for help . lakectl auth policies . Manage policies . Options . -h, --help help for policies . lakectl auth policies create . Create a policy . lakectl auth policies create [flags] . Options . -h, --help help for create --id string Policy identifier --statement-document string JSON statement document path (or \"-\" for stdin) . lakectl auth policies delete . Delete a policy . lakectl auth policies delete [flags] . Options . -h, --help help for delete --id string Policy identifier . lakectl auth policies help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type policies help [path to command] for full details. lakectl auth policies help [command] [flags] . Options . -h, --help help for help . lakectl auth policies list . List policies . lakectl auth policies list [flags] . Options . --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth policies show . Show a policy . lakectl auth policies show [flags] . Options . -h, --help help for show --id string Policy identifier . lakectl auth users . Manage users . Options . -h, --help help for users . lakectl auth users create . Create a user . lakectl auth users create [flags] . Options . -h, --help help for create --id string Username . lakectl auth users credentials . Manage user credentials . Options . -h, --help help for credentials . lakectl auth users credentials create . Create user credentials . lakectl auth users credentials create [flags] . Options . -h, --help help for create --id string Username (email for password-based users, default: current user) . lakectl auth users credentials delete . Delete user credentials . lakectl auth users credentials delete [flags] . Options . --access-key-id string Access key ID to delete -h, --help help for delete --id string Username (email for password-based users, default: current user) . lakectl auth users credentials help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type credentials help [path to command] for full details. lakectl auth users credentials help [command] [flags] . Options . -h, --help help for help . lakectl auth users credentials list . List user credentials . lakectl auth users credentials list [flags] . Options . --id string Username (email for password-based users, default: current user) --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth users delete . Delete a user . lakectl auth users delete [flags] . Options . -h, --help help for delete --id string Username (email for password-based users) . lakectl auth users groups . Manage user groups . Options . -h, --help help for groups . lakectl auth users groups help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type groups help [path to command] for full details. lakectl auth users groups help [command] [flags] . Options . -h, --help help for help . lakectl auth users groups list . List groups for the given user . lakectl auth users groups list [flags] . Options . --id string Username (email for password-based users) --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth users help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type users help [path to command] for full details. lakectl auth users help [command] [flags] . Options . -h, --help help for help . lakectl auth users list . List users . lakectl auth users list [flags] . Options . --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth users policies . Manage user policies . Synopsis . Manage user policies. Requires an external authorization server with matching support. Options . -h, --help help for policies . lakectl auth users policies attach . Attach a policy to a user . lakectl auth users policies attach [flags] . Options . -h, --help help for attach --id string Username (email for password-based users) --policy string Policy identifier . lakectl auth users policies detach . Detach a policy from a user . lakectl auth users policies detach [flags] . Options . -h, --help help for detach --id string Username (email for password-based users) --policy string Policy identifier . lakectl auth users policies help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type policies help [path to command] for full details. lakectl auth users policies help [command] [flags] . Options . -h, --help help for help . lakectl auth users policies list . List policies for the given user . lakectl auth users policies list [flags] . Options . --effective List all distinct policies attached to the user, including by group memberships --id string Username (email for password-based users) --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl bisect . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Binary search to find the commit that introduced a bug . Options . -h, --help help for bisect . lakectl bisect bad . Set ‘bad’ commit that is known to contain the bug . lakectl bisect bad [flags] . Options . -h, --help help for bad . lakectl bisect good . Set current commit as ‘good’ commit that is known to be before the bug was introduced . lakectl bisect good [flags] . Options . -h, --help help for good . lakectl bisect help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type bisect help [path to command] for full details. lakectl bisect help [command] [flags] . Options . -h, --help help for help . lakectl bisect log . Print out the current bisect state . lakectl bisect log [flags] . Options . -h, --help help for log . lakectl bisect reset . Clean up the bisection state . lakectl bisect reset [flags] . Options . -h, --help help for reset . lakectl bisect run . Bisecting based on command status code . lakectl bisect run &lt;command&gt; [flags] . Options . -h, --help help for run . lakectl bisect start . Start a bisect session . lakectl bisect start &lt;bad ref URI&gt; &lt;good ref URI&gt; [flags] . Options . -h, --help help for start . lakectl bisect view . Current bisect commits . lakectl bisect view [flags] . Options . -h, --help help for view . lakectl branch . Create and manage branches within a repository . Synopsis . Create delete and list branches within a lakeFS repository . Options . -h, --help help for branch . lakectl branch create . Create a new branch in a repository . lakectl branch create &lt;branch URI&gt; -s &lt;source ref URI&gt; [flags] . Examples . lakectl branch create lakefs://example-repo/new-branch -s lakefs://example-repo/main . Options . -h, --help help for create -s, --source string source branch uri . lakectl branch delete . Delete a branch in a repository, along with its uncommitted changes (CAREFUL) . lakectl branch delete &lt;branch URI&gt; [flags] . Examples . lakectl branch delete lakefs://my-repo/my-branch . Options . -h, --help help for delete -y, --yes Automatically say yes to all confirmations . lakectl branch help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type branch help [path to command] for full details. lakectl branch help [command] [flags] . Options . -h, --help help for help . lakectl branch list . List branches in a repository . lakectl branch list &lt;repository URI&gt; [flags] . Examples . lakectl branch list lakefs://my-repo . Options . --after string show results after this value (used for pagination) --amount int number of results to return (default 100) -h, --help help for list . lakectl branch reset . Reset uncommitted changes - all of them, or by path . Synopsis . reset changes. There are four different ways to reset changes: . | reset all uncommitted changes - reset lakefs://myrepo/main | reset uncommitted changes under specific path - reset lakefs://myrepo/main –prefix path | reset uncommitted changes for specific object - reset lakefs://myrepo/main –object path | . lakectl branch reset &lt;branch URI&gt; [--prefix|--object] [flags] . Examples . lakectl branch reset lakefs://my-repo/my-branch . Options . -h, --help help for reset --object string path to object to be reset --prefix string prefix of the objects to be reset -y, --yes Automatically say yes to all confirmations . lakectl branch revert . Given a commit, record a new commit to reverse the effect of this commit . Synopsis . The commits will be reverted in left-to-right order . lakectl branch revert &lt;branch URI&gt; &lt;commit ref to revert&gt; [&lt;more commits&gt;...] [flags] . Examples . lakectl branch revert lakefs://example-repo/example-branch commitA Revert the changes done by commitA in example-branch branch revert lakefs://example-repo/example-branch HEAD~1 HEAD~2 HEAD~3 Revert the changes done by the second last commit to the fourth last commit in example-branch . Options . -h, --help help for revert -m, --parent-number int the parent number (starting from 1) of the mainline. The revert will reverse the change relative to the specified parent. -y, --yes Automatically say yes to all confirmations . lakectl branch show . Show branch latest commit reference . lakectl branch show &lt;branch URI&gt; [flags] . Examples . lakectl branch show lakefs://my-repo/my-branch . Options . -h, --help help for show . lakectl branch-protect . Create and manage branch protection rules . Synopsis . Define branch protection rules to prevent direct changes. Changes to protected branches can only be done by merging from other branches. Options . -h, --help help for branch-protect . lakectl branch-protect add . Add a branch protection rule . Synopsis . Add a branch protection rule for a given branch name pattern . lakectl branch-protect add &lt;repository URI&gt; &lt;pattern&gt; [flags] . Examples . lakectl branch-protect add lakefs://my-repo 'stable_*' . Options . -h, --help help for add . lakectl branch-protect delete . Delete a branch protection rule . Synopsis . Delete a branch protection rule for a given branch name pattern . lakectl branch-protect delete &lt;repository URI&gt; &lt;pattern&gt; [flags] . Examples . lakectl branch-protect delete lakefs://my-repo stable_* . Options . -h, --help help for delete . lakectl branch-protect help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type branch-protect help [path to command] for full details. lakectl branch-protect help [command] [flags] . Options . -h, --help help for help . lakectl branch-protect list . List all branch protection rules . lakectl branch-protect list &lt;repository URI&gt; [flags] . Examples . lakectl branch-protect list lakefs://my-repo . Options . -h, --help help for list . lakectl cat-hook-output . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Cat actions hook output . lakectl cat-hook-output &lt;repository URI&gt; &lt;run_id&gt; &lt;hook_id&gt; [flags] . Examples . lakectl cat-hook-output lakefs://my-repo 20230719152411arS0z6I my_hook_name . Options . -h, --help help for cat-hook-output . lakectl cat-sst . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Explore lakeFS .sst files . lakectl cat-sst &lt;sst-file&gt; [flags] . Options . --amount int how many records to return, or -1 for all records (default -1) -f, --file string path to an sstable file, or \"-\" for stdin -h, --help help for cat-sst . lakectl cherry-pick . Apply the changes introduced by an existing commit . Synopsis . Apply the changes from the given commit to the tip of the branch. The changes will be added as a new commit. lakectl cherry-pick &lt;commit URI&gt; &lt;branch&gt; [flags] . Examples . lakectl cherry-pick lakefs://my-repo/600dc0ffee lakefs://my-repo/my-branch . Options . -h, --help help for cherry-pick -m, --parent-number int the parent number (starting from 1) of the cherry-picked commit. The cherry-pick will apply the change relative to the specified parent. lakectl commit . Commit changes on a given branch . lakectl commit &lt;branch URI&gt; [flags] . Options . --allow-empty-message allow an empty commit message -h, --help help for commit -m, --message string commit message --meta strings key value pair in the form of key=value . lakectl completion . Generate completion script . Synopsis . To load completions: . Bash: . $ source &lt;(lakectl completion bash) . To load completions for each session, execute once: Linux: . $ lakectl completion bash &gt; /etc/bash_completion.d/lakectl . MacOS: . $ lakectl completion bash &gt; /usr/local/etc/bash_completion.d/lakectl . Zsh: . If shell completion is not already enabled in your environment you will need to enable it. You can execute the following once: . $ echo \"autoload -U compinit; compinit\" &gt;&gt; ~/.zshrc . To load completions for each session, execute once: . $ lakectl completion zsh &gt; \"${fpath[1]}/_lakectl\" . You will need to start a new shell for this setup to take effect. Fish: . $ lakectl completion fish | source . To load completions for each session, execute once: . $ lakectl completion fish &gt; ~/.config/fish/completions/lakectl.fish . lakectl completion &lt;bash|zsh|fish&gt; . Options . -h, --help help for completion . lakectl config . Create/update local lakeFS configuration . lakectl config [flags] . Options . -h, --help help for config . lakectl diff . Show changes between two commits, or the currently uncommitted changes . lakectl diff &lt;ref URI&gt; [ref URI] [flags] . Examples . lakectl diff lakefs://example-repo/example-branch Show uncommitted changes in example-branch. lakectl diff lakefs://example-repo/main lakefs://example-repo/dev This shows the differences between master and dev starting at the last common commit. This is similar to the three-dot (...) syntax in git. Uncommitted changes are not shown. lakectl diff --two-way lakefs://example-repo/main lakefs://example-repo/dev Show changes between the tips of the main and dev branches. This is similar to the two-dot (..) syntax in git. Uncommitted changes are not shown. lakectl diff --two-way lakefs://example-repo/main lakefs://example-repo/dev$ Show changes between the tip of the main and the dev branch, including uncommitted changes on dev. Options . -h, --help help for diff --two-way Use two-way diff: show difference between the given refs, regardless of a common ancestor. lakectl docs . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. lakectl docs [outfile] [flags] . Options . -h, --help help for docs . lakectl doctor . Run a basic diagnosis of the LakeFS configuration . lakectl doctor [flags] . Options . -h, --help help for doctor . lakectl find-merge-base . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Find the commits for the merge operation . lakectl find-merge-base &lt;source ref URI&gt; &lt;destination ref URI&gt; [flags] . Options . -h, --help help for find-merge-base . lakectl fs . View and manipulate objects . Options . -h, --help help for fs . lakectl fs cat . Dump content of object to stdout . lakectl fs cat &lt;path URI&gt; [flags] . Options . -h, --help help for cat --pre-sign Use pre-signed URLs when downloading/uploading data (recommended) (default true) . lakectl fs download . Download object(s) from a given repository path . lakectl fs download &lt;path URI&gt; [&lt;destination path&gt;] [flags] . Options . -h, --help help for download -p, --parallelism int Max concurrent operations to perform (default 25) --pre-sign Use pre-signed URLs when downloading/uploading data (recommended) (default true) -r, --recursive recursively download all objects under path . lakectl fs help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type fs help [path to command] for full details. lakectl fs help [command] [flags] . Options . -h, --help help for help . lakectl fs ls . List entries under a given tree . lakectl fs ls &lt;path URI&gt; [flags] . Options . -h, --help help for ls -r, --recursive list all objects under the specified path . lakectl fs rm . Delete object . lakectl fs rm &lt;path URI&gt; [flags] . Options . -C, --concurrency int max concurrent single delete operations to send to the lakeFS server (default 50) -h, --help help for rm -r, --recursive recursively delete all objects under the specified path . lakectl fs stage . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Link an external object with a path in a repository . Synopsis . Link an external object with a path in a repository, creating an uncommitted change. The object location must be outside the repository’s storage namespace . lakectl fs stage &lt;path URI&gt; [flags] . Options . --checksum string Object MD5 checksum as a hexadecimal string --content-type string MIME type of contents -h, --help help for stage --location string fully qualified storage location (i.e. \"s3://bucket/path/to/object\") --meta strings key value pairs in the form of key=value --mtime int Object modified time (Unix Epoch in seconds). Defaults to current time --size int Object size in bytes . lakectl fs stat . View object metadata . lakectl fs stat &lt;path URI&gt; [flags] . Options . -h, --help help for stat --pre-sign Use pre-signed URLs when downloading/uploading data (recommended) (default true) . lakectl fs upload . Upload a local file to the specified URI . lakectl fs upload &lt;path URI&gt; [flags] . Options . --content-type string MIME type of contents -h, --help help for upload -p, --parallelism int Max concurrent operations to perform (default 25) --pre-sign Use pre-signed URLs when downloading/uploading data (recommended) (default true) -r, --recursive recursively copy all files under local source -s, --source string local file to upload, or \"-\" for stdin . lakectl gc . Manage the garbage collection policy . Options . -h, --help help for gc . lakectl gc delete-config . Deletes the garbage collection policy for the repository . lakectl gc delete-config &lt;repository URI&gt; [flags] . Examples . lakectl gc delete-config lakefs://my-repo . Options . -h, --help help for delete-config . lakectl gc get-config . Show the garbage collection policy for this repository . lakectl gc get-config &lt;repository URI&gt; [flags] . Examples . lakectl gc get-config lakefs://my-repo . Options . -h, --help help for get-config -p, --json get rules as JSON . lakectl gc help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type gc help [path to command] for full details. lakectl gc help [command] [flags] . Options . -h, --help help for help . lakectl gc set-config . Set garbage collection policy JSON . Synopsis . Sets the garbage collection policy JSON. Example configuration file: { “default_retention_days”: 21, “branches”: [ { “branch_id”: “main”, “retention_days”: 28 }, { “branch_id”: “dev”, “retention_days”: 14 } ] } . lakectl gc set-config &lt;repository URI&gt; [flags] . Examples . lakectl gc set-config lakefs://my-repo -f config.json . Options . -f, --filename string file containing the GC policy as JSON -h, --help help for set-config . lakectl help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type lakectl help [path to command] for full details. lakectl help [command] [flags] . Options . -h, --help help for help . lakectl import . Import data from external source to a destination branch . lakectl import --from &lt;object store URI&gt; --to &lt;lakeFS path URI&gt; [flags] . Options . --allow-empty-message allow an empty commit message (default true) --from string prefix to read from (e.g. \"s3://bucket/sub/path/\"). must not be in a storage namespace -h, --help help for import -m, --message string commit message --meta strings key value pair in the form of key=value --no-progress switch off the progress output --to string lakeFS path to load objects into (e.g. \"lakefs://repo/branch/sub/path/\") . lakectl ingest . Ingest objects from an external source into a lakeFS branch (without actually copying them) . lakectl ingest --from &lt;object store URI&gt; --to &lt;lakeFS path URI&gt; [--dry-run] [flags] . Options . -C, --concurrency int max concurrent API calls to make to the lakeFS server (default 64) --dry-run only print the paths to be ingested --from string prefix to read from (e.g. \"s3://bucket/sub/path/\"). must not be in a storage namespace -h, --help help for ingest --s3-endpoint-url string URL to access S3 storage API (by default, use regular AWS S3 endpoint --to string lakeFS path to load objects into (e.g. \"lakefs://repo/branch/sub/path/\") -v, --verbose print stats for each individual object staged . lakectl local . Sync local directories with lakeFS paths . Options . -h, --help help for local . lakectl local checkout . Sync local directory with the remote state. lakectl local checkout [directory] [flags] . Options . --all Checkout given source branch or reference for all linked directories -h, --help help for checkout -p, --parallelism int Max concurrent operations to perform (default 25) --pre-sign Use pre-signed URLs when downloading/uploading data (recommended) (default true) -r, --ref string Checkout the given reference -y, --yes Automatically say yes to all confirmations . lakectl local clone . Clone a path from a lakeFS repository into a new directory. lakectl local clone &lt;path URI&gt; [directory] [flags] . Options . --gitignore Update .gitignore file when working in a git repository context (default true) -h, --help help for clone -p, --parallelism int Max concurrent operations to perform (default 25) --pre-sign Use pre-signed URLs when downloading/uploading data (recommended) (default true) . lakectl local commit . Commit changes from local directory to the lakeFS branch it tracks. lakectl local commit [directory] [flags] . Options . --allow-empty-message allow an empty commit message -h, --help help for commit -m, --message string commit message --meta strings key value pair in the form of key=value -p, --parallelism int Max concurrent operations to perform (default 25) --pre-sign Use pre-signed URLs when downloading/uploading data (recommended) (default true) . lakectl local help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type local help [path to command] for full details. lakectl local help [command] [flags] . Options . -h, --help help for help . lakectl local init . set a local directory to sync with a lakeFS path. lakectl local init &lt;path URI&gt; [directory] [flags] . Options . --force Overwrites if directory already linked to a lakeFS path --gitignore Update .gitignore file when working in a git repository context (default true) -h, --help help for init . lakectl local list . find and list directories that are synced with lakeFS. lakectl local list [directory] [flags] . Options . -h, --help help for list . lakectl local pull . Fetch latest changes from lakeFS. lakectl local pull [directory] [flags] . Options . --force Reset any uncommitted local change -h, --help help for pull -p, --parallelism int Max concurrent operations to perform (default 25) --pre-sign Use pre-signed URLs when downloading/uploading data (recommended) (default true) . lakectl local status . show modifications (both remote and local) to the directory and the remote location it tracks . lakectl local status [directory] [flags] . Options . -h, --help help for status -l, --local Don't compare against remote changes . lakectl log . Show log of commits . Synopsis . Show log of commits for a given branch . lakectl log &lt;branch URI&gt; [flags] . Examples . lakectl log --dot lakefs://example-repository/main | dot -Tsvg &gt; graph.svg . Options . --after string show results after this value (used for pagination) --amount int number of results to return. By default, all results are returned --dot return results in a dotgraph format --first-parent follow only the first parent commit upon seeing a merge commit -h, --help help for log --limit limit result just to amount. By default, returns whether more items are available. --objects strings show results that contains changes to at least one path in that list of objects. Use comma separator to pass all objects together --prefixes strings show results that contains changes to at least one path in that list of prefixes. Use comma separator to pass all prefixes together --show-meta-range-id also show meta range ID --since string show results since this date-time (RFC3339 format) . lakectl merge . Merge &amp; commit changes from source branch into destination branch . Synopsis . Merge &amp; commit changes from source branch into destination branch . lakectl merge &lt;source ref&gt; &lt;destination ref&gt; [flags] . Options . --allow-empty-message allow an empty commit message (default true) -h, --help help for merge -m, --message string commit message --meta strings key value pair in the form of key=value --strategy string In case of a merge conflict, this option will force the merge process to automatically favor changes from the dest branch (\"dest-wins\") or from the source branch(\"source-wins\"). In case no selection is made, the merge process will fail in case of a conflict . lakectl metastore . Manage metastore commands . Options . -h, --help help for metastore . lakectl metastore copy . Copy or merge table . Synopsis . Copy or merge table. the destination table will point to the selected branch . lakectl metastore copy [flags] . Options . --catalog-id string Glue catalog ID --dbfs-root dbfs:/ dbfs location root will replace dbfs:/ in the location before transforming --from-client-type string metastore type [hive, glue] --from-schema string source schema name --from-table string source table name -h, --help help for copy --metastore-uri string Hive metastore URI -p, --partition strings partition to copy --serde string serde to set copy to [default is to-table] --to-branch string lakeFS branch name --to-client-type string metastore type [hive, glue] --to-schema string destination schema name [default is from-branch] --to-table string destination table name [default is from-table] . lakectl metastore copy-all . Copy from one metastore to another . Synopsis . copy or merge requested tables between hive metastores. the destination tables will point to the selected branch . lakectl metastore copy-all [flags] . Options . --branch string lakeFS branch name --continue-on-error prevent copy-all from failing when a single table fails --dbfs-root dbfs:/ dbfs location root will replace dbfs:/ in the location before transforming --from-address string source metastore address --from-client-type string metastore type [hive, glue] -h, --help help for copy-all --schema-filter string filter for schemas to copy in metastore pattern (default \".*\") --table-filter string filter for tables to copy in metastore pattern (default \".*\") --to-address string destination metastore address --to-client-type string metastore type [hive, glue] . lakectl metastore copy-schema . Copy schema . Synopsis . Copy schema (without tables). the destination schema will point to the selected branch . lakectl metastore copy-schema [flags] . Options . --catalog-id string Glue catalog ID --dbfs-root dbfs:/ dbfs location root will replace dbfs:/ in the location before transforming --from-client-type string metastore type [hive, glue] --from-schema string source schema name -h, --help help for copy-schema --metastore-uri string Hive metastore URI --to-branch string lakeFS branch name --to-client-type string metastore type [hive, glue] --to-schema string destination schema name [default is from-branch] . lakectl metastore create-symlink . Create symlink table and data . Synopsis . create table with symlinks, and create the symlinks in s3 in order to access from external services that could only access s3 directly (e.g athena) . lakectl metastore create-symlink [flags] . Options . --branch string lakeFS branch name --catalog-id string Glue catalog ID --from-client-type string metastore type [hive, glue] --from-schema string source schema name --from-table string source table name -h, --help help for create-symlink --path string path to table on lakeFS --repo string lakeFS repository name --to-schema string destination schema name --to-table string destination table name . lakectl metastore diff . Show column and partition differences between two tables . lakectl metastore diff [flags] . Options . --catalog-id string Glue catalog ID --from-address string source metastore address --from-client-type string metastore type [hive, glue] --from-schema string source schema name --from-table string source table name -h, --help help for diff --metastore-uri string Hive metastore URI --to-address string destination metastore address --to-client-type string metastore type [hive, glue] --to-schema string destination schema name --to-table string destination table name [default is from-table] . lakectl metastore help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type metastore help [path to command] for full details. lakectl metastore help [command] [flags] . Options . -h, --help help for help . lakectl metastore import-all . Import from one metastore to another . Synopsis . import requested tables between hive metastores. the destination tables will point to the selected repository and branch table with location s3://my-s3-bucket/path/to/table will be transformed to location s3://repo-param/bucket-param/path/to/table . lakectl metastore import-all [flags] . Options . --branch string lakeFS branch name --continue-on-error prevent import-all from failing when a single table fails --dbfs-root dbfs:/ dbfs location root will replace dbfs:/ in the location before transforming --from-address string source metastore address --from-client-type string metastore type [hive, glue] -h, --help help for import-all --repo string lakeFS repo name --schema-filter string filter for schemas to copy in metastore pattern (default \".*\") --table-filter string filter for tables to copy in metastore pattern (default \".*\") --to-address string destination metastore address --to-client-type string metastore type [hive, glue] . lakectl refs-dump . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Dumps refs (branches, commits, tags) to the underlying object store . lakectl refs-dump &lt;repository URI&gt; [flags] . Options . -h, --help help for refs-dump -o, --output string output filename (default stdout) --poll-interval duration poll status check interval (default 3s) --timeout duration timeout for polling status checks (default 1h0m0s) . lakectl refs-restore . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Restores refs (branches, commits, tags) from the underlying object store to a bare repository . Synopsis . restores refs (branches, commits, tags) from the underlying object store to a bare repository. This command is expected to run on a bare repository (i.e. one created with ‘lakectl repo create-bare’). Since a bare repo is expected, in case of transient failure, delete the repository and recreate it as bare and retry. lakectl refs-restore &lt;repository URI&gt; [flags] . Examples . aws s3 cp s3://bucket/_lakefs/refs_manifest.json - | lakectl refs-restore lakefs://my-bare-repository --manifest - . Options . -h, --help help for refs-restore --manifest refs-dump path to a refs manifest json file (as generated by refs-dump). Alternatively, use \"-\" to read from stdin --poll-interval duration poll status check interval (default 3s) --timeout duration timeout for polling status checks (default 1h0m0s) . lakectl repo . Manage and explore repos . Options . -h, --help help for repo . lakectl repo create . Create a new repository . lakectl repo create &lt;repository URI&gt; &lt;storage namespace&gt; [flags] . Examples . lakectl repo create lakefs://my-repo s3://my-bucket . Options . -d, --default-branch string the default branch of this repository (default \"main\") -h, --help help for create . lakectl repo create-bare . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Create a new repository with no initial branch or commit . lakectl repo create-bare &lt;repository URI&gt; &lt;storage namespace&gt; [flags] . Examples . lakectl create-bare lakefs://my-repo s3://my-bucket . Options . -d, --default-branch string the default branch name of this repository (will not be created) (default \"main\") -h, --help help for create-bare . lakectl repo delete . Delete existing repository . lakectl repo delete &lt;repository URI&gt; [flags] . Examples . lakectl repo delete lakefs://my-repo . Options . -h, --help help for delete -y, --yes Automatically say yes to all confirmations . lakectl repo help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type repo help [path to command] for full details. lakectl repo help [command] [flags] . Options . -h, --help help for help . lakectl repo list . List repositories . lakectl repo list [flags] . Options . --after string show results after this value (used for pagination) --amount int number of results to return (default 100) -h, --help help for list . lakectl show . See detailed information about an entity . Options . -h, --help help for show . lakectl show commit . See detailed information about a commit . lakectl show commit &lt;commit URI&gt; [flags] . Options . -h, --help help for commit --show-meta-range-id show meta range ID . lakectl show help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type show help [path to command] for full details. lakectl show help [command] [flags] . Options . -h, --help help for help . lakectl tag . Create and manage tags within a repository . Synopsis . Create delete and list tags within a lakeFS repository . Options . -h, --help help for tag . lakectl tag create . Create a new tag in a repository . lakectl tag create &lt;tag URI&gt; &lt;commit URI&gt; [flags] . Examples . lakectl tag create lakefs://example-repo/example-tag lakefs://example-repo/2397cc9a9d04c20a4e5739b42c1dd3d8ba655c0b3a3b974850895a13d8bf9917 . Options . -f, --force override the tag if it exists -h, --help help for create . lakectl tag delete . Delete a tag from a repository . lakectl tag delete &lt;tag URI&gt; [flags] . Options . -h, --help help for delete . lakectl tag help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type tag help [path to command] for full details. lakectl tag help [command] [flags] . Options . -h, --help help for help . lakectl tag list . List tags in a repository . lakectl tag list &lt;repository URI&gt; [flags] . Examples . lakectl tag list lakefs://my-repo . Options . --after string show results after this value (used for pagination) --amount int number of results to return (default 100) -h, --help help for list . lakectl tag show . Show tag’s commit reference . lakectl tag show &lt;tag URI&gt; [flags] . Options . -h, --help help for show . ",
    "url": "/v1.4/reference/cli.html#command-reference",
    
    "relUrl": "/reference/cli.html#command-reference"
  },"98": {
    "doc": "lakectl (lakeFS command-line tool)",
    "title": "lakectl (lakeFS command-line tool)",
    "content": " ",
    "url": "/v1.4/reference/cli.html",
    
    "relUrl": "/reference/cli.html"
  },"99": {
    "doc": "Cloudera",
    "title": "Using lakeFS with Cloudera Spark",
    "content": "Use the lakeFS Hadoop FileSystem to integrate lakeFS with Cloudera Spark. Review Cloudera Partner Listing for the Cloudera certification of lakeFS integration with Cloudera Data Platform (CDP) and Cloudera Spark. ",
    "url": "/v1.4/integrations/cloudera.html#using-lakefs-with-cloudera-spark",
    
    "relUrl": "/integrations/cloudera.html#using-lakefs-with-cloudera-spark"
  },"100": {
    "doc": "Cloudera",
    "title": "Cloudera",
    "content": " ",
    "url": "/v1.4/integrations/cloudera.html",
    
    "relUrl": "/integrations/cloudera.html"
  },"101": {
    "doc": "Migrating to 1.0",
    "title": "lakeFS 1.0 - Code Migration Guide",
    "content": "Version 1.0.0 promises API and SDK stability. By “API” we mean any access to a lakeFS REST endpoint. By “SDK” we mean auto-generated lakeFS clients: lakefs-sdk for Python and io.lakefs:sdk for Java. This guide details the steps to allow you to upgrade your code to enjoy this stability. Avoid using APIs and SDKs labeled as experimental, internal, or deprecated. If you must use them, be prepared to adjust your application to align with any lakeFS server updates. Your software developed without such APIs should be compatible with all minor version updates of the lakeFS server from the version you originally developed with. If you rely on a publicly released API and SDK, it will adhere to semantic versioning. Transitioning your application to a minor SDK version update should be smooth. The operation names and tags from the api/swagger.yml specification might differ based on the SDK or coding language in use. Deleted API Operations . The following API operations have been removed: . | updatePassword | forgotPassword | logBranchCommits | expandTemplate | createMetaRange | ingestRange | updateBranchToken | . Internal API Operations . The following operations are for internal use only and should not be used in your application code. Some deprecated operations have alternatives provided. | setupCommPrefs | getSetupState | setup | getAuthCapabilities | uploadObjectPreflight | setGarbageCollectionRulesPreflight | createBranchProtectionRulePreflight | postStatsEvents | dumpRefs (will be replaced with a long-running API later) | restoreRefs (will be replaced with a long-running API later) | createSymlinkFile (Deprecated) | getStorageConfig (Deprecated. Alternative: getConfig) | getLakeFSVersion (Deprecated. Alternative: getConfig) | stageObject (Deprecated. Alternatives: get/link physical address or import) | internalDeleteBranchProtectionRule (Deprecated. Temporary backward support. Alternative: setBranchProtectionRules) | internalCreateBranchProtectionRule (Deprecated. Temporary backward support. Alternative: setBranchProtectionRules) | internalGetBranchProtectionRule (Deprecated. Temporary backward support. Alternative: getBranchProtectionRules) | internalDeleteGarbageCollectionRules (Deprecated. Temporary backward support. Alternative: deleteGCRules) | internalSetGarbageCollectionRules (Deprecated. Temporary backward support. Alternative: setGCRules) | internalGetGarbageCollectionRules (Deprecated. Temporary backward support. Alternative: getGCRules) | prepareGarbageCollectionCommits | getGarbageCollectionConfig | . New/Updated API Operations . Here are the newly added or updated operations: . | getConfig (Retrieve lakeFS version and storage info) | setBranchProtectionRules (Route updated) | getBranchProtectionRules (Route updated) | getGCRules (New route introduced) | setGCRules (New route introduced) | deleteGCRules (New route introduced) | importStatus (Response structure updated: ‘ImportStatusResp’ to ‘ImportStatus’) | uploadObject (Parameters ‘if-none-match’ and ‘storageClass’ are now deprecated) | prepareGarbageCollectionCommits (Request body removed) | getOtfDiffs &amp; otfDiff (Removed from ‘otf diff’ tag; retained in ‘experimental’ tag) | . ",
    "url": "/v1.4/project/code-migrate-1.0-sdk.html#lakefs-10---code-migration-guide",
    
    "relUrl": "/project/code-migrate-1.0-sdk.html#lakefs-10---code-migration-guide"
  },"102": {
    "doc": "Migrating to 1.0",
    "title": "Migrating SDK Code for Java and JVM-based Languages",
    "content": "Introduction . If you are using the lakeFS client for Java or for any other JVM-based language, be aware that the current package is not stable with respect to minor version upgrades. Transitioning from io.lakefs:lakefs-client to io.lakefs:sdk will necessitate rewriting your API calls to fit the new design paradigm. Problem with the Old Style . Previously, API calls required developers to pass all parameters, including optional ones, in a single function call. As demonstrated in this older style: . ObjectStats objectStat = objectsApi.statObject( objectLoc.getRepository(), objectLoc.getRef(), objectLoc.getPath(), false, false); . This method posed a couple of challenges: . | Inflexibility with Upgrades: If an optional parameter were introduced in newer versions, existing code would fail to compile. | Maintenance Difficulty: Long argument lists can be challenging to manage and understand, leading to potential mistakes and readability issues. | . Adopting the Fluent Style . In the revised SDK, API calls adopt a fluent style, making the code more modular and adaptive to changes. Here’s an example of the new style: . ObjectStats objectStat = objectsApi .statObject( objectLoc.getRepository(), objectLoc.getRef(), objectLoc.getPath() ) .userMetadata(true) .execute(); . Here’s a breakdown of the changes: . | Initial Function Call: Begin by invoking the desired function with all required parameters. | Modifying Optional Parameters: Chain any modifications to optional parameters after the initial function. For instance, userMetadata is changed in the example above. | Unused Optional Parameters: You can safely ignore these. For instance, this code ignores the presign optional parameter because it never uses it. | Execution: Complete the call with the .execute() method. | . This new design offers several advantages: . | Compatibility with Upgrades: When a new optional parameter is introduced, existing code will use its default value, preserving compatibility with minor server version upgrades. | Improved Readability: The fluent style makes it evident which parameters are required and which ones are optional, enhancing code clarity. | . When migrating your code, ensure you refactor all your API calls to adopt the new fluent style. This ensures that your application remains maintainable and is safeguarded against potential issues arising from minor SDK version upgrades. For an illustrative example of the transition between styles, you can view the changes made in this pull request: lakeFS pull request #6529. ",
    "url": "/v1.4/project/code-migrate-1.0-sdk.html#migrating-sdk-code-for-java-and-jvm-based-languages",
    
    "relUrl": "/project/code-migrate-1.0-sdk.html#migrating-sdk-code-for-java-and-jvm-based-languages"
  },"103": {
    "doc": "Migrating to 1.0",
    "title": "Migrating SDK Code for Python",
    "content": "Introduction . If you are using the lakeFS client for Python, be aware that the current package is not stable with respect to minor version upgrades. Transitioning from lakefs-client to lakefs-sdk will necessitate rewriting your API calls. Here’s a breakdown of the changes: . | Modules change . | The previous model module was renamed to models, meaning that lakefs_client.model imports should be replaced with lakefs_sdk.models imports. | The apis module in lakefs_client is deprecated and no longer supported. To migrate to the new api module in lakefs_sdk, you should replace all imports of lakefs_client.apis with imports of lakefs_sdk.api. We still recommend using the lakefs_sdk.LakeFSClient class instead of using the api module directly. The LakeFSClient class provides a higher-level interface to the LakeFS API and makes it easier to use LakeFS in your applications. | . | upload_object API call: The content parameter value passed to the objects_api.upload_object method call should be either a string containing the path to the uploaded file, or bytes of data to be uploaded. | get_object API call: The return value of client.get_object(...) is a bytearray containing the content of the object. | **client.{operation}_api**: The lakefs-client package’s LakeFSClient class’s deprecation-marked operations (client.{operation}) will no longer be available in the lakefs-sdk package’s LakeFSClient class. In their place, the client.{operation}_api should be used. | Minimum Python Version: 3.7 | Fetching results from response objects: Instead of fetching the required results properties from a dictionary using response_result.get_property(prop_name), the response objects will include domain specific entities, thus referring to the properties in the results of the response - response_result.prop_name. For example, instead of: | . response = lakefs_client.branches.diff_branch(repository='repo', branch='main') diff = response.results[0] # 'results' is a 'DiffList' object path = diff.get_property('path') # 'diff' is a dictionary . You should use: . response = lakefs_client.branches_api.diff_branch(repository='repo', branch='main') diff = response.results[0] # 'results' is a 'DiffList' object path = diff.path # 'diff' is a 'Diff' object . ",
    "url": "/v1.4/project/code-migrate-1.0-sdk.html#migrating-sdk-code-for-python",
    
    "relUrl": "/project/code-migrate-1.0-sdk.html#migrating-sdk-code-for-python"
  },"104": {
    "doc": "Migrating to 1.0",
    "title": "Migrating to 1.0",
    "content": " ",
    "url": "/v1.4/project/code-migrate-1.0-sdk.html",
    
    "relUrl": "/project/code-migrate-1.0-sdk.html"
  },"105": {
    "doc": "4️⃣ Commit and Merge",
    "title": "Committing Changes in lakeFS",
    "content": "Having make the change to the datafile in the denmark-lakes branch, we now want to commit it. There are various options for interacting with the lakeFS API, including the web interface, a Python client, and lakectl which is what we’ll use here. Run the following from a terminal window: . docker exec lakefs \\ lakectl commit lakefs://quickstart/denmark-lakes \\ -m \"Create a dataset of just the lakes in Denmark\" . You will get confirmation of the commit including its hash. Branch: lakefs://quickstart/denmark-lakes Commit for branch \"denmark-lakes\" completed. ID: ba6d71d0965fa5d97f309a17ce08ad006c0dde15f99c5ea0904d3ad3e765bd74 Message: Create a dataset of just the lakes in Denmark Timestamp: 2023-03-15 08:09:36 +0000 UTC Parents: 3384cd7cdc4a2cd5eb6249b52f0a709b49081668bb1574ce8f1ef2d956646816 . With our change committed, it’s now time to merge it to back to the main branch. ",
    "url": "/v1.4/quickstart/commit-and-merge.html#committing-changes-in-lakefs",
    
    "relUrl": "/quickstart/commit-and-merge.html#committing-changes-in-lakefs"
  },"106": {
    "doc": "4️⃣ Commit and Merge",
    "title": "Merging Branches in lakeFS 🔀",
    "content": "As above, we’ll use lakectl to do this too. The syntax just requires us to specify the source and target of the merge. Run this from a terminal window. docker exec lakefs \\ lakectl merge \\ lakefs://quickstart/denmark-lakes \\ lakefs://quickstart/main . We can confirm that this has worked by returning to the same object view of lakes.parquet as before and clicking on Execute to rerun the same query. You’ll see that the country row counts have changed, and only Denmark is left in the data: . But…oh no! A slow chill creeps down your spine, and the bottom drops out of your stomach. What have you done! 😱 You were supposed to create a separate file of Denmark’s lakes - not replace the original one . Is all lost? Will our hero overcome the obstacles? No, and yes respectively! . Have no fear; lakeFS can revert changes. Tune in for the final part of the quickstart to see how. ",
    "url": "/v1.4/quickstart/commit-and-merge.html#merging-branches-in-lakefs-",
    
    "relUrl": "/quickstart/commit-and-merge.html#merging-branches-in-lakefs-"
  },"107": {
    "doc": "4️⃣ Commit and Merge",
    "title": "4️⃣ Commit and Merge",
    "content": "In the previous step we branched our data from main into a new denmark-lakes branch, and overwrote the lakes.parquet to hold solely information about lakes in Denmark. Now we’re going to commit that change (just like Git) and merge it back to main (just like git). ",
    "url": "/v1.4/quickstart/commit-and-merge.html",
    
    "relUrl": "/quickstart/commit-and-merge.html"
  },"108": {
    "doc": "lakeFS Server Configuration",
    "title": "lakeFS Server Configuration",
    "content": " ",
    "url": "/v1.4/reference/configuration.html",
    
    "relUrl": "/reference/configuration.html"
  },"109": {
    "doc": "lakeFS Server Configuration",
    "title": "Table of contents",
    "content": ". | Reference | Using Environment Variables | Example Configurations | . Configuring lakeFS is done using a YAML configuration file and/or environment variable. The configuration file’s location can be set with the ‘–config’ flag. If not specified, the first file found in the following order will be used: . | ./config.yaml | $HOME/lakefs/config.yaml | /etc/lakefs/config.yaml | $HOME/.lakefs.yaml | . Configuration items can each be controlled by an environment variable. The variable name will have a prefix of LAKEFS_, followed by the name of the configuration, replacing every ‘.’ with a ‘_’. Example: LAKEFS_LOGGING_LEVEL controls logging.level. This reference uses . to denote the nesting of values. ",
    "url": "/v1.4/reference/configuration.html#table-of-contents",
    
    "relUrl": "/reference/configuration.html#table-of-contents"
  },"110": {
    "doc": "lakeFS Server Configuration",
    "title": "Reference",
    "content": ". | logging.format (one of [\"json\", \"text\"] : \"text\") - Format to output log message in | logging.level (one of [\"TRACE\", \"DEBUG\", \"INFO\", \"WARN\", \"ERROR\", \"NONE\"] : \"INFO\") - Logging level to output | logging.audit_log_level (one of [\"TRACE\", \"DEBUG\", \"INFO\", \"WARN\", \"ERROR\", \"NONE\"] : \"DEBUG\") - Audit logs level to output. Note: In case you configure this field to be lower than the main logger level, you won’t be able to get the audit logs . | logging.output (string : \"-\") - A path or paths to write logs to. A - means the standard output, = means the standard error. | logging.file_max_size_mb (int : 100) - Output file maximum size in megabytes. | logging.files_keep (int : 0) - Number of log files to keep, default is all. | actions.enabled (bool : true) - Setting this to false will block hooks from being executed. | actions.lua.net_http_enabled (bool : false) - Setting this to true will load the net/http package. | actions.env.enabled (bool : true) - Environment variables accessible by hooks, disabled values evaluated to empty strings | actions.env.prefix (string : \"LAKEFSACTION_\") - Access to environment variables is restricted to those with the prefix. When environment access is enabled and no prefix is provided, all variables are accessible. Note: Deprecated - See database section . | database - Configuration section for the lakeFS key-value store database . | database.type (string [\"postgres\"|\"dynamodb\"|\"cosmosdb\"|\"local\"] : ) - lakeFS database type | database.postgres - Configuration section when using database.type=\"postgres\" . | database.postgres.connection_string (string : \"postgres://localhost:5432/postgres?sslmode=disable\") - PostgreSQL connection string to use | database.postgres.max_open_connections (int : 25) - Maximum number of open connections to the database | database.postgres.max_idle_connections (int : 25) - Maximum number of connections in the idle connection pool | database.postgres.connection_max_lifetime (duration : 5m) - Sets the maximum amount of time a connection may be reused (valid units: ns|us|ms|s|m|h) | . | database.dynamodb - Configuration section when using database.type=\"dynamodb\" . | database.dynamodb.table_name (string : \"kvstore\") - Table used to store the data | database.dynamodb.scan_limit (int : 1025) - Maximal number of items per page during scan operation . Note: Refer to the following AWS documentation for further information . | database.dynamodb.endpoint (string : ) - Endpoint URL for database instance | database.dynamodb.aws_region (string : ) - AWS Region of database instance | database.dynamodb.aws_profile (string : ) - AWS named profile to use | database.dynamodb.aws_access_key_id (string : ) - AWS access key ID | database.dynamodb.aws_secret_access_key (string : ) - AWS secret access key | Note: endpoint aws_region aws_access_key_id aws_secret_access_key are not required and used mainly for experimental purposes when working with DynamoDB with different AWS credentials. | database.dynamodb.health_check_interval (duration : 0s) - Interval to run health check for the DynamoDB instance (won’t run if equal to 0). | . | database.cosmosdb - Configuration section when using database.type=\"cosmosdb\" . | database.cosmosdb.key (string : \"\") - If specified, will be used to authenticate to the CosmosDB account. Otherwise, Azure SDK default authentication (with env vars) will be used. | database.cosmosdb.endpoint (string : \"\") - CosmosDB account endpoint, e.g. https://&lt;account&gt;.documents.azure.com/. | database.cosmosdb.database (string : \"\") - CosmosDB database name. | database.cosmosdb.container (string : \"\") - CosmosDB container name. | database.cosmosdb.throughput (int32 : ) - CosmosDB container’s RU/s. If not set - the default CosmosDB container throughput is used. | database.cosmosdb.autoscale (bool : false) - If set, CosmosDB container throughput is autoscaled (See CosmosDB docs for minimum throughput requirement). Otherwise, uses “Manual” mode (Docs). | . | database.local - Configuration section when using database.type=\"local\" . | database.local.path (string : \"~/lakefs/metadata\") - Local path on the filesystem to store embedded KV metadata, like branches and uncommitted entries | database.local.sync_writes (bool: true) - Ensure each write is written to the disk. Disable to increase performance | database.local.prefetch_size (int: 256) - How many items to prefetch when iterating over embedded KV records | database.local.enable_logging (bool: false) - Enable trace logging for local driver | . | . | listen_address (string : \"0.0.0.0:8000\") - A &lt;host&gt;:&lt;port&gt; structured string representing the address to listen on | tls.enabled (bool :false) - Enable TLS listening. The listen_address will be used to serve HTTPS requests. (mainly for local development) | tls.cert_file (string : ) - Server certificate file path used while serve HTTPS (.cert or .crt file - signed certificates). | tls.key_file (string : ) - Server secret key file path used whie serve HTTPS (.key file - private key). | auth.cache.enabled (bool : true) - Whether to cache access credentials and user policies in-memory. Can greatly improve throughput when enabled. | auth.cache.size (int : 1024) - How many items to store in the auth cache. Systems with a very high user count should use a larger value at the expense of ~1kb of memory per cached user. | auth.cache.ttl (time duration : \"20s\") - How long to store an item in the auth cache. Using a higher value reduces load on the database, but will cause changes longer to take effect for cached users. | auth.cache.jitter (time duration : \"3s\") - A random amount of time between 0 and this value is added to each item’s TTL. This is done to avoid a large bulk of keys expiring at once and overwhelming the database. | auth.encrypt.secret_key (string : required) - A random (cryptographically safe) generated string that is used for encryption and HMAC signing | auth.login_duration (time duration : \"168h\") - The duration the login token is valid for | auth.cookie_domain (string : \"\") - Domain attribute to set the access_token cookie on (the default is an empty string which defaults to the same host that sets the cookie) | auth.api.endpoint (string: https://external.service/api/v1) - URL to external Authorization Service described at authorization.yml; | auth.api.token (string: eyJhbGciOiJIUzI1NiIsInR5...) - API token used to authenticate requests to api endpoint | auth.api.health_check_timeout (time duration : \"20s\") - Timeout duration for external auth API health check | auth.api.skip_health_check (bool : false) - Skip external auth API health check . Note: It is best to keep this somewhere safe such as KMS or Hashicorp Vault, and provide it to the system at run time . | auth.remote_authenticator.enabled (bool : false) - If specified, also authenticate users via this Remote Authenticator server. | auth.remote_authenticator.endpoint (string : required) - Endpoint URL of the remote authentication service (e.g. https://my-auth.example.com/auth). | auth.remote_authenticator.default_user_group (string : Viewers) - Create users in this group (i.e Viewers, Developers, etc). | auth.remote_authenticator.request_timeout (duration : 10s) - If specified, timeout for remote authentication requests. | auth.cookie_auth_verification.validate_id_token_claims (map[string]string : ) - When a user tries to access lakeFS, validate that the ID token contains these claims with the corresponding values. | auth.cookie_auth_verification.default_initial_groups (string[] : [])` - By default, users will be assigned to these groups | auth.cookie_auth_verification.initial_groups_claim_name (string[] : []) - Use this claim from the ID token to provide the initial group for new users. This will take priority if auth.cookie_auth_verification.default_initial_groups is also set. | auth.cookie_auth_verification.friendly_name_claim_name (string[] : ) - If specified, the value from the claim with this name will be used as the user’s display name. | auth.cookie_auth_verification.external_user_id_claim_name - (string : ) - If specified, the value from the claim with this name will be used as the user’s id name. | auth.cookie_auth_verification.auth_source - (string : ) - If specified, user will be labeled with this auth source. | auth.oidc.default_initial_groups (string[] : []) - By default, OIDC users will be assigned to these groups | auth.oidc.initial_groups_claim_name (string[] : []) - Use this claim from the ID token to provide the initial group for new users. This will take priority if auth.oidc.default_initial_groups is also set. | auth.oidc.friendly_name_claim_name (string[] : ) - If specified, the value from the claim with this name will be used as the user’s display name. | auth.oidc.validate_id_token_claims (map[string]string : ) - When a user tries to access lakeFS, validate that the ID token contains these claims with the corresponding values. | auth.ui_config.rbac (string: \"simplified\") - “simplified”, “external” or “internal” (enterprise feature). In simplified mode, do not display policy in GUI. If you have configured an external auth server you can set this to “external” to support the policy editor. If you are using the enteprrise version of lakeFS, you can set this to “internal” to use the built-in policy editor. | blockstore.type (one of [\"local\", \"s3\", \"gs\", \"azure\", \"mem\"] : required). Block adapter to use. This controls where the underlying data will be stored | blockstore.default_namespace_prefix (string : ) - Use this to help your users choose a storage namespace for their repositories. If specified, the storage namespace will be filled with this default value as a prefix when creating a repository from the UI. The user may still change it to something else. | blockstore.local.path (string: \"~/lakefs/data\") - When using the local Block Adapter, which directory to store files in | blockstore.local.import_enabled (bool: false) - Enable import for local Block Adapter, relevant only if you are using shared location | blockstore.local.import_hidden (bool: false) - When enabled import will scan and import any file or folder that starts with a dot character. | blockstore.local.allowed_external_prefixes ([]string: []) - List of absolute path prefixes used to match any access for external location (ex: /var/data/). Empty list mean no access to external location. | blockstore.gs.credentials_file (string : ) - If specified will be used as a file path of the JSON file that contains your Google service account key | blockstore.gs.credentials_json (string : ) - If specified will be used as JSON string that contains your Google service account key (when credentials_file is not set) | blockstore.gs.pre_signed_expiry (time duration : \"15m\") - Expiry of pre-signed URL. | blockstore.gs.disable_pre_signed (bool : false) - Disable use of pre-signed URL. | blockstore.gs.disable_pre_signed_ui (bool : true) - Disable use of pre-signed URL in the UI. | blockstore.azure.storage_account (string : ) - If specified, will be used as the Azure storage account | blockstore.azure.storage_access_key (string : ) - If specified, will be used as the Azure storage access key | blockstore.azure.pre_signed_expiry (time duration : \"15m\") - Expiry of pre-signed URL. | blockstore.azure.disable_pre_signed (bool : false) - Disable use of pre-signed URL. | blockstore.azure.disable_pre_signed_ui (bool : true) - Disable use of pre-signed URL in the UI. | blockstore.s3.region (string : \"us-east-1\") - Default region for lakeFS to use when interacting with S3. | blockstore.s3.profile (string : ) - If specified, will be used as a named credentials profile | blockstore.s3.credentials_file (string : ) - If specified, will be used as a credentials file | blockstore.s3.credentials.access_key_id (string : ) - If specified, will be used as a static set of credential | blockstore.s3.credentials.secret_access_key (string : ) - If specified, will be used as a static set of credential | blockstore.s3.credentials.session_token (string : ) - If specified, will be used as a static session token | blockstore.s3.endpoint (string : ) - If specified, custom endpoint for the AWS S3 API (https://s3_compatible_service_endpoint:port) | blockstore.s3.force_path_style (bool : false) - When true, use path-style S3 URLs (https:/// instead of https://.) . | blockstore.s3.discover_bucket_region (bool : true) - (Can be turned off if the underlying S3 bucket doesn’t support the GetBucketRegion API). | blockstore.s3.skip_verify_certificate_test_only (boolean : false) - Skip certificate verification while connecting to the storage endpoint. Should be used only for testing. | blockstore.s3.server_side_encryption (string : ) - Server side encryption format used (Example on AWS using SSE-KMS while passing “aws:kms”) | blockstore.s3.server_side_encryption_kms_key_id (string : ) - Server side encryption KMS key ID | blockstore.s3.pre_signed_expiry (time duration : \"15m\") - Expiry of pre-signed URL. | blockstore.s3.disable_pre_signed (bool : false) - Disable use of pre-signed URL. | blockstore.s3.disable_pre_signed_ui (bool : true) - Disable use of pre-signed URL in the UI. | blockstore.s3.client_log_request (bool : false) - Set SDK logging bit to log requests | blockstore.s3.client_log_retries (bool : false) - Set SDK logging bit to log retries | graveler.reposiory_cache.size (int : 1000) - How many items to store in the repository cache. | graveler.reposiory_cache.ttl (time duration : \"5s\") - How long to store an item in the repository cache. | graveler.reposiory_cache.jitter (time duration : \"2s\") - A random amount of time between 0 and this value is added to each item’s TTL. | graveler.ensure_readable_root_namespace (bool: true) - When creating a new repository use this to verify that lakeFS has access to the root of the underlying storage namespace. Set false only if lakeFS should not have access (i.e pre-sign mode only). | graveler.commit_cache.size (int : 50000) - How many items to store in the commit cache. | graveler.commit_cache.ttl (time duration : \"10m\") - How long to store an item in the commit cache. | graveler.commit_cache.jitter (time duration : \"2s\") - A random amount of time between 0 and this value is added to each item’s TTL. | graveler.background.rate_limit (int : 0) - Advence configuration to control background work done rate limit in requests per second (default: 0 - unlimited). | committed.local_cache - an object describing the local (on-disk) cache of metadata from permanent storage: . | committed.local_cache.size_bytes (int : 1073741824) - bytes for local cache to use on disk. The cache may use more storage for short periods of time. | committed.local_cache.dir (string, ~/lakefs/local_tier) - directory to store local cache. | committed.local_cache.range_proportion (float : 0.9) - proportion of local cache to use for storing ranges (leaves of committed metadata storage). | committed.local_cache.range.open_readers (int : 500) - maximal number of unused open SSTable readers to keep for ranges. | committed.local_cache.range.num_shards (int : 30) - sharding factor for open SSTable readers for ranges. Should be at least sqrt(committed.local_cache.range.open_readers). | committed.local_cache.metarange_proportion (float : 0.1) - proportion of local cache to use for storing metaranges (roots of committed metadata storage). | committed.local_cache.metarange.open_readers (int : 50) - maximal number of unused open SSTable readers to keep for metaranges. | committed.local_cache.metarange.num_shards (int : 10) - sharding factor for open SSTable readers for metaranges. Should be at least sqrt(committed.local_cache.metarange.open_readers). | . | committed.block_storage_prefix (string : _lakefs) - Prefix for metadata file storage in each repository’s storage namespace | committed.permanent.min_range_size_bytes (int : 0) - Smallest allowable range in metadata. Increase to somewhat reduce random access time on committed metadata, at the cost of increased committed metadata storage cost. | committed.permanent.max_range_size_bytes (int : 20971520) - Largest allowable range in metadata. Should be close to the size at which fetching from remote storage becomes linear. | committed.permanent.range_raggedness_entries (int : 50_000) - Average number of object pointers to store in each range (subject to min_range_size_bytes and max_range_size_bytes). | committed.sstable.memory.cache_size_bytes (int : 200_000_000) - maximal size of in-memory cache used for each SSTable reader. | email.smtp_host (string) - A string representing the URL of the SMTP host. | email.smtp_port (int) - An integer representing the port of the SMTP service (465, 587, 993, 25 are some standard ports) | email.use_ssl (bool : false) - Use SSL connection with SMTP host. | email.username (string) - A string representing the username of the specific account at the SMTP. It’s recommended to provide this value at runtime from a secret vault of some sort. | email.password (string) - A string representing the password of the account. It’s recommended to provide this value at runtime from a secret vault of some sort. | email.local_name (string) - A string representing the hostname sent to the SMTP server with the HELO command. By default, “localhost” is sent. | email.sender (string) - A string representing the email account which is set as the sender. | email.limit_every_duration (duration : 1m) - The average time between sending emails. If zero is entered, there is no limit to the amount of emails that can be sent. | email.burst (int: 10) - Maximal burst of emails before applying limit_every_duration. The zero value means no burst and therefore no emails can be sent. | email.lakefs_base_url (string : \"http://localhost:8000\") - A string representing the base lakeFS endpoint to be directed to when emails are sent inviting users, reseting passwords etc. | gateways.s3.domain_name (string : \"s3.local.lakefs.io\") - a FQDN representing the S3 endpoint used by S3 clients to call this server (*.s3.local.lakefs.io always resolves to 127.0.0.1, useful for local development, if using virtual-host addressing. | gateways.s3.region (string : \"us-east-1\") - AWS region we’re pretending to be in, it should match the region configuration used in AWS SDK clients | gateways.s3.fallback_url (string) - If specified, requests with a non-existing repository will be forwarded to this URL. This can be useful for using lakeFS side-by-side with S3, with the URL pointing at an S3Proxy instance. | gateways.s3.verify_unsupported (bool : true) - The S3 gateway errors on unsupported requests, but when disabled, defers to target-based handlers. | stats.enabled (bool : true) - Whether to periodically collect anonymous usage statistics | stats.flush_interval (duration : 30s) - Interval used to post anonymous statistics collected | stats.flush_size (int : 100) - A size (in records) of anonymous statistics collected in which we post | security.audit_check_interval (duration : 24h) - Duration in which we check for security audit. | ui.enabled (bool: true) - Whether to server the embedded UI from the binary | ugc.prepare_max_file_size (int: 125829120) - Uncommitted garbage collection prepare request, limit the produced file maximum size | ugc.prepare_interval (duraction: 1m) - Uncommitted garbage collection prepare request, limit produce time to interval | diff.delta.plugin (string : ) - Name of the Delta Lake diff plugin. | plugins.default_path (string : ~/.lakefs/plugins) - Absolute path to the root of lakeFS’s plugins location. | plugins.properties.&lt;plugin name&gt;.path (string : ) - Absolute path to the location of &lt;plugin name&gt;’s binary location. | plugins.properties.&lt;plugin name&gt;.version (uint : ) - Version of the &lt;plugin name&gt; plugin. The version must be &gt; 0. | installation.user_name (string : ) - When specified, an initial admin user will be created when the server is first run. Works only when database.type is set to local. Requires installation.access_key_id and installation.secret_access_key. | installation.access_key_id (string : ) - Admin’s initial access key id (used once in the initial setup process) | installation.secret_access_key (string : ) - Admin’s initial secret access key (used once in the initial setup process) | . ",
    "url": "/v1.4/reference/configuration.html#reference",
    
    "relUrl": "/reference/configuration.html#reference"
  },"111": {
    "doc": "lakeFS Server Configuration",
    "title": "Using Environment Variables",
    "content": "All the configuration variables can be set or overridden using environment variables. To set an environment variable, prepend LAKEFS_ to its name, convert it to upper case, and replace . with _: . For example, logging.format becomes LAKEFS_LOGGING_FORMAT, blockstore.s3.region becomes LAKEFS_BLOCKSTORE_S3_REGION, etc. ",
    "url": "/v1.4/reference/configuration.html#using-environment-variables",
    
    "relUrl": "/reference/configuration.html#using-environment-variables"
  },"112": {
    "doc": "lakeFS Server Configuration",
    "title": "Example Configurations",
    "content": "Local Development with PostgreSQL database . --- listen_address: \"0.0.0.0:8000\" database: type: \"postgres\" postgres: connection_string: \"postgres://localhost:5432/postgres?sslmode=disable\" logging: format: text level: DEBUG output: \"-\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: local local: path: \"~/lakefs/dev/data\" gateways: s3: region: us-east-1 . AWS Deployment with DynamoDB database . --- logging: format: json level: WARN output: \"-\" database: type: \"dynamodb\" dynamodb: table_name: \"kvstore\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: s3 s3: region: us-east-1 # optional, fallback in case discover from bucket is not supported credentials_file: /secrets/aws/credentials profile: default . Google Storage . --- logging: format: json level: WARN output: \"-\" database: type: \"postgres\" postgres: connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: gs gs: credentials_file: /secrets/lakefs-service-account.json . MinIO . --- logging: format: json level: WARN output: \"-\" database: type: \"postgres\" postgres: connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: s3 s3: force_path_style: true endpoint: http://localhost:9000 discover_bucket_region: false credentials: access_key_id: minioadmin secret_access_key: minioadmin . Azure blob storage . --- logging: format: json level: WARN output: \"-\" database: type: \"cosmosdb\" cosmosdb: key: \"ExampleReadWriteKeyMD7nkPOWgV7d4BUjzLw==\" endpoint: \"https://lakefs-account.documents.azure.com:443/\" database: \"lakefs-db\" container: \"lakefs-container\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: azure azure: storage_account: exampleStorageAcount storage_access_key: ExampleAcessKeyMD7nkPOWgV7d4BUjzLw== . ",
    "url": "/v1.4/reference/configuration.html#example-configurations",
    
    "relUrl": "/reference/configuration.html#example-configurations"
  },"113": {
    "doc": "Contributing to lakeFS",
    "title": "Contributing to lakeFS",
    "content": "Thank you for your interest in contributing to our project. Whether it’s a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure that we have all the necessary information to effectively respond to your bug report or contribution. Don’t know where to start? Reach out on the #dev channel on our Slack and we will help you get started. We also recommend this free series about contributing to OSS projects. ",
    "url": "/v1.4/project/contributing.html",
    
    "relUrl": "/project/contributing.html"
  },"114": {
    "doc": "Contributing to lakeFS",
    "title": "Getting Started",
    "content": "Before you get started, we kindly ask that you: . | Check out the code of conduct. | Sign the lakeFS CLA when making your first pull request (individual / corporate) | Submit any security issues directly to security@treeverse.io. | Contributions should have an associated GitHub issue. | Before making major contributions, please reach out to us on the #dev channel on Slack. We will make sure no one else is working on the same feature. | . ",
    "url": "/v1.4/project/contributing.html#getting-started",
    
    "relUrl": "/project/contributing.html#getting-started"
  },"115": {
    "doc": "Contributing to lakeFS",
    "title": "Setting up an Environment",
    "content": "This section was tested on macOS and Linux (Fedora 32, Ubuntu 20.04) - Your mileage may vary . Our Go release workflow holds the Go and Node.js versions we currently use under go-version and node-version compatibly. The Java workflows use Maven 3.8.x (but any recent version of Maven should work). | Install the required dependencies for your OS: . | Git | GNU make (probably best to install from your OS package manager such as apt or brew) | Docker | Go | Node.js &amp; npm | Maven to build and test Spark client codes. | Java 8 . | Apple M1 users can install this from Azul Zulu Builds for Java JDK. Builds for Intel-based Macs are available from java.com. | . | Optional - PostgreSQL 11 (useful for running and debugging locally) | Optional - Rust &amp; Cargo (only needed if you want to build the Delta Lake diff plugin) | Optional - Buf CLI (only needed if you like to update Protocol Buffer files) | . | Clone the repository from GitHub. This gives you read-only access to the repository. To contribute, see the next section. | Build the project: . make build . Note: make build won’t work for Windows users. | Make sure tests are passing. The following should not return any errors: . make test . | . ",
    "url": "/v1.4/project/contributing.html#setting-up-an-environment",
    
    "relUrl": "/project/contributing.html#setting-up-an-environment"
  },"116": {
    "doc": "Contributing to lakeFS",
    "title": "Before creating a pull request",
    "content": ". | Review this document in full. | Make sure there’s an open issue on GitHub that this pull request addresses, and that it isn’t labeled x/wontfix. | Fork the lakeFS repository. | If you’re adding new functionality, create a new branch named feature/&lt;DESCRIPTIVE NAME&gt;. | If you’re fixing a bug, create a new branch named fix/&lt;DESCRIPTIVE NAME&gt;-&lt;ISSUE NUMBER&gt;. | . ",
    "url": "/v1.4/project/contributing.html#before-creating-a-pull-request",
    
    "relUrl": "/project/contributing.html#before-creating-a-pull-request"
  },"117": {
    "doc": "Contributing to lakeFS",
    "title": "Testing your change",
    "content": "Once you’ve made the necessary changes to the code, make sure the tests pass: . Run unit tests: . make test . Check that linting rules are passing. make checks-validator . You will need GNU diff to run this. On the macOS it can be installed with brew install diffutils . lakeFS uses go fmt as a style guide for Go code. Run system-tests: . make system-tests . Want to dive deeper into our system tests infrastructure? Need to debug the tests? Follow this documentation. ",
    "url": "/v1.4/project/contributing.html#testing-your-change",
    
    "relUrl": "/project/contributing.html#testing-your-change"
  },"118": {
    "doc": "Contributing to lakeFS",
    "title": "Submitting a pull request",
    "content": "Open a GitHub pull request with your change. The PR description should include a brief explanation of your change. You should also mention the related GitHub issue. If the issue should be automatically closed after the merge, please link it to the PR. After submitting your pull request, GitHub Actions will automatically run tests on your changes and make sure that your updated code builds and runs on Go 1.19.x. Check back shortly after submitting your pull request to make sure that your code passes these checks. If any of the checks come back with a red X, then do your best to address the errors. A developer from our team will review your pull request, and may request some changes to it. After the request is approved, it will be merged to our main branch. ",
    "url": "/v1.4/project/contributing.html#submitting-a-pull-request",
    
    "relUrl": "/project/contributing.html#submitting-a-pull-request"
  },"119": {
    "doc": "Contributing to lakeFS",
    "title": "Documentation",
    "content": "Any contribution to the docs, whether it is in conjunction with a code contribution or as a standalone, is appreciated. Documentation of features and changes in behaviour should be included in the pull request. You can create separate pull requests for documentation changes only. To learn how to contribute to the lakeFS documentation see this page, which also includes details on how to build the documentation locally. ",
    "url": "/v1.4/project/contributing.html#documentation",
    
    "relUrl": "/project/contributing.html#documentation"
  },"120": {
    "doc": "Contributing to lakeFS",
    "title": "CHANGELOG.md",
    "content": "Any user-facing change should be labeled with include-changelog. The PR title should contain a concise summary of the feature or fix and the description should have the GitHub issue number. When we publish a new version of lakeFS, we will add this to the relevant version section of the changelog. If the change should not be included in the changelog, label it with exclude-changelog. ",
    "url": "/v1.4/project/contributing.html#changelogmd",
    
    "relUrl": "/project/contributing.html#changelogmd"
  },"121": {
    "doc": "Copying data to/from lakeFS",
    "title": "Copying data to/from lakeFS",
    "content": " ",
    "url": "/v1.4/howto/copying.html",
    
    "relUrl": "/howto/copying.html"
  },"122": {
    "doc": "Copying data to/from lakeFS",
    "title": "Table of contents",
    "content": ". | Using DistCp | Using Rclone | . ",
    "url": "/v1.4/howto/copying.html#table-of-contents",
    
    "relUrl": "/howto/copying.html#table-of-contents"
  },"123": {
    "doc": "Copying data to/from lakeFS",
    "title": "Using DistCp",
    "content": "Apache Hadoop DistCp (distributed copy) is a tool used for large inter/intra-cluster copying. You can easily use it with your lakeFS repositories. Note . In the following examples, we set AWS credentials on the command line for clarity. In production, you should set these properties using one of Hadoop’s standard ways of Authenticating with S3. Between lakeFS repositories . You can use DistCP to copy between two different lakeFS repositories. Replace the access key pair with your lakeFS access key pair: . hadoop distcp \\ -Dfs.s3a.path.style.access=true \\ -Dfs.s3a.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\ -Dfs.s3a.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\ -Dfs.s3a.endpoint=\"https://lakefs.example.com\" \\ \"s3a://example-repo-1/main/example-file.parquet\" \\ \"s3a://example-repo-2/main/example-file.parquet\" . Between S3 buckets and lakeFS . To copy data from an S3 bucket to a lakeFS repository, use Hadoop’s per-bucket configuration. In the following examples, replace the first access key pair with your lakeFS key pair, and the second one with your AWS IAM key pair: . From S3 to lakeFs . hadoop distcp \\ -Dfs.s3a.path.style.access=true \\ -Dfs.s3a.bucket.example-repo.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\ -Dfs.s3a.bucket.example-repo.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\ -Dfs.s3a.bucket.example-repo.endpoint=\"https://lakefs.example.com\" \\ -Dfs.s3a.bucket.example-bucket.access.key=\"AKIAIOSFODNN3EXAMPLE\" \\ -Dfs.s3a.bucket.example-bucket.secret.key=\"wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY\" \\ \"s3a://example-bucket/example-file.parquet\" \\ \"s3a://example-repo/main/example-file.parquet\" . From lakeFS to S3 . hadoop distcp \\ -Dfs.s3a.path.style.access=true \\ -Dfs.s3a.bucket.example-repo.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\ -Dfs.s3a.bucket.example-repo.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\ -Dfs.s3a.bucket.example-repo.endpoint=\"https://lakefs.example.com\" \\ -Dfs.s3a.bucket.example-bucket.access.key=\"AKIAIOSFODNN3EXAMPLE\" \\ -Dfs.s3a.bucket.example-bucket.secret.key=\"wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY\" \\ \"s3a://example-repo/main/myfile\" \\ \"s3a://example-bucket/myfile\" . ",
    "url": "/v1.4/howto/copying.html#using-distcp",
    
    "relUrl": "/howto/copying.html#using-distcp"
  },"124": {
    "doc": "Copying data to/from lakeFS",
    "title": "Using Rclone",
    "content": "Rclone is a command line program to sync files and directories between cloud providers. To use it with lakeFS, create an Rclone remote as describe below and then use it as you would any other Rclone remote. Creating a remote for lakeFS in Rclone . To add the remote to Rclone, choose one of the following options: . Option 1: Add an entry in your Rclone configuration file . | Find the path to your Rclone configuration file and copy it for the next step. rclone config file # output: # Configuration file is stored at: # /home/myuser/.config/rclone/rclone.conf . | If your lakeFS access key is already set in an AWS profile or environment variables, run the following command, replacing the endpoint property with your lakeFS endpoint: . cat &lt;&lt;EOT &gt;&gt; /home/myuser/.config/rclone/rclone.conf [lakefs] type = s3 provider = Other endpoint = https://lakefs.example.com no_check_bucket = true EOT . | Otherwise, also include your lakeFS access key pair in the Rclone configuration file: . cat &lt;&lt;EOT &gt;&gt; /home/myuser/.config/rclone/rclone.conf [lakefs] type = s3 provider = Other env_auth = false access_key_id = AKIAIOSFODNN7EXAMPLE secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY endpoint = https://lakefs.example.com no_check_bucket = true EOT . | . Option 2: Use the Rclone interactive config command . Run this command and follow the instructions: . rclone config . Choose AWS S3 as your type of storage, and enter your lakeFS endpoint as your S3 endpoint. You will have to choose whether you use your environment for authentication (recommended), or enter the lakeFS access key pair into the Rclone configuration. Select “Edit advanced config” and accept defaults for all values except no_check_bucket: . If set, don't attempt to check the bucket exists or create it. This can be useful when trying to minimize the number of transactions Rclone carries out, if you know the bucket exists already. This might also be needed if the user you're using doesn't have bucket creation permissions. Before v1.52.0, this would have passed silently due to a bug. Enter a boolean value (true or false). Press Enter for the default (\"false\"). no_check_bucket&gt; yes . Syncing S3 and lakeFS . rclone sync mys3remote://mybucket/path/ lakefs:example-repo/main/path . Syncing a local directory and lakeFS . rclone sync /home/myuser/path/ lakefs:example-repo/main/path . ",
    "url": "/v1.4/howto/copying.html#using-rclone",
    
    "relUrl": "/howto/copying.html#using-rclone"
  },"125": {
    "doc": "In Test",
    "title": "In Test",
    "content": "As part of our routine work with data we develop new code, improve and upgrade old code, upgrade infrastructures, and test new technologies. lakeFS enables a safe test environment on your data lake without the need to copy or mock data, work on the pipelines or involve DevOps. Creating a branch provides you an isolated environment with a snapshot of your repository (any part of your data lake you chose to manage on lakeFS). While working on your own branch in isolation, all other data users will be looking at the repository’s main branch. They can’t see your changes, and you don’t see changes to main done after you created the branch. No worries, no data duplication is done, it’s all metadata management behind the scenes. Let’s look at 2 examples of a test environment and their branching models. Example 1: Upgrading Spark and using Reset action . You installed the latest version of Apache Spark. As a first step you’ll test your Spark jobs to see that the upgrade doesn’t have any undesired side effects. For this purpose, you may create a branch (testing-spark-3.0) which will only be used to test the Spark upgrade, and discarded later. Jobs may run smoothly (the theoretical possibility exists!), or they may fail halfway through, leaving you with some intermediate partitions, data and metadata. In this case, you can simply reset the branch to its original state, without worrying about the intermediate results of your last experiment, and perform another (hopefully successful) test in an isolated branch. Reset actions are atomic and immediate, so no manual cleanup is required. Once testing is completed, and you have achieved the desired result, you can delete this experimental branch, and all data not used on any other branch will be deleted with it. Creating a testing branch: . lakectl branch create \\ lakefs://example-repo/testing-spark-3 \\ --source lakefs://example-repo/main # output: # created branch 'testing-spark-3' . Resetting changes to a branch: . lakectl branch reset lakefs://example-repo/testing-spark-3 # are you sure you want to reset all uncommitted changes?: y█ . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. Example 2: Collaborate &amp; Compare - Which option is better? . Easily compare by testing which one performs better on your data set. Examples may be: . | Different computation tools, e.g Spark vs. Presto | Different compression algorithms | Different Spark configurations | Different code versions of an ETL | . Run each experiment on its own independent branch, while the main remains untouched. Once both experiments are done, create a comparison query (using Hive or Presto or any other tool of your choice) to compare data characteristics, performance or any other metric you see fit. With lakeFS you don’t need to worry about creating data paths for the experiments, copying data, and remembering to delete it. It’s substantially easier to avoid errors and maintain a clean lake after. Reading from and comparing branches using Spark: . val dfExperiment1 = sc.read.parquet(\"s3a://example-repo/experiment-1/events/by-date\") val dfExperiment2 = sc.read.parquet(\"s3a://example-repo/experiment-2/events/by-date\") dfExperiment1.groupBy(\"...\").count() dfExperiment2.groupBy(\"...\").count() // now we can compare the properties of the data itself . ",
    "url": "/v1.4/understand/data_lifecycle_management/data-devenv.html",
    
    "relUrl": "/understand/data_lifecycle_management/data-devenv.html"
  },"126": {
    "doc": "Delta Lake",
    "title": "Using lakeFS with Delta Lake",
    "content": "Delta Lake Delta Lake is an open-source storage framework designed to improve performance and provide transactional guarantees to data lake tables. Because lakeFS is format-agnostic, you can save data in Delta format within a lakeFS repository and benefit from the advantages of both technologies. Specifically: . | ACID operations can span across many Delta tables. | CI/CD hooks can validate Delta table contents, schema, or even referential integrity. | lakeFS supports zero-copy branching for quick experimentation with full isolation. | . ",
    "url": "/v1.4/integrations/delta.html#using-lakefs-with-delta-lake",
    
    "relUrl": "/integrations/delta.html#using-lakefs-with-delta-lake"
  },"127": {
    "doc": "Delta Lake",
    "title": "Table of contents",
    "content": ". | Using Delta Lake with lakeFS from Apache Spark | Using Delta Lake with lakeFS from Python | Viewing Delta Lake table changes in lakeFS BETA | Best Practices | Further Reading | . ",
    "url": "/v1.4/integrations/delta.html#table-of-contents",
    
    "relUrl": "/integrations/delta.html#table-of-contents"
  },"128": {
    "doc": "Delta Lake",
    "title": "Using Delta Lake with lakeFS from Apache Spark",
    "content": "Given the native integration between Delta Lake and Spark, it’s most common that you’ll interact with Delta tables in a Spark environment. To configure a Spark environment to read from and write to a Delta table within a lakeFS repository, you need to set the proper credentials and endpoint in the S3 Hadoop configuration, like you’d do with any Spark environment. Once set, you can interact with Delta tables using regular Spark path URIs. Make sure that you include the lakeFS repository and branch name: . df.write.format(\"delta\").save(\"s3a://&lt;repo-name&gt;/&lt;branch-name&gt;/path/to/delta-table\") . Note: If using the Databricks Analytics Platform, see the integration guide for configuring a Databricks cluster to use lakeFS. To see the integration in action see this notebook in the lakeFS Samples Repository. ",
    "url": "/v1.4/integrations/delta.html#using-delta-lake-with-lakefs-from-apache-spark",
    
    "relUrl": "/integrations/delta.html#using-delta-lake-with-lakefs-from-apache-spark"
  },"129": {
    "doc": "Delta Lake",
    "title": "Using Delta Lake with lakeFS from Python",
    "content": "The delta-rs library provides bindings for Python. This means that you can use Delta Lake and lakeFS directly from Python without needing Spark. Integration is done through the lakeFS S3 Gateway . The documentation for the deltalake Python module details how to read, write, and query Delta Lake tables. To use it with lakeFS use an s3a path for the table based on your repository and branch (for example, s3a://delta-lake-demo/main/my_table/) and specify the following storage_options: . storage_options = {\"AWS_ENDPOINT\": &lt;your lakeFS endpoint&gt;, \"AWS_ACCESS_KEY_ID\": &lt;your lakeFS access key&gt;, \"AWS_SECRET_ACCESS_KEY\": &lt;your lakeFS secret key&gt;, \"AWS_REGION\": \"us-east-1\", \"AWS_S3_ALLOW_UNSAFE_RENAME\": \"true\" } . If your lakeFS is not using HTTPS (for example, you’re just running it locally) then add the option . \"AWS_STORAGE_ALLOW_HTTP\": \"true\" . To see the integration in action see this notebook in the lakeFS Samples Repository. ",
    "url": "/v1.4/integrations/delta.html#using-delta-lake-with-lakefs-from-python",
    
    "relUrl": "/integrations/delta.html#using-delta-lake-with-lakefs-from-python"
  },"130": {
    "doc": "Delta Lake",
    "title": "Viewing Delta Lake table changes in lakeFS BETA",
    "content": "Using lakeFS you can . | Compare different versions of Delta Lake tables | Get a detailed view of all Delta Lake table operations performed since the tables diverged. | . For example, comparing branches dev and main, we can see that the movies table has changed on dev since the branches diverged. Expanding the delete operation, we learn that all movies with a rating &lt; 4 were deleted from the table on the dev branch. Note: The diff is available as long as the table history in Delta is retained (30 days by default). A delta lake table history is derived from the delta log JSON files. Installing the Delta Lake diff plugin . To enable the Delta Lake diff feature, you need to install a plugin on the lakeFS server. You will find the plugin binary in the release tarball (versions &gt;= 0.97.3). Rename the delta_diff binary to delta and put it under ~/.lakefs/plugins/diff on the machine where lakeFS is running. You can customize the location of the Delta Lake diff plugin by changing the diff.delta.plugin and plugin.properties.&lt;plugin name&gt;.path configurations in the .lakefs.yaml file. Notice: If you’re using the lakeFS docker image, the plugin is installed by default. ",
    "url": "/v1.4/integrations/delta.html#viewing-delta-lake-table-changes-in-lakefs-beta",
    
    "relUrl": "/integrations/delta.html#viewing-delta-lake-table-changes-in-lakefs-beta"
  },"131": {
    "doc": "Delta Lake",
    "title": "Best Practices",
    "content": "Production workflows should ideally write to a single lakeFS branch that could then be safely merged into main. This is because the Delta log is an auto-generated sequence of text files used to keep track of transactions on a Delta table sequentially. Writing to one Delta table from multiple lakeFS branches is possible, but note that it would result in conflicts if later attempting to merge one branch into the other. When running lakeFS inside your VPC (on AWS) . When lakeFS runs inside your private network, your Databricks cluster needs to be able to access it. This can be done by setting up a VPC peering between the two VPCs (the one where lakeFS runs and the one where Databricks runs). For this to work on Delta Lake tables, you would also have to disable multi-cluster writes with: . spark.databricks.delta.multiClusterWrites.enabled false . Using multi cluster writes (on AWS) . When using multi-cluster writes, Databricks overrides Delta’s S3-commit action. The new action tries to contact lakeFS from servers on Databricks’ own AWS account, which of course won’t be able to access your private network. So, if you must use multi-cluster writes, you’ll have to allow access from Databricks’ AWS account to lakeFS. If you are trying to achieve that, please reach out on Slack and the community will try to assist. ",
    "url": "/v1.4/integrations/delta.html#best-practices",
    
    "relUrl": "/integrations/delta.html#best-practices"
  },"132": {
    "doc": "Delta Lake",
    "title": "Further Reading",
    "content": "See Guaranteeing Consistency in Your Delta Lake Tables With lakeFS post on the lakeFS blog to learn how to guarantee data quality in a Delta table by utilizing lakeFS branches. ",
    "url": "/v1.4/integrations/delta.html#further-reading",
    
    "relUrl": "/integrations/delta.html#further-reading"
  },"133": {
    "doc": "Delta Lake",
    "title": "Delta Lake",
    "content": " ",
    "url": "/v1.4/integrations/delta.html",
    
    "relUrl": "/integrations/delta.html"
  },"134": {
    "doc": "Dremio",
    "title": "Using lakeFS with Dremio",
    "content": "Dremio is a next-generation data lake engine that liberates your data with live, interactive queries directly on cloud data lake storage, including S3 and lakeFS. ",
    "url": "/v1.4/integrations/dremio.html#using-lakefs-with-dremio",
    
    "relUrl": "/integrations/dremio.html#using-lakefs-with-dremio"
  },"135": {
    "doc": "Dremio",
    "title": "Configuration",
    "content": "Starting from version 3.2.3, Dremio supports Minio as an experimental S3-compatible plugin. Similarly, you can connect lakeFS with Dremio. Suppose you already have both lakeFS and Dremio deployed, and want to use Dremio to query your data in the lakeFS repositories. You can follow the steps listed below to configure on Dremio UI: . | click Add Data Lake. | Under File Stores, choose Amazon S3. | Under Advanced Options, check Enable compatibility mode (experimental). | Under Advanced Options &gt; Connection Properties, add fs.s3a.path.style.access and set the value to true. | Under Advanced Options &gt; Connection Properties, add fs.s3a.endpoint and set lakeFS S3 endpoint to the value. | Under the General tab, specify the access_key_id and secret_access_key provided by lakeFS server. | Click Save, and now you should be able to browse lakeFS repositories on Dremio. | . ",
    "url": "/v1.4/integrations/dremio.html#configuration",
    
    "relUrl": "/integrations/dremio.html#configuration"
  },"136": {
    "doc": "Dremio",
    "title": "Dremio",
    "content": " ",
    "url": "/v1.4/integrations/dremio.html",
    
    "relUrl": "/integrations/dremio.html"
  },"137": {
    "doc": "DuckDB",
    "title": "Using lakeFS with DuckDB",
    "content": "DuckDB is an in-process SQL OLAP database management system. You can access data in lakeFS from DuckDB, as well as use DuckDB from within the web interface of lakeFS . ",
    "url": "/v1.4/integrations/duckdb.html#using-lakefs-with-duckdb",
    
    "relUrl": "/integrations/duckdb.html#using-lakefs-with-duckdb"
  },"138": {
    "doc": "DuckDB",
    "title": "Table of contents",
    "content": ". | Accessing lakeFS from DuckDB . | Configuration | Querying Data | Writing Data | . | Using DuckDB in the lakeFS web UI | . ",
    "url": "/v1.4/integrations/duckdb.html#table-of-contents",
    
    "relUrl": "/integrations/duckdb.html#table-of-contents"
  },"139": {
    "doc": "DuckDB",
    "title": "Accessing lakeFS from DuckDB",
    "content": "Configuration . Querying data in lakeFS from DuckDB is similar to querying data in S3 from DuckDB. It is done using the httpfs extension connecting to the S3 Gateway that lakeFS provides. If not loaded already, install and load the HTTPFS extension: . INSTALL httpfs; LOAD httpfs; . Then run the following to configure the connection. SET s3_region='us-east-1'; SET s3_endpoint='lakefs.example.com'; SET s3_access_key_id='AKIAIOSFODNN7EXAMPLE'; SET s3_secret_access_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'; SET s3_url_style='path'; -- Uncomment in case the endpoint listen on non-secure, for example running lakeFS locally. -- SET s3_use_ssl=false; . | s3_endpoint is the host (and port, if necessary) of your lakeFS server | s3_access_key_id and s3_secret_access_key are the access credentials for your lakeFS user | s3_url_style needs to be set to path | s3_region is the S3 region on which your bucket resides. If local storage, or not S3, then just set it to us-east-1. | . Querying Data . Once configured, you can query data using the lakeFS S3 Gateway using the following URI pattern: . s3://&lt;REPOSITORY NAME&gt;/&lt;REFERENCE ID&gt;/&lt;PATH TO DATA&gt; . Since the S3 Gateway implemenets all S3 functionality required by DuckDB, you can query using globs and patterns, including support for Hive-partitioned data. Example: . SELECT * FROM parquet_scan('s3://example-repo/main/data/population/by-region/*.parquet', HIVE_PARTITIONING=1) ORDER BY name; . Writing Data . No special configuration required for writing to a branch. Assuming the configuration above and write permissions to a dev branch, a write operation would look like any DuckDB write: . CREATE TABLE sampled_population AS SELECT * FROM parquet_scan('s3://example-repo/main/data/population/by-region/*.parquet', HIVE_PARTITIONING=1) USING SAMPLE reservoir(50000 ROWS) REPEATABLE (100); COPY sampled_population TO 's3://example-repo/main/data/population/sample.parquet'; -- actual write happens here . ",
    "url": "/v1.4/integrations/duckdb.html#accessing-lakefs-from-duckdb",
    
    "relUrl": "/integrations/duckdb.html#accessing-lakefs-from-duckdb"
  },"140": {
    "doc": "DuckDB",
    "title": "Using DuckDB in the lakeFS web UI",
    "content": "The lakeFS web UI includes DuckDB in the Object viewer page. Using this you can query objects in lakeFS directly using a lakefs path: . lakefs://&lt;repository&gt;/&lt;branch&gt;/object/path/foo.parquet . The DuckDB query editor is provided by DuckDB WASM. It renders and provides querying capabilities for any objects of the following types: . | Parquet | CSV | TSV | . ",
    "url": "/v1.4/integrations/duckdb.html#using-duckdb-in-the-lakefs-web-ui",
    
    "relUrl": "/integrations/duckdb.html#using-duckdb-in-the-lakefs-web-ui"
  },"141": {
    "doc": "DuckDB",
    "title": "DuckDB",
    "content": " ",
    "url": "/v1.4/integrations/duckdb.html",
    
    "relUrl": "/integrations/duckdb.html"
  },"142": {
    "doc": "ETL Testing Environment",
    "title": "ETL Testing with Isolated Dev/Test Environments",
    "content": " ",
    "url": "/v1.4/understand/use_cases/etl_testing.html#etl-testing-with-isolated-devtest-environments",
    
    "relUrl": "/understand/use_cases/etl_testing.html#etl-testing-with-isolated-devtest-environments"
  },"143": {
    "doc": "ETL Testing Environment",
    "title": "Table of contents",
    "content": ". | Why are multiple environments so important? | How does lakeFS help with Dev/Test environments? | Using branches as development and testing environments | Try it out! Creating Dev/Test Environments with lakeFS for ETL Testing | Further Reading | . ",
    "url": "/v1.4/understand/use_cases/etl_testing.html#table-of-contents",
    
    "relUrl": "/understand/use_cases/etl_testing.html#table-of-contents"
  },"144": {
    "doc": "ETL Testing Environment",
    "title": "Why are multiple environments so important?",
    "content": "When working with a data lake, it’s useful to have replicas of your production environment. These replicas allow you to test these ETLs and understand changes to your data without impacting the consumers of the production data. Running ETL and transformation jobs directly in production without proper ETL testing presents a huge risk of having data issues flow into dashboards, ML models, and other consumers sooner or later. The most common approach to avoid making changes directly in production is to create and maintain multiple data environments and perform ETL testing on them. Dev environments give you a space in which to develop the data pipelines and test environment where pipeline changes are tested before pushing it to production. Without lakeFS, the challenge with this approach is that it can be time-consuming and costly to maintain these separate dev/test environments to enable thorough effective ETL testing. And for larger teams it forces multiple people to share these environments, requiring significant coordination. Depending on the size of the data involved there can also be high costs due to the duplication of data. ",
    "url": "/v1.4/understand/use_cases/etl_testing.html#why-are-multiple-environments-so-important",
    
    "relUrl": "/understand/use_cases/etl_testing.html#why-are-multiple-environments-so-important"
  },"145": {
    "doc": "ETL Testing Environment",
    "title": "How does lakeFS help with Dev/Test environments?",
    "content": "lakeFS makes creating isolated dev/test environments for ETL testing quick and cheap. lakeFS uses Copy-on-Write which means that there is no duplication of data when you create a new environment. This frees you from spending time on environment maintenance and makes it possible to create as many environments as needed. In a lakeFS repository, data is always located on a branch. You can think of each branch in lakeFS as its own environment. This is because branches are isolated, meaning changes on one branch have no effect other branches. Objects that remain unchanged between two branches are not copied, but rather shared to both branches via metadata pointers that lakeFS manages. If you make a change on one branch and want it reflected on another, you can perform a merge operation to update one branch with the changes from another. ",
    "url": "/v1.4/understand/use_cases/etl_testing.html#how-does-lakefs-help-with-devtest-environments",
    
    "relUrl": "/understand/use_cases/etl_testing.html#how-does-lakefs-help-with-devtest-environments"
  },"146": {
    "doc": "ETL Testing Environment",
    "title": "Using branches as development and testing environments",
    "content": "The key difference when using lakeFS for isolated data environments is that you can create them immediately before testing a change. And once new data is merged into production, you can delete the branch - effectively deleting the old environment. This is different from creating a long-living test environment used as a staging area to test all the updates. With lakeFS, we create a new branch for each change to production that we want to make. One benefit of this is the ability to test multiple changes at one time. ",
    "url": "/v1.4/understand/use_cases/etl_testing.html#using-branches-as-development-and-testing-environments",
    
    "relUrl": "/understand/use_cases/etl_testing.html#using-branches-as-development-and-testing-environments"
  },"147": {
    "doc": "ETL Testing Environment",
    "title": "Try it out! Creating Dev/Test Environments with lakeFS for ETL Testing",
    "content": "lakeFS supports UI, CLI (lakectl commandline utility) and several clients for the API to run the Git-like operations. Let us explore how to create dev/test environments using each of these options below. There are two ways that you can try out lakeFS: . | The lakeFS Playground on lakeFS Cloud - fully managed lakeFS with a 30-day free trial | Local Docker-based quickstart and samples | . You can also deploy lakeFS locally or self-managed on your cloud of choice. Using lakeFS Playground on lakeFS Cloud . In this tutorial, we will use a lakeFS playground environment to create dev/test data environments for ETL testing. This allows you to spin up a lakeFS instance in a click, create different data environments by simply branching out of your data repository and develop &amp; test data pipelines in these isolated branches. First, let us spin up a playground instance. Once you have a live environment, login to your instance with access and secret keys. Then, you can work with the sample data repository my-repo that is created for you. Click on my-repo and notice that by default, the repo has a main branch created and sample_data preloaded to work with. You can create a new branch (say, test-env) by going to the Branches tab and clicking Create Branch. Once it is successful, you will see two branches under the repo: main and test-env. Now you can add, modify or delete objects under the test-env branch without affecting the data in the main branch. Trying out lakeFS with Docker and Jupyter Notebooks . This use case shows how to create dev/test data environments for ETL testing using lakeFS branches. The following tutorial provides a lakeFS environment, a Jupyter notebook, and Python lakefs_client API to demonstrate integration of lakeFS with Spark. You can run this tutorial on your local machine. Follow the tutorial video below to get started with the playground and Jupyter notebook, or follow the instructions on this page. Prerequisites . Before getting started, you will need Docker installed on your machine. Running lakeFS and Jupyter Notebooks . Follow along the steps below to create dev/test environment with lakeFS. | Start by cloning the lakeFS samples Git repository: . git clone https://github.com/treeverse/lakeFS-samples.git . cd lakeFS-samples . | Run following commands to download and run Docker container which includes Python, Spark, Jupyter Notebook, JDK, Hadoop binaries, lakeFS Python client and Airflow (Docker image size is around 4.5GB): . git submodule init git submodule update docker compose up . | . Open the local Jupyter Notebook and go to the spark-demo.ipynb notebook. Configuring lakeFS Python Client . Setup lakeFS access credentials for the lakeFS instance running. The defaults for these that the samples repo Docker Compose uses are shown here: . lakefsAccessKey = 'AKIAIOSFODNN7EXAMPLE' lakefsSecretKey = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' lakefsEndPoint = 'http://lakefs:8000' . Next, setup the storage namespace to a location in the bucket you have configured. The storage namespace is a location in the underlying storage where data for this repository will be stored. storageNamespace = 's3://example/' . You can use lakeFS through the UI, API or lakectl commandline. For this use-case, we use python lakefs_client to run lakeFS core operations. import lakefs_client from lakefs_client import models from lakefs_client.client import LakeFSClient # lakeFS credentials and endpoint configuration = lakefs_client.Configuration() configuration.username = lakefsAccessKey configuration.password = lakefsSecretKey configuration.host = lakefsEndPoint client = LakeFSClient(configuration) . lakeFS can be configured to work with Spark in two ways: . | Access lakeFS using the S3-compatible API | Access lakeFS using the lakeFS-specific Hadoop FileSystem | . Upload the Sample Data to Main Branch . To upload an object to the my-repo, use the following command. import os contentToUpload = open('/data/lakefs_test.csv', 'rb') client.objects.upload_object( repository=\"my-repo\", branch=\"main\", path=fileName, content=contentToUpload) . Once uploaded, commit the changes to the main branch and attach some metadata to the commit as well. client.commits.commit( repository=\"my-repo\", branch=\"main\", commit_creation=models.CommitCreation( message='Added my first object!', metadata={'using': 'python_api'})) . In this example, we use lakeFS S3A gateway to read data from the storage bucket. dataPath = f\"s3a://my-repo/main/lakefs_test.csv\" df = spark.read.csv(dataPath) df.show() . Create a Test Branch . Let us start by creating a new branch test-env on the example repo my-repo. client.branches.create_branch( repository=\"my-repo\", branch_creation=models.BranchCreation( name=\"test-env\", source=\"main\")) . Now we can use Spark to write the csv file from main branch as a Parquet file to the test-env of our lakeFS repo. Suppose we accidentally write the dataframe back to “test-env” branch again, this time in append mode. df.write.mode('overwrite').parquet('s3a://my-repo/test-env/') df.write.mode('append').parquet('s3a://my-repo/test-env/') . What happens if we re-read in the data on both branches and perform a count on the resulting DataFrames? There will be twice as many rows in test-env branch. That is, we accidentally duplicated our data! Oh no! . Data duplication introduce errors into our data analytics, BI and machine learning efforts; hence we would like to avoid duplicating our data. On the main branch however, there is still just the original data - untouched by our Spark code. This shows the utility of branch-based isolated environments with lakeFS. You can safely continue working with the data from main which is unharmed due to lakeFS isolation capabilities. ",
    "url": "/v1.4/understand/use_cases/etl_testing.html#try-it-out-creating-devtest-environments-with-lakefs-for-etl-testing",
    
    "relUrl": "/understand/use_cases/etl_testing.html#try-it-out-creating-devtest-environments-with-lakefs-for-etl-testing"
  },"148": {
    "doc": "ETL Testing Environment",
    "title": "Further Reading",
    "content": ". | Case Study: How Enigma use lakeFS for isolated development and staging environments | ETL Testing: A Practical Guide | Top 5 ETL Testing Challenges - Solved! | . ",
    "url": "/v1.4/understand/use_cases/etl_testing.html#further-reading",
    
    "relUrl": "/understand/use_cases/etl_testing.html#further-reading"
  },"149": {
    "doc": "ETL Testing Environment",
    "title": "ETL Testing Environment",
    "content": " ",
    "url": "/v1.4/understand/use_cases/etl_testing.html",
    
    "relUrl": "/understand/use_cases/etl_testing.html"
  },"150": {
    "doc": "Export Data",
    "title": "Exporting Data",
    "content": "The export operation copies all data from a given lakeFS commit to a designated object store location. For instance, the contents lakefs://example/main might be exported on s3://company-bucket/example/latest. Clients entirely unaware of lakeFS could use that base URL to access latest files on main. Clients aware of lakeFS can continue to use the lakeFS S3 endpoint to access repository files on s3://example/main, as well as other versions and uncommitted versions. Possible use-cases: . | External consumers of data don’t have access to your lakeFS installation. | Some data pipelines in the organization are not fully migrated to lakeFS. | You want to experiment with lakeFS as a side-by-side installation first. | Create copies of your data lake in other regions (taking into account read pricing). | . ",
    "url": "/v1.4/howto/export.html#exporting-data",
    
    "relUrl": "/howto/export.html#exporting-data"
  },"151": {
    "doc": "Export Data",
    "title": "Table of contents",
    "content": ". | Exporting Data With Spark | Success/Failure Indications | Export Rounds (Spark success files) | Example | Exporting Data with Docker | . ",
    "url": "/v1.4/howto/export.html#table-of-contents",
    
    "relUrl": "/howto/export.html#table-of-contents"
  },"152": {
    "doc": "Export Data",
    "title": "Exporting Data With Spark",
    "content": "Using spark-submit . You can use the export main in three different modes: . | Export all the objects from branch example-branch on example-repo repository to S3 location s3://example-bucket/prefix/: .... example-repo s3://example-bucket/prefix/ --branch=example-branch . | Export all the objects from a commit c805e49bafb841a0875f49cd555b397340bbd9b8 on example-repo repository to S3 location s3://example-bucket/prefix/: .... example-repo s3://example-bucket/prefix/ --commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8 . | Export only the diff between branch example-branch and commit c805e49bafb841a0875f49cd555b397340bbd9b8 on example-repo repository to S3 location s3://example-bucket/prefix/: .... example-repo s3://example-bucket/prefix/ --branch=example-branch --prev_commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8 . | . The complete spark-submit command would look as follows: . spark-submit --conf spark.hadoop.lakefs.api.url=https://&lt;LAKEFS_ENDPOINT&gt;/api/v1 \\ --conf spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY_ID&gt; \\ --conf spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_ACCESS_KEY&gt; \\ --packages io.lakefs:lakefs-spark-client_2.12:0.11.0 \\ --class io.treeverse.clients.Main export-app example-repo s3://example-bucket/prefix \\ --branch=example-branch . The command assumes that the Spark cluster has permissions to write to s3://example-bucket/prefix. Otherwise, add spark.hadoop.fs.s3a.access.key and spark.hadoop.fs.s3a.secret.key with the proper credentials. Networking . Spark export communicates with the lakeFS server. Very large repositories may require increasing a read timeout. If you run into timeout errors during communication from the Spark job to lakeFS consider increasing these timeouts: . | Add -c spark.hadoop.lakefs.api.read.timeout_seconds=TIMEOUT_IN_SECONDS (default 10) to allow lakeFS more time to respond to requests. | Add -c spark.hadoop.lakefs.api.connection.timeout_seconds=TIMEOUT_IN_SECONDS (default 10) to wait longer for lakeFS to accept connections. | . Using custom code (Notebook/Spark) . Set up lakeFS Spark metadata client with the endpoint and credentials as instructed in the previous page. The client exposes the Exporter object with three export options: . | Export all the objects at the HEAD of a given branch. Does not include files that were added to that branch but were not committed. | . exportAllFromBranch(branch: String) . | Export ALL objects from a commit: | . exportAllFromCommit(commitID: String) . | Export just the diff between a commit and the HEAD of a branch. This is an ideal option for continuous exports of a branch since it will change only the files that have been changed since the previous commit. exportFrom(branch: String, prevCommitID: String) . | . ",
    "url": "/v1.4/howto/export.html#exporting-data-with-spark",
    
    "relUrl": "/howto/export.html#exporting-data-with-spark"
  },"153": {
    "doc": "Export Data",
    "title": "Success/Failure Indications",
    "content": "When the Spark export operation ends, an additional status file will be added to the root object storage destination. If all files were exported successfully, the file path will be of the form: EXPORT_&lt;commitID&gt;_&lt;ISO-8601-time-UTC&gt;_SUCCESS. For failures: the form will beEXPORT_&lt;commitID&gt;_&lt;ISO-8601-time-UTC&gt;_FAILURE, and the file will include a log of the failed files operations. ",
    "url": "/v1.4/howto/export.html#successfailure-indications",
    
    "relUrl": "/howto/export.html#successfailure-indications"
  },"154": {
    "doc": "Export Data",
    "title": "Export Rounds (Spark success files)",
    "content": "Some files should be exported before others, e.g., a Spark _SUCCESS file exported before other files under the same prefix might send the wrong indication. The export operation may contain several rounds within the same export. A failing round will stop the export of all the files of the next rounds. By default, lakeFS will use the SparkFilter and have 2 rounds for each export. The first round will export any non-Spark _SUCCESS files. Second round will export all Spark’s _SUCCESS files. You may override the default behavior by passing a custom filter to the Exporter. ",
    "url": "/v1.4/howto/export.html#export-rounds-spark-success-files",
    
    "relUrl": "/howto/export.html#export-rounds-spark-success-files"
  },"155": {
    "doc": "Export Data",
    "title": "Example",
    "content": ". | First configure the Exporter instance: . import io.treeverse.clients.{ApiClient, Exporter} import org.apache.spark.sql.SparkSession val endpoint = \"http://&lt;LAKEFS_ENDPOINT&gt;/api/v1\" val accessKey = \"&lt;LAKEFS_ACCESS_KEY_ID&gt;\" val secretKey = \"&lt;LAKEFS_SECRET_ACCESS_KEY&gt;\" val repo = \"example-repo\" val spark = SparkSession.builder().appName(\"I can export\").master(\"local\").getOrCreate() val sc = spark.sparkContext sc.hadoopConfiguration.set(\"lakefs.api.url\", endpoint) sc.hadoopConfiguration.set(\"lakefs.api.access_key\", accessKey) sc.hadoopConfiguration.set(\"lakefs.api.secret_key\", secretKey) // Add any required spark context configuration for s3 val rootLocation = \"s3://company-bucket/example/latest\" val apiClient = new ApiClient(endpoint, accessKey, secretKey) val exporter = new Exporter(spark, apiClient, repo, rootLocation) . | Now you can export all objects from main branch to s3://company-bucket/example/latest: . val branch = \"main\" exporter.exportAllFromBranch(branch) . | Assuming a previous successful export on commit f3c450d8cd0e84ac67e7bc1c5dcde9bef82d8ba7, you can alternatively export just the difference between main branch and the commit: . val branch = \"main\" val commit = \"f3c450d8cd0e84ac67e7bc1c5dcde9bef82d8ba7\" exporter.exportFrom(branch, commit) . | . ",
    "url": "/v1.4/howto/export.html#example",
    
    "relUrl": "/howto/export.html#example"
  },"156": {
    "doc": "Export Data",
    "title": "Exporting Data with Docker",
    "content": "This option is recommended if you don’t have Spark at your tool-set. It doesn’t support distribution across machines, therefore may have a lower performance. Using this method, you can export data from lakeFS to S3 using the export options (in a similar way to the Spark export): . | Export all objects from a branch example-branch on example-repo repository to S3 location s3://destination-bucket/prefix/: .... example-repo s3://destination-bucket/prefix/ --branch=\"example-branch\" . | Export all objects from a commit c805e49bafb841a0875f49cd555b397340bbd9b8 on example-repo repository to S3 location s3://destination-bucket/prefix/: .... example-repo s3://destination-bucket/prefix/ --commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8 . | Export only the diff between branch example-branch and commit c805e49bafb841a0875f49cd555b397340bbd9b8 on example-repo repository to S3 location s3://destination-bucket/prefix/: .... example-repo s3://destination-bucket/prefix/ --branch=\"example-branch\" --prev_commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8 . | . You will need to add the relevant environment variables. The complete docker run command would look like: . docker run \\ -e LAKEFS_ACCESS_KEY_ID=XXX -e LAKEFS_SECRET_ACCESS_KEY=YYY \\ -e LAKEFS_ENDPOINT=https://&lt;LAKEFS_ENDPOINT&gt;/ \\ -e AWS_ACCESS_KEY_ID=XXX -e AWS_SECRET_ACCESS_KEY=YYY \\ treeverse/lakefs-rclone-export:latest \\ example-repo \\ s3://destination-bucket/prefix/ \\ --branch=\"example-branch\" . Note: This feature uses rclone, and specifically rclone sync. This can change the destination path, therefore the s3 destination location must be designated to lakeFS export. ",
    "url": "/v1.4/howto/export.html#exporting-data-with-docker",
    
    "relUrl": "/howto/export.html#exporting-data-with-docker"
  },"157": {
    "doc": "Export Data",
    "title": "Export Data",
    "content": " ",
    "url": "/v1.4/howto/export.html",
    
    "relUrl": "/howto/export.html"
  },"158": {
    "doc": "FAQ",
    "title": "lakeFS Frequently Asked Questions (FAQ)",
    "content": "1. Is lakeFS open-source? . lakeFS is completely free, open-source, and licensed under the Apache 2.0 License. We maintain a public product roadmap and a Slack channel for open discussions. 2. How does lakeFS data versioning work? . lakeFS uses a copy-on-write mechanism to avoid data duplication. For example, creating a new branch is a metadata-only operation: no objects are actually copied. Only when an object changes does lakeFS create another version of the data in the storage. For more information, see Versioning internals. 3. How do I get support for my lakeFS installation? . We are extremely responsive on our Slack channel, and we make sure to prioritize the most pressing issues for the community. For SLA-based support, please contact us at support@treeverse.io. 4. Do you collect data from your active installations? . We collect anonymous usage statistics to understand the patterns of use and to detect product gaps we may have so we can fix them. This is optional and may be turned off by setting stats.enabled to false. See the configuration reference for more details. The data we gather is limited to the following: . | A UUID which is generated when setting up lakeFS for the first time and contains no personal or otherwise identifiable information, | The lakeFS version currently running, | The OS and architecture lakeFS is running on, | Metadata regarding the database used (version, installed extensions and parameters such as DB Timezone and work memory), | Periodic aggregated action counters (e.g. how many “get_object” operations occurred). | . 5. How is lakeFS different from Delta Lake / Hudi / Iceberg? . Delta Lake, Apache Hudi, and Apache Iceberg all define dedicated, structured data formats that allow deletes and upserts. lakeFS is format-agnostic and enables consistent cross-collection versioning of your data using Git-like operations. Read our comparison for a more detailed comparison. 6. What inspired the lakeFS logo? . The Axolotl – a species of salamander, also known as the Mexican Lake Monster or the Peter Pan of the animal kingdom. It’s a magical creature, living in a lake - just like us! :) . copyright . ",
    "url": "/v1.4/understand/faq.html#lakefs-frequently-asked-questions-faq",
    
    "relUrl": "/understand/faq.html#lakefs-frequently-asked-questions-faq"
  },"159": {
    "doc": "FAQ",
    "title": "FAQ",
    "content": " ",
    "url": "/v1.4/understand/faq.html",
    
    "relUrl": "/understand/faq.html"
  },"160": {
    "doc": "Garbage Collection",
    "title": "Garbage Collection",
    "content": "lakeFS Cloud users enjoy a managed garbage collection service, and do not need to run this Spark program. By default, lakeFS keeps all your objects forever. This allows you to travel back in time to previous versions of your data. However, sometimes you may want to remove the objects from the underlying storage completely. Reasons for this include cost-reduction and privacy policies. The garbage collection (GC) job is a Spark program that removes the following from the underlying storage: . | Committed objects that have been deleted (or replaced) in lakeFS, and are considered expired according to rules you define. | Uncommitted objects that are no longer accessible . | For example, objects deleted before ever being committed. | . | . ",
    "url": "/v1.4/howto/garbage-collection/gc.html",
    
    "relUrl": "/howto/garbage-collection/gc.html"
  },"161": {
    "doc": "Garbage Collection",
    "title": "Table of contents",
    "content": ". | Garbage collection rules | How to run the garbage collection job | Garbage collection notes | . ",
    "url": "/v1.4/howto/garbage-collection/gc.html#table-of-contents",
    
    "relUrl": "/howto/garbage-collection/gc.html#table-of-contents"
  },"162": {
    "doc": "Garbage Collection",
    "title": "Garbage collection rules",
    "content": "These rules only apply to objects that have been committed at some point. Without retention rules, only inaccessible uncommitted objects will be removed by the job. Garbage collection rules determine for how long an object is kept in the storage after it is deleted (or replaced) in lakeFS. For every branch, the GC job retains deleted objects for the number of days defined for the branch. In the absence of a branch-specific rule, the default rule for the repository is used. If an object is present in more than one branch ancestry, it is removed only after the retention period has ended for all relevant branches. Example GC rules for a repository: . { \"default_retention_days\": 14, \"branches\": [ {\"branch_id\": \"main\", \"retention_days\": 21}, {\"branch_id\": \"dev\", \"retention_days\": 7} ] } . In the above example, objects will be retained for 14 days after deletion by default. However, if present in the branch main, objects will be retained for 21 days. Objects present only in the dev branch will be retained for 7 days after they are deleted. How to configure garbage collection rules . To define retention rules, either use the lakectl command, the lakeFS web UI, or API: . | CLI | Web UI | . Create a JSON file with your GC rules: . cat &lt;&lt;EOT &gt;&gt; example_repo_gc_rules.json { \"default_retention_days\": 14, \"branches\": [ {\"branch_id\": \"main\", \"retention_days\": 21}, {\"branch_id\": \"dev\", \"retention_days\": 7} ] } EOT . Set the GC rules using lakectl: . lakectl gc set-config lakefs://example-repo -f example_repo_gc_rules.json . From the lakeFS web UI: . | Navigate to the main page of your repository. | Go to Settings -&gt; Garbage Collection. | Click Edit policy and paste your GC rule into the text box as a JSON. | Save your changes. | . ",
    "url": "/v1.4/howto/garbage-collection/gc.html#garbage-collection-rules",
    
    "relUrl": "/howto/garbage-collection/gc.html#garbage-collection-rules"
  },"163": {
    "doc": "Garbage Collection",
    "title": "How to run the garbage collection job",
    "content": "To run the job, use the following spark-submit command (or using your preferred method of running Spark programs). | On AWS | On Azure | On GCP | . spark-submit --class io.treeverse.gc.GarbageCollection \\ --packages org.apache.hadoop:hadoop-aws:2.7.7 \\ -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1 \\ -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\ -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\ -c spark.hadoop.fs.s3a.access.key=&lt;S3_ACCESS_KEY&gt; \\ -c spark.hadoop.fs.s3a.secret.key=&lt;S3_SECRET_KEY&gt; \\ http://treeverse-clients-us-east.s3-website-us-east-1.amazonaws.com/lakefs-spark-client/0.11.0/lakefs-spark-client-assembly-0.11.0.jar \\ example-repo us-east-1 . If you want to access your storage using the account key: . spark-submit --class io.treeverse.gc.GarbageCollection \\ --packages org.apache.hadoop:hadoop-aws:3.2.1 \\ -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1 \\ -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\ -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\ -c spark.hadoop.fs.azure.account.key.&lt;AZURE_STORAGE_ACCOUNT&gt;.dfs.core.windows.net=&lt;AZURE_STORAGE_ACCESS_KEY&gt; \\ http://treeverse-clients-us-east.s3-website-us-east-1.amazonaws.com/lakefs-spark-client/0.11.0/lakefs-spark-client-assembly-0.11.0.jar \\ example-repo . Or, if you want to access your storage using an Azure service principal: . spark-submit --class io.treeverse.gc.GarbageCollection \\ --packages org.apache.hadoop:hadoop-aws:3.2.1 \\ -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1 \\ -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\ -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\ -c spark.hadoop.fs.azure.account.auth.type.&lt;AZURE_STORAGE_ACCOUNT&gt;.dfs.core.windows.net=OAuth \\ -c spark.hadoop.fs.azure.account.oauth.provider.type.&lt;AZURE_STORAGE_ACCOUNT&gt;.dfs.core.windows.net=org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider \\ -c spark.hadoop.fs.azure.account.oauth2.client.id.&lt;AZURE_STORAGE_ACCOUNT&gt;.dfs.core.windows.net=&lt;application-id&gt; \\ -c spark.hadoop.fs.azure.account.oauth2.client.secret.&lt;AZURE_STORAGE_ACCOUNT&gt;.dfs.core.windows.net=&lt;service-credential-key&gt; \\ -c spark.hadoop.fs.azure.account.oauth2.client.endpoint.&lt;AZURE_STORAGE_ACCOUNT&gt;.dfs.core.windows.net=https://login.microsoftonline.com/&lt;directory-id&gt;/oauth2/token \\ http://treeverse-clients-us-east.s3-website-us-east-1.amazonaws.com/lakefs-spark-client/0.11.0/lakefs-spark-client-assembly-0.11.0.jar \\ example-repo . Notes: . | On Azure, GC was tested only on Spark 3.3.0, but may work with other Spark and Hadoop versions. | In case you don’t have hadoop-azure package as part of your environment, you should add the package to your spark-submit with --packages org.apache.hadoop:hadoop-azure:3.2.1 | For GC to work on Azure blob, soft delete should be disabled. | . ⚠️ At the moment, only the “mark” phase of the Garbage Collection is supported for GCP. That is, this program will output a list of expired objects, and you will have to delete them manually. We have concrete plans to extend this support to actually delete the objects. spark-submit --class io.treeverse.gc.GarbageCollection \\ --jars https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar \\ -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1 \\ -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\ -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\ -c spark.hadoop.google.cloud.auth.service.account.enable=true \\ -c spark.hadoop.google.cloud.auth.service.account.json.keyfile=&lt;PATH_TO_JSON_KEYFILE&gt; \\ -c spark.hadoop.fs.gs.project.id=&lt;GCP_PROJECT_ID&gt; \\ -c spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem \\ -c spark.hadoop.fs.AbstractFileSystem.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS \\ -c spark.hadoop.lakefs.gc.do_sweep=false \\ http://treeverse-clients-us-east.s3-website-us-east-1.amazonaws.com/lakefs-spark-client/0.11.0/lakefs-spark-client-assembly-0.11.0.jar \\ example-repo . This program will not delete anything. Instead, it will find all the objects that are safe to delete and save a list containing all their keys, in Parquet format. The list will then be found under the path: . gs://&lt;STORAGE_NAMESPACE&gt;/_lakefs/retention/gc/unified/&lt;RUN_ID&gt;/deleted/ . Note that this is a path in your Google Storage bucket, and not in your lakeFS repository. It is now safe to remove the objects that appear in this list directly from the storage. You will find the list of objects removed by the job in the storage namespace of the repository. It is saved in Parquet format under _lakefs/retention/gc/unified/&lt;RUN_ID&gt;/deleted/. Mark and Sweep stages . You can break the job into two stages: . | Mark: find objects to remove, without actually removing them. | Sweep: remove the objects. | . Mark-only mode . To make GC run the mark stage only, add the following to your spark-submit command: . spark.hadoop.lakefs.gc.do_sweep=false . In mark-only mode, GC will write the keys of the expired objects under: &lt;REPOSITORY_STORAGE_NAMESPACE&gt;/_lakefs/retention/gc/unified/&lt;MARK_ID&gt;/. MARK_ID is generated by the job. You can find it in the driver’s output: . Report for mark_id=gmc6523jatlleurvdm30 path=s3a://example-bucket/_lakefs/retention/gc/unified/gmc6523jatlleurvdm30 . Sweep-only mode . To make GC run the sweep stage only, add the following properties to your spark-submit command: . spark.hadoop.lakefs.gc.do_mark=false spark.hadoop.lakefs.gc.mark_id=&lt;MARK_ID&gt; # Replace &lt;MARK_ID&gt; with the identifier you obtained from a previous mark-only run . ",
    "url": "/v1.4/howto/garbage-collection/gc.html#how-to-run-the-garbage-collection-job",
    
    "relUrl": "/howto/garbage-collection/gc.html#how-to-run-the-garbage-collection-job"
  },"164": {
    "doc": "Garbage Collection",
    "title": "Garbage collection notes",
    "content": ". | In order for an object to be removed, it must not exist on the HEAD of any branch. You should remove stale branches to prevent them from retaining old objects. For example, consider a branch that has been merged to main and has become stale. An object which is later deleted from main will always be present in the stale branch, preventing it from being removed. | lakeFS will never delete objects outside your repository’s storage namespace. In particular, objects that were imported using lakectl import or the UI import wizard will not be affected by GC jobs. | In cases where deleted objects are brought back to life while a GC job is running (for example, by reverting a commit), the objects may or may not be deleted. | Garbage collection does not remove any commits: you will still be able to use commits containing removed objects, but trying to read these objects from lakeFS will result in a 410 Gone HTTP status. | . ",
    "url": "/v1.4/howto/garbage-collection/gc.html#garbage-collection-notes",
    
    "relUrl": "/howto/garbage-collection/gc.html#garbage-collection-notes"
  },"165": {
    "doc": "GCP",
    "title": "Deploy lakeFS on GCP",
    "content": "The instructions given here are for a self-managed deployment of lakeFS on GCP. For a hosted lakeFS service with guaranteed SLAs, please contact us for details of lakeFS Cloud on GCP. When you deploy lakeFS on GCP these are the options available to use: . ",
    "url": "/v1.4/howto/deploy/gcp.html#deploy-lakefs-on-gcp",
    
    "relUrl": "/howto/deploy/gcp.html#deploy-lakefs-on-gcp"
  },"166": {
    "doc": "GCP",
    "title": "Table of contents",
    "content": ". | Create a Database | Run the lakeFS Server | Load balancing | Create the admin user | Create your first repository | . ⏰ Expected deployment time: 25 min . ",
    "url": "/v1.4/howto/deploy/gcp.html#table-of-contents",
    
    "relUrl": "/howto/deploy/gcp.html#table-of-contents"
  },"167": {
    "doc": "GCP",
    "title": "Create a Database",
    "content": "lakeFS requires a PostgreSQL database to synchronize actions on your repositories. We will show you how to create a database on Google Cloud SQL, but you can use any PostgreSQL database as long as it’s accessible by your lakeFS installation. If you already have a database, take note of the connection string and skip to the next step . | Follow the official Google documentation on how to create a PostgreSQL instance. Make sure you’re using PostgreSQL version &gt;= 11. | On the Users tab in the console, create a user. The lakeFS installation will use it to connect to your database. | Choose the method by which lakeFS will connect to your database. Google recommends using the SQL Auth Proxy. | . ",
    "url": "/v1.4/howto/deploy/gcp.html#create-a-database",
    
    "relUrl": "/howto/deploy/gcp.html#create-a-database"
  },"168": {
    "doc": "GCP",
    "title": "Run the lakeFS Server",
    "content": ". | GCE Instance | Docker | GKE | . | Save the following configuration file as config.yaml: . --- database: type: \"postgres\" postgres: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: # replace this with a randomly-generated string: secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: gs # Uncomment the following lines to give lakeFS access to your buckets using a service account: # gs: # credentials_json: [YOUR SERVICE ACCOUNT JSON STRING] . | Download the binary to run on the GCE instance. | Run the lakefs binary on the GCE machine: lakefs --config config.yaml run . Note: it is preferable to run the binary as a service using systemd or your operating system’s facilities. | . To support container-based environments like Google Cloud Run, lakeFS can be configured using environment variables. Here is a docker run command to demonstrate starting lakeFS using Docker: . docker run \\ --name lakefs \\ -p 8000:8000 \\ -e LAKEFS_DATABASE_TYPE=\"postgres\" \\ -e LAKEFS_DATABASE_POSTGRES_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\ -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\ -e LAKEFS_BLOCKSTORE_TYPE=\"gs\" \\ treeverse/lakefs:latest run . See the reference for a complete list of environment variables. You can install lakeFS on Kubernetes using a Helm chart. To install lakeFS with Helm: . | Copy the Helm values file relevant for Google Storage: . secrets: # replace DATABASE_CONNECTION_STRING with the connection string of the database you created in a previous step. # e.g.: postgres://postgres:myPassword@localhost/postgres:5432 databaseConnectionString: [DATABASE_CONNECTION_STRING] # replace this with a randomly-generated string authEncryptSecretKey: [ENCRYPTION_SECRET_KEY] lakefsConfig: | blockstore: type: gs # Uncomment the following lines to give lakeFS access to your buckets using a service account: # gs: # credentials_json: [YOUR SERVICE ACCOUNT JSON STRING] . | Fill in the missing values and save the file as conf-values.yaml. For more configuration options, see our Helm chart README. The lakefsConfig parameter is the lakeFS configuration documented here but without sensitive information. Sensitive information like databaseConnectionString is given through separate parameters, and the chart will inject it into Kubernetes secrets. | In the directory where you created conf-values.yaml, run the following commands: . # Add the lakeFS repository helm repo add lakefs https://charts.lakefs.io # Deploy lakeFS helm install my-lakefs lakefs/lakefs -f conf-values.yaml . my-lakefs is the Helm Release name. | . ",
    "url": "/v1.4/howto/deploy/gcp.html#run-the-lakefs-server",
    
    "relUrl": "/howto/deploy/gcp.html#run-the-lakefs-server"
  },"169": {
    "doc": "GCP",
    "title": "Load balancing",
    "content": "To configure a load balancer to direct requests to the lakeFS servers you can use the LoadBalancer Service type or a Kubernetes Ingress. By default, lakeFS operates on port 8000 and exposes a /_health endpoint that you can use for health checks. 💡 The NGINX Ingress Controller by default limits the client body size to 1 MiB. Some clients use bigger chunks to upload objects - for example, multipart upload to lakeFS using the S3-compatible Gateway or a simple PUT request using the OpenAPI Server. Checkout Nginx documentation for increasing the limit, or an example of Nginx configuration with MinIO. ",
    "url": "/v1.4/howto/deploy/gcp.html#load-balancing",
    
    "relUrl": "/howto/deploy/gcp.html#load-balancing"
  },"170": {
    "doc": "GCP",
    "title": "Create the admin user",
    "content": "When you first open the lakeFS UI, you will be asked to create an initial admin user. | Open http://&lt;lakefs-host&gt;/ in your browser. If you haven’t set up a load balancer, this will likely be http://&lt;instance ip address&gt;:8000/ | On first use, you’ll be redirected to the setup page: . | Follow the steps to create an initial administrator user. Save the credentials you’ve received somewhere safe, you won’t be able to see them again! . | Follow the link and go to the login screen. Use the credentials from the previous step to log in. | . ",
    "url": "/v1.4/howto/deploy/gcp.html#create-the-admin-user",
    
    "relUrl": "/howto/deploy/gcp.html#create-the-admin-user"
  },"171": {
    "doc": "GCP",
    "title": "Create your first repository",
    "content": ". | Use the credentials from the previous step to log in | Click Create Repository and choose Blank Repository. | Under Storage Namespace, enter a path to your desired location on the object store. This is where data written to this repository will be stored. | Click Create Repository | You should now have a configured repository, ready to use! . | . Congratulations! Your environment is now ready 🤩 . ",
    "url": "/v1.4/howto/deploy/gcp.html#create-your-first-repository",
    
    "relUrl": "/howto/deploy/gcp.html#create-your-first-repository"
  },"172": {
    "doc": "GCP",
    "title": "GCP",
    "content": " ",
    "url": "/v1.4/howto/deploy/gcp.html",
    
    "relUrl": "/howto/deploy/gcp.html"
  },"173": {
    "doc": "Glossary",
    "title": "Glossary",
    "content": "This page has definition and explanations of all terms related to lakeFS technical internals and the architecture. ",
    "url": "/v1.4/understand/glossary.html",
    
    "relUrl": "/understand/glossary.html"
  },"174": {
    "doc": "Glossary",
    "title": "Table of contents",
    "content": ". | Auditing | Branch | Collection | Commit | Cross-Collection Consistency | Data Lake Governance | Data Lifecycle Management | Data Pipeline Reproducibility | Data Quality Testing | Data Versioning | Git-like Operations | Graveler | Hooks | Isolated Data Snapshot | Main Branch | Metadata Management | Merge | Repository | Rollback | Storage Namespace | Underlying Storage | Tag | Fluffy | . ",
    "url": "/v1.4/understand/glossary.html#table-of-contents",
    
    "relUrl": "/understand/glossary.html#table-of-contents"
  },"175": {
    "doc": "Glossary",
    "title": "Auditing",
    "content": "Data auditing is data assessment to ensure its accuracy, security, and efficacy for specific usage. It also involves assessing data quality through its lifecycle and understanding the impact of poor quality data on the organization’s performance and revenue. Ensuring data reproducibility, auditability, and governance is one of the key concerns of data engineers today. lakeFS commit history helps the data teams to keep track of all changes to the data, supporting data auditing. ",
    "url": "/v1.4/understand/glossary.html#auditing",
    
    "relUrl": "/understand/glossary.html#auditing"
  },"176": {
    "doc": "Glossary",
    "title": "Branch",
    "content": "Branches in lakeFS allow users to create their own “isolated” view of the repository. Read more. ",
    "url": "/v1.4/understand/glossary.html#branch",
    
    "relUrl": "/understand/glossary.html#branch"
  },"177": {
    "doc": "Glossary",
    "title": "Collection",
    "content": "A collection, roughly speaking, is a set of data. Collections may be structured or unstructured; a structured collection is often referred to as a table. ",
    "url": "/v1.4/understand/glossary.html#collection",
    
    "relUrl": "/understand/glossary.html#collection"
  },"178": {
    "doc": "Glossary",
    "title": "Commit",
    "content": "Using commits, you can view a repository at a certain point in its history and you’re guaranteed that the data you see is exactly as it was at the point of committing it. Read More. ",
    "url": "/v1.4/understand/glossary.html#commit",
    
    "relUrl": "/understand/glossary.html#commit"
  },"179": {
    "doc": "Glossary",
    "title": "Cross-Collection Consistency",
    "content": "It is unfortunate that the word ‘consistency’ has multiple meanings, at least four of them according to Martin Kleppmann. Consistency in the context of lakeFS and data versioning is, the guarantee that operations in a transaction are performed accurately, correctly and most important, atomically. A repository (and thus a branch) in lakeFS, can span multiple tables or collections. By providing branch, commit, merge and revert operations atomically on a branch, lakeFS achieves consistency guarantees across different logical collections. That is, data versioning is consistent across multiple collections within a repository. It is sometimes referred as multi-table transactions. That is, lakeFS offers transactional guarantees across multiple tables. ",
    "url": "/v1.4/understand/glossary.html#cross-collection-consistency",
    
    "relUrl": "/understand/glossary.html#cross-collection-consistency"
  },"180": {
    "doc": "Glossary",
    "title": "Data Lake Governance",
    "content": "The goal of data lake governance is to apply policies, standards and processes on the data. This allows creating high-quality data and ensuring that it’s used appropriately across the organization. Data lake governance improves the data quality and increases data usage for business decision-making, leading to operational improvements, better-informed business strategies, and stronger financial performance. lakeFS Cloud offers advanced data lake management features such as: Role-Based Access Control, Branch Aware Managed Garbage Collection, Data Lineage and Audit log. ",
    "url": "/v1.4/understand/glossary.html#data-lake-governance",
    
    "relUrl": "/understand/glossary.html#data-lake-governance"
  },"181": {
    "doc": "Glossary",
    "title": "Data Lifecycle Management",
    "content": "In data-intensive applications, data should be managed through its entire lifecycle similar to how teams manage code. By doing so, we could leverage the best practices and tools from application lifecycle management (like CI/CD operations) and apply them to data. lakeFS offers data lifecycle management via isolated data development environments instead of shared buckets. ",
    "url": "/v1.4/understand/glossary.html#data-lifecycle-management",
    
    "relUrl": "/understand/glossary.html#data-lifecycle-management"
  },"182": {
    "doc": "Glossary",
    "title": "Data Pipeline Reproducibility",
    "content": "Reproducibility in data pipelines is the ability to repeat a process. An example of this is recreating an issue that occurred in the production pipeline. Reproducibility allows for the controlled manufacture of an error to debug and troubleshoot it at a later point in time. Reproducing a data pipeline issue is a challenge that most data engineers face on a daily basis. Learn more about how lakeFS supports data pipeline reproducibility. Other use cases include running ad-hoc queries (useful for data science), review, and backfill. ",
    "url": "/v1.4/understand/glossary.html#data-pipeline-reproducibility",
    
    "relUrl": "/understand/glossary.html#data-pipeline-reproducibility"
  },"183": {
    "doc": "Glossary",
    "title": "Data Quality Testing",
    "content": "This term describes ways to test data for its accuracy, completeness, consistency, timeliness, validity, and integrity. lakeFS hooks can be used to implement and run data quality tests before promoting staging data into production. ",
    "url": "/v1.4/understand/glossary.html#data-quality-testing",
    
    "relUrl": "/understand/glossary.html#data-quality-testing"
  },"184": {
    "doc": "Glossary",
    "title": "Data Versioning",
    "content": "To version data means creating a unique point-in-time reference for data that can be accessed later. This reference can take the form of a query, an ID, or also commonly, a DateTime identifier. Data versioning may also include saving an entire copy of the data under a new name or file path every time you want to create a version of it. More advanced versioning solutions like lakeFS perform versioning through zero-copy data operations. lakeFS also optimizes storage usage between versions and exposes special operations to manage them. ",
    "url": "/v1.4/understand/glossary.html#data-versioning",
    
    "relUrl": "/understand/glossary.html#data-versioning"
  },"185": {
    "doc": "Glossary",
    "title": "Git-like Operations",
    "content": "lakeFS allows teams to treat their data lake as a Git repository. Git is used for code versioning, whereas lakeFS is used for data versioning. lakeFS provides Git-like operations such as branch, commit, merge and revert. ",
    "url": "/v1.4/understand/glossary.html#git-like-operations",
    
    "relUrl": "/understand/glossary.html#git-like-operations"
  },"186": {
    "doc": "Glossary",
    "title": "Graveler",
    "content": "Graveler is the core versioning engine of lakeFS. It handles versioning by translating lakeFS addresses to the actual stored objects. See the versioning internals section to learn how lakeFS stores metadata. ",
    "url": "/v1.4/understand/glossary.html#graveler",
    
    "relUrl": "/understand/glossary.html#graveler"
  },"187": {
    "doc": "Glossary",
    "title": "Hooks",
    "content": "lakeFS hooks allow you to automate and ensure that a given set of checks and validations happens before important lifecycle events. They are similar conceptually to Git Hooks, but in contrast, they run remotely on a server. Currently, lakeFS allows executing hooks when two types of events occur: pre-commit events that run before a commit is acknowledged and pre-merge events that trigger right before a merge operation. ",
    "url": "/v1.4/understand/glossary.html#hooks",
    
    "relUrl": "/understand/glossary.html#hooks"
  },"188": {
    "doc": "Glossary",
    "title": "Isolated Data Snapshot",
    "content": "Creating a branch in lakeFS provides an isolated environment containing a snapshot of your repository. While working on your branch in isolation, all other data users will be looking at the repository’s main branch. So they won’t see your changes, and you also won’t see the changes applied to the main branch. All of this happens without any data duplication but metadata management. ",
    "url": "/v1.4/understand/glossary.html#isolated-data-snapshot",
    
    "relUrl": "/understand/glossary.html#isolated-data-snapshot"
  },"189": {
    "doc": "Glossary",
    "title": "Main Branch",
    "content": "Every Git repository has the main branch (unless you take explicit steps to remove it) and it plays a key role in the software development process. In most projects, it represents the source of truth - all the code that works has been tested and is ready to be pushed to production. Similarly, main branch in lakeFS could be used as the single source of truth. For example, the live production data can be on the main branch. ",
    "url": "/v1.4/understand/glossary.html#main-branch",
    
    "relUrl": "/understand/glossary.html#main-branch"
  },"190": {
    "doc": "Glossary",
    "title": "Metadata Management",
    "content": "Where there is data, there is also metadata. lakeFS uses metadata to define schema, data types, data versions, relations to other datasets, etc. This helps to improve discoverability and manageability. lakeFS performs data versioning through metadata operations. ",
    "url": "/v1.4/understand/glossary.html#metadata-management",
    
    "relUrl": "/understand/glossary.html#metadata-management"
  },"191": {
    "doc": "Glossary",
    "title": "Merge",
    "content": "lakeFS merge command, similar to the Git merge functionality, allows you to merge data branches. Once you commit data, you can review it and then merge the committed data into the target branch. A merge generates a commit on the target branch with all your changes. lakeFS guarantees atomic merges that are fast, given they don’t involve copying data. Read More. ",
    "url": "/v1.4/understand/glossary.html#merge",
    
    "relUrl": "/understand/glossary.html#merge"
  },"192": {
    "doc": "Glossary",
    "title": "Repository",
    "content": "In lakeFS, a repository is a set of related objects (or collections of objects). Read More. ",
    "url": "/v1.4/understand/glossary.html#repository",
    
    "relUrl": "/understand/glossary.html#repository"
  },"193": {
    "doc": "Glossary",
    "title": "Rollback",
    "content": "A rollback is an atomic operation reversing the effects of a previous commit. If a developer introduces a new code version to production and discovers that it has a critical bug, they can simply roll back to the previous version. In lakeFS, a rollback is an atomic action that prevents the data consumers from receiving low-quality data until the issue is resolved. Learn more about how lakeFS supports the rollback operation. ",
    "url": "/v1.4/understand/glossary.html#rollback",
    
    "relUrl": "/understand/glossary.html#rollback"
  },"194": {
    "doc": "Glossary",
    "title": "Storage Namespace",
    "content": "The storage namespace is a location in the underlying storage dedicated to a specific repository. lakeFS uses it to store the repository’s objects and some of its metadata. ",
    "url": "/v1.4/understand/glossary.html#storage-namespace",
    
    "relUrl": "/understand/glossary.html#storage-namespace"
  },"195": {
    "doc": "Glossary",
    "title": "Underlying Storage",
    "content": "The underlying storage is a location in some object store where lakeFS keeps your objects and some metadata. ",
    "url": "/v1.4/understand/glossary.html#underlying-storage",
    
    "relUrl": "/understand/glossary.html#underlying-storage"
  },"196": {
    "doc": "Glossary",
    "title": "Tag",
    "content": "Tags are a way to give a meaningful name to a specific commit. Read More. ",
    "url": "/v1.4/understand/glossary.html#tag",
    
    "relUrl": "/understand/glossary.html#tag"
  },"197": {
    "doc": "Glossary",
    "title": "Fluffy",
    "content": "lakeFS Enterprise Single-Sign-On service, it’s delegated with lakeFS authentication requests and replies back to lakeFS with the authentication response. ",
    "url": "/v1.4/understand/glossary.html#fluffy",
    
    "relUrl": "/understand/glossary.html#fluffy"
  },"198": {
    "doc": "Glue / Hive metastore",
    "title": "Using lakeFS with the Glue/Hive Metastore",
    "content": "Deprecated Feature: Having heard the feedback from the community, we are planning to replace the below manual steps with an automated process. You can read more about it here. ",
    "url": "/v1.4/integrations/glue_hive_metastore.html#using-lakefs-with-the-gluehive-metastore",
    
    "relUrl": "/integrations/glue_hive_metastore.html#using-lakefs-with-the-gluehive-metastore"
  },"199": {
    "doc": "Glue / Hive metastore",
    "title": "Table of contents",
    "content": ". | About Glue / Hive Metastore . | Without lakeFS | With lakeFS | . | Managing Tables With lakeFS Branches . | Motivation | Configurations | Suggested Model | Commands | . | . ",
    "url": "/v1.4/integrations/glue_hive_metastore.html#table-of-contents",
    
    "relUrl": "/integrations/glue_hive_metastore.html#table-of-contents"
  },"200": {
    "doc": "Glue / Hive metastore",
    "title": "About Glue / Hive Metastore",
    "content": "This part explains about how Glue/Hive Metastore work with lakeFS. Glue and Hive Metastore store metadata related to Hive and other services (such as Spark and Trino). They contain metadata such as the location of the table, information about columns, partitions and many more. Without lakeFS . To query the table my_table, Spark will: . | Request the metadata from Hive metastore (steps 1,2), | Use the location from the metadata to access the data in S3 (steps 3,4). | . With lakeFS . When using lakeFS, the flow stays exactly the same. Note that the location of the table my_table now contains the branch s3://example/main/path/to/table . ",
    "url": "/v1.4/integrations/glue_hive_metastore.html#about-glue--hive-metastore",
    
    "relUrl": "/integrations/glue_hive_metastore.html#about-glue--hive-metastore"
  },"201": {
    "doc": "Glue / Hive metastore",
    "title": "Managing Tables With lakeFS Branches",
    "content": "Motivation . When creating a table in Glue/Hive metastore (using a client such as Spark, Hive, Presto), we specify the table location. Consider the table my_table that was created with the location s3://example/main/path/to/table. Suppose you created a new branch called DEV with main as the source branch. The data from s3://example/main/path/to/table is now accessible in s3://example/DEV/path/to/table. The metadata is not managed in lakeFS, meaning you don’t have any table pointing to s3://example/DEV/path/to/table. To address this, lakeFS introduces lakectl metastore commands. The case above can be handled using the copy command: it creates a copy of my_table with data located in s3://example/DEV/path/to/table. Note that this is a fast, metadata-only operation. Configurations . The lakectl metastore commands can run on Glue or Hive metastore. Add the following to the lakectl configuration file (by default ~/.lakectl.yaml): . Hive . metastore: type: hive hive: uri: hive-metastore:9083 . Glue . metastore: type: glue glue: catalog_id: 123456789012 region: us-east-1 profile: default # optional, implies using a credentials file credentials: access_key_id: AKIAIOSFODNN7EXAMPLE secret_access_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY . Note: It’s recommended to set type and catalog-id/metastore-uri in the lakectl configuration file. Suggested Model . For simplicity, we recommend creating a schema for each branch. That way, you can use the same table name across different schemas. For example: after creating branch example_branch, also create a schema named example_branch. For a table named my_table under the schema main, create a new table under the same name and under the schema example_branch. You now have two my_table, one in the main schema and one in the branch schema. Commands . Metastore tools support three commands: copy, diff, and create-symlink. copy and diff can work both on Glue and on Hive. create-symlink works only on Glue. Note: If to-schema or to-table are not specified, the destination branch and source table names will be used as per the suggested model. Note: Metastore commands can only run on tables located in lakeFS. You should not use tables that aren’t located in lakeFS. Copy . The copy command creates a copy of a table pointing to the defined branch. In case the destination table already exists, the command will only merge the changes. Example: . Suppose we created the table inventory on branch main on schema default. CREATE EXTERNAL TABLE `inventory`( `inv_item_sk` int, `inv_warehouse_sk` int, `inv_quantity_on_hand` int) PARTITIONED BY ( `inv_date_sk` int) STORED AS ORC LOCATION 's3a://my_repo/main/path/to/table'; . We create a new lakeFS branch example_branch: . lakectl branch create lakefs://my_repo/example_branch --source lakefs://my_repo/main . The data from s3://my_repo/main/path/to/table is now accessible in s3://my_repo/DEV/path/to/table. To query the data in s3://my_repo/DEV/path/to/table, you would like to create a copy of the table inventory in schema example_branch pointing to the new branch. lakectl metastore copy --from-schema default --from-table inventory --to-schema example_branch --to-table inventory --to-branch example_branch . After running this command, query the table example_branch.inventory to get the data from s3://my_repo/DEV/path/to/table . Copy Partition . After adding a partition to the branch table, you may want to copy the partition to the main table. For example, for the new partition 2020-08-01, run the following to copy the partition to the main table: . lakectl metastore copy --type hive --from-schema example_branch --from-table inventory --to-schema default --to-table inventory --to-branch main -p 2020-08-01 . For a table partitioned by more than one column, specify the partition flag for every column. For example, for the partition (year='2020',month='08',day='01'): . lakectl metastore copy --from-schema example_branch --from-table branch_inventory --to-schema default --to-branch main -p 2020 -p 08 -p 01 . Diff . Provides a two-way diff between two tables. Shows added+ , removed- and changed~ partitions and columns. Example: . Suppose you made some changes on the copied table inventory on schema example_branch and now want to view the changes before merging back to inventory on schema default. Hive: . lakectl metastore diff --type hive --address thrift://hive-metastore:9083 --from-schema example_branch --from-table branch --to-schema default --to-table inventory . The output will look like this: . Columns are identical Partitions - 2020-07-04 + 2020-07-05 + 2020-07-06 ~ 2020-07-08 . ",
    "url": "/v1.4/integrations/glue_hive_metastore.html#managing-tables-with-lakefs-branches",
    
    "relUrl": "/integrations/glue_hive_metastore.html#managing-tables-with-lakefs-branches"
  },"202": {
    "doc": "Glue / Hive metastore",
    "title": "Glue / Hive metastore",
    "content": " ",
    "url": "/v1.4/integrations/glue_hive_metastore.html",
    
    "relUrl": "/integrations/glue_hive_metastore.html"
  },"203": {
    "doc": "Glue Data Catalog",
    "title": "Using lakeFS with the Glue Catalog",
    "content": " ",
    "url": "/v1.4/integrations/glue_metastore.html#using-lakefs-with-the-glue-catalog",
    
    "relUrl": "/integrations/glue_metastore.html#using-lakefs-with-the-glue-catalog"
  },"204": {
    "doc": "Glue Data Catalog",
    "title": "Table of contents",
    "content": ". | Overview | Example: Using Athena to query lakeFS data . | Prerequisites | Add table descriptor | Write some table data | The exporter script | Configure Action Hooks | Use Athena | Cleanup | . | . ",
    "url": "/v1.4/integrations/glue_metastore.html#table-of-contents",
    
    "relUrl": "/integrations/glue_metastore.html#table-of-contents"
  },"205": {
    "doc": "Glue Data Catalog",
    "title": "Overview",
    "content": "The integration between Glue and lakeFS is based on Data Catalog Exports. This guide describes how to use lakeFS with the Glue Data Catalog. You’ll be able to query your lakeFS data by specifying the repository, branch and commit in your SQL query. Currently, only read operations are supported on the tables. You will set up the automation required to work with lakeFS on top of the Glue Data Catalog, including: . | Create a table descriptor under _lakefs_tables/&lt;your-table&gt;.yaml. This will represent your table schema. | Write an exporter script that will: . | Mirror your branch’s state into Hive Symlink files readable by Athena. | Export the table descriptors from your branch to the Glue Catalog. | . | Set up lakeFS hooks to trigger the above script when specific events occur. | . ",
    "url": "/v1.4/integrations/glue_metastore.html#overview",
    
    "relUrl": "/integrations/glue_metastore.html#overview"
  },"206": {
    "doc": "Glue Data Catalog",
    "title": "Example: Using Athena to query lakeFS data",
    "content": "Prerequisites . Before starting, make sure you have: . | An active lakeFS installation with S3 as the backing storage, and a repository in this installation. | A database in Glue Data Catalog (lakeFS does not create one). | AWS Credentials with permission to manage Glue, Athena Query and S3 access. | . Add table descriptor . Let’s define a table, and commit it to lakeFS. Save the YAML below as animals.yaml and upload it to lakeFS. lakectl fs upload lakefs://catalogs/main/_lakefs_tables/animals.yaml -s ./animals.yaml &amp;&amp; \\ lakectl commit lakefs://catalogs/main -m \"added table\" . name: animals type: hive # data location root in lakeFS path: tables/animals # partitions order partition_columns: ['type', 'weight'] schema: type: struct # all the columns spec fields: - name: type type: string nullable: true metadata: comment: axolotl, cat, dog, fish etc - name: weight type: integer nullable: false metadata: {} - name: name type: string nullable: false metadata: {} . Write some table data . Insert data into the table path, using your preferred method (e.g. Spark), and commit upon completion. This example uses CSV files, and the files added to lakeFS should look like this: . The exporter script . Upload the following script to your main branch under scripts/animals_exporter.lua (or a path of your choice). For code references check symlink_exporter and glue_exporter docs. local aws = require(\"aws\") local symlink_exporter = require(\"lakefs/catalogexport/symlink_exporter\") local glue_exporter = require(\"lakefs/catalogexport/glue_exporter\") -- settings local access_key = args.aws.aws_access_key_id local secret_key = args.aws.aws_secret_access_key local region = args.aws.aws_region local table_path = args.table_source -- table descriptor local db = args.catalog.db_name -- glue db local table_input = args.catalog.table_input -- table input (AWS input spec) for Glue -- export symlinks local s3 = aws.s3_client(access_key, secret_key, region) local result = symlink_exporter.export_s3(s3, table_path, action, {debug=true}) -- register glue table local glue = aws.glue_client(access_key, secret_key, region) local res = glue_exporter.export_glue(glue, db, table_path, table_input, action, {debug=true}) . Configure Action Hooks . Hooks serve as the mechanism that triggers the execution of the exporter. For more detailed information on how to configure exporter hooks, you can refer to Running an Exporter. The args.catalog.table_input argument in the Lua script is assumed to be passed from the action arguments, that way the same script can be reused for different tables. Check the example to construct the table input in the lua code. | Hook CSV Glue Table | Hook Parquet Glue Table | Multiple Hooks / Inline script | . Single hook with CSV Table . Upload to _lakefs_actions/animals_glue.yaml: . name: Glue Exporter on: post-commit: branches: [\"main\"] hooks: - id: animals_table_glue_exporter type: lua properties: script_path: \"scripts/animals_exporter.lua\" args: aws: aws_access_key_id: \"&lt;AWS_ACCESS_KEY_ID&gt;\" aws_secret_access_key: \"&lt;AWS_SECRET_ACCESS_KEY&gt;\" aws_region: \"&lt;AWS_REGION&gt;\" table_source: '_lakefs_tables/animals.yaml' catalog: db_name: \"my-glue-db\" table_input: StorageDescriptor: InputFormat: \"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat\" OutputFormat: \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\" SerdeInfo: SerializationLibrary: \"org.apache.hadoop.hive.serde2.OpenCSVSerde\" Parameters: separatorChar: \",\" Parameters: classification: \"csv\" \"skip.header.line.count\": \"1\" . Spark Parquet Example . When working with Parquet files, upload the following to _lakefs_actions/animals_glue.yaml: . name: Glue Exporter on: post-commit: branches: [\"main\"] hooks: - id: animals_table_glue_exporter type: lua properties: script_path: \"scripts/animals_exporter.lua\" args: aws: aws_access_key_id: \"&lt;AWS_ACCESS_KEY_ID&gt;\" aws_secret_access_key: \"&lt;AWS_SECRET_ACCESS_KEY&gt;\" aws_region: \"&lt;AWS_REGION&gt;\" table_source: '_lakefs_tables/animals.yaml' catalog: db_name: \"my-glue-db\" table_input: StorageDescriptor: InputFormat: \"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat\" OutputFormat: \"org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat\" SerdeInfo: SerializationLibrary: \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\" Parameters: classification: \"parquet\" EXTERNAL: \"TRUE\" \"parquet.compression\": \"SNAPPY\" . Multiple Hooks / Inline script . The following example demonstrates how to separate the symlink and glue exporter into building blocks running in separate hooks. It also shows how to run the lua script inline instead of a file, depending on user preference. name: Animal Table Exporter on: post-commit: branches: [\"main\"] hooks: - id: symlink_exporter type: lua properties: args: aws: aws_access_key_id: \"&lt;AWS_ACCESS_KEY_ID&gt;\" aws_secret_access_key: \"&lt;AWS_SECRET_ACCESS_KEY&gt;\" aws_region: \"&lt;AWS_REGION&gt;\" table_source: '_lakefs_tables/animals.yaml' script: | local exporter = require(\"lakefs/catalogexport/symlink_exporter\") local aws = require(\"aws\") local table_path = args.table_source local s3 = aws.s3_client(args.aws.aws_access_key_id, args.aws.aws_secret_access_key, args.aws.aws_region) exporter.export_s3(s3, table_path, action, {debug=true}) - id: glue_exporter type: lua properties: args: aws: aws_access_key_id: \"&lt;AWS_ACCESS_KEY_ID&gt;\" aws_secret_access_key: \"&lt;AWS_SECRET_ACCESS_KEY&gt;\" aws_region: \"&lt;AWS_REGION&gt;\" table_source: '_lakefs_tables/animals.yaml' catalog: db_name: \"my-glue-db\" table_input: # add glue table input here script: | local aws = require(\"aws\") local exporter = require(\"lakefs/catalogexport/glue_exporter\") local glue = aws.glue_client(args.aws.aws_access_key_id, args.aws.aws_secret_access_key, args.aws.aws_region) exporter.export_glue(glue, args.catalog.db_name, args.table_source, args.catalog.table_input, action, {debug=true}) . Adding the script and the action files to the repository and commit it. This is a post-commit action, meaning it will be executed after the commit operation has taken place. lakectl fs upload lakefs://catalogs/main/scripts/animals_exporter.lua -s ./animals_exporter.lua lakectl fs upload lakefs://catalogs/main/_lakefs_actions/animals_glue.yaml -s ./animals_glue.yaml lakectl commit lakefs://catalogs/main -m \"trigger first export hook\" . Once the action has completed its execution, you can review the results in the action logs. Use Athena . We can use the exported Glue table with any tool that supports Glue Catalog (or Hive compatible) such as Athena, Trino, Spark and others. To use Athena we can simply run MSCK REPAIR TABLE and then query the tables. In Athena, make sure that the correct database (my-glue-db in the example above) is configured, then run: . MSCK REPAIR TABLE `animals_catalogs_main_9255e5`; -- load partitions for the first time SELECT * FROM `animals_catalogs_main_9255e5` limit 50; . Cleanup . Users can use additional hooks / actions to implement a custom cleanup logic to delete the symlink in S3 and Glue Tables. glue.delete_table(db, '&lt;glue table name&gt;') s3.delete_recursive('bucket', 'path/to/symlinks/of/a/commit/') . ",
    "url": "/v1.4/integrations/glue_metastore.html#example-using-athena-to-query-lakefs-data",
    
    "relUrl": "/integrations/glue_metastore.html#example-using-athena-to-query-lakefs-data"
  },"207": {
    "doc": "Glue Data Catalog",
    "title": "Glue Data Catalog",
    "content": " ",
    "url": "/v1.4/integrations/glue_metastore.html",
    
    "relUrl": "/integrations/glue_metastore.html"
  },"208": {
    "doc": "Apache Hive",
    "title": "Using lakeFS with Apache Hive",
    "content": "The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive. ",
    "url": "/v1.4/integrations/hive.html#using-lakefs-with-apache-hive",
    
    "relUrl": "/integrations/hive.html#using-lakefs-with-apache-hive"
  },"209": {
    "doc": "Apache Hive",
    "title": "Table of contents",
    "content": ". | Configuration | Examples | . ",
    "url": "/v1.4/integrations/hive.html#table-of-contents",
    
    "relUrl": "/integrations/hive.html#table-of-contents"
  },"210": {
    "doc": "Apache Hive",
    "title": "Configuration",
    "content": "To configure Hive to work with lakeFS, you need to set the lakeFS credentials in the corresponding S3 credential fields. lakeFS endpoint: fs.s3a.endpoint . lakeFS access key: fs.s3a.access.key . lakeFS secret key: fs.s3a.secret.key . Note In the following examples, we set AWS credentials at runtime for clarity. In production, these properties should be set using one of Hadoop’s standard ways of Authenticating with S3. For example, you can add the configurations to the file hdfs-site.xml: . &lt;configuration&gt; ... &lt;property&gt; &lt;name&gt;fs.s3a.secret.key&lt;/name&gt; &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.access.key&lt;/name&gt; &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;https://lakefs.example.com&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . Note In this example, we set fs.s3a.path.style.access to true to remove the need for additional DNS records for virtual hosting fs.s3a.path.style.access that was introduced in Hadoop 2.8.0 . ",
    "url": "/v1.4/integrations/hive.html#configuration",
    
    "relUrl": "/integrations/hive.html#configuration"
  },"211": {
    "doc": "Apache Hive",
    "title": "Examples",
    "content": "Example with schema . CREATE SCHEMA example LOCATION 's3a://example/main/' ; CREATE TABLE example.request_logs ( request_time timestamp, url string, ip string, user_agent string ); . Example with an external table . CREATE EXTERNAL TABLE request_logs ( request_time timestamp, url string, ip string, user_agent string ) LOCATION 's3a://example/main/request_logs' ; . ",
    "url": "/v1.4/integrations/hive.html#examples",
    
    "relUrl": "/integrations/hive.html#examples"
  },"212": {
    "doc": "Apache Hive",
    "title": "Apache Hive",
    "content": " ",
    "url": "/v1.4/integrations/hive.html",
    
    "relUrl": "/integrations/hive.html"
  },"213": {
    "doc": "Apache Iceberg",
    "title": "Using lakeFS with Apache Iceberg",
    "content": " ",
    "url": "/v1.4/integrations/iceberg.html#using-lakefs-with-apache-iceberg",
    
    "relUrl": "/integrations/iceberg.html#using-lakefs-with-apache-iceberg"
  },"214": {
    "doc": "Apache Iceberg",
    "title": "Table of contents",
    "content": ". | Setup | Configure | Using Iceberg tables with lakeFS . | Create a table | Insert data into the table | Create a branch | Make changes on the branch | Query the table | . | Migrating an existing Iceberg Table to lakeFS Catalog | . To enrich your Iceberg tables with lakeFS capabilities, you can use the lakeFS implementation of the Iceberg catalog. You will then be able to query your Iceberg tables using lakeFS references, such as branches, tags, and commit hashes: . SELECT * FROM catalog.ref.db.table . ",
    "url": "/v1.4/integrations/iceberg.html#table-of-contents",
    
    "relUrl": "/integrations/iceberg.html#table-of-contents"
  },"215": {
    "doc": "Apache Iceberg",
    "title": "Setup",
    "content": ". | Maven | PySpark | . Use the following Maven dependency to install the lakeFS custom catalog: . &lt;dependency&gt; &lt;groupId&gt;io.lakefs&lt;/groupId&gt; &lt;artifactId&gt;lakefs-iceberg&lt;/artifactId&gt; &lt;version&gt;0.1.4&lt;/version&gt; &lt;/dependency&gt; . Include the lakefs-iceberg jar in your package list along with Iceberg. For example: .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.3.0,io.lakefs:lakefs-iceberg:0.1.4\") . ",
    "url": "/v1.4/integrations/iceberg.html#setup",
    
    "relUrl": "/integrations/iceberg.html#setup"
  },"216": {
    "doc": "Apache Iceberg",
    "title": "Configure",
    "content": ". | PySpark | Spark Shell | . Set up the Spark SQL catalog: .config(\"spark.sql.catalog.lakefs\", \"org.apache.iceberg.spark.SparkCatalog\") \\ .config(\"spark.sql.catalog.lakefs.catalog-impl\", \"io.lakefs.iceberg.LakeFSCatalog\") \\ .config(\"spark.sql.catalog.lakefs.warehouse\", f\"lakefs://{repo_name}\") \\ .config(\"spark.sql.catalog.lakefs.cache-enabled\", \"false\") . Configure the S3A Hadoop FileSystem with your lakeFS connection details. Note that these are your lakeFS endpoint and credentials, not your S3 ones.config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\ .config(\"spark.hadoop.fs.s3a.endpoint\", \"https://example-org.us-east-1.lakefscloud.io\") \\ .config(\"spark.hadoop.fs.s3a.access.key\", \"AKIAIO5FODNN7EXAMPLE\") \\ .config(\"spark.hadoop.fs.s3a.secret.key\", \"wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY\") \\ .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") . spark-shell --conf spark.sql.catalog.lakefs=\"org.apache.iceberg.spark.SparkCatalog\" \\ --conf spark.sql.catalog.lakefs.catalog-impl=\"io.lakefs.iceberg.LakeFSCatalog\" \\ --conf spark.sql.catalog.lakefs.warehouse=\"lakefs://example-repo\" \\ --conf spark.sql.catalog.lakefs.cache-enabled=\"false\" \\ --conf spark.hadoop.fs.s3.impl=\"org.apache.hadoop.fs.s3a.S3AFileSystem\" \\ --conf spark.hadoop.fs.s3a.endpoint=\"https://example-org.us-east-1.lakefscloud.io\" \\ --conf spark.hadoop.fs.s3a.access.key=\"AKIAIO5FODNN7EXAMPLE\" \\ --conf spark.hadoop.fs.s3a.secret.key=\"wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY\" \\ --conf spark.hadoop.fs.s3a.path.style.access=\"true\" . ",
    "url": "/v1.4/integrations/iceberg.html#configure",
    
    "relUrl": "/integrations/iceberg.html#configure"
  },"217": {
    "doc": "Apache Iceberg",
    "title": "Using Iceberg tables with lakeFS",
    "content": "Create a table . To create a table on your main branch, use the following syntax: . CREATE TABLE lakefs.main.table1 (id int, data string); . Insert data into the table . INSERT INTO lakefs.main.table1 VALUES (1, 'data1'); INSERT INTO lakefs.main.table1 VALUES (2, 'data2'); . Create a branch . We can now commit the creation of the table to the main branch: . lakectl commit lakefs://example-repo/main -m \"my first iceberg commit\" . Then, create a branch: . lakectl branch create lakefs://example-repo/dev -s lakefs://example-repo/main . Make changes on the branch . We can now make changes on the branch: . INSERT INTO lakefs.dev.table1 VALUES (3, 'data3'); . Query the table . If we query the table on the branch, we will see the data we inserted: . SELECT * FROM lakefs.dev.table1; . Results in: . +----+------+ | id | data | +----+------+ | 1 | data1| 2 | data2| 3 | data3| +----+------+ . However, if we query the table on the main branch, we will not see the new changes: . SELECT * FROM lakefs.main.table1; . Results in: . +----+------+ | id | data | +----+------+ | 1 | data1| 2 | data2| +----+------+ . ",
    "url": "/v1.4/integrations/iceberg.html#using-iceberg-tables-with-lakefs",
    
    "relUrl": "/integrations/iceberg.html#using-iceberg-tables-with-lakefs"
  },"218": {
    "doc": "Apache Iceberg",
    "title": "Migrating an existing Iceberg Table to lakeFS Catalog",
    "content": "This is done through an incremental copy from the original table into lakeFS. | Create a new lakeFS repository lakectl repo create lakefs://example-repo &lt;base storage path&gt; | Initiate a spark session that can interact with the source iceberg table and the target lakeFS catalog. Here’s an example of Hadoop and S3 session and lakeFS catalog with per-bucket config: . SparkConf conf = new SparkConf(); conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\"); // set hadoop on S3 config (source tables we want to copy) for spark conf.set(\"spark.sql.catalog.hadoop_prod\", \"org.apache.iceberg.spark.SparkCatalog\"); conf.set(\"spark.sql.catalog.hadoop_prod.type\", \"hadoop\"); conf.set(\"spark.sql.catalog.hadoop_prod.warehouse\", \"s3a://my-bucket/warehouse/hadoop/\"); conf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"); conf.set(\"spark.hadoop.fs.s3a.bucket.my-bucket.access.key\", \"&lt;AWS_ACCESS_KEY&gt;\"); conf.set(\"spark.hadoop.fs.s3a.bucket.my-bucket.secret.key\", \"&lt;AWS_SECRET_KEY&gt;\"); // set lakeFS config (target catalog and repository) conf.set(\"spark.sql.catalog.lakefs\", \"org.apache.iceberg.spark.SparkCatalog\"); conf.set(\"spark.sql.catalog.lakefs.catalog-impl\", \"io.lakefs.iceberg.LakeFSCatalog\"); conf.set(\"spark.sql.catalog.lakefs.warehouse\", \"lakefs://example-repo\"); conf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"); conf.set(\"spark.hadoop.fs.s3a.bucket.example-repo.access.key\", \"&lt;LAKEFS_ACCESS_KEY&gt;\"); conf.set(\"spark.hadoop.fs.s3a.bucket.example-repo.secret.key\", \"&lt;LAKEFS_SECRET_KEY&gt;\"); conf.set(\"spark.hadoop.fs.s3a.bucket.example-repo.endpoint\" , \"&lt;LAKEFS_ENDPOINT&gt;\"); . | Create Schema in lakeFS and copy the data . Example of copy with spark-sql: . -- Create Iceberg Schema in lakeFS CREATE SCHEMA IF NOT EXISTS &lt;lakefs-catalog&gt;.&lt;branch&gt;.&lt;db&gt; -- Create new iceberg table in lakeFS from the source table (pre-lakeFS) CREATE TABLE IF NOT EXISTS &lt;lakefs-catalog&gt;.&lt;branch&gt;.&lt;db&gt; USING iceberg AS SELECT * FROM &lt;iceberg-original-table&gt; . | . ",
    "url": "/v1.4/integrations/iceberg.html#migrating-an-existing-iceberg-table-to-lakefs-catalog",
    
    "relUrl": "/integrations/iceberg.html#migrating-an-existing-iceberg-table-to-lakefs-catalog"
  },"219": {
    "doc": "Apache Iceberg",
    "title": "Apache Iceberg",
    "content": " ",
    "url": "/v1.4/integrations/iceberg.html",
    
    "relUrl": "/integrations/iceberg.html"
  },"220": {
    "doc": "Import Data",
    "title": "Importing data into lakeFS",
    "content": " ",
    "url": "/v1.4/howto/import.html#importing-data-into-lakefs",
    
    "relUrl": "/howto/import.html#importing-data-into-lakefs"
  },"221": {
    "doc": "Import Data",
    "title": "Table of contents",
    "content": ". | Prerequisites | Using the lakeFS UI | Using the CLI: lakectl import | Examples | . ",
    "url": "/v1.4/howto/import.html#table-of-contents",
    
    "relUrl": "/howto/import.html#table-of-contents"
  },"222": {
    "doc": "Import Data",
    "title": "Prerequisites",
    "content": ". | Importing is permitted for users in the Supers (open-source) group or the SuperUsers (Cloud/Enterprise) group. To learn how lakeFS Cloud and lakeFS Enterprise users can fine-tune import permissions, see Fine-grained permissions below. | The lakeFS server must have permissions to list the objects in the source bucket. | The source bucket must be on the same cloud provider and in the same region as your repository. | . ",
    "url": "/v1.4/howto/import.html#prerequisites",
    
    "relUrl": "/howto/import.html#prerequisites"
  },"223": {
    "doc": "Import Data",
    "title": "Using the lakeFS UI",
    "content": ". | In your repository’s main page, click the Import button to open the import dialog. | Under Import from, fill in the location on your object store you would like to import from. | Fill in the import destination in lakeFS. This should be a path under the current branch. | Add a commit message, and optionally commit metadata. | Press Import. | . Once the import is complete, a new commit containing the imported objects will be created in the destination branch. ",
    "url": "/v1.4/howto/import.html#using-the-lakefs-ui",
    
    "relUrl": "/howto/import.html#using-the-lakefs-ui"
  },"224": {
    "doc": "Import Data",
    "title": "Using the CLI: lakectl import",
    "content": "The lakectl import command acts the same as the UI import wizard. It commits the changes to the selected branch. | AWS S3 or S3 API Compatible storage | Azure Blob | Google Cloud Storage | . lakectl import \\ --from s3://bucket/optional/prefix/ \\ --to lakefs://my-repo/my-branch/optional/path/ . lakectl import \\ --from https://storageAccountName.blob.core.windows.net/container/optional/prefix/ \\ --to lakefs://my-repo/my-branch/optional/path/ . lakectl import \\ --from gs://bucket/optional/prefix/ \\ --to lakefs://my-repo/my-branch/optional/path/ . ",
    "url": "/v1.4/howto/import.html#using-the-cli-lakectl-import",
    
    "relUrl": "/howto/import.html#using-the-cli-lakectl-import"
  },"225": {
    "doc": "Import Data",
    "title": "Notes",
    "content": ". | Any previously existing objects under the destination prefix will be deleted. | The import duration depends on the amount of imported objects, but will roughly be a few thousand objects per second. | For security reasons, if you are using lakeFS on top of your local disk (blockstore.type=local), you need to enable the import feature explicitly. To do so, set the blockstore.local.import_enabled to true and specify the allowed import paths in blockstore.local.allowed_external_prefixes (see configuration reference). When using lakectl or the lakeFS UI, you can currently import only directories locally. If you need to import a single file, use the HTTP API or API Clients with type=object in the request body and destination=&lt;full-path-to-file&gt;. | Making changes to data in the original bucket will not be reflected in lakeFS, and may cause inconsistencies. | . ",
    "url": "/v1.4/howto/import.html#notes",
    
    "relUrl": "/howto/import.html#notes"
  },"226": {
    "doc": "Import Data",
    "title": "Examples",
    "content": "To explore practical examples and real-world use cases of importing data into lakeFS, we recommend checking out our comprehensive blog post on the subject. ",
    "url": "/v1.4/howto/import.html#examples",
    
    "relUrl": "/howto/import.html#examples"
  },"227": {
    "doc": "Import Data",
    "title": "Fine-grained permissions",
    "content": "lakeFS Cloud . lakeFS Enterprise . With RBAC support, The lakeFS user running the import command should have the following permissions in lakeFS: fs:WriteObject, fs:CreateMetaRange, fs:CreateCommit, fs:ImportFromStorage and fs:ImportCancel. As mentioned above, all of these permissions are available by default to the Supers (open-source) group or the SuperUsers (Cloud/Enterprise). ",
    "url": "/v1.4/howto/import.html#fine-grained-permissions",
    
    "relUrl": "/howto/import.html#fine-grained-permissions"
  },"228": {
    "doc": "Import Data",
    "title": "Provider-specific permissions",
    "content": "In addition, the following for provider-specific permissions may be required: . | AWS S3 | Azure Storage | Google Cloud Storage | . ",
    "url": "/v1.4/howto/import.html#provider-specific-permissions",
    
    "relUrl": "/howto/import.html#provider-specific-permissions"
  },"229": {
    "doc": "Import Data",
    "title": "AWS S3: Importing from public buckets",
    "content": "lakeFS needs access to the imported location to first list the files to import and later read the files upon users request. There are some use cases where the user would like to import from a destination which isn’t owned by the account running lakeFS. For example, importing public datasets to experiment with lakeFS and Spark. lakeFS will require additional permissions to read from public buckets. For example, for S3 public buckets, the following policy needs to be attached to the lakeFS S3 service-account to allow access to public buckets, while blocking access to other owned buckets: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"PubliclyAccessibleBuckets\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetBucketVersioning\", \"s3:ListBucket\", \"s3:GetBucketLocation\", \"s3:ListBucketMultipartUploads\", \"s3:ListBucketVersions\", \"s3:GetObject\", \"s3:GetObjectVersion\", \"s3:AbortMultipartUpload\", \"s3:ListMultipartUploadParts\" ], \"Resource\": [\"*\"], \"Condition\": { \"StringNotEquals\": { \"s3:ResourceAccount\": \"&lt;YourAccountID&gt;\" } } } ] } . See Azure deployment on limitations when using account credentials. Azure Data Lake Gen2 . lakeFS requires a hint in the import source URL to understand that the provided storage account is ADLS Gen2 . For source account URL: https://&lt;my-account&gt;.core.windows.net/path/to/import/ Please add the *adls* subdomain to the URL as follows: https://&lt;my-account&gt;.adls.core.windows.net/path/to/import/ . No specific prerequisites . ",
    "url": "/v1.4/howto/import.html#aws-s3-importing-from-public-buckets",
    
    "relUrl": "/howto/import.html#aws-s3-importing-from-public-buckets"
  },"230": {
    "doc": "Import Data",
    "title": "Import Data",
    "content": "This section describes how to import existing data into a lakeFS repository, without copying it. If you are interested in copying data into lakeFS, see Copying data to/from lakeFS. ",
    "url": "/v1.4/howto/import.html",
    
    "relUrl": "/howto/import.html"
  },"231": {
    "doc": "Integrations",
    "title": "Integrations with lakeFS",
    "content": "You can use lakeFS with a wide range of tools and frameworks. lakeFS provides several clients directly, as well as an S3-compatible gateway. This gateway means that if you want to use something with lakeFS, so long as that technology can interface with S3, it can interface with lakeFS. See below for detailed instructions for using different technologies with lakeFS. | Airbyte | Amazon Athena | Amazon SageMaker | Apache Airflow | . | Apache Hive | Apache Iceberg | Apache Kafka | Apache Spark | . | AWS CLI | Cloudera | Delta Lake | . | Dremio | DuckDB | Glue / Hive metastore | Kubeflow | . | Presto / Trino | Python | R | Vertex AI | . If there is a technology not listed here that you would like to use with lakeFS, please drop by our Slack and we’ll help you get started with it. ",
    "url": "/v1.4/integrations/#integrations-with-lakefs",
    
    "relUrl": "/integrations/#integrations-with-lakefs"
  },"232": {
    "doc": "Integrations",
    "title": "Integrations",
    "content": " ",
    "url": "/v1.4/integrations/",
    
    "relUrl": "/integrations/"
  },"233": {
    "doc": "Security",
    "title": "lakeFS Security Reference",
    "content": ". | Authentication | Remote Authenticator | Role-Based Access Control (RBAC) | Presigned URL | Access Control Lists (ACLs) | Single Sign On (SSO) | . ",
    "url": "/v1.4/reference/security/#lakefs-security-reference",
    
    "relUrl": "/reference/security/#lakefs-security-reference"
  },"234": {
    "doc": "Security",
    "title": "Security",
    "content": " ",
    "url": "/v1.4/reference/security/",
    
    "relUrl": "/reference/security/"
  },"235": {
    "doc": "Reference",
    "title": "lakeFS Reference",
    "content": ". ",
    "url": "/v1.4/reference/#lakefs-reference",
    
    "relUrl": "/reference/#lakefs-reference"
  },"236": {
    "doc": "Reference",
    "title": "API",
    "content": ". | lakeFS API | S3 Gateway API | . ",
    "url": "/v1.4/reference/#api",
    
    "relUrl": "/reference/#api"
  },"237": {
    "doc": "Reference",
    "title": "Components",
    "content": ". | Server Configuration | lakeFS command-line tool lakectl | . ",
    "url": "/v1.4/reference/#components",
    
    "relUrl": "/reference/#components"
  },"238": {
    "doc": "Reference",
    "title": "Clients",
    "content": ". | Spark Metadata Client | lakeFS Hadoop FileSystem | Python Client | . ",
    "url": "/v1.4/reference/#clients",
    
    "relUrl": "/reference/#clients"
  },"239": {
    "doc": "Reference",
    "title": "Security",
    "content": ". | Authentication | Remote Authenticator | Role-Based Access Control (RBAC) | Presigned URL | Access Control Lists (ACLs) | Single Sign On (SSO) | . ",
    "url": "/v1.4/reference/#security",
    
    "relUrl": "/reference/#security"
  },"240": {
    "doc": "Reference",
    "title": "Other Reference Documentation",
    "content": ". | Monitoring using Prometheus | Auditing | . ",
    "url": "/v1.4/reference/#other-reference-documentation",
    
    "relUrl": "/reference/#other-reference-documentation"
  },"241": {
    "doc": "Reference",
    "title": "Reference",
    "content": " ",
    "url": "/v1.4/reference/",
    
    "relUrl": "/reference/"
  },"242": {
    "doc": "⭐ Quickstart",
    "title": "lakeFS Quickstart",
    "content": "Welcome to lakeFS! . lakeFS provides a “Git for data” platform enabling you to implement best practices from software engineering on your data lake, including branching and merging, CI/CD, and production-like dev/test environments. This quickstart will introduce you to some of the core ideas in lakeFS and show what you can do by illustrating the concept of branching, merging, and rolling back changes to data. It’s laid out in five short sections: . Launch . Spin up the quickstart environment locally under Docker . Query . Query the pre-populated data on the main branch . Branch . Make changes to the data on a new branch . Merge . Merge the changed data back to the main branch . Rollback . Change our mind and revert the changes . Actions . Use Actions to trigger code when an event occurs . You can use the 30-day free trial of lakeFS Cloud if you want to try out lakeFS without installing anything. ",
    "url": "/v1.4/quickstart/#lakefs-quickstart",
    
    "relUrl": "/quickstart/#lakefs-quickstart"
  },"243": {
    "doc": "⭐ Quickstart",
    "title": "⭐ Quickstart",
    "content": " ",
    "url": "/v1.4/quickstart/",
    
    "relUrl": "/quickstart/"
  },"244": {
    "doc": "Install lakeFS",
    "title": "Deploy and Setup lakeFS",
    "content": "The instructions given here are for a self-managed deployment of lakeFS. For a hosted lakeFS service with guaranteed SLAs, try lakeFS Cloud . This section will guide you through deploying lakeFS on top of an object store. You will require a database, and can optionally configure authentication using providers specific to your deployment platform. Which options are available depends on your deployment platform. For example, the object store available on Azure differs from that on AWS. ",
    "url": "/v1.4/howto/deploy/#deploy-and-setup-lakefs",
    
    "relUrl": "/howto/deploy/#deploy-and-setup-lakefs"
  },"245": {
    "doc": "Install lakeFS",
    "title": "Deployment and Setup Details",
    "content": "lakeFS releases include binaries for common operating systems, a containerized option or a Helm chart. Check out our guides below for full deployment details: . | AWS | Azure | GCP | On-premises and other cloud providers | . ",
    "url": "/v1.4/howto/deploy/#deployment-and-setup-details",
    
    "relUrl": "/howto/deploy/#deployment-and-setup-details"
  },"246": {
    "doc": "Install lakeFS",
    "title": "Install lakeFS",
    "content": " ",
    "url": "/v1.4/howto/deploy/",
    
    "relUrl": "/howto/deploy/"
  },"247": {
    "doc": "Actions and Hooks",
    "title": "Actions and Hooks in lakeFS",
    "content": " ",
    "url": "/v1.4/howto/hooks/#actions-and-hooks-in-lakefs",
    
    "relUrl": "/howto/hooks/#actions-and-hooks-in-lakefs"
  },"248": {
    "doc": "Actions and Hooks",
    "title": "Table of contents",
    "content": ". | Overview | Configuration | Supported Events | Runs API &amp; CLI | Result Files | . Like other version control systems, lakeFS allows you to configure Actions to trigger when predefined events occur. There are numerous uses for Actions, including: . | Format Validator: A webhook that checks new files to ensure they are of a set of allowed data formats. | Schema Validator: A webhook that reads new Parquet and ORC files to ensure they don’t contain a block list of column names (or name prefixes). This is useful for avoiding accidental PII exposure. | Integration with external systems: Post-merge and post-commit hooks could be used to export metadata about the change to another system. A common example is exporting symlink.txt files that allow e.g. AWS Athena to read data from lakeFS. | Notifying downstream consumers: Running a post-merge hook to trigger an Airflow DAG or to send a Webhook to an API, notifying it of the change that happened | . For step-by-step examples of hooks in action check out the lakeFS Quickstart and the lakeFS samples repository. ",
    "url": "/v1.4/howto/hooks/#table-of-contents",
    
    "relUrl": "/howto/hooks/#table-of-contents"
  },"249": {
    "doc": "Actions and Hooks",
    "title": "Overview",
    "content": "An action defines one or more hooks to execute. lakeFS supports three types of hook: . | Lua - uses an embedded Lua VM | Webhook - makes a REST call to an external URL | Airflow - triggers a DAG in Airflow | . “Before” hooks must run successfully before their action. If the hook fails, it aborts the action. Lua hooks and Webhooks are synchronous, and lakeFS waits for them to run to completion. Airflow hooks are asynchronous: lakeFS stops waiting as soon as Airflow accepts triggering the DAG. ",
    "url": "/v1.4/howto/hooks/#overview",
    
    "relUrl": "/howto/hooks/#overview"
  },"250": {
    "doc": "Actions and Hooks",
    "title": "Configuration",
    "content": "There are two parts to configuration an Action: . | Create an Action file and upload it to the lakeFS repository | Configure the hook(s) that you specified in the Action file. How these are configured will depend on the type of hook. | . Action files . An Action is a list of Hooks with the same trigger configuration, i.e. an event will trigger all Hooks under an Action or none at all. The Hooks under an Action are ordered and so is their execution. Before each hook execution the if boolean expression is evaluated. The expression can use the functions success() and failure(), which return true if the hook’s actions succeeded or failed, respectively. By default, when if is empty or omitted, the step will run only if no error occurred (the same as success()). Action File schema . | Property | Description | Data Type | Required | Default Value | . | name | Identifes the Action file | String | no | Action filename | . | on | List of events that will trigger the hooks | List | yes |   | . | on&lt;event&gt;.branches | Glob pattern list of branches that triggers the hooks | List | no | Not applicable to Tag events. If empty, Action runs on all branches | . | hooks | List of hooks to be executed | List | yes |   | . | hook.id | ID of the hook, must be unique within the action. | String | yes |   | . | hook.type | Type of the hook (types) | String | yes |   | . | hook.description | Description for the hook | String | no |   | . | hook.if | Expression that will be evaluated before execute the hook | String | no | No value is the same as evaluate success() | . | hook.properties | Hook’s specific configuration, see Lua, WebHook, and Airflow for details | Dictionary | true |   | . Example Action File . name: Good files check description: set of checks to verify that branch is good on: pre-commit: pre-merge: branches: - main hooks: - id: no_temp type: webhook description: checking no temporary files found properties: url: \"https://example.com/webhook?notmp=true?t=1za2PbkZK1bd4prMuTDr6BeEQwWYcX2R\" - id: no_freeze type: webhook description: check production is not in dev freeze properties: url: \"https://example.com/webhook?nofreeze=true?t=1za2PbkZK1bd4prMuTDr6BeEQwWYcX2R\" - id: alert type: webhook if: failure() description: notify alert system when check failed properties: url: \"https://example.com/alert\" query_params: title: good files webhook failed - id: notification type: webhook if: true description: notify that will always run - no matter if one of the previous steps failed properties: url: \"https://example.com/notification\" query_params: title: good files completed . Note: lakeFS will validate action files only when an Event has occurred. Use lakectl actions validate &lt;path&gt; to validate your action files locally. Uploading Action files . Action files should be uploaded with the prefix _lakefs_actions/ to the lakeFS repository. When an actionable event (see Supported Events above) takes place, lakeFS will read all files with prefix _lakefs_actions/ in the repository branch where the action occurred. A failure to parse an Action file will result with a failing Run. For example, lakeFS will search and execute all the matching Action files with the prefix lakefs://example-repo/feature-1/_lakefs_actions/ on: . | Commit to feature-1 branch on example-repo repository. | Merge to main branch from feature-1 branch on repo1 repository. | . ",
    "url": "/v1.4/howto/hooks/#configuration",
    
    "relUrl": "/howto/hooks/#configuration"
  },"251": {
    "doc": "Actions and Hooks",
    "title": "Supported Events",
    "content": "| Event | Description | . | pre-commit | Runs when the commit occurs, before the commit is finalized | . | post-commit | Runs after the commit is finalized | . | pre-merge | Runs on the source branch when the merge occurs, before the merge is finalized | . | post-merge | Runs on the merge result, after the merge is finalized | . | pre-create-branch | Runs on the source branch prior to creating a new branch | . | post-create-branch | Runs on the new branch after the branch was created | . | pre-delete-branch | Runs prior to deleting a branch | . | post-delete-branch | Runs after the branch was deleted | . | pre-create-tag | Runs prior to creating a new tag | . | post-create-tag | Runs after the tag was created | . | pre-delete-tag | Runs prior to deleting a tag | . | post-delete-tag | Runs after the tag was deleted | . lakeFS Actions are handled per repository and cannot be shared between repositories. A failure of any Hook under any Action of a pre-* event will result in aborting the lakeFS operation that is taking place. Hook failures under any Action of a post-* event will not revert the operation. Hooks are managed by Action files that are written to a prefix in the lakeFS repository. This allows configuration-as-code inside lakeFS, where Action files are declarative and written in YAML. ",
    "url": "/v1.4/howto/hooks/#supported-events",
    
    "relUrl": "/howto/hooks/#supported-events"
  },"252": {
    "doc": "Actions and Hooks",
    "title": "Runs API &amp; CLI",
    "content": "A Run is an instantiation of the repository’s Action files when the triggering event occurs. For example, if your repository contains a pre-commit hook, every commit would generate a Run for that specific commit. lakeFS will fetch, parse and filter the repository Action files and start to execute the Hooks under each Action. All executed Hooks (each with hook_run_id) exist in the context of that Run (run_id). The lakeFS API and lakectl expose the results of executions per repository, branch, commit, and specific Action. The endpoint also allows to download the execution log of any executed Hook under each Run for observability. ",
    "url": "/v1.4/howto/hooks/#runs-api--cli",
    
    "relUrl": "/howto/hooks/#runs-api--cli"
  },"253": {
    "doc": "Actions and Hooks",
    "title": "Result Files",
    "content": "The metadata section of lakeFS repository with each Run contains two types of files: . | _lakefs/actions/log/&lt;runID&gt;/&lt;hookRunID&gt;.log - Execution log of the specific Hook run. | _lakefs/actions/log/&lt;runID&gt;/run.manifest - Manifest with all Hooks execution for the run with their results and additional metadata. | . Note: Metadata section of a lakeFS repository is where lakeFS keeps its metadata, like commits and metaranges. Metadata files stored in the metadata section aren’t accessible like user stored files. ",
    "url": "/v1.4/howto/hooks/#result-files",
    
    "relUrl": "/howto/hooks/#result-files"
  },"254": {
    "doc": "Actions and Hooks",
    "title": "Actions and Hooks",
    "content": " ",
    "url": "/v1.4/howto/hooks/",
    
    "relUrl": "/howto/hooks/"
  },"255": {
    "doc": "Garbage Collection",
    "title": "Garbage Collection",
    "content": "lakeFS Cloud users enjoy a managed garbage collection service, and do not need to run this Spark program. ",
    "url": "/v1.4/howto/garbage-collection/",
    
    "relUrl": "/howto/garbage-collection/"
  },"256": {
    "doc": "How-To",
    "title": "lakeFS - How To",
    "content": ". ",
    "url": "/v1.4/howto/#lakefs---how-to",
    
    "relUrl": "/howto/#lakefs---how-to"
  },"257": {
    "doc": "How-To",
    "title": "Installation and upgrades",
    "content": ". | Step-by-step instructions for deploying and configuring lakeFS on AWS, GCP, Azure, and on-premises. | Details on how to upgrade lakeFS . | . ",
    "url": "/v1.4/howto/#installation-and-upgrades",
    
    "relUrl": "/howto/#installation-and-upgrades"
  },"258": {
    "doc": "How-To",
    "title": "Getting data in and out of lakeFS",
    "content": ". | Import and Export Data from lakeFS | Copy data to/from lakeFS | Using external Data Catalogs with data stored on lakeFS | Migrating away from lakeFS | . ",
    "url": "/v1.4/howto/#getting-data-in-and-out-of-lakefs",
    
    "relUrl": "/howto/#getting-data-in-and-out-of-lakefs"
  },"259": {
    "doc": "How-To",
    "title": "Actions and Hooks in lakeFS",
    "content": ". | Use Actions and Hooks as part of your workflow to validate data, enforce constraints, and do more when events occur. | . ",
    "url": "/v1.4/howto/#actions-and-hooks-in-lakefs",
    
    "relUrl": "/howto/#actions-and-hooks-in-lakefs"
  },"260": {
    "doc": "How-To",
    "title": "Branch Protection",
    "content": ". | Branch Protection prevents commits directly to a branch. This is a good way to enforce good practice and make sure that changes to important branches are only done by a merge. | . ",
    "url": "/v1.4/howto/#branch-protection",
    
    "relUrl": "/howto/#branch-protection"
  },"261": {
    "doc": "How-To",
    "title": "lakeFS Sizing Guide",
    "content": ". | This comprehensive guide details all you need to know to correctly size and test your lakeFS deployment for production use at scale, including: . | System Requirements | Scaling factors | Benchmarks | Important metrics | Reference architectures | . | . ",
    "url": "/v1.4/howto/#lakefs-sizing-guide",
    
    "relUrl": "/howto/#lakefs-sizing-guide"
  },"262": {
    "doc": "How-To",
    "title": "Garbage Collection",
    "content": ". | lakeFS will keep all of your objects forever, unless you tell it otherwise. Use Garbage Collection (GC) to remove objects from the underlying storage. If you want GC to happen automagically then you can use Managed Garbage Collection which is available as part of lakeFS Cloud. | . ",
    "url": "/v1.4/howto/#garbage-collection",
    
    "relUrl": "/howto/#garbage-collection"
  },"263": {
    "doc": "How-To",
    "title": "Private Link",
    "content": ". | Private Link enables lakeFS Cloud to interact with your infrastructure using private networking. | . ",
    "url": "/v1.4/howto/#private-link",
    
    "relUrl": "/howto/#private-link"
  },"264": {
    "doc": "How-To",
    "title": "Unity Delta Sharing",
    "content": ". | lakeFS Unity Delta Sharing provides a read-only experience from Unity Catalog for lakeFS customers. | . ",
    "url": "/v1.4/howto/#unity-delta-sharing",
    
    "relUrl": "/howto/#unity-delta-sharing"
  },"265": {
    "doc": "How-To",
    "title": "How-To",
    "content": " ",
    "url": "/v1.4/howto/",
    
    "relUrl": "/howto/"
  },"266": {
    "doc": "Use Cases",
    "title": "lakeFS Use Cases",
    "content": "lakeFS has many uses in the data world, including . | CI/CD for Data Lakes | ETL Testing Environment | Reproducibility | Rollback | . One of the important things that lakeFS provides is full support for Data Lifecycle Management through all stages: . | In Test | During Deployment | In Production | . ",
    "url": "/v1.4/understand/use_cases/#lakefs-use-cases",
    
    "relUrl": "/understand/use_cases/#lakefs-use-cases"
  },"267": {
    "doc": "Use Cases",
    "title": "Use Cases",
    "content": " ",
    "url": "/v1.4/understand/use_cases/",
    
    "relUrl": "/understand/use_cases/"
  },"268": {
    "doc": "How lakeFS Works",
    "title": "How lakeFS Works",
    "content": "The Architecture page includes a logical overview of lakeFS and its components. For deep-dive content about lakeFS see: . | Internal database structure | Merges in lakeFS | Versioning Internals | . ",
    "url": "/v1.4/understand/how/",
    
    "relUrl": "/understand/how/"
  },"269": {
    "doc": "Data Lifecycle Management",
    "title": "Data Lifecycle Management in lakeFS",
    "content": "lakeFS provides full support for Data Lifecycle Management through all stages: . | In Test | During Deployment | In Production | . ",
    "url": "/v1.4/understand/data_lifecycle_management/#data-lifecycle-management-in-lakefs",
    
    "relUrl": "/understand/data_lifecycle_management/#data-lifecycle-management-in-lakefs"
  },"270": {
    "doc": "Data Lifecycle Management",
    "title": "Data Lifecycle Management",
    "content": " ",
    "url": "/v1.4/understand/data_lifecycle_management/",
    
    "relUrl": "/understand/data_lifecycle_management/"
  },"271": {
    "doc": "Understanding lakeFS",
    "title": "Understanding lakeFS",
    "content": ". ",
    "url": "/v1.4/understand/",
    
    "relUrl": "/understand/"
  },"272": {
    "doc": "Understanding lakeFS",
    "title": "Architecture and Internals",
    "content": "The Architecture page includes a logical overview of lakeFS and its components. For deep-dive content about lakeFS see: . | Internal database structure | Merges in lakeFS | Versioning Internals | . ",
    "url": "/v1.4/understand/#architecture-and-internals",
    
    "relUrl": "/understand/#architecture-and-internals"
  },"273": {
    "doc": "Understanding lakeFS",
    "title": "lakeFS Use Cases",
    "content": "lakeFS has many uses in the data world, including . | CI/CD for Data Lakes | ETL Testing Environment | Reproducibility | Rollback | . One of the important things that lakeFS provides is full support for Data Lifecycle Management through all stages: . | In Test | During Deployment | In Production | . ",
    "url": "/v1.4/understand/#lakefs-use-cases",
    
    "relUrl": "/understand/#lakefs-use-cases"
  },"274": {
    "doc": "Understanding lakeFS",
    "title": "lakeFS Concepts and Model",
    "content": "lakeFS adopts many of the terms and concepts from git. This page goes into details on the similarities and differences, and provides a good background to the concepts used in lakeFS. ",
    "url": "/v1.4/understand/#lakefs-concepts-and-model",
    
    "relUrl": "/understand/#lakefs-concepts-and-model"
  },"275": {
    "doc": "Understanding lakeFS",
    "title": "Performance",
    "content": "Check out the Performance best practices guide for useful hints and tips on ensuring high performance from lakeFS. ",
    "url": "/v1.4/understand/#performance",
    
    "relUrl": "/understand/#performance"
  },"276": {
    "doc": "Understanding lakeFS",
    "title": "FAQ and Glossary",
    "content": "The FAQ covers many common questions around lakeFS, and the glossary provides a useful reference for the terms used in lakeFS. ",
    "url": "/v1.4/understand/#faq-and-glossary",
    
    "relUrl": "/understand/#faq-and-glossary"
  },"277": {
    "doc": "Understanding lakeFS",
    "title": "Products",
    "content": "lakeFS is an Apache 2.0 licensed open-source project. Treeverse also provides two commercial products: . | lakeFS Cloud | lakeFS Enterprise | . Find out more on the pricing page, or contact us for details. ",
    "url": "/v1.4/understand/#products",
    
    "relUrl": "/understand/#products"
  },"278": {
    "doc": "Documentation",
    "title": "lakeFS Documentation",
    "content": "Any contribution to the docs, whether it is in conjunction with a code contribution or as a standalone, is appreciated. Please see the contributing guide for details on contributing to lakeFS in general. ",
    "url": "/v1.4/project/docs/#lakefs-documentation",
    
    "relUrl": "/project/docs/#lakefs-documentation"
  },"279": {
    "doc": "Documentation",
    "title": "Table of contents",
    "content": ". | lakeFS Documentation Philosophy | lakeFS Style Guide: | Headings and Table of Contents | Callouts 💬 | Links 🔗 | Test your changes locally | Link Checking locally | . 📝 Notice! lakeFS documentation is written using Markdown. Make sure to familiarize yourself with the Markdown Guide. Customizing the lakeFS docs site should follow the following guidelines: Just The Docs Customization and style-guide. ",
    "url": "/v1.4/project/docs/#table-of-contents",
    
    "relUrl": "/project/docs/#table-of-contents"
  },"280": {
    "doc": "Documentation",
    "title": "lakeFS Documentation Philosophy",
    "content": "We are heavily inspired by the Diátaxis approach to documentation. At a very high-level, it defines documentation as falling into one of four categories: . | How To | Tutorial | Reference | Explanation | . There is a lot more to it than this, and you are encouraged to read the Diátaxis website for more details. Its application to lakeFS was discussed in #6197 . ",
    "url": "/v1.4/project/docs/#lakefs-documentation-philosophy",
    
    "relUrl": "/project/docs/#lakefs-documentation-philosophy"
  },"281": {
    "doc": "Documentation",
    "title": "lakeFS Style Guide:",
    "content": ". | Don’t use unnecessary tech jargon or vague/wordy constructions - keep it friendly, not condescending. | Be inclusive and welcoming - use gender-neutral words and pronouns when talking about abstract people like users and developers. | Replace complex expressions with simpler ones. | Keep it short - 25-30 words max per sentence. Otherwise, your readers might get lost on the way. | Use active voice instead of passive. For example: This feature can be used to do task X. vs. You can use this feature to do task X. The second one reads much better, right? | You can explain things better by including examples. Show, not tell. Use illustrations, images, gifs, code snippets, etc. | Establish a visual hierarchy to help people quickly find the information they need. Use text formatting to create levels of title and subtitle (such as # to ###### markdown headings). The title of every page should use the topmost heading #; all other headings on the page should use lower headers ## to ######. | . ",
    "url": "/v1.4/project/docs/#lakefs-style-guide",
    
    "relUrl": "/project/docs/#lakefs-style-guide"
  },"282": {
    "doc": "Documentation",
    "title": "Headings and Table of Contents",
    "content": "The title of the page should be H1 (# in markdown). Use headings in descending order and do not skip any. Pages should generally have a table of contents to help the user navigate it. Use the following snippet to add it to your page: . &lt;div class=\"toc-block\"&gt; ## {: .no_toc .text-delta } 1. TOC {:toc} {::options toc_levels=\"2\" /} &lt;/div&gt; . By default the page’s Table of Contents will include only H2 headings. If you want to include H2 and H3 then use this snippet instead: . &lt;div class=\"toc-block\"&gt; ## {: .no_toc .text-delta } 1. TOC {:toc} {::options toc_levels=\"2..3\" /} &lt;/div&gt; . Both of these snippets invoke {:toc} which is used by Kramdown (the Markdown processor that Jekyll uses) to insert a table of contents from the headings present in the markdown. ",
    "url": "/v1.4/project/docs/#headings-and-table-of-contents",
    
    "relUrl": "/project/docs/#headings-and-table-of-contents"
  },"283": {
    "doc": "Documentation",
    "title": "Callouts 💬",
    "content": "Multiple callout types are available. Please review this page for details. ",
    "url": "/v1.4/project/docs/#callouts-",
    
    "relUrl": "/project/docs/#callouts-"
  },"284": {
    "doc": "Documentation",
    "title": "Links 🔗",
    "content": "Links should use absolute paths in conjunction with {% link %}, e.g. {% link foo/example.md %}. Adding a link with an anchor is a bit trickier. Create a reference [link text][link-reference] and then define the anchor at the end of the document: . [link-reference]: {% link foo.example.md %}#anchor . This is so that references work within the versioned documentation that is deployed. Relative links, unless within the local folder, are discouraged as it can cause additional work when moving pages at a later date. ",
    "url": "/v1.4/project/docs/#links-",
    
    "relUrl": "/project/docs/#links-"
  },"285": {
    "doc": "Documentation",
    "title": "Test your changes locally",
    "content": "If you have the necessary dependencies installed, you can run Jekyll to build and serve the documentation from your machine using the provided Makefile target: . make docs-serve . The alternative is to use Docker which has the benefit of handling all the dependencies for you. Docker . | Launch the Docker container: . docker run --rm \\ --name lakefs_docs \\ -e TZ=\"Etc/UTC\" \\ --publish 4000:4000 --publish 35729:35729 \\ --volume=\"$PWD/docs:/srv/jekyll:Z\" \\ --volume=\"$PWD/docs/.jekyll-bundle-cache:/usr/local/bundle:Z\" \\ --interactive --tty \\ jekyll/jekyll:3.8 \\ jekyll serve --livereload . If you have make installed, you can also run make docs-serve-docker instead. | The first time you run the container it will need to download dependencies and will take several minutes to be ready. Once you see the following output, the docs server is ready to open in your web browser: . Server running... press ctrl-c to stop. | When you make a change to a page’s source the server will automatically rebuild the page which will be shown in the server log by this entry: . Regenerating: 1 file(s) changed at 2023-01-26 08:34:47 contributing.md Remote Theme: Using theme pmarsceill/just-the-docs . This can take a short while—you’ll see something like this in the server’s output when it’s done...done in 34.714073460 seconds. Your page will automatically reload to show the changes. | . If you are doing lots of work on the docs you may want to leave the Docker container in place (so that you don’t have to wait for the dependencies to load each time you re-create it). To do this replace the --rm with --detach in the docker run command, and use docker logs -f lakefs_docs to view the server log. ",
    "url": "/v1.4/project/docs/#test-your-changes-locally",
    
    "relUrl": "/project/docs/#test-your-changes-locally"
  },"286": {
    "doc": "Documentation",
    "title": "Link Checking locally",
    "content": "When making a pull request to lakeFS that involves a docs/* file, a GitHub action will automagically check the links. You can also run this link checker manually on your local machine: . | Build the site: . docker run --rm \\ --name lakefs_docs \\ -e TZ=\"Etc/UTC\" \\ --volume=\"$PWD/docs:/srv/jekyll:Z\" \\ --volume=\"$PWD/docs/.jekyll-bundle-cache:/usr/local/bundle:Z\" \\ --interactive --tty \\ jekyll/jekyll:3.8 \\ jekyll build --config _config.yml -d _site --watch . | Check the links: . docker run --rm \\ --name lakefs_docs_lychee \\ --volume \"$PWD:/data\"\\ --volume \"/tmp:/output\"\\ --tty \\ lycheeverse/lychee:master \\ --exclude-file /data/docs/.lycheeignore \\ --output /output/lychee_report.md \\ --format markdown \\ /data/docs/_site . | Review the lychee_report.md in your local /tmp folder . | . ",
    "url": "/v1.4/project/docs/#link-checking-locally",
    
    "relUrl": "/project/docs/#link-checking-locally"
  },"287": {
    "doc": "Documentation",
    "title": "Documentation",
    "content": " ",
    "url": "/v1.4/project/docs/",
    
    "relUrl": "/project/docs/"
  },"288": {
    "doc": "The lakeFS Project",
    "title": "The lakeFS Project",
    "content": "lakeFS provides version control over the data lake and lakehouse, and uses Git-like semantics to create and access those versions. If you know git, you’ll be right at home with lakeFS. lakeFS is an open-source project under the Apache 2.0 license. The project was created and is supported by Treeverse, a commercial company founded by engineers passionate about providing solutions to the evolving world of data engineering. ",
    "url": "/v1.4/project/",
    
    "relUrl": "/project/"
  },"289": {
    "doc": "The lakeFS Project",
    "title": "Table of contents",
    "content": ". | Our commitment to open source | Roadmap | . ",
    "url": "/v1.4/project/#table-of-contents",
    
    "relUrl": "/project/#table-of-contents"
  },"290": {
    "doc": "The lakeFS Project",
    "title": "Our commitment to open source",
    "content": "lakeFS is an open-source project under the Apache 2.0 license. The project was created and is supported by Treeverse, a commercial company founded by engineers passionate about providing solutions to the evolving world of data engineering. Why did we choose to open the source of our core capabilities? . We believe in the bottom-up adoption of technologies. We believe collaborative communities have the power to bring the best solutions to the community. We believe that every engineer should be able to use, contribute to, and influence cutting edge technologies, so they can innovate in their domain. What is our commitment to open source? . We created lakeFS, our open-source project, to provide a Git-like interface on top of object stores - so that you can fully take advantage of with any data application at any scale. For that reason, we commit that the following capabilities are and will remain open-source as part of lakeFS: . | All versioning capabilities, | Git-Like interface for the versioning operations, | Support for public object store APIs, | Integrations with publicly available applications accessing an object store, | CLI, API, and GUI interfaces. | . We also commit to keeping lakeFS scalable in throughput and performance. We are deeply committed to our community of engineers who use and contribute to the project. We are and will continue to be highly responsive and shape lakeFS together to provide the data versioning capabilities we are all looking for. What is lakeFS Cloud? . Treeverse offers lakeFS Cloud, which provides all the same benefits of the Git-like interface on top of object stores as a fully-managed service. The vision behind lakeFS Cloud is to provide a managed data versioning and management solution for data practitioners. lakeFS Cloud will leverage the lakeFS open-source technology, integrate capabilities and unique features, and lead its users to implement best practices. As part of our commitment to the open source values of transparency and interoperability, we believe everyone should be able to enjoy these benefits, regardless of whether or not they choose to use the managed offering. Because of that, we will not intentionally make it harder to build these features independently on top of the open source solution. ",
    "url": "/v1.4/project/#our-commitment-to-open-source",
    
    "relUrl": "/project/#our-commitment-to-open-source"
  },"291": {
    "doc": "The lakeFS Project",
    "title": "Roadmap",
    "content": "This is the lakeFS public roadmap. As the project evolves and the community grows, this roadmap might change. Please share your feedback on the #dev on Slack or by starting a GitHub Discussion! . ",
    "url": "/v1.4/project/#roadmap",
    
    "relUrl": "/project/#roadmap"
  },"292": {
    "doc": "Slack",
    "title": "Slack",
    "content": " ",
    "url": "/v1.4/slack/",
    
    "relUrl": "/slack/"
  },"293": {
    "doc": "Welcome to lakeFS",
    "title": "Welcome to the Lake!",
    "content": ". lakeFS brings software engineering best practices and applies them to data engineering. lakeFS provides version control over the data lake, and uses Git-like semantics to create and access those versions. If you know git, you’ll be right at home with lakeFS. With lakeFS, you can use concepts on your data lake such as branch to create an isolated version of the data, commit to create a reproducible point in time, and merge in order to incorporate your changes in one atomic action. ",
    "url": "/v1.4/#welcome-to-the-lake",
    
    "relUrl": "/#welcome-to-the-lake"
  },"294": {
    "doc": "Welcome to lakeFS",
    "title": "How Do I Get Started?",
    "content": "The hands-on quickstart guides you through some of core features of lakeFS. These include branching, merging, and rolling back changes to data. You can use the 30-day free trial of lakeFS Cloud if you want to try out lakeFS without installing anything. ",
    "url": "/v1.4/#how-do-i-get-started",
    
    "relUrl": "/#how-do-i-get-started"
  },"295": {
    "doc": "Welcome to lakeFS",
    "title": "Key lakeFS Features",
    "content": ". | It is format-agnostic. | It works with numerous data tools and platforms. | Your data stays in place. | It minimizes data duplication via a copy-on-write mechanism. | It maintains high performance over data lakes of any size. | It includes configurable garbage collection capabilities. | It is proven in production and has an active community. | . ",
    "url": "/v1.4/#key-lakefs-features",
    
    "relUrl": "/#key-lakefs-features"
  },"296": {
    "doc": "Welcome to lakeFS",
    "title": "How Does lakeFS Work With Other Tools?",
    "content": "lakeFS is an open source project that supports managing data in AWS S3, Azure Blob Storage, Google Cloud Storage (GCS) and any other object storage with an S3 interface. It integrates seamlessly with popular data frameworks such as Spark, Hive Metastore, dbt, Trino, Presto, and many others and includes an S3 compatibility layer. For more details and a full list see the integrations pages. lakeFS maintains compatibility with the S3 API to minimize adoption friction. You can use it as a drop-in replacement for S3 from the perspective of any tool interacting with a data lake. For example, take the common operation of reading a collection of data from an object storage into a Spark DataFrame. For data outside a lakeFS repo, the code will look like this: . df = spark.read.parquet(\"s3a://my-bucket/collections/foo/\") . After adding the data collections into my-bucket to a repository, the same operation becomes: . df = spark.read.parquet(\"s3a://my-repo/main-branch/collections/foo/\") . You can use the same methods and syntax you are already using to read and write data when using a lakeFS repository. This simplifies the adoption of lakeFS - minimal changes are needed to get started, making further changes an incremental process. ",
    "url": "/v1.4/#how-does-lakefs-work-with-other-tools",
    
    "relUrl": "/#how-does-lakefs-work-with-other-tools"
  },"297": {
    "doc": "Welcome to lakeFS",
    "title": "lakeFS is Git for Data",
    "content": "Git conquered the world of code because it had best supported engineering best practices required by developers, in particular: . | Collaborate during development. | Develop and Test in isolation | Revert code repository to a sable version in case of an error | Reproduce and troubleshoot issues with a given version of the code | Continuously integrate and deploy new code (CI/CD) | . lakeFS provides these exact benefits, that are data practitioners are missing today, and enables them a clear intuitive Git-like interface to easily manage their data like they manage code. Through its versioning engine, lakeFS enables the following built-in operations familiar from Git: . | branch: a consistent copy of a repository, isolated from other branches and their changes. Initial creation of a branch is a metadata operation that does not duplicate objects. | commit: an immutable checkpoint containing a complete snapshot of a repository. | merge: performed between two branches — merges atomically update one branch with the changes from another. | revert: returns a repo to the exact state of a previous commit. | tag: a pointer to a single immutable commit with a readable, meaningful name. | . See the object model for an in-depth definition of these, and the CLI reference for the full list of commands. Incorporating these operations into your data lake pipelines provides the same collaboration and organizational benefits you get when managing application code with source control. The lakeFS Promotion Workflow . Here’s how lakeFS branches and merges improve the universal process of updating collections with the latest data. | First, create a new branch from main to instantly generate a complete “copy” of your production data. | Apply changes or make updates on the isolated branch to understand their impact prior to exposure. | And finally, perform a merge from the feature branch back to main to atomically promote the updates into production. | . Following this pattern, lakeFS facilitates a streamlined data deployment workflow that consistently produces data assets you can have total confidence in. ",
    "url": "/v1.4/#lakefs-is-git-for-data",
    
    "relUrl": "/#lakefs-is-git-for-data"
  },"298": {
    "doc": "Welcome to lakeFS",
    "title": "How Can lakeFS Help Me?",
    "content": "lakeFS helps you maintain a tidy data lake in several ways, including: . Isolated Dev/Test Environments with copy-on-write . lakeFS makes creating isolated dev/test environments for ETL testing instantaneous, and through its use of copy-on-write, cheap. This enables you to test and validate code changes on production data without impacting it, as well as run analysis and experiments on production data in an isolated clone. 👉🏻 Read more . Reproducibility: What Did My Data Look Like at a Point In Time? . Being able to look at data as it was at a given point is particularly useful in at least two scenarios: . | Reproducibility of ML experiments . ML experimentation is usually an iterative process, and being able to reproduce a specific iteration is important. With lakeFS you can version all components of an ML experiment including its data, as well as make use of copy-on-write to minimise the footprint of versions of the data . | Troubleshooting production problems . Data engineers are often asked to validate the data. A user might report inconsistencies, question the accuracy, or simply report it to be incorrect. Since the data continuously changes, it is challenging to understand its state at the time of the error. With lakeFS you can create a branch from a commit to debug an issue in isolation. | . 👉🏻 Read More . Rollback of Data Changes and Recovery from Data Errors . Human error, misconfiguration, or wide-ranging systematic effects are unavoidable. When they do happen, erroneous data may make it into production or critical data assets might accidentally be deleted. By their nature, backups are a wrong tool for recovering from such events. Backups are periodic events that are usually not tied to performing erroneous operations. So, they may be out of date, and will require sifting through data at the object level. This process is inefficient and can take hours, days, or in some cases, weeks to complete. By quickly committing entire snapshots of data at well-defined times, recovering data in deletion or corruption events becomes an instant one-line operation with lakeFS: just identify a good historical commit, and then restore to it or copy from it. 👉🏻 Read more . Multi-Table Transactions guarantees . Data engineers typically need to implement custom logic in scripts to guarantee two or more data assets are updated synchronously. This logic often requires extensive rewrites or periods during which data is unavailable. The lakeFS merge operation from one branch into another removes the need to implement this logic yourself. Instead, make updates to the desired data assets on a branch and then utilize a lakeFS merge to atomically expose the data to downstream consumers. To learn more about atomic cross-collection updates, check out this video which describes the concept in more detail, along with this notebook in the lakeFS samples repository. Establishing data quality guarantees - CI/CD for data . The best way to deal with mistakes is to avoid them. A data source that is ingested into the lake introducing low-quality data should be blocked before exposure if possible. With lakeFS, you can achieve this by tying data quality tests to commit and merge operations via lakeFS hooks. 👉🏻 Read more . ",
    "url": "/v1.4/#how-can-lakefs-help-me",
    
    "relUrl": "/#how-can-lakefs-help-me"
  },"299": {
    "doc": "Welcome to lakeFS",
    "title": "Downloads",
    "content": "lakeFS is also available as a fully-managed hosted service on lakeFS Cloud . Binary Releases . Binary packages are available for Linux/macOS/Windows on GitHub Releases . Or using Homebrew for Linux/macOS: . # add repository brew tap treeverse/lakefs # installing lakefs/lakectl brew install lakefs . Docker Images . The official Docker images are available at https://hub.docker.com/r/treeverse/lakefs . ",
    "url": "/v1.4/#downloads",
    
    "relUrl": "/#downloads"
  },"300": {
    "doc": "Welcome to lakeFS",
    "title": "Welcome to lakeFS",
    "content": " ",
    "url": "/v1.4/",
    
    "relUrl": "/"
  },"301": {
    "doc": "Apache Kafka",
    "title": "Using lakeFS with Apache Kafka",
    "content": "Apache Kafka provides a unified, high-throughput, low-latency platform for handling real-time data feeds. Different distributions of Kafka offer different methods for exporting data to S3 called Kafka Sink Connectors. The most commonly used Connector for S3 is Confluent’s S3 Sink Connector. Add the following to connector.properties file for lakeFS support: . # Your lakeFS repository s3.bucket.name=example-repo # Your lakeFS S3 endpoint and credentials store.url=https://lakefs.example.com aws.access.key.id=AKIAIOSFODNN7EXAMPLE aws.secret.access.key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # main being the branch we want to write to topics.dir=main/topics . ",
    "url": "/v1.4/integrations/kafka.html#using-lakefs-with-apache-kafka",
    
    "relUrl": "/integrations/kafka.html#using-lakefs-with-apache-kafka"
  },"302": {
    "doc": "Apache Kafka",
    "title": "Apache Kafka",
    "content": " ",
    "url": "/v1.4/integrations/kafka.html",
    
    "relUrl": "/integrations/kafka.html"
  },"303": {
    "doc": "Kubeflow",
    "title": "Using lakeFS with Kubeflow pipelines",
    "content": "Kubeflow is a project dedicated to making deployments of ML workflows on Kubernetes simple, portable, and scalable. A Kubeflow pipeline is a portable and scalable definition of an ML workflow composed of steps. Each step in the pipeline is an instance of a component represented as an instance of ContainerOp. ",
    "url": "/v1.4/integrations/kubeflow.html#using-lakefs-with-kubeflow-pipelines",
    
    "relUrl": "/integrations/kubeflow.html#using-lakefs-with-kubeflow-pipelines"
  },"304": {
    "doc": "Kubeflow",
    "title": "Table of contents",
    "content": ". | Add pipeline steps for lakeFS operations . | Function-based ContainerOps | Non-function-based ContainerOps | . | Add the lakeFS steps to your pipeline . | Example pipeline | . | . ",
    "url": "/v1.4/integrations/kubeflow.html#table-of-contents",
    
    "relUrl": "/integrations/kubeflow.html#table-of-contents"
  },"305": {
    "doc": "Kubeflow",
    "title": "Add pipeline steps for lakeFS operations",
    "content": "To integrate lakeFS into your Kubeflow pipeline, you need to create Kubeflow components that perform lakeFS operations. Currently, there are two methods to create lakeFS ContainerOps: . | Implement a function-based ContainerOp that uses the lakeFS Python API to invoke lakeFS operations. | Implement a ContainerOp that uses the lakectl CLI docker image to invoke lakeFS operations. | . Function-based ContainerOps . To implement a function-based component that invokes lakeFS operations, you should use the Python OpenAPI client lakeFS provides. See the example below that demonstrates how to make the client’s package available to your ContainerOp. Example operations . Create a new branch: A function-based ContainerOp that creates a branch called example-branch based on the main branch of example-repo. from kfp import components def create_branch(repo_name, branch_name, source_branch): import lakefs_client from lakefs_client import models from lakefs_client.client import LakeFSClient # lakeFS credentials and endpoint configuration = lakefs_client.Configuration() configuration.username = 'AKIAIOSFODNN7EXAMPLE' configuration.password = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' configuration.host = 'https://lakefs.example.com' client = LakeFSClient(configuration) client.branches.create_branch(repository=repo_name, branch_creation=models.BranchCreation(name=branch_name, source=source_branch)) # Convert the function to a lakeFS pipeline step. create_branch_op = components.func_to_container_op( func=create_branch, packages_to_install=['lakefs_client==&lt;lakeFS version&gt;']) # Type in the lakeFS version you are using . You can invoke any lakeFS operation supported by lakeFS OpenAPI. For example, you could implement a commit and merge function-based ContainerOps. Check out the full API reference. Non-function-based ContainerOps . To implement a non-function based ContainerOp, you should use the treeverse/lakectl docker image. With this image, you can run lakectl commands to execute the desired lakeFS operation. For lakectl to work with Kubeflow, you will need to pass your lakeFS configurations as environment variables named: . | LAKECTL_CREDENTIALS_ACCESS_KEY_ID: AKIAIOSFODNN7EXAMPLE | LAKECTL_SECRET_ACCESS_KEY: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY | LAKECTL_SERVER_ENDPOINT_URL: https://lakefs.example.com | . Example operations . | Commit changes to a branch: A ContainerOp that commits uncommitted changes to example-branch on example-repo. from kubernetes.client.models import V1EnvVar def commit_op(): return dsl.ContainerOp( name='commit', image='treeverse/lakectl', arguments=['commit', 'lakefs://example-repo/example-branch', '-m', 'commit message']).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_ACCESS_KEY_ID',value='AKIAIOSFODNN7EXAMPLE')).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY',value='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')).add_env_variable(V1EnvVar(name='LAKECTL_SERVER_ENDPOINT_URL',value='https://lakefs.example.com')) . | Merge two lakeFS branches: A ContainerOp that merges example-branch into the main branch of example-repo. def merge_op(): return dsl.ContainerOp( name='merge', image='treeverse/lakectl', arguments=['merge', 'lakefs://example-repo/example-branch', 'lakefs://example-repo/main']).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_ACCESS_KEY_ID',value='AKIAIOSFODNN7EXAMPLE')).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY',value='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')).add_env_variable(V1EnvVar(name='LAKECTL_SERVER_ENDPOINT_URL',value='https://lakefs.example.com')) . | . You can invoke any lakeFS operation supported by lakectl by implementing it as a ContainerOp. Check out the complete CLI reference for the list of supported operations. Note The lakeFS Kubeflow integration that uses lakectl is supported on lakeFS version &gt;= v0.43.0. ",
    "url": "/v1.4/integrations/kubeflow.html#add-pipeline-steps-for-lakefs-operations",
    
    "relUrl": "/integrations/kubeflow.html#add-pipeline-steps-for-lakefs-operations"
  },"306": {
    "doc": "Kubeflow",
    "title": "Add the lakeFS steps to your pipeline",
    "content": "Add the steps created in the previous step to your pipeline before compiling it. Example pipeline . A pipeline that implements a simple ETL that has steps for branch creation and commits. def lakectl_pipeline(): create_branch_task = create_branch_op('example-repo', 'example-branch', 'main') # A function-based component extract_task = example_extract_op() commit_task = commit_op() transform_task = example_transform_op() commit_task = commit_op() load_task = example_load_op() . Note It’s recommended to store credentials as Kubernetes secrets and pass them as environment variables to Kubeflow operations using V1EnvVarSource. ",
    "url": "/v1.4/integrations/kubeflow.html#add-the-lakefs-steps-to-your-pipeline",
    
    "relUrl": "/integrations/kubeflow.html#add-the-lakefs-steps-to-your-pipeline"
  },"307": {
    "doc": "Kubeflow",
    "title": "Kubeflow",
    "content": " ",
    "url": "/v1.4/integrations/kubeflow.html",
    
    "relUrl": "/integrations/kubeflow.html"
  },"308": {
    "doc": "Internal database structure",
    "title": "Internal database structure",
    "content": " ",
    "url": "/v1.4/understand/how/kv.html",
    
    "relUrl": "/understand/how/kv.html"
  },"309": {
    "doc": "Internal database structure",
    "title": "Table of contents",
    "content": ". | Optimistic Locking with KV | DB Transactions and Atomic Updates | So, Which Approach is Better? | . Starting at version 0.80.2, lakeFS abandoned the tight coupling to PostgreSQL and moved all database operations to work over Key-Value Store . While SQL databases, and Postgres among them, have their obvious advantages, we felt that the tight coupling to Postgres is limiting our users and so, lakeFS with Key Value Store is introduced. Our KV Store implements a generic interface, with methods for Get, Set, Compare-and-Set, Delete and Scan. Each entry is represented by a [partition, key, value] triplet. All these fields are generic byte-array, and the using module has maximal flexibility on the format to use for each field . Under the hood, our KV implementation relies on a backing DB, which persists the data. Theoretically, it could be any type of database and out of the box, we already implemented drivers for DynamoDB, for AWS users, and PostgreSQL, using its relational nature to store a KV Store. More databases will be supported in the future, and lakeFS users and contributors can develop their own driver to use their own favorite database. For experimenting purposes, an in-memory KV store can be used, though it obviously lack the persistency aspect . In order to store its metadata objects (that is Repositories, Branches, Commits, Tags, and Uncommitted Objects), lakeFS implements another layer over the generic KV Store, which supports serialization and deserialization of these objects as protobuf. As this layer relies on the generic interface of the KV Store layer, it is totally agnostic to whichever store implementation is in use, gaining our users the maximal flexibility . For further reading, please refer to our KV Design . ",
    "url": "/v1.4/understand/how/kv.html#table-of-contents",
    
    "relUrl": "/understand/how/kv.html#table-of-contents"
  },"310": {
    "doc": "Internal database structure",
    "title": "Optimistic Locking with KV",
    "content": "One important key difference between SQL databases and Key Value Store is the ability to lock resources. While this is a common practice with relational databases, Key Value stores not always support this ability. When designing our KV Store, we tried to support the most simplistic straight-forward interface, with flexibility in backing DB selection, and so, we decided not to support locking. This decision brought some concurrency challenges we had to overcome. Let us take a look at a common lakeFS flow, Commit, during which several database operations are performed: . | All relevant (Branch correlated) uncommitted objects are collected and marked as committed | A new Commit object is created | The relevant Branch is updated to point to the new commit | . The Commit flow includes multiple database accesses and modifications, and is very sensitive to concurrent executions: If 2 Commit flows run in parallel, we must guarantee correctness of the data. lakeFS with PostgreSQL simply locks the Branch for the entire Commit operation, preventing concurrent execution of such flows. Now, with KV Store replacing the SQL database, this easy solution is no longer available. Instead, we implemented an Optimistic Locking algorithm, which leverages the KV Store Compare-And-Set (CAS) functionality to remember the Branch state at the beginning of the Commit flow, and updating the branch at the end, only if it remains unchanged, using CAS, with the former Branch state, used as a comparison criteria. If the sampled Branch state and the current state differ, it could only mean that another, later, Commit is in progress, causing the first Commit to fail, and give the later Commit a chance to complete. Here’s a running example: . | Commit A sets the StagingToken to tokenA and samples the Branch, | Commit B sets the StagingToken to tokenB and samples the Branch, | Commit A finishes, tries to update the Branch and fails due to the recent modification by Commit B - the StagingToken is set to tokenB and not tokenA as expected by Commit A, | Commit B finishes and updates the branch, as tokenB is set as StagingToken and it matches the flow expectation | . An important detail to note, is that as a Commit starts, and the StagingToken is set a new value, the former value is added to a list of ‘still valid’ StagingToken_s - _SealedToken - on the Branch, which makes sure no StagingToken and no object are lost due to a failed Commit . You can read more on the Commit Flow in the dedicated section in the KV Design . ",
    "url": "/v1.4/understand/how/kv.html#optimistic-locking-with-kv",
    
    "relUrl": "/understand/how/kv.html#optimistic-locking-with-kv"
  },"311": {
    "doc": "Internal database structure",
    "title": "DB Transactions and Atomic Updates",
    "content": "Another notable difference is the existence of DB transactions with PostgreSQL, ability that our KV Store lacks. This ability was leveraged by lakeFS to construct several DB updates, into one “atomic” operation - each failure, in each step, rolled back the entire operation, keeping the DB consistent and clean. With KV Store, this ability is gone, and we had to come up with various solutions. As a starting point, the DB consistency is, obviously, not anything we can risk. On the other hand, maintaining the DB clean, and as a result smaller, is something that can be sacrificed, at least as a first step. Let us take a look at a relatively simple flow of a new Repository creation: A brand new Repository has 3 objects: The Repository object itself, an initial Branch object and an initial Commit, which the Branch points to. With SQL DB, it was as simple as creating all 3 objects in the DB under one transaction (at this order). Any failure resulted in a rollback and no redundant leftovers in our DB. With no transaction in KV Store, if for example the Branch creation fails, it will leave the Repository without an initial Branch (or a Branch at all), yet the Repository will be accessible. Trying to delete the Repository as a response to Branch creation failure is ony a partial solution as this operation can fail as well. To mitigate this we introduced a per-_Repository_-partition, which holds all repository related objects (the Branch and Commit in this scenario). The partition key can only be derived from the specific Repository instance itself. In addition we first create the Repository objects, the Commit and the Branch, under the Repository’s partition key, and then the Repository is created. The Repository and its objects will be accessible only after a successful creation of all 3 entities. A failure in this flow might leave some dangling objects, but consistency is maintained. The number of such dangling objects is not expected to be significant, and we plan to implement a cleaning algorithm to keep our KV Store neat and clean . ",
    "url": "/v1.4/understand/how/kv.html#db-transactions-and-atomic-updates",
    
    "relUrl": "/understand/how/kv.html#db-transactions-and-atomic-updates"
  },"312": {
    "doc": "Internal database structure",
    "title": "So, Which Approach is Better?",
    "content": "This documents provides a peek into our new database approach - Key Value Store instead of a Relational SQL. It discusses the challenges we faced, and the solutions we provided to overcome these challenges. Considering the fact that lakeFS over with relational database did work, you might ask yourself why did we bother to develop another solution. The simple answer, is that while PostgreSQL was not a bad option, it was the only option, and any drawback of PostgreSQL, reflected on our users: . | PostgreSQL can only scale vertically and that is a limitation. At some point this might not hold. | PostgreSQL is not a managed solution, meaning that users had to take care of all maintenance tasks, including the above mentioned scale (when needed) | As an unmanaged database, scaling means downtime - is that acceptable? | It might even get to the point that your organization is not willing to work with PostgreSQL due to various business considerations | . If none of the above apply, and you have no seemingly reason to switch from PostgreSQL, it can definitely still be used as an excellent option for the backing database for the lakeFS KV Store. If you do need another solution, you have DynamoDB support, out of the box. DynamoDB, as a fully managed solution, with horizontal scalability support and optimized partitions support, answers all the pain-points specified above. It is definitely an option to consider, if you need to overcome these And, of course, you can always decide to implement your own KV Store driver to use your database of choice - we would love to add your contribution to lakeFS . ",
    "url": "/v1.4/understand/how/kv.html#so-which-approach-is-better",
    
    "relUrl": "/understand/how/kv.html#so-which-approach-is-better"
  },"313": {
    "doc": "lakeFS Cloud",
    "title": "lakeFS Cloud",
    "content": "lakeFS Cloud is a fully-managed lakeFS solution provided by Treeverse, implemented using our best practices, providing high availability, auto-scaling, support and enterprise-ready features. ",
    "url": "/v1.4/understand/lakefs-cloud.html",
    
    "relUrl": "/understand/lakefs-cloud.html"
  },"314": {
    "doc": "lakeFS Cloud",
    "title": "lakeFS Cloud Features",
    "content": ". | Role-Based Access Control | Auditing | Single-Sign-On (including support for SAML, OIDC, AD FS, Okta, and Azure AD) | Managed Garbage Collection | Private-Link | Unity Delta Sharing | SOC 2 Type II Compliance | . ",
    "url": "/v1.4/understand/lakefs-cloud.html#lakefs-cloud-features",
    
    "relUrl": "/understand/lakefs-cloud.html#lakefs-cloud-features"
  },"315": {
    "doc": "lakeFS Cloud",
    "title": "How lakeFS Cloud interacts with your infrastructure",
    "content": "Treeverse hosts and manages a dedicated lakeFS instance that interfaces with data held in your object store, such as S3. flowchart TD U[Users] --&gt; LFC subgraph Your Infrastructure IAMM[lakeFS Managed GC IAM Role] --&gt; ObjectStore[Client's Object Store] IAMA[lakeFS Application IAM Role] --&gt; ObjectStore end subgraph Treeverse's Infrastructure MGC[Managed Garbage Collection] --&gt; EMR[Elastic Map Reduce] EMR --&gt; IAMM MGC --&gt; CP CP[Control Plane] LFC --&gt; CP subgraph Client's Tenant LFC[lakeFS Cloud] --&gt; DB[Refstore Database] end LFC --&gt; IAMC[lakeFS Connector IAM Role] IAMC --&gt;|ExternalID| IAMA end . ",
    "url": "/v1.4/understand/lakefs-cloud.html#how-lakefs-cloud-interacts-with-your-infrastructure",
    
    "relUrl": "/understand/lakefs-cloud.html#how-lakefs-cloud-interacts-with-your-infrastructure"
  },"316": {
    "doc": "lakeFS Cloud",
    "title": "Setting up lakeFS Cloud",
    "content": "AWS / Azure . Please follow the self-service setup wizard on lakeFS Cloud . GCP . Please contact us for onboarding instructions. ",
    "url": "/v1.4/understand/lakefs-cloud.html#setting-up-lakefs-cloud",
    
    "relUrl": "/understand/lakefs-cloud.html#setting-up-lakefs-cloud"
  },"317": {
    "doc": "lakeFS Enterprise",
    "title": "lakeFS Enterprise",
    "content": "lakeFS Enterprise is an enterprise-ready lakeFS solution that provides a support SLA and additional features to the open-source version of lakeFS. The additional features are: . | RBAC | SSO | Support SLA | . For more details and pricing, please contact sales. ",
    "url": "/v1.4/understand/lakefs-enterprise.html",
    
    "relUrl": "/understand/lakefs-enterprise.html"
  },"318": {
    "doc": "1️⃣ Run lakeFS",
    "title": "Spin up the environment",
    "content": "If you don’t want to use Docker, you can use the 30-day free trial of lakeFS Cloud. Once you launch the free trial you will have access to the same content as this quickstart within the provided repository once you login. The quickstart uses Docker to bring up the lakeFS container, pre-populate it with some data, and also provides DuckDB from where we can interact with the data. You’ll need Docker installed to run this. Launch the lakeFS container: . docker run --name lakefs --pull always \\ --rm --publish 8000:8000 \\ treeverse/lakefs:latest \\ run --quickstart . After a few moments you should see the lakeFS container ready to use: . │ │ lakeFS running in quickstart mode. │ Login at http://127.0.0.1:8000/ │ │ Access Key ID : AKIAIOSFOLQUICKSTART │ Secret Access Key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY │ . You’re now ready to dive into lakeFS! . | Open lakeFS’s web interface at http://127.0.0.1:8000/ . | Login with the quickstart credentials. | Access Key ID: AKIAIOSFOLQUICKSTART | Secret Access Key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY | . | You’ll notice that there aren’t any repositories created yet. Click the Create Sample Repository button. | . You will see the sample repository created and the quickstart guide within it. You can follow along there, or here - it’s the same :) . ",
    "url": "/v1.4/quickstart/launch.html#spin-up-the-environment",
    
    "relUrl": "/quickstart/launch.html#spin-up-the-environment"
  },"319": {
    "doc": "1️⃣ Run lakeFS",
    "title": "1️⃣ Run lakeFS",
    "content": " ",
    "url": "/v1.4/quickstart/launch.html",
    
    "relUrl": "/quickstart/launch.html"
  },"320": {
    "doc": "Learn more about lakeFS",
    "title": "Learn more about lakeFS",
    "content": "The lakeFS quickstart is just the beginning of your lakeFS journey 🛣️ . Here are some more resources to help you find out more about lakeFS. ",
    "url": "/v1.4/quickstart/learning-more-lakefs.html",
    
    "relUrl": "/quickstart/learning-more-lakefs.html"
  },"321": {
    "doc": "Learn more about lakeFS",
    "title": "Connecting lakeFS to your own object storage",
    "content": "Enjoyed the quickstart and want to try out lakeFS against your own data? Here’s how to run lakeFS locally as a Docker container locally connecting to an object store. | AWS S3 | Azure Blob Storage | Google Cloud Storage | MinIO | . Note: Make sure the Quickstart Docker Compose from the previous steps isn’t also running as you’ll get a port conflict. docker run --pull always -p 8000:8000 \\ -e LAKEFS_BLOCKSTORE_TYPE='s3' \\ -e AWS_ACCESS_KEY_ID='YourAccessKeyValue' \\ -e AWS_SECRET_ACCESS_KEY='YourSecretKeyValue' \\ treeverse/lakefs run --local-settings . Note: Make sure the Quickstart Docker Compose from the previous steps isn’t also running as you’ll get a port conflict. docker run --pull always -p 8000:8000 \\ -e LAKEFS_BLOCKSTORE_TYPE='azure' \\ -e LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCOUNT='YourAzureStorageAccountName' \\ -e LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCESS_KEY='YourAzureStorageAccessKey' \\ treeverse/lakefs run --local-settings . Note: Make sure the Quickstart Docker Compose from the previous steps isn’t also running as you’ll get a port conflict. docker run --pull always -p 8000:8000 \\ -e LAKEFS_BLOCKSTORE_TYPE='gs' \\ -e LAKEFS_BLOCKSTORE_GS_CREDENTIALS_JSON='YourGoogleServiceAccountKeyJSON' \\ treeverse/lakefs run --local-settings . where you will replace YourGoogleServiceAccountKeyJSON with JSON string that contains your Google service account key. If you want to use the JSON file that contains your Google service account key instead of JSON string (as in the previous command) then go to the directory where JSON file is stored and run the command with local parameters: . docker run --pull always -p 8000:8000 \\ -v $PWD:/myfiles \\ -e LAKEFS_BLOCKSTORE_TYPE='gs' \\ -e LAKEFS_BLOCKSTORE_GS_CREDENTIALS_FILE='/myfiles/YourGoogleServiceAccountKey.json' \\ treeverse/lakefs run --local-settings . This command will mount your present working directory (PWD) within the container and will read the JSON file from your PWD. To use lakeFS with MinIO (or other S3-compatible object storage), use the following example: . Note: Make sure the Quickstart Docker Compose from the previous steps isn’t also running as you’ll get a port conflict. docker run --pull always -p 8000:8000 \\ -e LAKEFS_BLOCKSTORE_TYPE='s3' \\ -e LAKEFS_BLOCKSTORE_S3_FORCE_PATH_STYLE='true' \\ -e LAKEFS_BLOCKSTORE_S3_ENDPOINT='http://&lt;minio_endpoint&gt;' \\ -e LAKEFS_BLOCKSTORE_S3_DISCOVER_BUCKET_REGION='false' \\ -e LAKEFS_BLOCKSTORE_S3_CREDENTIALS_ACCESS_KEY_ID='&lt;minio_access_key&gt;' \\ -e LAKEFS_BLOCKSTORE_S3_CREDENTIALS_SECRET_ACCESS_KEY='&lt;minio_secret_key&gt;' \\ treeverse/lakefs run --local-settings . ",
    "url": "/v1.4/quickstart/learning-more-lakefs.html#connecting-lakefs-to-your-own-object-storage",
    
    "relUrl": "/quickstart/learning-more-lakefs.html#connecting-lakefs-to-your-own-object-storage"
  },"322": {
    "doc": "Learn more about lakeFS",
    "title": "Deploying lakeFS",
    "content": "Ready to do this thing for real? The deployment guides show you how to deploy lakeFS locally (including on Kubernetes) or on AWS, Azure, or GCP. Alternatively you might want to have a look at lakeFS Cloud which provides a fully-managed, SOC-2 compliant, lakeFS service. ",
    "url": "/v1.4/quickstart/learning-more-lakefs.html#deploying-lakefs",
    
    "relUrl": "/quickstart/learning-more-lakefs.html#deploying-lakefs"
  },"323": {
    "doc": "Learn more about lakeFS",
    "title": "lakeFS Samples",
    "content": "The lakeFS Samples GitHub repository includes some excellent examples including: . | How to implement multi-table transaction on multiple Delta Tables | Notebooks to show integration of lakeFS with Spark, Python, Delta Lake, Airflow and Hooks. | Examples of using lakeFS webhooks to run automated data quality checks on different branches. | Using lakeFS branching features to create dev/test data environments for ETL testing and experimentation. | Reproducing ML experiments with certainty using lakeFS tags. | . ",
    "url": "/v1.4/quickstart/learning-more-lakefs.html#lakefs-samples",
    
    "relUrl": "/quickstart/learning-more-lakefs.html#lakefs-samples"
  },"324": {
    "doc": "Learn more about lakeFS",
    "title": "lakeFS Community",
    "content": "The lakeFS community is important to us. Our guiding principles are: . | Fully open, in code and conversation | We learn and grow together | Compassion and respect in every interaction | . We’d love for you to join our Slack group and come and introduce yourself on #announcements-and-more. Or just lurk and soak up the vibes 😎 . If you’re interested in getting involved in the development of lakeFS, head over our the GitHub repo to look at the code and peruse the issues. The comprehensive contributing document should have you covered on next steps but if you’ve any questions the #dev channel on Slack will be delighted to help. We love speaking at meetups and chatting to community members at them - you can find a list of these here. Finally, make sure to drop by to say hi on Twitter, Mastodon, and LinkedIn 👋🏻 . ",
    "url": "/v1.4/quickstart/learning-more-lakefs.html#lakefs-community",
    
    "relUrl": "/quickstart/learning-more-lakefs.html#lakefs-community"
  },"325": {
    "doc": "Learn more about lakeFS",
    "title": "lakeFS Concepts and Internals",
    "content": "We describe lakeFS as “Git for data” but what does that actually mean? Have a look at the concepts and architecture guides, as well as the explanation of how merges are handled. To go deeper you might be interested in the internals of versioning and our internal database structure. ",
    "url": "/v1.4/quickstart/learning-more-lakefs.html#lakefs-concepts-and-internals",
    
    "relUrl": "/quickstart/learning-more-lakefs.html#lakefs-concepts-and-internals"
  },"326": {
    "doc": "Lua Hooks",
    "title": "Lua Hooks",
    "content": "lakeFS supports running hooks without relying on external components using an embedded Lua VM . Using Lua hooks, it is possible to pass a Lua script to be executed directly by the lakeFS server when an action occurs. The Lua runtime embedded in lakeFS is limited for security reasons. It provides a narrow set of APIs and functions that by default do not allow: . | Accessing any of the running lakeFS server’s environment | Accessing the local filesystem available the lakeFS process | . ",
    "url": "/v1.4/howto/hooks/lua.html",
    
    "relUrl": "/howto/hooks/lua.html"
  },"327": {
    "doc": "Lua Hooks",
    "title": "Table of contents",
    "content": ". | Action File Lua Hook Properties | Example Lua Hooks | Lua Library reference | . ",
    "url": "/v1.4/howto/hooks/lua.html#table-of-contents",
    
    "relUrl": "/howto/hooks/lua.html#table-of-contents"
  },"328": {
    "doc": "Lua Hooks",
    "title": "Action File Lua Hook Properties",
    "content": "See the Action configuration for overall configuration schema and details. | Property | Description | Data Type | Required | Default Value | . | args | One or more arguments to pass to the hook | Dictionary | false |   | . | script | An inline Lua script | String | either this or script_path must be specified |   | . | script_path | The path in lakeFS to a Lua script | String | either this or script must be specified |   | . ",
    "url": "/v1.4/howto/hooks/lua.html#action-file-lua-hook-properties",
    
    "relUrl": "/howto/hooks/lua.html#action-file-lua-hook-properties"
  },"329": {
    "doc": "Lua Hooks",
    "title": "Example Lua Hooks",
    "content": "For more examples and configuration samples, check out the examples/hooks/ directory in the lakeFS repository. You’ll also find step-by-step examples of hooks in action in the lakeFS samples repository. Display information about an event . This example will print out a JSON representation of the event that occurred: . name: dump_all on: post-commit: post-merge: post-create-tag: post-create-branch: hooks: - id: dump_event type: lua properties: script: | json = require(\"encoding/json\") print(json.marshal(action)) . Ensure that a commit includes a mandatory metadata field . A more useful example: ensure every commit contains a required metadata field: . name: pre commit metadata field check on: pre-commit: branches: - main - dev hooks: - id: ensure_commit_metadata type: lua properties: args: notebook_url: {\"pattern\": \"my-jupyter.example.com/.*\"} spark_version: {} script_path: lua_hooks/ensure_metadata_field.lua . Lua code at lakefs://repo/main/lua_hooks/ensure_metadata_field.lua: . regexp = require(\"regexp\") for k, props in pairs(args) do current_value = action.commit.metadata[k] if current_value == nil then error(\"missing mandatory metadata field: \" .. k) end if props.pattern and not regexp.match(props.pattern, current_value) then error(\"current value for commit metadata field \" .. k .. \" does not match pattern: \" .. props.pattern .. \" - got: \" .. current_value) end end . For more examples and configuration samples, check out the examples/hooks/ directory in the lakeFS repository. ",
    "url": "/v1.4/howto/hooks/lua.html#example-lua-hooks",
    
    "relUrl": "/howto/hooks/lua.html#example-lua-hooks"
  },"330": {
    "doc": "Lua Hooks",
    "title": "Lua Library reference",
    "content": "The Lua runtime embedded in lakeFS is limited for security reasons. The provided APIs are shown below. array(table) . Helper function to mark a table object as an array for the runtime by setting _is_array: true metatable field. aws/s3.get_object(bucket, key) . Returns the body (as a Lua string) of the requested object and a boolean value that is true if the requested object exists . aws/s3.put_object(bucket, key, value) . Sets the object at the given bucket and key to the value of the supplied value string . aws/s3.delete_object(bucket [, key]) . Deletes the object at the given key . aws/s3.list_objects(bucket [, prefix, continuation_token, delimiter]) . Returns a table of results containing the following structure: . | is_truncated: (boolean) whether there are more results to paginate through using the continuation token | next_continuation_token: (string) to pass in the next request to get the next page of results | results (table of tables) information about the objects (and prefixes if a delimiter is used) | . a result could in one of the following structures . { [\"key\"] = \"a/common/prefix/\", [\"type\"] = \"prefix\" } . or: . { [\"key\"] = \"path/to/object\", [\"type\"] = \"object\", [\"etag\"] = \"etagString\", [\"size\"] = 1024, [\"last_modified\"] = \"2023-12-31T23:10:00Z\" } . aws/s3.delete_recursive(bucket, prefix) . Deletes all objects under the given prefix . aws/glue . Glue client library. local aws = require(\"aws\") -- pass valid AWS credentials local glue = aws.glue_client(\"ACCESS_KEY_ID\", \"SECRET_ACCESS_KEY\", \"REGION\") . aws/glue.get_table(database, table [, catalog_id) . Describe a table from the Glue catalog. Example: . local table, exists = glue.get_table(db, table_name) if exists then print(json.marshal(table)) . aws/glue.create_table(database, table_input, [, catalog_id]) . Create a new table in Glue Catalog. The table_input argument is a JSON that is passed “as is” to AWS and is parallel to the AWS SDK TableInput . Example: . local json = require(\"encoding/json\") local input = { Name = \"my-table\", PartitionKeys = array(partitions), -- etc... } local json_input = json.marshal(input) glue.create_table(\"my-db\", table_input) . aws/glue.update_table(database, table_input, [, catalog_id, version_id, skip_archive]) . Update an existing Table in Glue Catalog. The table_input is the same as the argument in glue.create_table function. aws/glue.delete_table(database, table_input, [, catalog_id]) . Delete an existing Table in Glue Catalog. crypto/aes/encryptCBC(key, plaintext) . Returns a ciphertext for the aes encrypted text . crypto/aes/decryptCBC(key, ciphertext) . Returns the decrypted (plaintext) string for the encrypted ciphertext . crypto/hmac/sign_sha256(message, key) . Returns a SHA256 hmac signature for the given message with the supplied key (using the SHA256 hashing algorithm) . crypto/hmac/sign_sha1(message, key) . Returns a SHA1 hmac signature for the given message with the supplied key (using the SHA1 hashing algorithm) . crypto/md5/digest(data) . Returns the MD5 digest (string) of the given data . crypto/sha256/digest(data) . Returns the SHA256 digest (string) of the given data . databricks/client(databricks_host, databricks_service_principal_token) . Returns a table representing a Databricks client with the register_external_table and create_or_get_schema methods. databricks/client.create_schema(schema_name, catalog_name, get_if_exists) . Creates a schema, or retrieves it if exists, in the configured Databricks host’s Unity catalog. If a schema doesn’t exist, a new schema with the given schema_name will be created under the given catalog_name. Returns the created/fetched schema name. Parameters: . | schema_name(string): The required schema name | catalog_name(string): The catalog name under which the schema will be created (or from which it will be fetched) | get_if_exists(boolean): In case of failure due to an existing schema with the given schema_name in the given catalog_name, return the schema. | . Example: . local databricks = require(\"databricks\") local client = databricks.client(\"https://my-host.cloud.databricks.com\", \"my-service-principal-token\") local schema_name = client.create_schema(\"main\", \"mycatalog\", true) . databricks/client.register_external_table(table_name, physical_path, warehouse_id, catalog_name, schema_name) . Registers an external table under the provided warehouse ID, catalog name, and schema name. In order for this method call to succeed, an external location should be configured in the catalog, with the physical_path’s root storage URI (for example: s3://mybucket). Returns the table’s creation status. Parameters: . | table_name(string): Table name. | physical_path(string): A location to which the external table will refer, e.g. s3://mybucket/the/path/to/mytable. | warehouse_id(string): The SQL warehouse ID used in Databricks to run the CREATE TABLE query (fetched from the SQL warehouse Connection Details, or by running databricks warehouses get, choosing your SQL warehouse and fetching its ID). | catalog_name(string): The name of the catalog under which a schema will be created (or fetched from). | schema_name(string): The name of the schema under which the table will be created. | . Example: . local databricks = require(\"databricks\") local client = databricks.client(\"https://my-host.cloud.databricks.com\", \"my-service-principal-token\") local status = client.register_external_table(\"mytable\", \"s3://mybucket/the/path/to/mytable\", \"examwarehouseple\", \"my-catalog-name\", \"myschema\") . | For the Databricks permissions needed to run this method, check out the Unity Catalog Exporter docs. | . encoding/base64/encode(data) . Encodes the given data to a base64 string . encoding/base64/decode(data) . Decodes the given base64 encoded data and return it as a string . encoding/base64/url_encode(data) . Encodes the given data to an unpadded alternate base64 encoding defined in RFC 4648. encoding/base64/url_decode(data) . Decodes the given unpadded alternate base64 encoding defined in RFC 4648 and return it as a string . encoding/hex/encode(value) . Encode the given value string to hexadecimal values (string) . encoding/hex/decode(value) . Decode the given hexadecimal string back to the string it represents (UTF-8) . encoding/json/marshal(table) . Encodes the given table into a JSON string . encoding/json/unmarshal(string) . Decodes the given string into the equivalent Lua structure . encoding/yaml/marshal(table) . Encodes the given table into a YAML string . encoding/yaml/unmarshal(string) . Decodes the given YAML encoded string into the equivalent Lua structure . encoding/parquet/get_schema(payload) . Read the payload (string) as the contents of a Parquet file and return its schema in the following table structure: . { { [\"name\"] = \"column_a\", [\"type\"] = \"INT32\" }, { [\"name\"] = \"column_b\", [\"type\"] = \"BYTE_ARRAY\" } } . formats . formats/delta_client(key, secret, region) . Creates a new Delta Lake client used to interact with the lakeFS server. | key: lakeFS access key id | secret: lakeFS secret access key | region: The region in which your lakeFS server is configured at. | . formats/delta_client.get_table(repository_id, reference_id, prefix) . Returns a representation of a Delta Lake table under the given repository, reference, and prefix. The format of the response is a table {number, {string}} where number is a version in the Delta Log, and the mapped {string} table (list) contains JSON strings of the different Delta Lake log operations listed in the mapped version entry. gcloud . gcloud/gs_client(gcs_credentials_json_string) . Create a new Google Cloud Storage client using a string that contains a valid credentials.json file content. gcloud/gs.write_fuse_symlink(source, destination, mount_info) . Will create a gcsfuse symlink from the source (typically a lakeFS physical address for an object) to a given destination. mount_info is a Lua table with \"from\" and \"to\" keys - since symlinks don’t work for gs://... URIs, they need to point to the mounted location instead. from will be removed from the beginning of source, and destination will be added instead. Example: . source = \"gs://bucket/lakefs/data/abc/def\" destination = \"gs://bucket/exported/path/to/object\" mount_info = { [\"from\"] = \"gs://bucket\", [\"to\"] = \"/home/user/gcs-mount\" } gs.write_fuse_symlink(source, destination, mount_info) -- Symlink: \"/home/user/gcs-mount/exported/path/to/object\" -&gt; \"/home/user/gcs-mount/lakefs/data/abc/def\" . lakefs . The Lua Hook library allows calling back to the lakeFS API using the identity of the user that triggered the action. For example, if user A tries to commit and triggers a pre-commit hook - any call made inside that hook to the lakeFS API, will automatically use user A’s identity for authorization and auditing purposes. lakefs/create_tag(repository_id, reference_id, tag_id) . Create a new tag for the given reference . lakefs/diff_refs(repository_id, lef_reference_id, right_reference_id [, after, prefix, delimiter, amount]) . Returns an object-wise diff between left_reference_id and right_reference_id. lakefs/list_objects(repository_id, reference_id [, after, prefix, delimiter, amount]) . List objects in the specified repository and reference (branch, tag, commit ID, etc.). If delimiter is empty, will default to a recursive listing. Otherwise, common prefixes up to delimiter will be shown as a single entry. lakefs/get_object(repository_id, reference_id, path) . Returns 2 values: . | The HTTP status code returned by the lakeFS API | The content of the specified object as a lua string | . lakefs/diff_branch(repository_id, branch_id [, after, amount, prefix, delimiter]) . Returns an object-wise diff of uncommitted changes on branch_id. lakefs/stat_object(repository_id, ref_id, path) . Returns a stat object for the given path under the given reference and repository. lakefs/catalogexport/glue_exporter.get_full_table_name(descriptor, action_info) . Generate glue table name. Parameters: . | descriptor(Table): Object from (e.g. _lakefs_tables/my_table.yaml). | action_info(Table): The global action object. | . lakefs/catalogexport/delta_exporter . A package used to export Delta Lake tables from lakeFS to an external cloud storage. lakefs/catalogexport/delta_exporter.export_delta_log(action, table_names, writer, delta_client, table_descriptors_path) . The function used to export Delta Lake tables. The return value is a table with mapping of table names to external table location (from which it is possible to query the data). Parameters: . | action: The global action object | table_names: Delta tables name list (e.g. {\"table1\", \"table2\"}) | writer: A writer function with function(bucket, key, data) signature, used to write the exported Delta Log (e.g. aws/s3.s3_client.put_object) | delta_client: A Delta Lake client that implements get_table: function(repo, ref, prefix) | table_descriptors_path: The path under which the table descriptors of the provided table_names reside | . Example: . --- name: delta_exporter on: post-commit: null hooks: - id: delta_export type: lua properties: script: | local aws = require(\"aws\") local formats = require(\"formats\") local delta_exporter = require(\"lakefs/catalogexport/delta_exporter\") local table_descriptors_path = \"_lakefs_tables\" local sc = aws.s3_client(args.aws.access_key_id, args.aws.secret_access_key, args.aws.region) local delta_client = formats.delta_client(args.lakefs.access_key_id, args.lakefs.secret_access_key, args.aws.region) local delta_table_locations = delta_exporter.export_delta_log(action, args.table_names, sc.put_object, delta_client, table_descriptors_path) for t, loc in pairs(delta_table_locations) do print(\"Delta Lake exported table \\\"\" .. t .. \"\\\"'s location: \" .. loc .. \"\\n\") end args: aws: access_key_id: &lt;AWS_ACCESS_KEY_ID&gt; secret_access_key: &lt;AWS_SECRET_ACCESS_KEY&gt; region: us-east-1 lakefs: access_key_id: &lt;LAKEFS_ACCESS_KEY_ID&gt; secret_access_key: &lt;LAKEFS_SECRET_ACCESS_KEY&gt; table_names: - mytable . For the table descriptor under the _lakefs_tables/mytable.yaml: . --- name: myTableActualName type: delta path: a/path/to/my/delta/table . lakefs/catalogexport/table_extractor . Utility package to parse _lakefs_tables/ descriptors. lakefs/catalogexport/table_extractor.list_table_descriptor_entries(client, repo_id, commit_id) . List all YAML files under _lakefs_tables/* and return a list of type [{physical_address, path}], ignores hidden files. The client is lakefs client. lakefs/catalogexport/table_extractor.get_table_descriptor(client, repo_id, commit_id, logical_path) . Read a table descriptor and parse YAML object. Will set partition_columns to {} if no partitions are defined. The client is lakefs client. lakefs/catalogexport/hive.extract_partition_pager(client, repo_id, commit_id, base_path, partition_cols, page_size) . Hive format partition iterator each result set is a collection of files under the same partition in lakeFS. Example: . local lakefs = require(\"lakefs\") local pager = hive.extract_partition_pager(lakefs, repo_id, commit_id, prefix, partitions, 10) for part_key, entries in pager do print(\"partition: \" .. part_key) for _, entry in ipairs(entries) do print(\"path: \" .. entry.path .. \" physical: \" .. entry.physical_address) end end . lakefs/catalogexport/symlink_exporter . Writes metadata for a table using Hive’s SymlinkTextInputFormat. Currently only S3 is supported. The default export paths per commit: . ${storageNamespace} _lakefs/ exported/ ${ref}/ ${commitId}/ ${tableName}/ p1=v1/symlink.txt p1=v2/symlink.txt p1=v3/symlink.txt ... lakefs/catalogexport/symlink_exporter.export_s3(s3_client, table_src_path, action_info [, options]) . Export Symlink files that represent a table to S3 location. Parameters: . | s3_client: Configured client. | table_src_path(string): Path to the table spec YAML file in _lakefs_tables (e.g. _lakefs_tables/my_table.yaml). | action_info(table): The global action object. | options(table): . | debug(boolean): Print extra info. | export_base_uri(string): Override the prefix in S3 e.g. s3://other-bucket/path/. | writer(function(bucket, key, data)): If passed then will not use s3 client, helpful for debug. | . | . Example: . local exporter = require(\"lakefs/catalogexport/symlink_exporter\") local aws = require(\"aws\") -- args are user inputs from a lakeFS action. local s3 = aws.s3_client(args.aws.aws_access_key_id, args.aws.aws_secret_access_key, args.aws.aws_region) exporter.export_s3(s3, args.table_descriptor_path, action, {debug=true}) . lakefs/catalogexport/glue_exporter . A Package for automating the export process from lakeFS stored tables into Glue catalog. lakefs/catalogexport/glue_exporter.export_glue(glue, db, table_src_path, create_table_input, action_info, options) . Represent lakeFS table in Glue Catalog. This function will create a table in Glue based on configuration. It assumes that there is a symlink location that is already created and only configures it by default for the same commit. Parameters: . | glue: AWS glue client | db(string): glue database name | table_src_path(string): path to table spec (e.g. _lakefs_tables/my_table.yaml) | create_table_input(Table): Input equal mapping to table_input in AWS, the same as we use for glue.create_table. should contain inputs describing the data format (e.g. InputFormat, OutputFormat, SerdeInfo) since the exporter is agnostic to this. by default this function will configure table location and schema. | action_info(Table): the global action object. | options(Table): . | table_name(string): Override default glue table name | debug(boolean | export_base_uri(string): Override the default prefix in S3 for symlink location e.g. s3://other-bucket/path/ | . | . When creating a glue table, the final table input will consist of the create_table_input input parameter and lakeFS computed defaults that will override it: . | Name Gable table name get_full_table_name(descriptor, action_info). | PartitionKeys Partition columns usually deduced from _lakefs_tables/${table_src_path}. | TableType = “EXTERNAL_TABLE” | StorageDescriptor: Columns usually deduced from _lakefs_tables/${table_src_path}. | StorageDescriptor.Location = symlink_location | . Example: . local aws = require(\"aws\") local exporter = require(\"lakefs/catalogexport/glue_exporter\") local glue = aws.glue_client(args.aws_access_key_id, args.aws_secret_access_key, args.aws_region) -- table_input can be passed as a simple Key-Value object in YAML as an argument from an action, this is inline example: local table_input = { StorageDescriptor: InputFormat: \"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat\" OutputFormat: \"org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat\" SerdeInfo: SerializationLibrary: \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\" Parameters: classification: \"parquet\" EXTERNAL: \"TRUE\" \"parquet.compression\": \"SNAPPY\" } exporter.export_glue(glue, \"my-db\", \"_lakefs_tables/animals.yaml\", table_input, action, {debug=true}) . lakefs/catalogexport/glue_exporter.get_full_table_name(descriptor, action_info) . Generate glue table name. Parameters: . | descriptor(Table): Object from (e.g. _lakefs_tables/my_table.yaml). | action_info(Table): The global action object. | . lakefs/catalogexport/delta_exporter . A package used to export Delta Lake tables from lakeFS to an external cloud storage. lakefs/catalogexport/delta_exporter.export_delta_log(action, table_paths, writer, delta_client, table_descriptors_path) . The function used to export Delta Lake tables. The return value is a table with mapping of table names to external table location (from which it is possible to query the data). Parameters: . | action: The global action object | table_paths: Paths list in lakeFS to Delta Tables (e.g. {\"path/to/table1\", \"path/to/table2\"}) | writer: A writer function with function(bucket, key, data) signature, used to write the exported Delta Log (e.g. aws/s3.s3_client.put_object) | delta_client: A Delta Lake client that implements get_table: function(repo, ref, prefix) | table_descriptors_path: The path under which the table descriptors of the provided table_paths reside | . Example: . --- name: delta_exporter on: post-commit: null hooks: - id: delta type: lua properties: script: | local aws = require(\"aws\") local formats = require(\"formats\") local delta_exporter = require(\"lakefs/catalogexport/delta_exporter\") local table_descriptors_path = \"_lakefs_tables\" local sc = aws.s3_client(args.aws.access_key_id, args.aws.secret_access_key, args.aws.region) local delta_client = formats.delta_client(args.lakefs.access_key_id, args.lakefs.secret_access_key, args.aws.region) local delta_table_locations = delta_exporter.export_delta_log(action, args.table_paths, sc.put_object, delta_client, table_descriptors_path) for t, loc in pairs(delta_table_locations) do print(\"Delta Lake exported table \\\"\" .. t .. \"\\\"'s location: \" .. loc .. \"\\n\") end args: aws: access_key_id: &lt;AWS_ACCESS_KEY_ID&gt; secret_access_key: &lt;AWS_SECRET_ACCESS_KEY&gt; region: us-east-1 lakefs: access_key_id: &lt;LAKEFS_ACCESS_KEY_ID&gt; secret_access_key: &lt;LAKEFS_SECRET_ACCESS_KEY&gt; table_paths: - my/delta/table/path . lakefs/catalogexport/unity_exporter . A package used to register exported Delta Lake tables to Databricks’ Unity catalog. lakefs/catalogexport/unity_exporter.register_tables(action, table_descriptors_path, delta_table_paths, databricks_client, warehouse_id) . The function used to register exported Delta Lake tables in Databricks’ Unity Catalog. The registration will use the following paths to register the table: &lt;catalog&gt;.&lt;branch name&gt;.&lt;table_name&gt; where the branch name will be used as the schema name. The return value is a table with mapping of table names to registration request status. Parameters: . | action(table): The global action table | table_descriptors_path(string): The path under which the table descriptors of the provided table_paths reside. | delta_table_paths(table): Table names to physical paths mapping (e.g. { table1 = \"s3://mybucket/mytable1\", table2 = \"s3://mybucket/mytable2\" }) | databricks_client(table): A Databricks client that implements create_or_get_schema: function(id, catalog_name) and register_external_table: function(table_name, physical_path, warehouse_id, catalog_name, schema_name) | warehouse_id(string): Databricks warehouse ID. | . Example: The following registers an exported Delta Lake table to Unity Catalog. local databricks = require(\"databricks\") local unity_export = require(\"lakefs/catalogexport/unity_exporter\") local delta_table_locations = { [\"table1\"] = \"s3://mybucket/mytable1\", } -- Register the exported table in Unity Catalog: local action_details = { repository_id = \"my-repo\", commit_id = \"commit_id\", branch_id = \"main\", } local databricks_client = databricks.client(\"&lt;DATABRICKS_HOST&gt;\", \"&lt;DATABRICKS_TOKEN&gt;\") local registration_statuses = unity_export.register_tables(action_details, \"_lakefs_tables\", delta_table_locations, databricks_client, \"&lt;WAREHOUSE_ID&gt;\") for t, status in pairs(registration_statuses) do print(\"Unity catalog registration for table \\\"\" .. t .. \"\\\" completed with status: \" .. status .. \"\\n\") end . For the table descriptor under the _lakefs_tables/delta-table-descriptor.yaml: . --- name: my_table_name type: delta path: path/to/delta/table/data catalog: my-catalog . For detailed step-by-step guide on how to use unity_exporter.register_tables as a part of a lakeFS action refer to the Unity Catalog docs. path/parse(path_string) . Returns a table for the given path string with the following structure: . &gt; require(\"path\") &gt; path.parse(\"a/b/c.csv\") { [\"parent\"] = \"a/b/\" [\"base_name\"] = \"c.csv\" } . path/join(*path_parts) . Receives a variable number of strings and returns a joined string that represents a path: . &gt; require(\"path\") &gt; path.join(\"path/\", \"to\", \"a\", \"file.data\") path/o/a/file.data . path/is_hidden(path_string [, seperator, prefix]) . returns a boolean - true if the given path string is hidden (meaning it starts with prefix) - or if any of its parents start with prefix. &gt; require(\"path\") &gt; path.is_hidden(\"a/b/c\") -- false &gt; path.is_hidden(\"a/b/_c\") -- true &gt; path.is_hidden(\"a/_b/c\") -- true &gt; path.is_hidden(\"a/b/_c/\") -- true . path/default_separator() . Returns a constant string (/) . regexp/match(pattern, s) . Returns true if the string s matches pattern. This is a thin wrapper over Go’s regexp.MatchString. regexp/quote_meta(s) . Escapes any meta-characters in string s and returns a new string . regexp/compile(pattern) . Returns a regexp match object for the given pattern . regexp/compiled_pattern.find_all(s, n) . Returns a table list of all matches for the pattern, (up to n matches, unless n == -1 in which case all possible matches will be returned) . regexp/compiled_pattern.find_all_submatch(s, n) . Returns a table list of all sub-matches for the pattern, (up to n matches, unless n == -1 in which case all possible matches will be returned). Submatches are matches of parenthesized subexpressions (also known as capturing groups) within the regular expression, numbered from left to right in order of opening parenthesis. Submatch 0 is the match of the entire expression, submatch 1 is the match of the first parenthesized subexpression, and so on . regexp/compiled_pattern.find(s) . Returns a string representing the left-most match for the given pattern in string s . regexp/compiled_pattern.find_submatch(s) . find_submatch returns a table of strings holding the text of the leftmost match of the regular expression in s and the matches, if any, of its submatches . strings/split(s, sep) . returns a table of strings, the result of splitting s with sep. strings/trim(s) . Returns a string with all leading and trailing white space removed, as defined by Unicode . strings/replace(s, old, new, n) . Returns a copy of the string s with the first n non-overlapping instances of old replaced by new. If old is empty, it matches at the beginning of the string and after each UTF-8 sequence, yielding up to k+1 replacements for a k-rune string. If n &lt; 0, there is no limit on the number of replacements . strings/has_prefix(s, prefix) . Returns true if s begins with prefix . strings/has_suffix(s, suffix) . Returns true if s ends with suffix . strings/contains(s, substr) . Returns true if substr is contained anywhere in s . time/now() . Returns a float64 representing the amount of nanoseconds since the unix epoch (01/01/1970 00:00:00). time/format(epoch_nano, layout, zone) . Returns a string representation of the given epoch_nano timestamp for the given Timezone (e.g. \"UTC\", \"America/Los_Angeles\", …) The layout parameter should follow Go’s time layout format. time/format_iso(epoch_nano, zone) . Returns a string representation of the given epoch_nano timestamp for the given Timezone (e.g. \"UTC\", \"America/Los_Angeles\", …) The returned string will be in ISO8601 format. time/sleep(duration_ns) . Sleep for duration_ns nanoseconds . time/since(epoch_nano) . Returns the amount of nanoseconds elapsed since epoch_nano . time/add(epoch_time, duration_table) . Returns a new timestamp (in nanoseconds passed since 01/01/1970 00:00:00) for the given duration. The duration should be a table with the following structure: . &gt; require(\"time\") &gt; time.add(time.now(), { [\"hour\"] = 1, [\"minute\"] = 20, [\"second\"] = 50 }) . You may omit any of the fields from the table, resulting in a default value of 0 for omitted fields . time/parse(layout, value) . Returns a float64 representing the amount of nanoseconds since the unix epoch (01/01/1970 00:00:00). This timestamp will represent date value parsed using the layout format. The layout parameter should follow Go’s time layout format . time/parse_iso(value) . Returns a float64 representing the amount of nanoseconds since the unix epoch (01/01/1970 00:00:00 for value. The value string should be in ISO8601 format . uuid/new() . Returns a new 128-bit RFC 4122 UUID in string representation. net/url . Provides a parse function parse a URL string into parts, returns a table with the URL’s host, path, scheme, query and fragment. &gt; local url = require(\"net/url\") &gt; url.parse(\"https://example.com/path?p1=a#section\") { [\"host\"] = \"example.com\" [\"path\"] = \"/path\" [\"scheme\"] = \"https\" [\"query\"] = \"p1=a\" [\"fragment\"] = \"section\" } . net/http (optional) . Provides a request function that performs an HTTP request. For security reasons, this package is not available by default as it enables http requests to be sent out from the lakeFS instance network. The feature should be enabled under actions.lua.net_http_enabled configuration. Request will time out after 30 seconds. http.request(url [, body]) http.request{ url = string, [method = string,] [headers = header-table,] [body = string,] } . Returns a code (number), body (string), headers (table) and status (string). | code - status code number | body - string with the response body | headers - table with the response request headers (key/value or table of values) | status - status code text | . The first form of the call will perform GET requests or POST requests if the body parameter is passed. The second form accepts a table and allows you to customize the request method and headers. Example of a GET request . local http = require(\"net/http\") local code, body = http.request(\"https://example.com\") if code == 200 then print(body) else print(\"Failed to get example.com - status code: \" .. code) end . Example of a POST request . local http = require(\"net/http\") local code, body = http.request{ url=\"https://httpbin.org/post\", method=\"POST\", body=\"custname=tester\", headers={[\"Content-Type\"]=\"application/x-www-form-urlencoded\"}, } if code == 200 then print(body) else print(\"Failed to post data - status code: \" .. code) end . ",
    "url": "/v1.4/howto/hooks/lua.html#lua-library-reference",
    
    "relUrl": "/howto/hooks/lua.html#lua-library-reference"
  },"331": {
    "doc": "Managed Garbage Collection",
    "title": "Managed Garbage Collection",
    "content": "lakeFS Cloud . Managed GC is only available for lakeFS Cloud. If you are using the self-managed lakeFS, garbage collection is available to run manually. ",
    "url": "/v1.4/howto/garbage-collection/managed-gc.html",
    
    "relUrl": "/howto/garbage-collection/managed-gc.html"
  },"332": {
    "doc": "Managed Garbage Collection",
    "title": "Benefits of using managed GC",
    "content": ". | The quick and safe way to delete your unnecessary objects | No operational overhead | SLA for when your objects are deleted | Support from the Treeverse team | . ",
    "url": "/v1.4/howto/garbage-collection/managed-gc.html#benefits-of-using-managed-gc",
    
    "relUrl": "/howto/garbage-collection/managed-gc.html#benefits-of-using-managed-gc"
  },"333": {
    "doc": "Managed Garbage Collection",
    "title": "How it works",
    "content": "Similarly to the self-managed lakeFS, managed GC uses garbage collection rules to determine which objects to delete. However, it uses our super-fast and efficient engine to detect stale objects and branches (depends on your configuration) and prioritize them for deletion. ",
    "url": "/v1.4/howto/garbage-collection/managed-gc.html#how-it-works",
    
    "relUrl": "/howto/garbage-collection/managed-gc.html#how-it-works"
  },"334": {
    "doc": "Managed Garbage Collection",
    "title": "Setting up",
    "content": "Enable managed GC through the lakeFS Cloud onboarding setup wizard. This will create additional cloud resources for us to use and have access to delete those objects. ",
    "url": "/v1.4/howto/garbage-collection/managed-gc.html#setting-up",
    
    "relUrl": "/howto/garbage-collection/managed-gc.html#setting-up"
  },"335": {
    "doc": "Merge",
    "title": "Merges in lakeFS",
    "content": "The merge operation in lakeFS is similar to Git. It incorporates changes from a merge source (a commit/reference) into a merge destination (a branch). ",
    "url": "/v1.4/understand/how/merge.html#merges-in-lakefs",
    
    "relUrl": "/understand/how/merge.html#merges-in-lakefs"
  },"336": {
    "doc": "Merge",
    "title": "How does it work?",
    "content": "lakeFS first finds the merge base: the nearest common ancestor of the two commits. It can now perform a three-way merge, by examining the presence and identity of files in each commit. In the table below, “A”, “B” and “C” are possible file contents, “X” is a missing file, and “conflict” (which only appears as a result) is a merge failure. | In base | In source | In destination | Result | Comment | . | A | A | A | A | Unchanged file | . | A | B | B | B | Files changed on both sides in same way | . | A | B | C | conflict | Files changed on both sides differently | . | A | A | B | B | File changed only on one branch | . | A | B | A | B | File changed only on one branch | . | A | X | X | X | Files deleted on both sides | . | A | B | X | conflict | File changed on one side, deleted on the other | . | A | X | B | conflict | File changed on one side, deleted on the other | . | A | A | X | X | File deleted on one side | . | A | X | A | X | File deleted on one side | . ",
    "url": "/v1.4/understand/how/merge.html#how-does-it-work",
    
    "relUrl": "/understand/how/merge.html#how-does-it-work"
  },"337": {
    "doc": "Merge",
    "title": "Merge Strategies",
    "content": "The API and lakectl allow passing an optional strategy flag with the following values: . source-wins . In case of a conflict, merge will pick the source objects. Example . lakectl merge lakefs://example-repo/validated-data lakefs://example-repo/production --strategy source-wins . When a merge conflict arises, the conflicting objects in the validated-data branch will be chosen to end up in production. dest-wins . In case of a conflict, merge will pick the destination objects. Example . lakectl merge lakefs://example-repo/validated-data lakefs://example-repo/production --strategy dest-wins . When a merge conflict arises, the conflicting objects in the production branch will be chosen to end up in validated-data. The production branch will not be affected by object changes from validated-data conflicting objects. The strategy will affect all conflicting objects in the merge if it is set. Currently it is not possible to treat conflicts individually. As a format-agnostic system, lakeFS currently merges by complete files. Format-specific and other user-defined merge strategies for handling conflicts are on the roadmap. ",
    "url": "/v1.4/understand/how/merge.html#merge-strategies",
    
    "relUrl": "/understand/how/merge.html#merge-strategies"
  },"338": {
    "doc": "Merge",
    "title": "Merge",
    "content": " ",
    "url": "/v1.4/understand/how/merge.html",
    
    "relUrl": "/understand/how/merge.html"
  },"339": {
    "doc": "Migrating away from lakeFS",
    "title": "Migrating away from lakeFS",
    "content": " ",
    "url": "/v1.4/howto/migrate-away.html",
    
    "relUrl": "/howto/migrate-away.html"
  },"340": {
    "doc": "Migrating away from lakeFS",
    "title": "Copying data from a lakeFS repository to an S3 bucket",
    "content": "The simplest way to migrate away from lakeFS is by copying data from a lakeFS repository to an S3 bucket (or any other object store). For smaller repositories, you can do this by using the AWS CLI or Rclone. For larger repositories, running distcp with lakeFS as the source is also an option. ",
    "url": "/v1.4/howto/migrate-away.html#copying-data-from-a-lakefs-repository-to-an-s3-bucket",
    
    "relUrl": "/howto/migrate-away.html#copying-data-from-a-lakefs-repository-to-an-s3-bucket"
  },"341": {
    "doc": "Concepts and  Model",
    "title": "lakeFS Concepts and Model",
    "content": " ",
    "url": "/v1.4/understand/model.html#lakefs-concepts-and-model",
    
    "relUrl": "/understand/model.html#lakefs-concepts-and-model"
  },"342": {
    "doc": "Concepts and  Model",
    "title": "Table of contents",
    "content": ". | Objects | Version Control . | Repository | Commits | Branches | Tags | History | Merge | Ref expressions | . | Concepts unique to lakeFS . | lakefs protocol URIs | . | . lakeFS blends concepts from object stores such as S3 with concepts from Git. This reference defines the common concepts of lakeFS. ",
    "url": "/v1.4/understand/model.html#table-of-contents",
    
    "relUrl": "/understand/model.html#table-of-contents"
  },"343": {
    "doc": "Concepts and  Model",
    "title": "Objects",
    "content": "lakeFS is an interface to manage objects in an object store. The actual data itself is not stored inside lakeFS directly but in an underlying object store. lakeFS manages pointers and additional metadata about these objects. ",
    "url": "/v1.4/understand/model.html#objects",
    
    "relUrl": "/understand/model.html#objects"
  },"344": {
    "doc": "Concepts and  Model",
    "title": "Version Control",
    "content": "lakeFS is spearheading version control semantics for data. Most of these concepts will be familiar to Git users: . Repository . In lakeFS, a repository is a set of related objects (or collections of objects). In many cases, these represent tables of various formats for tabular data, semi-structured data such as JSON or log files - or a set of unstructured objects such as images, videos, sensor data, etc. lakeFS represents repositories as a logical namespace used to group together objects, branches, and commits - analogous to a repository in Git. lakeFS repository naming requirements are as follows: . | Start with a lower case letter or number | Contain only lower case letters, numbers and hyphens | Be between 3 and 63 characters long | . Commits . Using commits, you can view a repository at a certain point in its history and you’re guaranteed that the data you see is exactly as it was at the point of committing it. These commits are immutable “checkpoints” containing all contents of a repository at a given point in the repository’s history. Each commit contains metadata - the committer, timestamp, a commit message, as well as arbitrary key/value pairs you can choose to add. Identifying Commits A commit is identified by its commit ID, a digest of all contents of the commit. Commit IDs are by nature long, so you may use a unique prefix to abbreviate them. A commit may also be identified by using a textual definition, called a ref. Examples of refs include tags, branch names, and expressions. Branches . Branches in lakeFS allow users to create their own “isolated” view of the repository. Changes on one branch do not appear on other branches. Users can take changes from one branch and apply it to another by merging them. Under the hood, branches are simply a pointer to a commit along with a set of uncommitted changes. Tags . Tags are a way to give a meaningful name to a specific commit. Using tags allow users to reference specific releases, experiments or versions by using a human friendly name. Example tags: . | v2.3 to mark a release. | dev:jane-before-v2.3-merge to mark Jane’s private temporary point. | . History . The history of the branch is the list of commits from the branch tip through the first parent of each commit. Histories go back in time. Merge . Merging is the way to integrate changes from a branch into another branch. The result of a merge is a new commit, with the destination as the first parent and the source as the second. To learn more about how merging works in lakeFS, see the merge reference . Ref expressions . lakeFS also supports expressions for creating a ref. These are similar to revisions in Git; indeed all ~ and ^ examples at the end of that section will work unchanged in lakeFS. | A branch or a tag are ref expressions. | If &lt;ref&gt; is a ref expression, then: . | &lt;ref&gt;^ is a ref expression referring to its first parent. | &lt;ref&gt;^N is a ref expression referring to its N’th parent; in particular &lt;ref&gt;^1 is the same as &lt;ref&gt;^. | &lt;ref&gt;~ is a ref expression referring to its first parent; in particular &lt;ref&gt;~ is the same as &lt;ref&gt;^ and &lt;ref&gt;~. | &lt;ref&gt;~N is a ref expression referring to its N’th parent, always traversing to the first parent. So &lt;ref&gt;~N is the same as &lt;ref&gt;^^...^ with N consecutive carets ^. | . | . ",
    "url": "/v1.4/understand/model.html#version-control",
    
    "relUrl": "/understand/model.html#version-control"
  },"345": {
    "doc": "Concepts and  Model",
    "title": "Concepts unique to lakeFS",
    "content": "The underlying storage is a location in an object store where lakeFS keeps your objects and some immutable metadata. When creating a lakeFS repository, you assign it with a storage namespace. The repository’s storage namespace is a location in the underlying storage where data for this repository will be stored. We sometimes refer to underlying storage as physical. The path used to store the contents of an object is then termed a physical path. Once lakeFS saves an object in the underlying storage it is never modified, except to remove it entirely during some cleanups. A lot of what lakeFS does is to manage how lakeFS paths translate to physical paths on the object store. This mapping is generally not straightforward. Importantly (and contrary to many object stores), lakeFS may map multiple paths to the same object on backing storage, and always does this for objects that are unchanged across versions. lakefs protocol URIs . lakeFS uses a specific format for path URIs. The URI lakefs://&lt;REPO&gt;/&lt;REF&gt;/&lt;KEY&gt; is a path to objects in the given repo and ref expression under key. This is used both for path prefixes and for full paths. In similar fashion, lakefs://&lt;REPO&gt;/&lt;REF&gt; identifies the repository at a ref expression, and lakefs://&lt;REPO&gt; identifes a repo. ",
    "url": "/v1.4/understand/model.html#concepts-unique-to-lakefs",
    
    "relUrl": "/understand/model.html#concepts-unique-to-lakefs"
  },"346": {
    "doc": "Concepts and  Model",
    "title": "Concepts and  Model",
    "content": " ",
    "url": "/v1.4/understand/model.html",
    
    "relUrl": "/understand/model.html"
  },"347": {
    "doc": "Monitoring using Prometheus",
    "title": "Monitoring using Prometheus",
    "content": " ",
    "url": "/v1.4/reference/monitor.html",
    
    "relUrl": "/reference/monitor.html"
  },"348": {
    "doc": "Monitoring using Prometheus",
    "title": "Table of contents",
    "content": ". | Example prometheus.yml | Metrics exposed by lakeFS | Example queries | . ",
    "url": "/v1.4/reference/monitor.html#table-of-contents",
    
    "relUrl": "/reference/monitor.html#table-of-contents"
  },"349": {
    "doc": "Monitoring using Prometheus",
    "title": "Example prometheus.yml",
    "content": "lakeFS exposes metrics through the same port used by the lakeFS service, using the standard /metrics path. An example prometheus.yml could look like this: . scrape_configs: - job_name: lakeFS scrape_interval: 10s metrics_path: /metrics static_configs: - targets: - lakefs.example.com:8000 . ",
    "url": "/v1.4/reference/monitor.html#example-prometheusyml",
    
    "relUrl": "/reference/monitor.html#example-prometheusyml"
  },"350": {
    "doc": "Monitoring using Prometheus",
    "title": "Metrics exposed by lakeFS",
    "content": "By default, Prometheus exports metrics with OS process information like memory and CPU. It also includes Go-specific metrics such as details about GC and a number of goroutines. You can learn about these default metrics in this post. In addition, lakeFS exposes the following metrics to help monitor your deployment: . | Name in Prometheus | Description | Labels | . | api_requests_total | lakeFS API requests (counter) | code: http statusmethod: http method | . | api_request_duration_seconds | Durations of lakeFS API requests (histogram) | operation: name of API operationcode: http status | . | gateway_request_duration_seconds | lakeFS S3-compatible endpoint request (histogram) | operation: name of gateway operationcode: http status | . | s3_operation_duration_seconds | Outgoing S3 operations (histogram) | operation: operation nameerror: “true” if error, “false” otherwise | . | gs_operation_duration_seconds | Outgoing Google Storage operations (histogram) | operation: operation nameerror: “true” if error, “false” otherwise | . | azure_operation_duration_seconds | Outgoing Azure storage operations (histogram) | operation: operation nameerror: “true” if error, “false” otherwise | . | dynamo_request_duration_seconds | Time spent doing DynamoDB requests | operation: DynamoDB operation name | . | dynamo_consumed_capacity_total | The capacity units consumed by operation | operation: DynamoDB operation name | . | dynamo_failures_total | The total number of errors while working for kv store | operation: DynamoDB operation name | . | pgxpool_acquire_count | PostgreSQL cumulative count of successful acquires from the pool | db_name default to the kv table name (kv) | . | pgxpool_acquire_duration_ns | PostgreSQL total duration of all successful acquires from the pool in nanoseconds | db_name default to the kv table name (kv) | . | pgxpool_acquired_conns | PostgreSQL number of currently acquired connections in the pool | db_name default to the kv table name (kv) | . | pgxpool_canceled_acquire_count | PostgreSQL cumulative count of acquires from the pool that were canceled by a context | db_name default to the kv table name (kv) | . | pgxpool_constructing_conns | PostgreSQL number of conns with construction in progress in the pool | db_name default to the kv table name (kv) | . | pgxpool_empty_acquire | PostgreSQL cumulative count of successful acquires from the pool that waited for a resource to be released or constructed because the pool was empty | db_name default to the kv table name (kv) | . | pgxpool_idle_conns | PostgreSQL number of currently idle conns in the pool | db_name default to the kv table name (kv) | . | pgxpool_max_conns | PostgreSQL maximum size of the pool | db_name default to the kv table name (kv) | . | pgxpool_total_conns | PostgreSQL total number of resources currently in the pool | db_name default to the kv table name (kv) | . ",
    "url": "/v1.4/reference/monitor.html#metrics-exposed-by-lakefs",
    
    "relUrl": "/reference/monitor.html#metrics-exposed-by-lakefs"
  },"351": {
    "doc": "Monitoring using Prometheus",
    "title": "Example queries",
    "content": "Note: when using Prometheus functions like rate or increase, results are extrapolated and may not be exact. 99th percentile of API request latencies . sum by (operation)(histogram_quantile(0.99, rate(api_request_duration_seconds_bucket[1m]))) . 50th percentile of S3-compatible API latencies . sum by (operation)(histogram_quantile(0.5, rate(gateway_request_duration_seconds_bucket[1m]))) . Number of errors in outgoing S3 requests . sum by (operation) (increase(s3_operation_duration_seconds_count{error=\"true\"}[1m])) . Number of open connections to the database . go_sql_stats_connections_open . Example Grafana dashboard . ",
    "url": "/v1.4/reference/monitor.html#example-queries",
    
    "relUrl": "/reference/monitor.html#example-queries"
  },"352": {
    "doc": "On-Premises",
    "title": "On-Premises Deployment",
    "content": "The instructions given here are for a self-managed deployment of lakeFS. For a hosted lakeFS service with guaranteed SLAs, try lakeFS Cloud . ",
    "url": "/v1.4/howto/deploy/onprem.html#on-premises-deployment",
    
    "relUrl": "/howto/deploy/onprem.html#on-premises-deployment"
  },"353": {
    "doc": "On-Premises",
    "title": "Table of contents",
    "content": ". | Prerequisites | Setting up a database | Setting up a lakeFS Server | Load balancing | Secure connection | Local Blockstore | Create the admin user | Create your first repository | . ⏰ Expected deployment time: 25 min . ",
    "url": "/v1.4/howto/deploy/onprem.html#table-of-contents",
    
    "relUrl": "/howto/deploy/onprem.html#table-of-contents"
  },"354": {
    "doc": "On-Premises",
    "title": "Prerequisites",
    "content": "To use lakeFS, you’ll need to have access to an S3-compatible object store such as MinIO . For more information on how to set up MinIO, see the official deployment guide . ",
    "url": "/v1.4/howto/deploy/onprem.html#prerequisites",
    
    "relUrl": "/howto/deploy/onprem.html#prerequisites"
  },"355": {
    "doc": "On-Premises",
    "title": "Setting up a database",
    "content": "lakeFS requires a PostgreSQL database to synchronize actions on your repositories. This section assumes that you already have a PostgreSQL &gt;= 11.0 database accessible. ",
    "url": "/v1.4/howto/deploy/onprem.html#setting-up-a-database",
    
    "relUrl": "/howto/deploy/onprem.html#setting-up-a-database"
  },"356": {
    "doc": "On-Premises",
    "title": "Setting up a lakeFS Server",
    "content": ". | Linux | Docker | Kubernetes | . Connect to your host using SSH: . | Create a config.yaml on your VM, with the following parameters: . --- database: type: \"postgres\" postgres: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: # replace this with a randomly-generated string. Make sure to keep it safe! secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: s3 s3: force_path_style: true endpoint: http://&lt;minio_endpoint&gt; discover_bucket_region: false credentials: access_key_id: &lt;minio_access_key&gt; secret_access_key: &lt;minio_secret_key&gt; . ⚠️ Notice that the lakeFS Blockstore type is set to s3 - This configuration works with S3-compatible storage engines such as MinIO. | Download the binary to the server. | Run the lakefs binary: . lakefs --config config.yaml run . | . Note: It’s preferable to run the binary as a service using systemd or your operating system’s facilities. To support container-based environments, you can configure lakeFS using environment variables. Here is a docker run command to demonstrate starting lakeFS using Docker: . docker run \\ --name lakefs \\ -p 8000:8000 \\ -e LAKEFS_DATABASE_TYPE=\"postgres\" \\ -e LAKEFS_DATABASE_POSTGRES_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\ -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\ -e LAKEFS_BLOCKSTORE_TYPE=\"s3\" \\ -e LAKEFS_BLOCKSTORE_S3_FORCE_PATH_STYLE=\"true\" \\ -e LAKEFS_BLOCKSTORE_S3_ENDPOINT=\"http://&lt;minio_endpoint&gt;\" \\ -e LAKEFS_BLOCKSTORE_S3_DISCOVER_BUCKET_REGION=\"false\" \\ -e LAKEFS_BLOCKSTORE_S3_CREDENTIALS_ACCESS_KEY_ID=\"&lt;minio_access_key&gt;\" \\ -e LAKEFS_BLOCKSTORE_S3_CREDENTIALS_SECRET_ACCESS_KEY=\"&lt;minio_secret_key&gt;\" \\ treeverse/lakefs:latest run . ⚠️ Notice that the lakeFS Blockstore type is set to s3 - This configuration works with S3-compatible storage engines such as MinIO. See the reference for a complete list of environment variables. You can install lakeFS on Kubernetes using a Helm chart. To install lakeFS with Helm: . | Copy the Helm values file relevant for S3-Compatible storage (MinIO in this example): . secrets: # replace this with the connection string of the database you created in a previous step: databaseConnectionString: [DATABASE_CONNECTION_STRING] # replace this with a randomly-generated string authEncryptSecretKey: [ENCRYPTION_SECRET_KEY] lakefsConfig: | blockstore: type: s3 s3: force_path_style: true endpoint: http://&lt;minio_endpoint&gt; discover_bucket_region: false credentials: access_key_id: &lt;minio_access_key&gt; secret_access_key: &lt;minio_secret_key&gt; . ⚠️ Notice that the lakeFS Blockstore type is set to s3 - This configuration works with S3-compatible storage engines such as MinIO. | Fill in the missing values and save the file as conf-values.yaml. For more configuration options, see our Helm chart README. The lakefsConfig parameter is the lakeFS configuration documented here but without sensitive information. Sensitive information like databaseConnectionString is given through separate parameters, and the chart will inject it into Kubernetes secrets. | In the directory where you created conf-values.yaml, run the following commands: . # Add the lakeFS repository helm repo add lakefs https://charts.lakefs.io # Deploy lakeFS helm install my-lakefs lakefs/lakefs -f conf-values.yaml . my-lakefs is the Helm Release name. ",
    "url": "/v1.4/howto/deploy/onprem.html#setting-up-a-lakefs-server",
    
    "relUrl": "/howto/deploy/onprem.html#setting-up-a-lakefs-server"
  },"357": {
    "doc": "On-Premises",
    "title": "Load balancing",
    "content": "To configure a load balancer to direct requests to the lakeFS servers you can use the LoadBalancer Service type or a Kubernetes Ingress. By default, lakeFS operates on port 8000 and exposes a /_health endpoint that you can use for health checks. 💡 The NGINX Ingress Controller by default limits the client body size to 1 MiB. Some clients use bigger chunks to upload objects - for example, multipart upload to lakeFS using the S3-compatible Gateway or a simple PUT request using the OpenAPI Server. Checkout Nginx documentation for increasing the limit, or an example of Nginx configuration with MinIO. | . ",
    "url": "/v1.4/howto/deploy/onprem.html#load-balancing",
    
    "relUrl": "/howto/deploy/onprem.html#load-balancing"
  },"358": {
    "doc": "On-Premises",
    "title": "Secure connection",
    "content": "Using a load balancer or cluster manager for TLS/SSL termination is recommended. It helps speed the decryption process and reduces the processing burden from lakeFS. In case lakeFS needs to listen and serve with HTTPS, for example for development purposes, update its config yaml with the following section: . tls: enabled: true cert_file: server.crt # provide path to your certificate file key_file: server.key # provide path to your server private key . ",
    "url": "/v1.4/howto/deploy/onprem.html#secure-connection",
    
    "relUrl": "/howto/deploy/onprem.html#secure-connection"
  },"359": {
    "doc": "On-Premises",
    "title": "Local Blockstore",
    "content": "You can configure a block adapter to a POSIX compatible storage location shared by all lakeFS instances. Using the shared storage location, both data and metadata will be stored there. Using the local blockstore import and allowing lakeFS access to a specific prefix, it is possible to import files from a shared location. Import is not enabled by default, as it doesn’t assume the local path is shared and there is a security concern about accessing a path outside the specified in the blockstore configuration. Enabling is done by blockstore.local.import_enabled and blockstore.local.allowed_external_prefixes as described in the configuration reference. Sample configuration using local blockstore . database: type: \"postgres\" postgres: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: # replace this with a randomly-generated string. Make sure to keep it safe! secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: local local: path: /shared/location/lakefs_data # location where data and metadata kept by lakeFS import_enabled: true # required to be true to enable import files # from `allowed_external_prefixes` locations allowed_external_prefixes: - /shared/location/files_to_import # location with files we can import into lakeFS, require access from lakeFS . Limitations . | Using a local adapter on a shared location is relativly new and not battle-tested yet | lakeFS doesn’t control the way a shared location is managed across machines | When using lakectl or the lakeFS UI, you can currently import only directories. If you need to import a single file, use the HTTP API or API Clients with type=object in the request body and destination=&lt;full-path-to-file&gt;. | Garbage collector (for committed and uncommitted) and lakeFS Hadoop FileSystem currently unsupported | . ",
    "url": "/v1.4/howto/deploy/onprem.html#local-blockstore",
    
    "relUrl": "/howto/deploy/onprem.html#local-blockstore"
  },"360": {
    "doc": "On-Premises",
    "title": "Create the admin user",
    "content": "When you first open the lakeFS UI, you will be asked to create an initial admin user. | Open http://&lt;lakefs-host&gt;/ in your browser. If you haven’t set up a load balancer, this will likely be http://&lt;instance ip address&gt;:8000/ | On first use, you’ll be redirected to the setup page: . | Follow the steps to create an initial administrator user. Save the credentials you’ve received somewhere safe, you won’t be able to see them again! . | Follow the link and go to the login screen. Use the credentials from the previous step to log in. | . ",
    "url": "/v1.4/howto/deploy/onprem.html#create-the-admin-user",
    
    "relUrl": "/howto/deploy/onprem.html#create-the-admin-user"
  },"361": {
    "doc": "On-Premises",
    "title": "Create your first repository",
    "content": ". | Use the credentials from the previous step to log in | Click Create Repository and choose Blank Repository. | Under Storage Namespace, enter a path to your desired location on the object store. This is where data written to this repository will be stored. | Click Create Repository | You should now have a configured repository, ready to use! . | . Congratulations! Your environment is now ready 🤩 . ",
    "url": "/v1.4/howto/deploy/onprem.html#create-your-first-repository",
    
    "relUrl": "/howto/deploy/onprem.html#create-your-first-repository"
  },"362": {
    "doc": "On-Premises",
    "title": "On-Premises",
    "content": " ",
    "url": "/v1.4/howto/deploy/onprem.html",
    
    "relUrl": "/howto/deploy/onprem.html"
  },"363": {
    "doc": "Performance Best Practices",
    "title": "Performance Best Practices",
    "content": " ",
    "url": "/v1.4/understand/performance-best-practices.html",
    
    "relUrl": "/understand/performance-best-practices.html"
  },"364": {
    "doc": "Performance Best Practices",
    "title": "Table of contents",
    "content": ". | Overview | Avoid concurrent commits/merges | Perform meaningful commits | Use zero-copy import | Read data using the commit ID | Operate directly on the storage | Zero-copy | . ",
    "url": "/v1.4/understand/performance-best-practices.html#table-of-contents",
    
    "relUrl": "/understand/performance-best-practices.html#table-of-contents"
  },"365": {
    "doc": "Performance Best Practices",
    "title": "Overview",
    "content": "Use this guide to achieve the best performance with lakeFS. ",
    "url": "/v1.4/understand/performance-best-practices.html#overview",
    
    "relUrl": "/understand/performance-best-practices.html#overview"
  },"366": {
    "doc": "Performance Best Practices",
    "title": "Avoid concurrent commits/merges",
    "content": "Just like in Git, branch history is composed by commits and is linear by nature. Concurrent commits/merges on the same branch result in a race. The first operation will finish successfully while the rest will retry. ",
    "url": "/v1.4/understand/performance-best-practices.html#avoid-concurrent-commitsmerges",
    
    "relUrl": "/understand/performance-best-practices.html#avoid-concurrent-commitsmerges"
  },"367": {
    "doc": "Performance Best Practices",
    "title": "Perform meaningful commits",
    "content": "It’s a good idea to perform commits that are meaningful in the senese that they represent a logical point in your data’s lifecycle. While lakeFS supports arbirartily large commits, avoiding commits with a huge number of objects will result in a more comprehensible commit history. ",
    "url": "/v1.4/understand/performance-best-practices.html#perform-meaningful-commits",
    
    "relUrl": "/understand/performance-best-practices.html#perform-meaningful-commits"
  },"368": {
    "doc": "Performance Best Practices",
    "title": "Use zero-copy import",
    "content": "To import object into lakeFS, either a single time or regularly, lakeFS offers a zero-copy import feature. Use this feature to import a large number of objects to lakeFS, instead of simply copying them into your repository. This feature will create a reference to the existing objects on your bucket and avoids the copy. ",
    "url": "/v1.4/understand/performance-best-practices.html#use-zero-copy-import",
    
    "relUrl": "/understand/performance-best-practices.html#use-zero-copy-import"
  },"369": {
    "doc": "Performance Best Practices",
    "title": "Read data using the commit ID",
    "content": "In cases where you are only interested in reading committed data: . | Use a commit ID (or a tag ID) in your path (e.g: lakefs://repo/a1b2c3). | Add @ before the path lakefs://repo/main@/path. | . When accessing data using the branch name (e.g. lakefs://repo/main/path) lakeFS will also try to fetch uncommitted data, which may result in reduced performance. For more information, see how uncommitted data is managed in lakeFS. ",
    "url": "/v1.4/understand/performance-best-practices.html#read-data-using-the-commit-id",
    
    "relUrl": "/understand/performance-best-practices.html#read-data-using-the-commit-id"
  },"370": {
    "doc": "Performance Best Practices",
    "title": "Operate directly on the storage",
    "content": "Sometimes, storage operations can become a bottleneck. For example, when your data pipelines upload many big objects. In such cases, it can be beneficial to perform only versioning operations on lakeFS, while performing storage reads/writes directly on the object store. lakeFS offers multiple ways to do that: . | The lakectl fs upload --pre-sign command (or download). | The lakeFS Hadoop Filesystem. | The staging API which can be used to add lakeFS references to objects after having written them to the storage. | . Accessing the object store directly is a faster way to interact with your data. ",
    "url": "/v1.4/understand/performance-best-practices.html#operate-directly-on-the-storage",
    
    "relUrl": "/understand/performance-best-practices.html#operate-directly-on-the-storage"
  },"371": {
    "doc": "Performance Best Practices",
    "title": "Zero-copy",
    "content": "lakeFS provides a zero-copy mechanism to data. Instead of copying the data, we can check out to a new branch. Creating a new branch will take constant time as the new branch points to the same data as its parent. It will also lower the storage cost. ",
    "url": "/v1.4/understand/performance-best-practices.html#zero-copy",
    
    "relUrl": "/understand/performance-best-practices.html#zero-copy"
  },"372": {
    "doc": "Presigned URL",
    "title": "Configuring lakeFS to use presigned URLs",
    "content": " ",
    "url": "/v1.4/reference/security/presigned-url.html#configuring-lakefs-to-use-presigned-urls",
    
    "relUrl": "/reference/security/presigned-url.html#configuring-lakefs-to-use-presigned-urls"
  },"373": {
    "doc": "Presigned URL",
    "title": "Table of contents",
    "content": ". | Using presigned URLs in the UI . | Example: AWS S3 | Example: Google Storage | Example: Azure blob storage | . | . With lakeFS, you can access data directly from the storage and not through lakeFS using a presigned URL. Based on the user’s access to an object in the object store, the presigned URL will get read or write access. The presign support is enabled for block adapter that supports it (S3, GCP, Azure), and can be disabled by the configuration (blockstore.blockstore-name.disable_pre_signed). Note that the UI support is disabled by default. ",
    "url": "/v1.4/reference/security/presigned-url.html#table-of-contents",
    
    "relUrl": "/reference/security/presigned-url.html#table-of-contents"
  },"374": {
    "doc": "Presigned URL",
    "title": "Using presigned URLs in the UI",
    "content": "For using presigned URLs in the UI: . | Enable the presigned URL support UI in the lakeFS configuration (blockstore.blockstore-name.disable_pre_signed_ui). | Add CORS (Cross-Origin Resource Sharing) permissions to the bucket for the UI to fetch objects using a presigned URL (instead of through lakeFS). | The disable_pre_signed needs to be enabled to enable it in the UI. | . ⚠️ Note Currently DuckDB fetching data from lakeFS does not support fetching data using presigned URL. Example: AWS S3 . [ { \"AllowedHeaders\": [ \"*\" ], \"AllowedMethods\": [ \"GET\", \"PUT\" ], \"AllowedOrigins\": [ \"lakefs.endpoint\" ], \"ExposeHeaders\": [ \"ETag\" ] } ] . Example: Google Storage . [ { \"origin\": [\"lakefs.endpoint\"], \"responseHeader\": [\"ETag\"], \"method\": [\"PUT\", \"GET\"], \"maxAgeSeconds\": 3600 } ] . Example: Azure blob storage . &lt;Cors&gt; &lt;CorsRule&gt; &lt;AllowedOrigins&gt;lakefs.endpoint&lt;/AllowedOrigins&gt; &lt;AllowedMethods&gt;PUT,GET&lt;/AllowedMethods&gt; &lt;AllowedHeaders&gt;*&lt;/AllowedHeaders&gt; &lt;ExposedHeaders&gt;ETag,x-ms-*&lt;/ExposedHeaders&gt; &lt;MaxAgeInSeconds&gt;3600&lt;/MaxAgeInSeconds&gt; &lt;/CorsRule&gt; &lt;/Cors&gt; . ",
    "url": "/v1.4/reference/security/presigned-url.html#using-presigned-urls-in-the-ui",
    
    "relUrl": "/reference/security/presigned-url.html#using-presigned-urls-in-the-ui"
  },"375": {
    "doc": "Presigned URL",
    "title": "Presigned URL",
    "content": " ",
    "url": "/v1.4/reference/security/presigned-url.html",
    
    "relUrl": "/reference/security/presigned-url.html"
  },"376": {
    "doc": "Presto / Trino",
    "title": "Using lakeFS with Presto/Trino",
    "content": "Presto and Trino are a distributed SQL query engines designed to query large data sets distributed over one or more heterogeneous data sources. Querying data in lakeFS from Presto/Trino is similar to querying data in S3 from Presto/Trino. It is done using the Presto Hive connector or Trino Hive connector. ",
    "url": "/v1.4/integrations/presto_trino.html#using-lakefs-with-prestotrino",
    
    "relUrl": "/integrations/presto_trino.html#using-lakefs-with-prestotrino"
  },"377": {
    "doc": "Presto / Trino",
    "title": "Table of contents",
    "content": ". | Configuration . | Configure the Hive connector | Configure Hive | . | Examples . | Example with schema | Example with External table | Example of copying a table with metastore tools: | . | . Credentials . In the following examples, we set AWS credentials at runtime for clarity. In production, these properties should be set using one of Hadoop’s standard ways of Authenticating with S3. ",
    "url": "/v1.4/integrations/presto_trino.html#table-of-contents",
    
    "relUrl": "/integrations/presto_trino.html#table-of-contents"
  },"378": {
    "doc": "Presto / Trino",
    "title": "Configuration",
    "content": "Configure the Hive connector . Create /etc/catalog/hive.properties with the following contents to mount the hive-hadoop2 connector as the Hive catalog, replacing example.net:9083 with the correct host and port for your Hive Metastore Thrift service: . connector.name=hive-hadoop2 hive.metastore.uri=thrift://example.net:9083 . Add the lakeFS configurations to /etc/catalog/hive.properties in the corresponding S3 configuration properties: . hive.s3.aws-access-key=AKIAIOSFODNN7EXAMPLE hive.s3.aws-secret-key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY hive.s3.endpoint=https://lakefs.example.com hive.s3.path-style-access=true . Configure Hive . Presto/Trino uses Hive Metastore Service (HMS) or a compatible implementation of the Hive Metastore such as AWS Glue Data Catalog to write data to S3. In case you are using Hive Metastore, you will need to configure Hive as well. In file hive-site.xml add to the configuration: . &lt;configuration&gt; ... &lt;property&gt; &lt;name&gt;fs.s3a.access.key&lt;/name&gt; &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.secret.key&lt;/name&gt; &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;https://lakefs.example.com&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . ",
    "url": "/v1.4/integrations/presto_trino.html#configuration",
    
    "relUrl": "/integrations/presto_trino.html#configuration"
  },"379": {
    "doc": "Presto / Trino",
    "title": "Examples",
    "content": "Here are some examples based on examples from the Presto Hive connector examples and Trino Hive connector examples . Example with schema . Create a new schema named main that will store tables in a lakeFS repository named example branch: master: . CREATE SCHEMA main WITH (location = 's3a://example/main') . Create a new Hive table named page_views in the web schema stored using the ORC file format, partitioned by date and country, and bucketed by user into 50 buckets (note that Hive requires the partition columns to be the last columns in the table): . CREATE TABLE main.page_views ( view_time timestamp, user_id bigint, page_url varchar, ds date, country varchar ) WITH ( format = 'ORC', partitioned_by = ARRAY['ds', 'country'], bucketed_by = ARRAY['user_id'], bucket_count = 50 ) . Example with External table . Create an external Hive table named request_logs that points at existing data in lakeFS: . CREATE TABLE main.request_logs ( request_time timestamp, url varchar, ip varchar, user_agent varchar ) WITH ( format = 'TEXTFILE', external_location = 's3a://example/main/data/logs/' ) . Example of copying a table with metastore tools: . Deprecated Feature: Having heard the feedback from the community, we are planning to replace the below manual steps with an automated process. You can read more about it here. Copy the created table page_views on schema main to schema example_branch with location s3a://example/example_branch/page_views/ . lakectl metastore copy --from-schema main --from-table page_views --to-branch example_branch . ",
    "url": "/v1.4/integrations/presto_trino.html#examples",
    
    "relUrl": "/integrations/presto_trino.html#examples"
  },"380": {
    "doc": "Presto / Trino",
    "title": "Presto / Trino",
    "content": " ",
    "url": "/v1.4/integrations/presto_trino.html",
    
    "relUrl": "/integrations/presto_trino.html"
  },"381": {
    "doc": "Private Link",
    "title": "Private Link",
    "content": "lakeFS Cloud . Private Link enables lakeFS Cloud to interact with your infrastructure using private networking. ",
    "url": "/v1.4/howto/private-link.html",
    
    "relUrl": "/howto/private-link.html"
  },"382": {
    "doc": "Private Link",
    "title": "Table of contents",
    "content": ". | Supported Vendors | Access Methods | Setting up Private Link | Register your Azure subscription | Create an Azure Private Link connection to lakeFS Cloud | Create a DNS entry for your private endpoint | . ",
    "url": "/v1.4/howto/private-link.html#table-of-contents",
    
    "relUrl": "/howto/private-link.html#table-of-contents"
  },"383": {
    "doc": "Private Link",
    "title": "Supported Vendors",
    "content": "At the moment, we support Private-Link with AWS and Azure. If you are looking for Private Link for GCP please contact us. | AWS | Azure | . ",
    "url": "/v1.4/howto/private-link.html#supported-vendors",
    
    "relUrl": "/howto/private-link.html#supported-vendors"
  },"384": {
    "doc": "Private Link",
    "title": "Access Methods",
    "content": "There are two types of Private Link implementation: . | Front-End Access refers to API and UI access. Use this option if you’d like your lakeFS application to be exposed only to your infrastructure and not to the whole internet. | Back-End Access refers to the network communication between the lakeFS clusters we host, and your infrastructure. Use this option if you’d like lakeFS to communicate with your servers privately and not over the internet. | . The two types of access are not mutually exclusive nor are they dependent on each other. ",
    "url": "/v1.4/howto/private-link.html#access-methods",
    
    "relUrl": "/howto/private-link.html#access-methods"
  },"385": {
    "doc": "Private Link",
    "title": "Setting up Private Link",
    "content": "Front-End Access . Prerequisites: . | Administrator access to your AWS account | In order for us to communicate with your account privately, we’ll need to create a service endpoint on our end first. | . Steps: . | Login to your AWS account | Go to AWS VPC Service | Filter the relevant VPC &amp; Navigate to Endpoints | Click Create endpoint | Fill in the following: . | Name: lakefs-cloud | Service category: Other endpoint services | Service name: input from Treeverse team (see prerequisites) | Click Verify service | Pick the VPC you’d like to expose this service to. | Click Create endpoint | . | . Now you can access your infrastructure privately using the endpoint DNS name. If you would like to change the DNS name to a friendly one please contact support@treeverse.io. Back-End Access . Prerequisites: . | Administrator access to your AWS account | . Steps: . | Login to your AWS account | Go to AWS VPC Service | Filter the relevant VPC &amp; Navigate to Endpoints | Click endpoint service | Fill in the following: . | Name: lakefs-cloud | Load Balancer Type: Network | Available load balancers: pick the load balancer you’d like lakefs-cloud to send events to. | Click Create | . | Pick the newly created Endpoint Service from within the Endpoint Services page. | Navigate to the Allow principals tab. | Click Allow principals | Fill in the following ARN: arn:aws:iam::924819537486:root | Click Allow principals | . That’s it on your end! Now, we’ll need the service name you’ve just created in order to associate it with our infrastructure, once we do, we’ll be ready to use the back-end access privately. Azure Private Link enables secure access to Azure services from a private endpoint within your virtual network. By using Azure Private Link with lakeFS, you can securely access lakeFS services without exposing traffic to the public internet. In this manual, we will guide you through the steps to enable Azure Private Link to your lakeFS instance. ",
    "url": "/v1.4/howto/private-link.html#setting-up-private-link",
    
    "relUrl": "/howto/private-link.html#setting-up-private-link"
  },"386": {
    "doc": "Private Link",
    "title": "Register your Azure subscription",
    "content": "To automatically approve private endpoint connections to the lakeFS network, please provide us with your subscription. If required, you can register multiple subscriptions. ",
    "url": "/v1.4/howto/private-link.html#register-your-azure-subscription",
    
    "relUrl": "/howto/private-link.html#register-your-azure-subscription"
  },"387": {
    "doc": "Private Link",
    "title": "Create an Azure Private Link connection to lakeFS Cloud",
    "content": "Once your subscription is in our trusted subscriptions navigate to the Azure portal and do the following steps: . | Navigate to the private endpoint | Click Create | On the first step (basics): . | Select your subscription | Specify the desired resource group used to access lakeFS | Provide a name for your private endpoint instance | Specify the region of your lakeFS instance | . | On the second step (Resource) . | In connection method select connect to an Azure resource by resource ID or alias | Insert the alias provided by us into the Resource ID or alias | No need to add a request message | . | Continue with the steps and run Review + Create | . ",
    "url": "/v1.4/howto/private-link.html#create-an-azure-private-link-connection-to-lakefs-cloud",
    
    "relUrl": "/howto/private-link.html#create-an-azure-private-link-connection-to-lakefs-cloud"
  },"388": {
    "doc": "Private Link",
    "title": "Create a DNS entry for your private endpoint",
    "content": "Update your DNS server to resolve your account URL (which will be provided by us) to the Private Link IP address. You can add the DNS entry to your on-premises DNS server or private DNS on your VNet, to access lakeFS services. ",
    "url": "/v1.4/howto/private-link.html#create-a-dns-entry-for-your-private-endpoint",
    
    "relUrl": "/howto/private-link.html#create-a-dns-entry-for-your-private-endpoint"
  },"389": {
    "doc": "In Production",
    "title": "In Production",
    "content": "Errors with data in production inevitably occur. When they do, they best thing we can do is remove the erroneous data, understand why the issue happened, and deploy changes that prevent it from occurring again. Example 1: RollBack! - Data ingested from a Kafka stream . If you introduce a new code version to production and discover it has a critical bug, you can simply roll back to the previous version. But you also need to roll back the results of running it. Similar to Git, lakeFS allows you to revert your commits in case they introduced low-quality data. Revert in lakeFS is an atomic action that prevents the data consumers from receiving low quality data until the issue is resolved. As previously mentioned, with lakeFS the recommended branching schema is to ingest data to a dedicated branch. When streaming data, we can decide to merge the incoming data to main at a given time interval or checkpoint, depending on how we chose to write it from Kafka. You can run quality tests for each merge (as discussed in the During Deployment section). Alas, tests are not perfect and we might still introduce low quality data to our main branch at some point. In such a case, we can revert the bad commits from main to the last known high quality commit. This will record new commits reversing the effect of the bad commits. Reverting commits using the CLI . lakectl branch revert lakefs://example-repo/main 20c30c96 ababea32 . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. Example 2: Troubleshoot - Reproduce a bug in production . You upgraded spark and deployed changes in production. A few days or weeks later, you identify a data quality issue, a performance degradation, or an increase to your infra costs. Something that requires investigation and fixing (aka, a bug). lakeFS allows you to open a branch of your lake from the specific merge/commit that introduced the changes to production. Using the metadata saved on the merge/commit you can reproduce all aspects of the environment, then reproduce the issue on the branch and debug it. Meanwhile, you can revert the main to a previous point in time, or keep it as is, depending on the use case . Reading from a historic version (a previous commit) using Spark . // represents the data as existed at commit \"11eef40b\": spark.read.parquet(\"s3://example-repo/11eef40b/events/by-date\") . Example 3: Cross collection consistency . We often need consistency between different data collections. A few examples may be: . | To join different collections in order to create a unified view of an account, a user or another entity we measure. | To introduce the same data in different formats | To introduce the same data with a different leading index or sorting due to performance considerations | . lakeFS will help ensure you introduce only consistent data to your consumers by exposing the new collections and their join in one atomic action to main. Once you consumed the collections on a different branch, and only when both are synchronized, we calculated the join and merged to main. In this example you can see two data sets (Sales data and Marketing data) consumed each to its own independent branch, and after the write of both data sets is completed, they are merged to a different branch (leads branch) where the join ETL runs and creates a joined collection by account. The joined table is then merged to main. The same logic can apply if the data is ingested in streaming, using standard formats, or formats that allow upsert/delete such as Apache Hudi, Delta Lake or Iceberg. ",
    "url": "/v1.4/understand/data_lifecycle_management/production.html",
    
    "relUrl": "/understand/data_lifecycle_management/production.html"
  },"390": {
    "doc": "In Production",
    "title": "Case Study: Windward",
    "content": "See how Windward is using lakeFS’ isolation and atomic commits to achieve consistency on top of S3. ",
    "url": "/v1.4/understand/data_lifecycle_management/production.html#case-study-windward",
    
    "relUrl": "/understand/data_lifecycle_management/production.html#case-study-windward"
  },"391": {
    "doc": "Protect Branches",
    "title": "Branch Protection Rules",
    "content": "Define branch protection rules to prevent direct changes and commits to specific branches. Only merges are allowed into protected branches. Together with the power of pre-merge hooks, you can run validations on your data before it reaches your important branches and is exposed to consumers. You can create rules for a specific branch or any branch that matches a name pattern you specify with glob syntax (supporting ? and * wildcards). ",
    "url": "/v1.4/howto/protect-branches.html#branch-protection-rules",
    
    "relUrl": "/howto/protect-branches.html#branch-protection-rules"
  },"392": {
    "doc": "Protect Branches",
    "title": "How it works",
    "content": "When at least one protection rule applies to a branch, the branch is protected. The following operations will fail on protected branches: . | Object write operations: upload and delete objects. | Branch operations: commit and reset uncommitted changes. | . To operate on a protected branch, merge commits from other branches into it. Use pre-merge hooks to validate the changes before they are merged. Reverting a previous commit using lakectl branch revert is allowed on a protected branch. ",
    "url": "/v1.4/howto/protect-branches.html#how-it-works",
    
    "relUrl": "/howto/protect-branches.html#how-it-works"
  },"393": {
    "doc": "Protect Branches",
    "title": "Managing branch protection rules",
    "content": "This section explains how to use the lakeFS UI to manage rules. You can also use the command line and API. Reaching the branch protection rules page . | On lakeFS, navigate to the main page of the repository. | Click on the Settings tab. | In the left menu, click Branches. | . Adding a rule . To add a new rule, click the Add button. In the dialog, enter the branch name pattern and then click Create. Deleting a rule . To delete a rule, click the Delete button next to it. ",
    "url": "/v1.4/howto/protect-branches.html#managing-branch-protection-rules",
    
    "relUrl": "/howto/protect-branches.html#managing-branch-protection-rules"
  },"394": {
    "doc": "Protect Branches",
    "title": "Protect Branches",
    "content": " ",
    "url": "/v1.4/howto/protect-branches.html",
    
    "relUrl": "/howto/protect-branches.html"
  },"395": {
    "doc": "Python",
    "title": "Use Python to interact with your objects on lakeFS",
    "content": " ",
    "url": "/v1.4/integrations/python.html#use-python-to-interact-with-your-objects-on-lakefs",
    
    "relUrl": "/integrations/python.html#use-python-to-interact-with-your-objects-on-lakefs"
  },"396": {
    "doc": "Python",
    "title": "Table of contents",
    "content": ". | Using the lakeFS SDK . | Installing | Initializing | Usage Examples | List branches | . | IO . | Upload | Uncommitted changes | Read data from main branch | Python SDK documentation and API reference | . | Using Boto . | Initializing | Usage Examples | . | . High Level Python SDK New We’ve just released a new High Level Python SDK library, and we’re super excited to tell you about it! Continue reading to get the full story! Though our previous SDK client is still supported and maintained, we highly recommend using the new High Level SDK. For previous Python SDKs follow these links: lakefs-sdk legacy-sdk (Depracated) . There are two primary ways to work with lakeFS from Python: . | Use Boto to perform object operations through the lakeFS S3 gateway. | Use the High Level lakeFS SDK to perform object operations, versioning and other lakeFS-specific operations. | . ",
    "url": "/v1.4/integrations/python.html#table-of-contents",
    
    "relUrl": "/integrations/python.html#table-of-contents"
  },"397": {
    "doc": "Python",
    "title": "Using the lakeFS SDK",
    "content": "Installing . Install the Python client using pip: . pip install lakefs . Initializing . The High Level SDK by default will try to collect authentication parameters from the environment and attempt to create a default client. When working in an environment where lakectl is configured it is not necessary to instantiate a lakeFS client or provide it for creating the lakeFS objects. In case no authentication parameters exist, it is also possible to explicitly create a lakeFS client . Here’s how to instantiate a client: . from lakefs.client import Client clt = Client( host=\"http://localhost:8000\", username=\"AKIAIOSFODNN7EXAMPLE\", password=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", ) . For testing SSL endpoints you may wish to use a self-signed certificate. If you do this and receive an SSL: CERTIFICATE_VERIFY_FAILED error message you might add the following configuration to your client: . clt.config.verify_ssl= False . This setting allows well-known “man-in-the-middle”, impersonation, and credential stealing attacks. Never use this in any production setting. Optionally, to enable communication via proxies, simply set the proxy configuration: . clt.config.ssl_ca_cert = &lt;path to a file of concatenated CA certificates in PEM format&gt; # Set this to customize the certificate file to verify the peer clt.config.proxy = &lt;proxy server URL&gt; . Usage Examples . Lets see how we can interact with lakeFS using the High Level SDK. Creating a repository . import lakefs repo = lakefs.repository(\"example-repo\").create(storage_namespace=\"s3://storage-bucket/repos/example-repo\") print(repo) . If using an explicit client, create the Repository object and pass the client to it (note the changed syntax). import lakefs from lakefs.client import Client clt = Client( host=\"http://localhost:8000\", username=\"AKIAIOSFODNN7EXAMPLE\", password=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", ) repo = lakefs.Repository(\"example-repo\", client=clt).create(storage_namespace=\"s3://storage-bucket/repos/example-repo\") print(repo) . Output . {id: 'example-repo', creation_date: 1697815536, default_branch: 'main', storage_namespace: 's3://storage-bucket/repos/example-repo'} . List repositories . import lakefs print(\"Listing repositories:\") for repo in lakefs.repositories(): print(repo) . Output . Listing repositories: {id: 'example-repo', creation_date: 1697815536, default_branch: 'main', storage_namespace: 's3://storage-bucket/repos/example-repo'} . Creating a branch . import lakefs branch1 = lakefs.repository(\"example-repo\").branch(\"experiment1\").create(source_reference_id=\"main\") print(\"experiment1 ref:\", branch1.get_commit().id) branch1 = lakefs.repository(\"example-repo\").branch(\"experiment2\").create(source_reference_id=\"main\") print(\"experiment2 ref:\", branch2.get_commit().id) . Output . experiment1 ref: 7a300b41a8e1ca666c653171a364c08f640549c24d7e82b401bf077c646f8859 experiment2 ref: 7a300b41a8e1ca666c653171a364c08f640549c24d7e82b401bf077c646f8859 . List branches . import lakefs for branch in lakefs.repository(\"example-repo\").branches(): print(branch) . Output . experiment1 experiment2 main . ",
    "url": "/v1.4/integrations/python.html#using-the-lakefs-sdk",
    
    "relUrl": "/integrations/python.html#using-the-lakefs-sdk"
  },"398": {
    "doc": "Python",
    "title": "IO",
    "content": "Great, now lets see some IO operations in action! The new High Level SDK provide IO semantics which allow to work with lakeFS objects as if they were files in your filesystem. This is extremely useful when working with data transformation packages that accept file descriptors and streams. Upload . A simple way to upload data is to use the upload method which accepts contents as str/bytes . obj = branch1.object(path=\"text/sample_data.txt\").upload(content_type=\"text/plain\", data=\"This is my object data\") print(obj.stats()) . Output . {'path': 'text/sample_data.txt', 'physical_address': 's3://storage-bucket/repos/example-repo/data/gke0ignnl531fa6k90p0/ckpfk4fnl531fa6k90pg', 'physical_address_expiry': None, 'checksum': '4a09d10820234a95bb548f14e4435bba', 'size_bytes': 15, 'mtime': 1701865289, 'metadata': {}, 'content_type': 'text/plain'} . Reading the data is just as simple: . print(obj.reader(mode='r').read()) . Output . This is my object data . Now let’s generate a “sample_data.csv” file and write it directly to a lakeFS writer object . import csv sample_data = [ [1, \"Alice\", \"alice@example.com\"], [2, \"Bob\", \"bob@example.com\"], [3, \"Carol\", \"carol@example.com\"], ] obj = branch1.object(path=\"csv/sample_data.csv\") with obj.writer(mode='w', pre_sign=True, content_type=\"text/csv\") as fd: writer = csv.writer(fd) writer.writerow([\"ID\", \"Name\", \"Email\"]) for row in sample_data: writer.writerow(row) . On context exit the object will be uploaded to lakeFS . print(obj.stats()) . Output . {'path': 'csv/sample_data.csv', 'physical_address': 's3://storage-bucket/repos/example-repo/data/gke0ignnl531fa6k90p0/ckpfk4fnl531fa6k90pg', 'physical_address_expiry': None, 'checksum': 'f181262c138901a74d47652d5ea72295', 'size_bytes': 88, 'mtime': 1701865939, 'metadata': {}, 'content_type': 'text/csv'} . We can also upload raw byte contents: . obj = branch1.object(path=\"raw/file1.data\").upload(data=b\"Hello Object World\", pre_sign=True) print(obj.stats()) . Output . {'path': 'raw/file1.data', 'physical_address': 's3://storage-bucket/repos/example-repo/data/gke0ignnl531fa6k90p0/ckpfltvnl531fa6k90q0', 'physical_address_expiry': None, 'checksum': '0ef432f8eb0305f730b0c57bbd7a6b08', 'size_bytes': 18, 'mtime': 1701866323, 'metadata': {}, 'content_type': 'application/octet-stream'} . Uncommitted changes . Using the branch uncommmitted method will show all the uncommitted changes on that branch: . for diff in branch1.uncommitted(): print(diff) . Output . {'type': 'added', 'path': 'text/sample_data.txt', 'path_type': 'object', 'size_bytes': 15} {'type': 'added', 'path': 'csv/sample_data.csv', 'path_type': 'object', 'size_bytes': 88} {'type': 'added', 'path': 'raw/file1.data', 'path_type': 'object', 'size_bytes': 18} . As expected, our change appears here. Let’s commit it and attach some arbitrary metadata: . ref = branch1.commit(message='Add some data!', metadata={'using': 'python_sdk'}) print(ref.get_commit()) . Output . {'id': 'c4666db80d2a984b4eab8ce02b6a60830767eba53995c26350e0ad994e15fedb', 'parents': ['a7a092a5a32a2cd97f22abcc99414f6283d29f6b9dd2725ce89f90188c5901e5'], 'committer': 'admin', 'message': 'Add some data!', 'creation_date': 1701866838, 'meta_range_id': '999bedeab1b740f83d2cf8c52548d55446f9038c69724d399adc4438412cade2', 'metadata': {'using': 'python_sdk'}} . Calling uncommitted again on the same branch, this time there should be no uncommitted files: . print(len(list(branch1.uncommitted()))) . Output . 0 . Merging changes from a branch into main . Let’s diff between your branch and the main branch: . main = repo.branch(\"main\") for diff in main.diff(other_ref=branch1): print(diff) . Output . {'type': 'added', 'path': 'text/sample_data.txt', 'path_type': 'object', 'size_bytes': 15} {'type': 'added', 'path': 'csv/sample_data.csv', 'path_type': 'object', 'size_bytes': 88} {'type': 'added', 'path': 'raw/file1.data', 'path_type': 'object', 'size_bytes': 18} . Looks like we have some changes. Let’s merge them: . res = branch1.merge_into(main) print(res) # output: # cfddb68b7265ae0b17fafa1a2068f8414395e0a8b8bc0f8d741cbcce1e67e394 . Let’s diff again - there should be no changes as all changes are on our main branch already: . print(len(list(main.diff(other_ref=branch1)))) . Output . 0 . Read data from main branch . import csv obj = main.object(path=\"csv/sample_data.csv\") for row in csv.reader(obj.reader(mode='r')): print(row) . Output . ['ID', 'Name', 'Email'] ['1', 'Alice', 'alice@example.com'] ['2', 'Bob', 'bob@example.com'] ['3', 'Carol', 'carol@example.com'] . Python SDK documentation and API reference . For the documentation of lakeFS’s Python package and full api reference, see https://pydocs-lakefs.lakefs.io . ",
    "url": "/v1.4/integrations/python.html#io",
    
    "relUrl": "/integrations/python.html#io"
  },"399": {
    "doc": "Python",
    "title": "Using Boto",
    "content": "💡 To use Boto with lakeFS alongside S3, check out Boto S3 Router. It will route requests to either S3 or lakeFS according to the provided bucket name. lakeFS exposes an S3-compatible API, so you can use Boto to interact with your objects on lakeFS. Initializing . Create a Boto3 S3 client with your lakeFS endpoint and key-pair: . import boto3 s3 = boto3.client('s3', endpoint_url='https://lakefs.example.com', aws_access_key_id='AKIAIOSFODNN7EXAMPLE', aws_secret_access_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY') . The client is now configured to operate on your lakeFS installation. Usage Examples . Put an object into lakeFS . Use a branch name and a path to put an object in lakeFS: . with open('/local/path/to/file_0', 'rb') as f: s3.put_object(Body=f, Bucket='example-repo', Key='main/example-file.parquet') . You can now commit this change using the lakeFS UI or CLI. List objects . List the branch objects starting with a prefix: . list_resp = s3.list_objects_v2(Bucket='example-repo', Prefix='main/example-prefix') for obj in list_resp['Contents']: print(obj['Key']) . Or, use a lakeFS commit ID to list objects for a specific commit: . list_resp = s3.list_objects_v2(Bucket='example-repo', Prefix='c7a632d74f/example-prefix') for obj in list_resp['Contents']: print(obj['Key']) . Get object metadata . Get object metadata using branch and path: . s3.head_object(Bucket='example-repo', Key='main/example-file.parquet') # output: # {'ResponseMetadata': {'RequestId': '72A9EBD1210E90FA', # 'HostId': '', # 'HTTPStatusCode': 200, # 'HTTPHeaders': {'accept-ranges': 'bytes', # 'content-length': '1024', # 'etag': '\"2398bc5880e535c61f7624ad6f138d62\"', # 'last-modified': 'Sun, 24 May 2020 10:42:24 GMT', # 'x-amz-request-id': '72A9EBD1210E90FA', # 'date': 'Sun, 24 May 2020 10:45:42 GMT'}, # 'RetryAttempts': 0}, # 'AcceptRanges': 'bytes', # 'LastModified': datetime.datetime(2020, 5, 24, 10, 42, 24, tzinfo=tzutc()), # 'ContentLength': 1024, # 'ETag': '\"2398bc5880e535c61f7624ad6f138d62\"', # 'Metadata': {}} . ",
    "url": "/v1.4/integrations/python.html#using-boto",
    
    "relUrl": "/integrations/python.html#using-boto"
  },"400": {
    "doc": "Python",
    "title": "Python",
    "content": " ",
    "url": "/v1.4/integrations/python.html",
    
    "relUrl": "/integrations/python.html"
  },"401": {
    "doc": "2️⃣ Query the data",
    "title": "Let’s Query Something",
    "content": "The lakeFS server has been loaded with a sample parquet datafile. Fittingly enough for a piece of software to help users of data lakes, the lakes.parquet file holds data about lakes around the world. You’ll notice that the branch is set to main. This is conceptually the same as your main branch in Git against which you develop software code. Let’s have a look at the data, ahead of making some changes to it on a branch in the following steps. Click on lakes.parquet and notice that the built-it DuckDB runs a query to show a preview of the file’s contents. Now we’ll run our own query on it to look at the top five countries represented in the data. Copy and paste the following SQL statement into the DuckDB query panel and click on Execute. SELECT country, COUNT(*) FROM READ_PARQUET('lakefs://quickstart/main/lakes.parquet') GROUP BY country ORDER BY COUNT(*) DESC LIMIT 5; . Next we’re going to make some changes to the data—but on a development branch so that the data in the main branch remains untouched. ",
    "url": "/v1.4/quickstart/query.html#lets-query-something",
    
    "relUrl": "/quickstart/query.html#lets-query-something"
  },"402": {
    "doc": "2️⃣ Query the data",
    "title": "2️⃣ Query the data",
    "content": " ",
    "url": "/v1.4/quickstart/query.html",
    
    "relUrl": "/quickstart/query.html"
  },"403": {
    "doc": "R",
    "title": "Using R with lakeFS",
    "content": "R is a powerful language used widely in data science. lakeFS interfaces with R in two ways: . | To read and write data in lakeFS use standard S3 tools such as the aws.s3 library. lakeFS has a S3 gateway which presents a lakeFS repository as an S3 bucket. | For working with lakeFS operations such as branches and commits use the API for which can be accessed from R using the httr library. | . To see examples of R in action with lakeFS please visit the lakeFS-samples repository and the sample notebooks. ",
    "url": "/v1.4/integrations/r.html#using-r-with-lakefs",
    
    "relUrl": "/integrations/r.html#using-r-with-lakefs"
  },"404": {
    "doc": "R",
    "title": "Table of contents",
    "content": ". | Reading and Writing from lakeFS with R | Performing lakeFS Operations using the lakeFS API from R | . ",
    "url": "/v1.4/integrations/r.html#table-of-contents",
    
    "relUrl": "/integrations/r.html#table-of-contents"
  },"405": {
    "doc": "R",
    "title": "Reading and Writing from lakeFS with R",
    "content": "Working with data stored in lakeFS from R is the same as you would with an S3 bucket, via the S3 Gateway that lakeFS provides. You can use any library that interfaces with S3. In this example we’ll use the aws.s3 library. install.packages(c(\"aws.s3\")) library(aws.s3) . Configuration . The R S3 client documentation includes full details of the configuration options available. A good approach for using it with lakeFS set the endpoint and authentication details as environment variables: . Sys.setenv(\"AWS_ACCESS_KEY_ID\" = \"AKIAIOSFODNN7EXAMPLE\", \"AWS_SECRET_ACCESS_KEY\" = \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"AWS_S3_ENDPOINT\" = \"lakefs.mycorp.com:8000\") . Note: it is generally best practice to set these environment variables outside of the R script; it is done so here for convenience of the example. In conjunction with this you must also specify region and use_https in each call of an aws.s3 function as these cannot be set globally. For example: . bucketlist( region = \"\", use_https = FALSE ) . | region should always be empty | use_https should be set to TRUE or FALSE depending on whether your lakeFS endpoint uses HTTPS. | . Listing repositories . The S3 gateway exposes a repository as a bucket, and so using the aws.s3 function bucketlist will return a list of available repositories on lakeFS: . bucketlist( region = \"\", use_https = FALSE ) . Writing to lakeFS from R . Assuming you’re using the aws.s3 library there various functions available including s3save, s3saveRDS, and put_object. Here’s an example of writing an R object to lakeFS: . repo_name &lt;- \"example\" branch &lt;- \"development\" s3saveRDS(x=my_df, bucket = repo_name, object = paste0(branch,\"/my_df.R\"), region = \"\", use_https = FALSE) . You can also upload local files to lakeFS using R and the put_object function: . repo_name &lt;- \"example\" branch &lt;- \"development\" local_file &lt;- \"/tmp/never.gonna\" put_object(file = local_file, bucket = repo_name, object = paste0(branch,\"/give/you/up\"), region = \"\", use_https = FALSE) . Reading from lakeFS with R . As with writing data from R to lakeFS, there is a similar set of functions for reading data. These include s3load, s3readRDS, and get_object. Here’s an example of reading an R object from lakeFS: . repo_name &lt;- \"example\" branch &lt;- \"development\" my_df &lt;- s3readRDS(bucket = repo_name, object = paste0(branch,\"/my_data.R\"), region = \"\", use_https = FALSE) . Listing Objects . In general you should always specify a branch prefix when listing objects. Here’s an example to list the main branch in the quickstart repository: . get_bucket_df(bucket = \"quickstart\", prefix = \"main/\", region = \"\", use_https = FALSE) . When listing objects in lakeFS there is a special case which is the repository/bucket level. When you list at this level you will get the branches returned as folders. These are not listed recursively, unless you list something under the branch. To understand more about this please refer to #5441 . Working with Arrow . Arrow’s R library includes powerful support for data analysis, including reading and writing multiple file formats including Parquet, Arrow, CSV, and JSON. It has functionality for connecting to S3, and thus integrates perfectly with lakeFS. To start with install and load the library . install.packages(\"arrow\") library(arrow) . Then create an S3FileSystem object to connect to your lakeFS instance . lakefs &lt;- S3FileSystem$create( endpoint_override = \"lakefs.mycorp.com:8000\", scheme = \"http\" access_key = \"AKIAIOSFODNN7EXAMPLE\", secret_key = \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", region = \"\", ) . From here you can list the contents of a particular lakeFS repository and branch . lakefs$ls(path = \"quickstart/main\") . To read a Parquet from lakeFS with R use the read_parquet function . lakes &lt;- read_parquet(lakefs$path(\"quickstart/main/lakes.parquet\")) . Writing a file follows a similar pattern. Here is rewriting the same file as above but in Arrow format . write_feather(x = lakes, sink = lakefs$path(\"quickstart/main/lakes.arrow\")) . ",
    "url": "/v1.4/integrations/r.html#reading-and-writing-from-lakefs-with-r",
    
    "relUrl": "/integrations/r.html#reading-and-writing-from-lakefs-with-r"
  },"406": {
    "doc": "R",
    "title": "Performing lakeFS Operations using the lakeFS API from R",
    "content": "As well as reading and writing data, you will also want to carry out lakeFS operations from R including creating branches, committing data, and more. To do this call the lakeFS API from the httr library. You should refer to the API documentation for full details of the endpoints and their behaviour. Below are a few examples to illustrate the usage. Check the lakeFS Server Version . This is a useful API call to establish connectivity and test authentication. library(httr) lakefs_api_url &lt;- \"lakefs.mycorp.com:8000/api/v1\" lakefsAccessKey &lt;- \"AKIAIOSFODNN7EXAMPLE\" lakefsSecretKey &lt;- \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" r=GET(url=paste0(lakefs_api_url, \"/config/version\"), authenticate(lakefsAccessKey, lakefsSecretKey)) . The returned object r can be inspected to determine the outcome of the operation by comparing it to the status codes specified in the API. Here is some example R code to demonstrate the idea: . if (r$status_code == 200) { print(paste0(\"✅lakeFS credentials and connectivity verified. ℹ️lakeFS version \",content(r)$version)) } else { print(\"🛑 failed to get lakeFS version\") print(content(r)$message) } . Create a Repository . library(httr) lakefs_api_url &lt;- \"lakefs.mycorp.com:8000/api/v1\" lakefsAccessKey &lt;- \"AKIAIOSFODNN7EXAMPLE\" lakefsSecretKey &lt;- \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" repo_name &lt;- \"my_new_repo\" # Define the payload body=list(name=repo_name, storage_namespace=\"s3://example-bucket/foo\") # Call the API r=POST(url=paste0(lakefs_api_url, \"/repositories\"), authenticate(lakefsAccessKey, lakefsSecretKey), body=body, encode=\"json\") . Commit Data . library(httr) lakefs_api_url &lt;- \"lakefs.mycorp.com:8000/api/v1\" lakefsAccessKey &lt;- \"AKIAIOSFODNN7EXAMPLE\" lakefsSecretKey &lt;- \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" repo_name &lt;- \"my_new_repo\" branch &lt;- \"example\" # Define the payload body=list(message=\"add some data and charts\", metadata=list( client=\"httr\", author=\"rmoff\")) # Call the API r=POST(url=paste0(lakefs_api_url, \"/repositories/\", repo_name, \"/branches/\", branch, \"/commits\"), authenticate(lakefsAccessKey, lakefsSecretKey), body=body, encode=\"json\") . ",
    "url": "/v1.4/integrations/r.html#performing-lakefs-operations-using-the-lakefs-api-from-r",
    
    "relUrl": "/integrations/r.html#performing-lakefs-operations-using-the-lakefs-api-from-r"
  },"407": {
    "doc": "R",
    "title": "R",
    "content": " ",
    "url": "/v1.4/integrations/r.html",
    
    "relUrl": "/integrations/r.html"
  },"408": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Role-Based Access Control (RBAC)",
    "content": "lakeFS Cloud . lakeFS Enterprise . RBAC is available on lakeFS Cloud and lakeFS Enterprise. If you’re using the open source version of lakeFS then the ACL-based authorization mechanism is an alternative to RBAC. ",
    "url": "/v1.4/reference/security/rbac.html",
    
    "relUrl": "/reference/security/rbac.html"
  },"409": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Table of contents",
    "content": ". | RBAC Model | Authorization process | Policy Precedence | Resource naming - ARNs | Actions and Permissions | Preconfigured Policies | Additional Policies | Preconfigured Groups | Pluggable Authentication and Authorization | . ",
    "url": "/v1.4/reference/security/rbac.html#table-of-contents",
    
    "relUrl": "/reference/security/rbac.html#table-of-contents"
  },"410": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "RBAC Model",
    "content": "Access to resources is managed very much like AWS IAM. There are five basic components to the system: . | Users - Representing entities that access and use the system. A user is given one or more Access Credentials for authentication. | Actions - Representing a logical action within the system - reading a file, creating a repository, etc. | Resources - A unique identifier representing a specific resource in the system - a repository, an object, a user, etc. | Policies - Representing a set of Actions, a Resource and an effect: whether or not these actions are allowed or denied for the given resource(s). | Groups - A named collection of users. Users can belong to multiple groups. | . Controlling access is done by attaching Policies, either directly to Users, or to Groups they belong to. ",
    "url": "/v1.4/reference/security/rbac.html#rbac-model",
    
    "relUrl": "/reference/security/rbac.html#rbac-model"
  },"411": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Authorization process",
    "content": "Every action in the system - be it an API request, UI interaction, S3 Gateway call, or CLI command - requires a set of actions to be allowed for one or more resources. When a user makes a request to perform that action, the following process takes place: . | Authentication - the credentials passed in the request are evaluated and the user’s identity is extracted. | Action permission resolution - lakeFS then calculates the set of allowed actions and resources that this request requires. | Effective policy resolution - the user’s policies (either attached directly or through group memberships) are calculated. | Policy/Permission evaluation - lakeFS will compare the given user policies with the request actions and determine whether or not the request is allowed to continue. | . ",
    "url": "/v1.4/reference/security/rbac.html#authorization-process",
    
    "relUrl": "/reference/security/rbac.html#authorization-process"
  },"412": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Policy Precedence",
    "content": "Each policy attached to a user or a group has an Effect - either allow or deny. During evaluation of a request, deny would take precedence over any other allow policy. This helps us compose policies together. For example, we could attach a very permissive policy to a user and use deny rules to then selectively restrict what that user can do. ",
    "url": "/v1.4/reference/security/rbac.html#policy-precedence",
    
    "relUrl": "/reference/security/rbac.html#policy-precedence"
  },"413": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Resource naming - ARNs",
    "content": "lakeFS uses ARN identifier - very similar in structure to those used by AWS. The resource segment of the ARN supports wildcards: use * to match 0 or more characters, or ? to match exactly one character. Additionally, the current user’s ID is interpolated in runtime into the ARN using the ${user} placeholder. Here are a few examples of valid ARNs within lakeFS: . arn:lakefs:auth:::user/jane.doe arn:lakefs:auth:::user/* arn:lakefs:fs:::repository/myrepo/* arn:lakefs:fs:::repository/myrepo/object/foo/bar/baz arn:lakefs:fs:::repository/myrepo/object/* arn:lakefs:fs:::repository/* arn:lakefs:fs:::* . this allows us to create fine-grained policies affecting only a specific subset of resources. See below for a full reference of ARNs and actions. ",
    "url": "/v1.4/reference/security/rbac.html#resource-naming---arns",
    
    "relUrl": "/reference/security/rbac.html#resource-naming---arns"
  },"414": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Actions and Permissions",
    "content": "For the full list of actions and their required permissions see the following table: . | Action name | required action | Resource | API endpoint | S3 gateway operation | . | List Repositories | fs:ListRepositories | * | GET /repositories | ListBuckets | . | Get Repository | fs:ReadRepository | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId} | HeadBucket | . | Get Commit | fs:ReadCommit | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/commits/{commitId} | - | . | Create Commit | fs:CreateCommit | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | POST /repositories/{repositoryId}/branches/{branchId}/commits | - | . | Get Commit log | fs:ReadBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | GET /repositories/{repositoryId}/branches/{branchId}/commits | - | . | Create Repository | fs:CreateRepository | arn:lakefs:fs:::repository/{repositoryId} | POST /repositories | - | . | Namespace Attach to Repository | fs:AttachStorageNamespace | arn:lakefs:fs:::namespace/{storageNamespace} | POST /repositories | - | . | Import From Source | fs:ImportFromStorage | arn:lakefs:fs:::namespace/{storageNamespace} | POST /repositories/{repositoryId}/branches/{branchId}/import | - | . | Cancel Import | fs:ImportCancel | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | DELETE /repositories/{repositoryId}/branches/{branchId}/import | - | . | Delete Repository | fs:DeleteRepository | arn:lakefs:fs:::repository/{repositoryId} | DELETE /repositories/{repositoryId} | - | . | List Branches | fs:ListBranches | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/branches | ListObjects/ListObjectsV2 (with delimiter = / and empty prefix) | . | Get Branch | fs:ReadBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | GET /repositories/{repositoryId}/branches/{branchId} | - | . | Create Branch | fs:CreateBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | POST /repositories/{repositoryId}/branches | - | . | Delete Branch | fs:DeleteBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | DELETE /repositories/{repositoryId}/branches/{branchId} | - | . | Merge branches | fs:CreateCommit | arn:lakefs:fs:::repository/{repositoryId}/branch/{destinationBranchId} | POST /repositories/{repositoryId}/refs/{sourceBranchId}/merge/{destinationBranchId} | - | . | Diff branch uncommitted changes | fs:ListObjects | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/branches/{branchId}/diff | - | . | Diff refs | fs:ListObjects | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/refs/{leftRef}/diff/{rightRef} | - | . | Stat object | fs:ReadObject | arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey} | GET /repositories/{repositoryId}/refs/{ref}/objects/stat | HeadObject | . | Get Object | fs:ReadObject | arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey} | GET /repositories/{repositoryId}/refs/{ref}/objects | GetObject | . | List Objects | fs:ListObjects | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/refs/{ref}/objects/ls | ListObjects, ListObjectsV2 (no delimiter, or “/” + non-empty prefix) | . | Upload Object | fs:WriteObject | arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey} | POST /repositories/{repositoryId}/branches/{branchId}/objects | PutObject, CreateMultipartUpload, UploadPart, CompleteMultipartUpload | . | Delete Object | fs:DeleteObject | arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey} | DELETE /repositories/{repositoryId}/branches/{branchId}/objects | DeleteObject, DeleteObjects, AbortMultipartUpload | . | Revert Branch | fs:RevertBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | PUT /repositories/{repositoryId}/branches/{branchId} | - | . | Get Branch Protection Rules | branches:GetBranchProtectionRules | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repository}/branch_protection | - | . | Set Branch Protection Rules | branches:SetBranchProtectionRules | arn:lakefs:fs:::repository/{repositoryId} | POST /repositories/{repository}/branch_protection | - | . | Delete Branch Protection Rules | branches:SetBranchProtectionRules | arn:lakefs:fs:::repository/{repositoryId} | DELETE /repositories/{repository}/branch_protection | - | . | Create User | auth:CreateUser | arn:lakefs:auth:::user/{userId} | POST /auth/users | - | . | List Users | auth:ListUsers | * | GET /auth/users | - | . | Get User | auth:ReadUser | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId} | - | . | Delete User | auth:DeleteUser | arn:lakefs:auth:::user/{userId} | DELETE /auth/users/{userId} | - | . | Get Group | auth:ReadGroup | arn:lakefs:auth:::group/{groupId} | GET /auth/groups/{groupId} | - | . | List Groups | auth:ListGroups | * | GET /auth/groups | - | . | Create Group | auth:CreateGroup | arn:lakefs:auth:::group/{groupId} | POST /auth/groups | - | . | Delete Group | auth:DeleteGroup | arn:lakefs:auth:::group/{groupId} | DELETE /auth/groups/{groupId} | - | . | List Policies | auth:ListPolicies | * | GET /auth/policies | - | . | Create Policy | auth:CreatePolicy | arn:lakefs:auth:::policy/{policyId} | POST /auth/policies | - | . | Update Policy | auth:UpdatePolicy | arn:lakefs:auth:::policy/{policyId} | POST /auth/policies | - | . | Delete Policy | auth:DeletePolicy | arn:lakefs:auth:::policy/{policyId} | DELETE /auth/policies/{policyId} | - | . | Get Policy | auth:ReadPolicy | arn:lakefs:auth:::policy/{policyId} | GET /auth/policies/{policyId} | - | . | List Group Members | auth:ReadGroup | arn:lakefs:auth:::group/{groupId} | GET /auth/groups/{groupId}/members | - | . | Add Group Member | auth:AddGroupMember | arn:lakefs:auth:::group/{groupId} | PUT /auth/groups/{groupId}/members/{userId} | - | . | Remove Group Member | auth:RemoveGroupMember | arn:lakefs:auth:::group/{groupId} | DELETE /auth/groups/{groupId}/members/{userId} | - | . | List User Credentials | auth:ListCredentials | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId}/credentials | - | . | Create User Credentials | auth:CreateCredentials | arn:lakefs:auth:::user/{userId} | POST /auth/users/{userId}/credentials | - | . | Delete User Credentials | auth:DeleteCredentials | arn:lakefs:auth:::user/{userId} | DELETE /auth/users/{userId}/credentials/{accessKeyId} | - | . | Get User Credentials | auth:ReadCredentials | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId}/credentials/{accessKeyId} | - | . | List User Groups | auth:ReadUser | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId}/groups | - | . | List User Policies | auth:ReadUser | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId}/policies | - | . | Attach Policy To User | auth:AttachPolicy | arn:lakefs:auth:::user/{userId} | PUT /auth/users/{userId}/policies/{policyId} | - | . | Detach Policy From User | auth:DetachPolicy | arn:lakefs:auth:::user/{userId} | DELETE /auth/users/{userId}/policies/{policyId} | - | . | List Group Policies | auth:ReadGroup | arn:lakefs:auth:::group/{groupId} | GET /auth/groups/{groupId}/policies | - | . | Attach Policy To Group | auth:AttachPolicy | arn:lakefs:auth:::group/{groupId} | PUT /auth/groups/{groupId}/policies/{policyId} | - | . | Detach Policy From Group | auth:DetachPolicy | arn:lakefs:auth:::group/{groupId} | DELETE /auth/groups/{groupId}/policies/{policyId} | - | . | Read Storage Config | fs:ReadConfig | * | GET /config/storage | - | . | Get Garbage Collection Rules | retention:GetGarbageCollectionRules | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/gc/rules | - | . | Set Garbage Collection Rules | retention:SetGarbageCollectionRules | arn:lakefs:fs:::repository/{repositoryId} | POST /repositories/{repositoryId}/gc/rules | - | . | Prepare Garbage Collection Commits | retention:PrepareGarbageCollectionCommits | arn:lakefs:fs:::repository/{repositoryId} | POST /repositories/{repositoryId}/gc/prepare_commits | - | . | List Repository Action Runs | ci:ReadAction | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repository}/actions/runs | - | . | Get Action Run | ci:ReadAction | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repository}/actions/runs/{run_id} | - | . | List Action Run Hooks | ci:ReadAction | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repository}/actions/runs/{run_id}/hooks | - | . | Get Action Run Hook Output | ci:ReadAction | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repository}/actions/runs/{run_id}/hooks/{hook_run_id}/output | - | . Some APIs may require more than one action.For instance, in order to create a repository (POST /repositories), you need permission to fs:CreateRepository for the name of the repository and also fs:AttachStorageNamespace for the storage namespace used. ",
    "url": "/v1.4/reference/security/rbac.html#actions-and-permissions",
    
    "relUrl": "/reference/security/rbac.html#actions-and-permissions"
  },"415": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Preconfigured Policies",
    "content": "The following Policies are created during initial setup: . FSFullAccess . { \"statement\": [ { \"action\": [ \"fs:*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . FSReadAll . { \"statement\": [ { \"action\": [ \"fs:List*\", \"fs:Read*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . FSReadWriteAll . { \"statement\": [ { \"action\": [ \"fs:Read*\", \"fs:List*\", \"fs:WriteObject\", \"fs:DeleteObject\", \"fs:RevertBranch\", \"fs:CreateBranch\", \"fs:CreateTag\", \"fs:DeleteBranch\", \"fs:DeleteTag\", \"fs:CreateCommit\", \"fs:CreateMetaRange\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . AuthFullAccess . { \"statement\": [ { \"action\": [ \"auth:*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . AuthManageOwnCredentials . { \"statement\": [ { \"action\": [ \"auth:CreateCredentials\", \"auth:DeleteCredentials\", \"auth:ListCredentials\", \"auth:ReadCredentials\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:auth:::user/${user}\" } ] } . RepoManagementFullAccess . { \"statement\": [ { \"action\": [ \"ci:*\" ], \"effect\": \"allow\", \"resource\": \"*\" }, { \"action\": [ \"retention:*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . RepoManagementReadAll . { \"statement\": [ { \"action\": [ \"ci:Read*\" ], \"effect\": \"allow\", \"resource\": \"*\" }, { \"action\": [ \"retention:Get*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . ",
    "url": "/v1.4/reference/security/rbac.html#preconfigured-policies",
    
    "relUrl": "/reference/security/rbac.html#preconfigured-policies"
  },"416": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Additional Policies",
    "content": "You can create additional policies to further limit user access. Use the web UI or the lakectl auth command to create policies. Here is an example to define read/write access for a specific repository: . { \"statement\": [ { \"action\": [ \"fs:ReadRepository\", \"fs:ReadCommit\", \"fs:ListBranches\", \"fs:ListTags\", \"fs:ListObjects\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;\" }, { \"action\": [ \"fs:RevertBranch\", \"fs:ReadBranch\", \"fs:CreateBranch\", \"fs:DeleteBranch\", \"fs:CreateCommit\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;/branch/*\" }, { \"action\": [ \"fs:ListObjects\", \"fs:ReadObject\", \"fs:WriteObject\", \"fs:DeleteObject\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;/object/*\" }, { \"action\": [ \"fs:ReadTag\", \"fs:CreateTag\", \"fs:DeleteTag\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;/tag/*\" }, { \"action\": [\"fs:ReadConfig\"], \"effect\": \"allow\", \"resource\": \"*\" } ] } . ",
    "url": "/v1.4/reference/security/rbac.html#additional-policies",
    
    "relUrl": "/reference/security/rbac.html#additional-policies"
  },"417": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Preconfigured Groups",
    "content": "lakeFS has four preconfigured groups: . | Admins | SuperUsers | Developers | Viewers | . They have the following policies granted to them: . | Policy | Admins | SuperUsers | Developers | Viewers | . | FSFullAccess | ✅ | ✅ |   |   | . | AuthFullAccess | ✅ |   |   |   | . | RepoManagementFullAccess | ✅ |   |   |   | . | AuthManageOwnCredentials |   | ✅ | ✅ | ✅ | . | RepoManagementReadAll |   | ✅ | ✅ |   | . | FSReadWriteAll |   |   | ✅ |   | . | FSReadAll |   |   |   | ✅ | . ",
    "url": "/v1.4/reference/security/rbac.html#preconfigured-groups",
    
    "relUrl": "/reference/security/rbac.html#preconfigured-groups"
  },"418": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Pluggable Authentication and Authorization",
    "content": "Authorization and authentication is pluggable in lakeFS. If lakeFS is attached to a remote authentication server (or you are using lakeFS Cloud) then the role-based access control user interface can be used. If you are using RBAC with your self-managed lakeFS then the lakeFS configuration element auth.ui_config.rbac should be set to external. An enterprise (paid) solution of lakeFS should set auth.ui_config.rbac as internal. If you are using self-managed lakeFS and do not have a remote authentication server then you should set auth.ui_config.rbac to simplified and refer to the access control list documentation instead. ",
    "url": "/v1.4/reference/security/rbac.html#pluggable-authentication-and-authorization",
    
    "relUrl": "/reference/security/rbac.html#pluggable-authentication-and-authorization"
  },"419": {
    "doc": "Remote Authenticator",
    "title": "Remote Authenticator",
    "content": "Remote Authenticator is a pluggable architecture for lakeFS which allows you to use existing organizational identity policies and infrastructure with the authentication mechanism of lakeFS. The Remote Authenticator’s job is to abstract away the complexities of existing infrastructure and implement a standard interface, which lakeFS can use to resolve user identity and manage access to lakeFS. This loose coupling allows you to implement federated identity without providing lakeFS with direct access to your identity infrastructure. ",
    "url": "/v1.4/reference/security/remote-authenticator.html",
    
    "relUrl": "/reference/security/remote-authenticator.html"
  },"420": {
    "doc": "Remote Authenticator",
    "title": "Architecture",
    "content": "Here’s the authentication flow that lakeFS uses when configured with a remote authenticator: . sequenceDiagram participant A as lakeFS Client participant B as lakeFS Server participant C as Remote Authenticator participant D as IdP A-&gt;&gt;B: Submit login form B-&gt;&gt;C: POST user credentials C-&gt;&gt;D: IdP request D-&gt;&gt;C: IdP response C-&gt;&gt;B: Auth response B-&gt;&gt;A: auth JWT . ",
    "url": "/v1.4/reference/security/remote-authenticator.html#architecture",
    
    "relUrl": "/reference/security/remote-authenticator.html#architecture"
  },"421": {
    "doc": "Remote Authenticator",
    "title": "The Interface",
    "content": "To configure lakeFS to work with a Remote Authenticator add the following YAML to your lakeFS configuration: . auth: remote_authenticator: enabled: true endpoint: &lt;url-to-remote-authenticator-endpoint&gt; default_user_group: \"Developers\" ui_config: logout_url: /logout login_cookie_names: - internal_auth_session . | auth.remote_authenticator.enabled - set lakeFS to use the remote authenticator | auth.remote_authenticator.endpoint - an endpoint where the remote authenticator is able to receive a POST request from lakeFS | auth.remote_authenticator.default_user_group - the group assigned by default to new users | auth.ui_config.logout_url - the URL to redirect the browser when clicking the logout link in the user menu | auth.ui_config.login_cookie_names - the name of the cookie(s) lakeFS will set following a successful authentication. The value is the authenticated user’s JWT | . A Remote Authenticator implementation should expose a single endpoint, which expects the following JSON request: . { \"username\": \"testy.mctestface@example.com\", \"password\": \"Password1\" } . and returns a JSON response like this: . { \"external_user_identifier\": \"TestyMcTestface\" } . Example Request &amp; Responses . Request . POST https://remote-authenticator.example.com/auth Content-Type: application/json { \"username\": \"testy.mctestface@example.com\", \"password\": \"Password1\" } . Successful Response . HTTP/1.1 200 OK Content-Type: application/json { \"external_user_identifier\": \"TestyMcTestface\" } . Unauthorized Response . HTTP/1.1 401 Unauthorized Content-Type: application/json { \"external_user_identifier\": \"\" } . If the Remote Authenticator returns any HTTP status in the 2xx range, lakeFS considers this a successful authentication. Any HTTP status &lt; 200 or &gt; 300 is considered a failed authentication. If the Remote Authenticator returns a non-empty value for the external_user_identifier property along with a success HTTP status, lakeFS will show this identifier instead of an internal lakeFS user identifier in the UI. ",
    "url": "/v1.4/reference/security/remote-authenticator.html#the-interface",
    
    "relUrl": "/reference/security/remote-authenticator.html#the-interface"
  },"422": {
    "doc": "Remote Authenticator",
    "title": "Sample Implementation",
    "content": "Here is a sample Remote Authenticator implemented using node and express and written in TypeScript. This example implementation doesn’t integrate with any real IdP but illustrates the expected request/response patterns that you need to implement. import dotenv from \"dotenv\"; import express, { Express, Request, Response } from \"express\"; import { StatusCodes } from \"http-status-codes\"; type AuthRequestBody = { username: string; password: string; }; type AuthResponseBody = { external_user_identifier: string; }; const DEFAULT_PORT = 80; dotenv.config(); const port = process.env.PORT || DEFAULT_PORT; const app: Express = express(); app.post( \"/auth\", (req: Request&lt;AuthResponseBody, {}, AuthRequestBody&gt;, res: Response) =&gt; { const { username, password } = req.body; if (!username?.length || !password?.length) { return res.status(StatusCodes.BAD_REQUEST).json({ external_user_identifier: \"\", }); } // 👇🏻 This is where you would implement your own authentication logic if ( username === \"testy.mctestface@example.com\" &amp;&amp; password === \"Password1\" ) { return res.status(StatusCodes.OK).json({ external_user_identifier: \"TestyMcTestface\", }); } else { return res.status(StatusCodes.UNAUTHORIZED).json({ external_user_identifier: \"\", }); } } ); app.listen(port, () =&gt; { console.log(`Remote Authenticator listening on port ${port}`); }); . To run this service on the sub-domain idp.example.com, use a lakeFS configuration that looks like this: . auth: remote_authenticator: enabled: true endpoint: https://idp.example.com/auth default_user_group: \"Developers\" ui_config: logout_url: /logout login_cookie_names: - internal_auth_session . ",
    "url": "/v1.4/reference/security/remote-authenticator.html#sample-implementation",
    
    "relUrl": "/reference/security/remote-authenticator.html#sample-implementation"
  },"423": {
    "doc": "Reproducibility",
    "title": "The Benefits of Reproducible Data",
    "content": "Data changes frequently. This makes the task of keeping track of its exact state over time difficult. Oftentimes, people maintain only one state of their data––its current state. This has a negative impact on the work, as it becomes hard to: . | Debug a data issue. | Validate machine learning training accuracy (re-running a model over different data gives different results). | Comply with data audits. | . In comparison, lakeFS exposes a Git-like interface to data that allows keeping track of more than just the current state of data. This makes reproducing its state at any point in time straightforward. ",
    "url": "/v1.4/understand/use_cases/reproducibility.html#the-benefits-of-reproducible-data",
    
    "relUrl": "/understand/use_cases/reproducibility.html#the-benefits-of-reproducible-data"
  },"424": {
    "doc": "Reproducibility",
    "title": "Achieving Reproducibility with lakeFS",
    "content": "To make data reproducible, we recommend taking a new commit of your lakeFS repository every time the data in it changes. As long as there’s a commit taken, the process to reproduce a given state is as simple as reading the data from a path that includes the unique commit_id generated for each commit. To read data at it’s current state, we can use a static path containing the repository and branch names. To give an example, if you have a repository named example with a branch named main, reading the latest state of this data into a Spark Dataframe is always: . df = spark.read.parquet(‘s3://example/main/”) . Note: The code above assumes that all objects in the repository under this path are stored in parquet format. If a different format is used, the applicable Spark read method should be used. In a lakeFS repository, we are capable of taking many commits over the data, making many points in time reproducible. In the repository above, a new commit is taken each time a model training script is run, and the commit message includes the specific run number. If we wanted to re-run the model training script and reproduce the exact same results for a historical run, say run #435, we could copy the commit ID associated with the run and read the data into a dataframe like so: . df = spark.read.parquet(\"s3://example/296e54fbee5e176f3f4f4aeb7e087f9d57515750e8c3d033b8b841778613cb23/training_dataset/”) . The ability to reference a specific commit_id in code simplifies reproducing the specific state a data collection or even multiple collections. This has many applications that are common in data development, such as historical debugging, identifying deltas in a data collection, audit compliance, and more. ",
    "url": "/v1.4/understand/use_cases/reproducibility.html#achieving-reproducibility-with-lakefs",
    
    "relUrl": "/understand/use_cases/reproducibility.html#achieving-reproducibility-with-lakefs"
  },"425": {
    "doc": "Reproducibility",
    "title": "Reproducibility",
    "content": " ",
    "url": "/v1.4/understand/use_cases/reproducibility.html",
    
    "relUrl": "/understand/use_cases/reproducibility.html"
  },"426": {
    "doc": "5️⃣ Roll back Changes",
    "title": "Rolling back Changes in lakeFS",
    "content": "Our intrepid user (you) merged a change back into the main branch and realised that they had made a mistake 🤦🏻. The good news for them (you) is that lakeFS can revert changes made, similar to how you would in Git 😅. From your terminal window run lakectl with the revert command: . docker exec -it lakefs \\ lakectl branch revert \\ lakefs://quickstart/main \\ main --parent-number 1 --yes . You should see a confirmation of a successful rollback: . Branch: lakefs://quickstart/main commit main successfully reverted . Back in the object page and the DuckDB query we can see that the original file is now back to how it was: . ",
    "url": "/v1.4/quickstart/rollback.html#rolling-back-changes-in-lakefs",
    
    "relUrl": "/quickstart/rollback.html#rolling-back-changes-in-lakefs"
  },"427": {
    "doc": "5️⃣ Roll back Changes",
    "title": "5️⃣ Roll back Changes",
    "content": " ",
    "url": "/v1.4/quickstart/rollback.html",
    
    "relUrl": "/quickstart/rollback.html"
  },"428": {
    "doc": "Rollback",
    "title": "Rollbacks",
    "content": " ",
    "url": "/v1.4/understand/use_cases/rollback.html#rollbacks",
    
    "relUrl": "/understand/use_cases/rollback.html#rollbacks"
  },"429": {
    "doc": "Rollback",
    "title": "What Is a Rollback?",
    "content": "A rollback operation is used to to fix critical data errors immediately. What is a critical data error? Think of a situation where erroneous or misformatted data causes a significant issue with an important service or function. In such situations, the first thing to do is stop the bleeding. Rolling back returns data to a state in the past, before the error was present. You might not be showing all the latest data after a rollback, but at least you aren’t showing incorrect data or raising errors. ",
    "url": "/v1.4/understand/use_cases/rollback.html#what-is-a-rollback",
    
    "relUrl": "/understand/use_cases/rollback.html#what-is-a-rollback"
  },"430": {
    "doc": "Rollback",
    "title": "Why Rollbacks Are Useful",
    "content": "A Rollback is used as a stopgap measure to “put out the fire” as quickly as possible while RCA (root cause analysis) is performed to understand 1) exactly how the error happened, and 2) what can be done to prevent it from happening again. It can be a pressured, stressful situation to deal with a critical data error. Having the ability to employ a rollback relieves some of the pressure and makes it more likely you can figure out what happened without creating additional issues. As a real world example, the 14-day outage some Atlassian users experienced in May 2022 could have been an uninteresting minor incident had rolling back the deleted customer data been an option. ",
    "url": "/v1.4/understand/use_cases/rollback.html#why-rollbacks-are-useful",
    
    "relUrl": "/understand/use_cases/rollback.html#why-rollbacks-are-useful"
  },"431": {
    "doc": "Rollback",
    "title": "Performing Rollbacks with lakeFS",
    "content": "lakeFS lets you develop in your data lake in such a way that rollbacks are simple to perform. This starts by taking a commit over your lakeFS repository whenever a change to its state occurs. Using the lakeFS UI or CLI, you can set the current state, or HEAD, of a branch to any historical commit in seconds, effectively performing a rollback. To demonstrate how this works, let’s take the example of a lakeFS repo with the following commit history: . As can be inferred from the history, this repo is updated every minute with a data sync from some data source. An example data sync is a typical ETL job that replicates data from an internal database or any other data source. After each sync, a commit is taken in lakeFS to save a snapshot of data at that point in time. How to Rollback From a Bad Data Sync? . Say a situation occurs where one of the syncs had bad data and is causing downstream dashboards to fail to load. Since we took a commit of the repo right after the sync ran, we can use a revert operation to undo the changes introduced in that sync. Step 1: Copy the commit_id associated with the commit we want to revert. As the screenshot above shows, you can use the Copy ID to Clipboard button to do this. Step 2: Run the revert command using lakectl, the lakeFS CLI. In this example, the command will be as follows: . lakectl branch revert lakefs://example/main 9666d7c9daf37b3ba6964e733d08596ace2ec2c7bc3a4023ad8e80737a6c3e9d . This will undo the changes introduced by this commit, completing the rollback! . The rollback operation is that simple, even if many changes were introduced in a commit, spanning acrossmultiple data collections. In lakeFS, rolling back data is always a one-liner. ",
    "url": "/v1.4/understand/use_cases/rollback.html#performing-rollbacks-with-lakefs",
    
    "relUrl": "/understand/use_cases/rollback.html#performing-rollbacks-with-lakefs"
  },"432": {
    "doc": "Rollback",
    "title": "Rollback",
    "content": " ",
    "url": "/v1.4/understand/use_cases/rollback.html",
    
    "relUrl": "/understand/use_cases/rollback.html"
  },"433": {
    "doc": "S3 Gateway API",
    "title": "S3-Supported API",
    "content": "The S3 Gateway emulates a subset of the API exposed by S3. This subset includes all API endpoints relevant to data systems. For more information, see architecture. lakeFS supports the following API operations: . | Identity and authorization . | SIGv2 | SIGv4 | . | Bucket operations: . | HEAD bucket | . | Object operations: . | DeleteObject | DeleteObjects | GetObject . | Support for caching headers, ETag | Support for range requests | No support for SSE | No support for SelectObject operations | . | HeadObject | PutObject . | Support multi-part uploads | No support for storage classes | No object level tagging | . | CopyObject | . | Object Listing: . | ListObjects | ListObjectsV2 | Delimiter support (for \"/\" only) | . | Multipart Uploads: . | AbortMultipartUpload | CompleteMultipartUpload | CreateMultipartUpload | ListParts | Upload Part | UploadPartCopy | . | . ",
    "url": "/v1.4/reference/s3.html#s3-supported-api",
    
    "relUrl": "/reference/s3.html#s3-supported-api"
  },"434": {
    "doc": "S3 Gateway API",
    "title": "S3 Gateway API",
    "content": " ",
    "url": "/v1.4/reference/s3.html",
    
    "relUrl": "/reference/s3.html"
  },"435": {
    "doc": "Amazon SageMaker",
    "title": "Using lakeFS with Amazon SageMaker",
    "content": "Amazon SageMaker helps to prepare, build, train and deploy ML models quickly by bringing together a broad set of capabilities purpose-built for ML. ",
    "url": "/v1.4/integrations/sagemaker.html#using-lakefs-with-amazon-sagemaker",
    
    "relUrl": "/integrations/sagemaker.html#using-lakefs-with-amazon-sagemaker"
  },"436": {
    "doc": "Amazon SageMaker",
    "title": "Table of contents",
    "content": ". | Initializing session and client | Usage Examples | . ",
    "url": "/v1.4/integrations/sagemaker.html#table-of-contents",
    
    "relUrl": "/integrations/sagemaker.html#table-of-contents"
  },"437": {
    "doc": "Amazon SageMaker",
    "title": "Initializing session and client",
    "content": "Initialize a Sagemaker session and an S3 client with lakeFS as the endpoint: . import sagemaker import boto3 endpoint_url = '&lt;LAKEFS_ENDPOINT&gt;' aws_access_key_id = '&lt;LAKEFS_ACCESS_KEY_ID&gt;' aws_secret_access_key = '&lt;LAKEFS_SECRET_ACCESS_KEY&gt;' repo = 'example-repo' sm = boto3.client('sagemaker', endpoint_url=endpoint_url, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key) s3_resource = boto3.resource('s3', endpoint_url=endpoint_url, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key) session = sagemaker.Session(boto3.Session(), sagemaker_client=sm, default_bucket=repo) session.s3_resource = s3_resource . ",
    "url": "/v1.4/integrations/sagemaker.html#initializing-session-and-client",
    
    "relUrl": "/integrations/sagemaker.html#initializing-session-and-client"
  },"438": {
    "doc": "Amazon SageMaker",
    "title": "Usage Examples",
    "content": "Upload train and test data . Let’s use the created session for uploading data to the ‘main’ branch: . prefix = \"/prefix-within-branch\" branch = 'main' train_file = 'train_data.csv'; train_data.to_csv(train_file, index=False, header=True) train_data_s3_path = session.upload_data(path=train_file, key_prefix=branch + prefix + \"/train\") test_file = 'test_data.csv'; test_data_no_target.to_csv(test_file, index=False, header=False) test_data_s3_path = session.upload_data(path=test_file, key_prefix=branch + prefix + \"/test\") . Download objects . You can use the integration with lakeFS to download a portion of the data you see fit: . repo = 'example-repo' prefix = \"/prefix-to-download\" branch = 'main' localpath = './' + branch session.download_data(path=localpath, bucket=repo, key_prefix = branch + prefix) . Note: Advanced AWS SageMaker features, like Autopilot jobs, are encapsulated and don’t have the option to override the S3 endpoint. However, it is possible to export the required inputs from lakeFS to S3. If you’re using SageMaker features that aren’t supported by lakeFS, we’d love to hear from you. ",
    "url": "/v1.4/integrations/sagemaker.html#usage-examples",
    
    "relUrl": "/integrations/sagemaker.html#usage-examples"
  },"439": {
    "doc": "Amazon SageMaker",
    "title": "Amazon SageMaker",
    "content": " ",
    "url": "/v1.4/integrations/sagemaker.html",
    
    "relUrl": "/integrations/sagemaker.html"
  },"440": {
    "doc": "Sizing Guide",
    "title": "Sizing guide",
    "content": "Note: For a scalable managed lakeFS service with guaranteed SLAs, try lakeFS Cloud . ",
    "url": "/v1.4/howto/sizing-guide.html#sizing-guide",
    
    "relUrl": "/howto/sizing-guide.html#sizing-guide"
  },"441": {
    "doc": "Sizing Guide",
    "title": "Table of contents",
    "content": ". | System Requirements | Scaling factors | Benchmarks | Important metrics | Reference architectures | . ",
    "url": "/v1.4/howto/sizing-guide.html#table-of-contents",
    
    "relUrl": "/howto/sizing-guide.html#table-of-contents"
  },"442": {
    "doc": "Sizing Guide",
    "title": "System Requirements",
    "content": "Operating Systems and ISA . lakeFS can run on MacOS and Linux. Windows binaries are available but not rigorously tested - we don’t recommend deploying lakeFS to production on Windows. x86_64 and arm64 architectures are supported for both MacOS and Linux. Memory and CPU requirements . lakeFS servers require a minimum of 512mb of RAM and 1 CPU core. For high throughput, additional CPUs help scale requests across different cores. “Expensive” operations such as large diff or commit operations can take advantage of multiple cores. Network . If using the data APIs such as the S3 Gateway, lakeFS will require enough network bandwidth to support the planned concurrent network upload/download operations. For most cloud providers, more powerful machines (i.e., more expensive and usually containing more CPU cores) also provide increased network bandwidth. If using only the metadata APIs (for example, only using the Hadoop/Spark clients), network bandwidth is minimal, at roughly 1Kb per request. Disk . lakeFS greatly benefits from fast local disks. A lakeFS instance doesn’t require any strong durability guarantees from the underlying storage, as the disk is only ever used as a local caching layer for lakeFS metadata and not for long-term storage. lakeFS is designed to work with ephemeral disks - these are usually based on NVMe and are tied to the machine’s lifecycle. Using ephemeral disks lakeFS can provide a very high throughput/cost ratio, probably the best that could be achieved on a public cloud, so we recommend those. A local cache of at least 512 MiB should be provided. For large installations (managing &gt;100 concurrently active branches, with &gt;100M objects per commit), we recommend allocating at least 10 GiB - since it’s a caching layer over a relatively slow storage (the object store), see Important metrics below to understand how to size this: it should be big enough to hold all commit metadata for actively referenced commits. lakeFS KV Store . lakeFS uses a key-value database to manage branch references, authentication and authorization information and to keep track of currently uncommitted data across branches. Please refer to the relevant driver tab for best practices, requirements and benchmarks. Storage . The dataset stored in the metadata store is relatively modest as most metadata is pushed down into the object store. Required storage is mostly a factor of the amount of uncommitted writes across all branches at any given point in time: in the range of 150 MiB per every 100,000 uncommitted writes. We recommend starting at 10 GiB for a production deployment, as it will likely be more than enough. | PostgreSQL | DynamoDB | . RAM Since the data size is small, it’s recommended to provide enough memory to hold the vast majority of that data in RAM. Cloud providers will save you the need to tune this parameter - it will be set to a fixed percentage the chosen instance’s available RAM (25% on AWS RDS, 30% on Google Cloud SQL). It is recommended that you check with your selected cloud provider for configuration and provisioning information for you database. For self-managed database instances follow these best practices . Ideally, configure the shared_buffers of your PostgreSQL instances to be large enough to contain the currently active dataset. Pick a database instance with enough RAM to accommodate this buffer size at roughly x4 the size given for shared_buffers. For example, if an installation has ~500,000 uncommitted writes at any given time, it would require about 750 MiB of shared_buffers that would require about 3 GiB of RAM. CPU PostgreSQL CPU cores help scale concurrent requests. 1 CPU core for every 5,000 requests/second is ideal. lakeFS will create a DynamoDB table for you, defaults to on-demand capacity setting. No need to specify how much read and write throughput you expect your application to perform, as DynamoDB instantly accommodates your workloads as they ramp up or down. You can customize the table settings to provisioned capacity which allows you to manage and optimize your costs by allocating read/write capacity in advance (see Benchmarks) . Notes: . | Using DynamoDB on-demand capacity might generate unwanted costs if the table is abused, if you’d like to cap your costs, make sure to change the table to use provisioned capacity instead. | lakeFS doesn’t manage the DynamoDB’s table lifecycle, we’ve included the table creation in order to help evaluating the system with minimal effort, any change to the table beyond the table creation - will need to be handled manually or by 3rd party tools. | . RAM Managed by AWS. CPU Managed by AWS. ",
    "url": "/v1.4/howto/sizing-guide.html#system-requirements",
    
    "relUrl": "/howto/sizing-guide.html#system-requirements"
  },"443": {
    "doc": "Sizing Guide",
    "title": "Scaling factors",
    "content": "Scaling lakeFS, like most data systems, moves across two axes: throughput of requests (amount per given timeframe) and latency (time to complete a single request). Understanding latency and throughput considerations . Most lakeFS operations are designed to be very low in latency. Assuming a well-tuned local disk cache (see Storage above), most critical path operations (writing objects, requesting objects, deleting objects) are designed to complete in &lt;25ms at p90. Listing objects obviously requires accessing more data, but should always be on-par with what the underlying object store can provide, and in most cases, it’s actually faster. At the worst case, for directory listing with 1,000 common prefixes returned, expect a latency of 75ms at p90. Managing branches (creating them, listing them and deleting them) are all constant-time operations, generally taking &lt;30ms at p90. Committing and merging can take longer, as they are proportional to the amount of changes introduced. This is what makes lakeFS optimal for large Data Lakes - the amount of changes introduced per commit usually stays relatively stable while the entire data set usually grows over time. This means lakeFS will provide predictable performance: committing 100 changes will take roughly the same amount of time whether the resulting commit contains 500 or 500 million objects. See Data Model for more information. Scaling throughput depends very much on the amount of CPU cores available to lakeFS. In many cases, it’s easier to scale lakeFS across a fleet of smaller cloud instances (or containers) than scale up with machines that have many cores. In fact, lakeFS works well in both cases. Most critical path operations scale very well across machines. ",
    "url": "/v1.4/howto/sizing-guide.html#scaling-factors",
    
    "relUrl": "/howto/sizing-guide.html#scaling-factors"
  },"444": {
    "doc": "Sizing Guide",
    "title": "Benchmarks",
    "content": ". | PostgreSQL | DynamoDB | . PostgresSQL . All benchmarks below were measured using 2 x c5ad.4xlarge instances on AWS us-east-1. Similar results can be achieved on Google Cloud using a c2-standard-16 machine type, with an attached local SSD. On Azure, you can use a Standard_F16s_v2 virtual machine. The PostgreSQL instance that was used is a db.m6g.2xlarge (8 vCPUs, 32 GB RAM). Equivalent machines on Google Cloud or Azure should yield similar results. The example repository we tested against contains the metadata of a large lakeFS installation, where each commit contains ~180,000,000 objects (representing ~7.5 Petabytes of data). All tests are reproducible using the lakectl abuse command, so use it to properly size and tune your setup. All tests are accompanied by the relevant lakectl abuse command that generated them. Random reads . This test generates random read requests to lakeFS, in a given commit. Paths are requested randomly from a file containing a set of preconfigured (and existing) paths. command executed: . lakectl abuse random-read \\ --from-file randomly_selected_paths.txt \\ --amount 500000 \\ --parallelism 128 \\ lakefs://example-repo/&lt;commit hash&gt; . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and commit hash. Result Histogram (raw): . Histogram (ms): 1 0 2 0 5 37945 7 179727 10 296964 15 399682 25 477502 50 499625 75 499998 100 499998 250 500000 350 500000 500 500000 750 500000 1000 500000 5000 500000 min 3 max 222 total 500000 . So 50% of all requests took &lt;10ms, while 99.9% of them took &lt;50ms . throughput: . Average throughput during the experiment was 10851.69 requests/second . Random Writes . This test generates random write requests to a given lakeFS branch. All the paths are pre-generated and don’t overwrite each other (as overwrites are relatively rare in a Data Lake setup). command executed: . lakectl abuse random-write \\ --amount 500000 \\ --parallelism 64 \\ lakefs://example-repo/main . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. Result Histogram (raw): . Histogram (ms): 1 0 2 0 5 30715 7 219647 10 455807 15 498144 25 499535 50 499742 75 499784 100 499802 250 500000 350 500000 500 500000 750 500000 1000 500000 5000 500000 min 3 max 233 total 500000 . So, 50% of all requests took &lt;10ms, while 99.9% of them took &lt;25ms. throughput: . The average throughput during the experiment was 7595.46 requests/second. Branch creation . This test creates branches from a given reference. command executed: . lakectl abuse create-branches \\ --amount 500000 \\ --branch-prefix \"benchmark-\" \\ --parallelism 256 \\ lakefs://example-repo/&lt;commit hash&gt; . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and commit hash. Result Histogram (raw): . Histogram (ms): 1 0 2 1 5 5901 7 39835 10 135863 15 270201 25 399895 50 484932 75 497180 100 499303 250 499996 350 500000 500 500000 750 500000 1000 500000 5000 500000 min 2 max 304 total 500000 . So, 50% of all requests took &lt;15ms, while 99.9% of them took &lt;100ms. throughput: . The average throughput during the experiment was 7069.03 requests/second. DynamoDB . All benchmarks below were measured using m5.xlarge instance on AWS us-east-1. The DynamoDB table that was used was provisioned with 500/1000 read/write capacity. The example repository we tested against contains the metadata of a large lakeFS installation, where each commit contains ~100,000,000 objects (representing ~3.5 Petabytes of data). All tests are reproducible using the lakectl abuse command, so use it to properly size and tune your setup. All tests are accompanied by the relevant lakectl abuse command that generated them. Random reads . This test generates random read requests to lakeFS, in a given commit. Paths are requested randomly from a file containing a set of preconfigured (and existing) paths. command executed: . lakectl abuse random-read \\ --from-file randomly_selected_paths.txt \\ --amount 500000 \\ --parallelism 128 \\ lakefs://example-repo/&lt;commit hash&gt; . Result Histogram (raw): Provisioned read capacity units = 1000 Provisioned write capacity units = 1000 . Histogram (ms): 1 0 2 0 5 0 7 0 10 0 15 0 25 122 50 47364 75 344489 100 460404 250 497912 350 498016 500 498045 750 498111 1000 498176 5000 499478 min 18 max 52272 total 500000 . Result Histogram (raw): Provisioned read capacity units = 500 Provisioned write capacity units = 500 . Histogram (ms): 1 0 2 0 5 0 7 0 10 0 15 1 25 2672 50 239661 75 420171 100 470146 250 486603 350 486715 500 486789 750 487443 1000 488113 5000 493201 min 14 max 648085 total 499998 . Random Writes . This test generates random write requests to a given lakeFS branch. All the paths are pre-generated and don’t overwrite each other (as overwrites are relatively rare in a Data Lake setup). command executed: . lakectl abuse random-write \\ --amount 500000 \\ --parallelism 64 \\ lakefs://example-repo/main . Result Histogram (raw): Provisioned read capacity units = 1000 Provisioned write capacity units = 1000 . Histogram (ms): 1 0 2 0 5 0 7 0 10 0 15 0 25 24 50 239852 75 458504 100 485225 250 493687 350 493872 500 493960 750 496239 1000 499194 5000 500000 min 23 max 4437 total 500000 . Result Histogram (raw): Provisioned read capacity units = 500 Provisioned write capacity units = 500 . Histogram (ms): 1 0 2 0 5 0 7 0 10 0 15 0 25 174 50 266460 75 462641 100 484486 250 490633 350 490856 500 490984 750 492973 1000 495605 5000 498920 min 21 max 50157 total 500000 . Branch creation . This test creates branches from a given reference. command executed: . lakectl abuse create-branches \\ --amount 500000 \\ --branch-prefix \"benchmark-\" \\ --parallelism 256 \\ lakefs://example-repo/&lt;commit hash&gt; . Result Histogram (raw): Provisioned read capacity units = 1000 Provisioned write capacity units = 1000 . Histogram (ms): 1 0 2 0 5 0 7 0 10 0 15 0 25 0 50 628 75 26153 100 58099 250 216160 350 307078 500 406165 750 422898 1000 431332 5000 475848 min 41 max 430725 total 490054 . Result Histogram (raw): Provisioned read capacity units = 500 Provisioned write capacity units = 500 . Histogram (ms): 1 0 2 0 5 0 7 0 10 0 15 0 25 0 50 3132 75 155570 100 292745 250 384224 350 397258 500 431141 750 441360 1000 445597 5000 469538 min 39 max 760626 total 497520 . ",
    "url": "/v1.4/howto/sizing-guide.html#benchmarks",
    
    "relUrl": "/howto/sizing-guide.html#benchmarks"
  },"445": {
    "doc": "Sizing Guide",
    "title": "Important metrics",
    "content": "lakeFS exposes metrics using the Prometheus protocol. Every lakeFS instance exposes a /metrics endpoint that could be used to extract them. Here are a few notable metrics to keep track of when sizing lakeFS: . api_requests_total - Tracks throughput of API requests over time. api_request_duration_seconds - Histogram of latency per operation type. gateway_request_duration_seconds - Histogram of latency per S3 Gateway operation. | PostgreSQL | DynamoDB | . dynamo_request_duration_seconds - Time spent doing DynamoDB requests. dynamo_consumed_capacity_total - The capacity units consumed by operation. dynamo_failures_total - The total number of errors while working for kv store. ",
    "url": "/v1.4/howto/sizing-guide.html#important-metrics",
    
    "relUrl": "/howto/sizing-guide.html#important-metrics"
  },"446": {
    "doc": "Sizing Guide",
    "title": "Reference architectures",
    "content": "Below are a few example architectures for lakeFS deployment. Reference Architecture: Data Science/Research environment . Use case: Manage Machine learning or algorithms development. Use lakeFS branches to achieve both isolation and reproducibility of experiments. Data being managed by lakeFS is both structured tabular data, as well as unstructured sensor and image data used for training. Assuming a team of 20-50 researchers, with a dataset size of 500 TiB across 20M objects. Environment: lakeFS will be deployed on Kubernetes. managed by AWS EKS with PostgreSQL on AWS RDS Aurora . Sizing: Since most of the work is done by humans (vs. automated pipelines), most experiments tend to be small in scale, reading and writing 10s to 1000s of objects. The expected amount of branches active in parallel is relatively low, around 1-2 per user, each representing a small amount of uncommitted changes at any given point in time. Let’s assume 5,000 uncommitted writes per branch = ~500k. To support the expected throughput, a single moderate lakeFS instance should be more than enough, since requests per second would be on the order of 10s to 100s. For high availability, we’ll deploy 2 pods with 1 CPU core and 1 GiB of RAM each. Since the PostgreSQL instance is expected to hold a very small dataset (at 500k, expected dataset size is 150MiB (for 100k records) * 5 = 750MiB). To ensure we have enough RAM to hold this, we’ll need 3 GiB of RAM, so, a very moderate Aurora instance db.t3.large (2 vCPUs, 8 GB RAM) will be more than enough. An equivalent database instance on GCP or Azure should give similar results. Reference Architecture: Automated Production Pipelines . Use case: Manage multiple concurrent data pipelines using Apache Spark and Airflow. Airflow DAGs start by creating a branch for isolation and for CI/CD. Data being managed by lakeFS is structured, tabular data. The total dataset size is 10 PiB, spanning across 500M objects. The expected throughput is 10k reads/second + 2k writes per second across 100 concurrent branches. Environment: lakeFS will be deployed on Kubernetes. managed by AWS EKS with PostgreSQL on AWS RDS . Sizing: Data pipelines tend to be bursty in nature: reading in a lot of objects concurrently, doing some calculation or aggregation, and then writing many objects concurrently. The expected amount of branches active in parallel is high, with many Airflow DAGs running per day, each representing a moderate amount of uncommitted changes at any given point in time. Let’s assume 1,000 uncommitted writes/branch * 2500 branches = ~2.5M records. To support the expected throughput, looking the benchmarking numbers above, we’re doing roughly 625 requests/core, so 24 cores should cover our peak traffic. We can deploy 6 * 4 CPU core pods. On to the PostgreSQL instance - at 500k, the expected dataset size is 150MiB (for 100k records) * 25 = 3750 MiB. To ensure we have enough RAM to hold this, we’ll need at least 15 GiB of RAM, so we’ll go with a db.r5.xlarge (4 vCPUs, 32GB RAM) Aurora instance. An equivalent database instance on GCP or Azure should give similar results. ",
    "url": "/v1.4/howto/sizing-guide.html#reference-architectures",
    
    "relUrl": "/howto/sizing-guide.html#reference-architectures"
  },"447": {
    "doc": "Sizing Guide",
    "title": "Sizing Guide",
    "content": " ",
    "url": "/v1.4/howto/sizing-guide.html",
    
    "relUrl": "/howto/sizing-guide.html"
  },"448": {
    "doc": "Spark Client",
    "title": "lakeFS Spark Metadata Client",
    "content": "Utilize the power of Spark to interact with the metadata on lakeFS. Possible use cases include: . | Creating a DataFrame for listing the objects in a specific commit or branch. | Computing changes between two commits. | Exporting your data for consumption outside lakeFS. | Bulk operations on the underlying storage. | . ",
    "url": "/v1.4/reference/spark-client.html#lakefs-spark-metadata-client",
    
    "relUrl": "/reference/spark-client.html#lakefs-spark-metadata-client"
  },"449": {
    "doc": "Spark Client",
    "title": "Getting Started",
    "content": "Please note that Spark 2 is no longer supported with the lakeFS metadata client. Start Spark Shell / PySpark with the --packages flag: . This client is compiled for Spark 3.1.2 with Hadoop 3.2.1, but can work for other Spark versions and higher Hadoop versions. spark-shell --packages io.lakefs:lakefs-spark-client_2.12:0.11.0 . Alternatively an assembled jar is available on S3, at s3://treeverse-clients-us-east/lakefs-spark-client/0.11.0/lakefs-spark-client-assembly-0.11.0.jar &lt;/div&gt; . ",
    "url": "/v1.4/reference/spark-client.html#getting-started",
    
    "relUrl": "/reference/spark-client.html#getting-started"
  },"450": {
    "doc": "Spark Client",
    "title": "Configuration",
    "content": ". | To read metadata from lakeFS, the client should be configured with your lakeFS endpoint and credentials, using the following Hadoop configurations: . | Configuration | Description | . | spark.hadoop.lakefs.api.url | lakeFS API endpoint, e.g: http://lakefs.example.com/api/v1 | . | spark.hadoop.lakefs.api.access_key | The access key to use for fetching metadata from lakeFS | . | spark.hadoop.lakefs.api.secret_key | Corresponding lakeFS secret key | . | The client will also directly interact with your storage using Hadoop FileSystem. Therefore, your Spark session must be able to access the underlying storage of your lakeFS repository. There are various ways to do this, but for a non-production environment you can use the following Hadoop configurations: . | Configuration | Description | . | spark.hadoop.fs.s3a.access.key | Access key to use for accessing underlying storage on S3 | . | spark.hadoop.fs.s3a.secret.key | Corresponding secret key to use with S3 access key | . Assuming role on S3 (Hadoop 3 only) . The client includes support for assuming a separate role on S3 when running on Hadoop 3. It uses the same configuration used by S3AFileSystem to assume the role on S3A. Apache Hadoop AWS documentation has details under “Working with IAM Assumed Roles”. You will need to use the following Hadoop configurations: . | Configuration | Description | . | fs.s3a.aws.credentials.provider | Set to org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider | . | fs.s3a.assumed.role.arn | Set to the ARN of the role to assume | . | . ",
    "url": "/v1.4/reference/spark-client.html#configuration",
    
    "relUrl": "/reference/spark-client.html#configuration"
  },"451": {
    "doc": "Spark Client",
    "title": "Examples",
    "content": ". | Get a DataFrame for listing all objects in a commit: . import io.treeverse.clients.LakeFSContext val commitID = \"a1b2c3d4\" val df = LakeFSContext.newDF(spark, \"example-repo\", commitID) df.show /* output example: +------------+--------------------+--------------------+-------------------+----+ | key | address| etag| last_modified|size| +------------+--------------------+--------------------+-------------------+----+ | file_1 |791457df80a0465a8...|7b90878a7c9be5a27...|2021-03-05 11:23:30| 36| file_2 |e15be8f6e2a74c329...|95bee987e9504e2c3...|2021-03-05 11:45:25| 36| file_3 |f6089c25029240578...|32e2f296cb3867d57...|2021-03-07 13:43:19| 36| file_4 |bef38ef97883445c8...|e920efe2bc220ffbb...|2021-03-07 13:43:11| 13| +------------+--------------------+--------------------+-------------------+----+ */ . | Run SQL queries on your metadata: . df.createOrReplaceTempView(\"files\") spark.sql(\"SELECT DATE(last_modified), COUNT(*) FROM files GROUP BY 1 ORDER BY 1\") /* output example: +----------+--------+ | dt|count(1)| +----------+--------+ |2021-03-05| 2|2021-03-07| 2| +----------+--------+ */ . | . ",
    "url": "/v1.4/reference/spark-client.html#examples",
    
    "relUrl": "/reference/spark-client.html#examples"
  },"452": {
    "doc": "Spark Client",
    "title": "Spark Client",
    "content": " ",
    "url": "/v1.4/reference/spark-client.html",
    
    "relUrl": "/reference/spark-client.html"
  },"453": {
    "doc": "Apache Spark",
    "title": "Using lakeFS with Apache Spark",
    "content": "There are several ways to use lakeFS with Spark: . | The S3-compatible API: Scalable and best to get started. All Storage Vendors | The lakeFS FileSystem: Direct data flow from client to storage, highly scalable. AWS S3 . | lakeFS FileSystem in Presigned mode: Best of both worlds. AWS S3Azure Blob | . | . See how SimilarWeb is using lakeFS with Spark to manage algorithm changes in data pipelines. ",
    "url": "/v1.4/integrations/spark.html#using-lakefs-with-apache-spark",
    
    "relUrl": "/integrations/spark.html#using-lakefs-with-apache-spark"
  },"454": {
    "doc": "Apache Spark",
    "title": "Table of contents",
    "content": ". | S3-compatible API | lakeFS Hadoop FileSystem | Hadoop FileSystem in Presigned mode | . ",
    "url": "/v1.4/integrations/spark.html#table-of-contents",
    
    "relUrl": "/integrations/spark.html#table-of-contents"
  },"455": {
    "doc": "Apache Spark",
    "title": "S3-compatible API",
    "content": "lakeFS has an S3-compatible endpoint which you can point Spark at to get started quickly. You will access your data using S3-style URIs, e.g. s3a://example-repo/example-branch/example-table. You can use the S3-compatible API regardless of where your data is hosted. Configuration . To configure Spark to work with lakeFS, we set S3A Hadoop configuration to the lakeFS endpoint and credentials: . | fs.s3a.access.key: lakeFS access key | fs.s3a.secret.key: lakeFS secret key | fs.s3a.endpoint: lakeFS S3-compatible API endpoint (e.g. https://example-org.us-east-1.lakefscloud.io) | fs.s3a.path.style.access: true | . Here is how to do it: . | CLI | Scala | XML Configuration | EMR | . spark-shell --conf spark.hadoop.fs.s3a.access.key='AKIAlakefs12345EXAMPLE' \\ --conf spark.hadoop.fs.s3a.secret.key='abc/lakefs/1234567bPxRfiCYEXAMPLEKEY' \\ --conf spark.hadoop.fs.s3a.path.style.access=true \\ --conf spark.hadoop.fs.s3a.endpoint='https://example-org.us-east-1.lakefscloud.io' ... spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", \"AKIAlakefs12345EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.endpoint\", \"https://example-org.us-east-1.lakefscloud.io\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.path.style.access\", \"true\") . Add these into a configuration file, e.g. $SPARK_HOME/conf/hdfs-site.xml: . &lt;?xml version=\"1.0\"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.s3a.access.key&lt;/name&gt; &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.secret.key&lt;/name&gt; &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;https://example-org.us-east-1.lakefscloud.io&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . Use the below configuration when creating the cluster. You may delete any app configuration that is not suitable for your use case: . [ { \"Classification\": \"spark-defaults\", \"Properties\": { \"spark.sql.catalogImplementation\": \"hive\" } }, { \"Classification\": \"core-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3.path.style.access\": \"true\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3a.path.style.access\": \"true\" } }, { \"Classification\": \"emrfs-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3.path.style.access\": \"true\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3a.path.style.access\": \"true\" } }, { \"Classification\": \"presto-connector-hive\", \"Properties\": { \"hive.s3.aws-access-key\": \"AKIAIOSFODNN7EXAMPLE\", \"hive.s3.aws-secret-key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"hive.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"hive.s3.path-style-access\": \"true\", \"hive.s3-file-system-type\": \"PRESTO\" } }, { \"Classification\": \"hive-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3.path.style.access\": \"true\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3a.path.style.access\": \"true\" } }, { \"Classification\": \"hdfs-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3.path.style.access\": \"true\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3a.path.style.access\": \"true\" } }, { \"Classification\": \"mapred-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3.path.style.access\": \"true\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3a.path.style.access\": \"true\" } } ] . Alternatively, you can pass these configuration values when adding a step. For example: . aws emr add-steps --cluster-id j-197B3AEGQ9XE4 \\ --steps=\"Type=Spark,Name=SparkApplication,ActionOnFailure=CONTINUE, \\ Args=[--conf,spark.hadoop.fs.s3a.access.key=AKIAIOSFODNN7EXAMPLE, \\ --conf,spark.hadoop.fs.s3a.secret.key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY, \\ --conf,spark.hadoop.fs.s3a.endpoint=https://example-org.us-east-1.lakefscloud.io, \\ --conf,spark.hadoop.fs.s3a.path.style.access=true, \\ s3a://&lt;lakefs-repo&gt;/&lt;lakefs-branch&gt;/path/to/jar]\" . Per-bucket configuration . The above configuration will use lakeFS as the sole S3 endpoint. To use lakeFS in parallel with S3, you can configure Spark to use lakeFS only for specific bucket names. For example, to configure only example-repo to use lakeFS, set the following configurations: . | CLI | Scala | XML Configuration | EMR | . spark-shell --conf spark.hadoop.fs.s3a.bucket.example-repo.access.key='AKIAlakefs12345EXAMPLE' \\ --conf spark.hadoop.fs.s3a.bucket.example-repo.secret.key='abc/lakefs/1234567bPxRfiCYEXAMPLEKEY' \\ --conf spark.hadoop.fs.s3a.bucket.example-repo.endpoint='https://example-org.us-east-1.lakefscloud.io' \\ --conf spark.hadoop.fs.s3a.path.style.access=true . spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.access.key\", \"AKIAlakefs12345EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.endpoint\", \"https://example-org.us-east-1.lakefscloud.io\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.path.style.access\", \"true\") . Add these into a configuration file, e.g. $SPARK_HOME/conf/hdfs-site.xml: . &lt;?xml version=\"1.0\"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.s3a.bucket.example-repo.access.key&lt;/name&gt; &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.bucket.example-repo.secret.key&lt;/name&gt; &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.bucket.example-repo.endpoint&lt;/name&gt; &lt;value&gt;https://example-org.us-east-1.lakefscloud.io&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . Use the below configuration when creating the cluster. You may delete any app configuration that is not suitable for your use case: . [ { \"Classification\": \"spark-defaults\", \"Properties\": { \"spark.sql.catalogImplementation\": \"hive\" } }, { \"Classification\": \"core-site\", \"Properties\": { \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3.bucket.example-repo.path.style.access\": \"true\", \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3a.bucket.example-repo.path.style.access\": \"true\" } }, { \"Classification\": \"emrfs-site\", \"Properties\": { \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3.bucket.example-repo.path.style.access\": \"true\", \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3a.bucket.example-repo.path.style.access\": \"true\" } }, { \"Classification\": \"presto-connector-hive\", \"Properties\": { \"hive.s3.aws-access-key\": \"AKIAIOSFODNN7EXAMPLE\", \"hive.s3.aws-secret-key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"hive.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"hive.s3.path-style-access\": \"true\", \"hive.s3-file-system-type\": \"PRESTO\" } }, { \"Classification\": \"hive-site\", \"Properties\": { \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3.bucket.example-repo.path.style.access\": \"true\", \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3a.bucket.example-repo.path.style.access\": \"true\" } }, { \"Classification\": \"hdfs-site\", \"Properties\": { \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3.bucket.example-repo.path.style.access\": \"true\", \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3a.bucket.example-repo.path.style.access\": \"true\" } }, { \"Classification\": \"mapred-site\", \"Properties\": { \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3.bucket.example-repo.path.style.access\": \"true\", \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3a.bucket.example-repo.path.style.access\": \"true\" } } ] . Alternatively, you can pass these configuration values when adding a step. For example: . aws emr add-steps --cluster-id j-197B3AEGQ9XE4 \\ --steps=\"Type=Spark,Name=SparkApplication,ActionOnFailure=CONTINUE, \\ Args=[--conf,spark.hadoop.fs.s3a.bucket.example-repo.access.key=AKIAIOSFODNN7EXAMPLE, \\ --conf,spark.hadoop.fs.s3a.bucket.example-repo.secret.key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY, \\ --conf,spark.hadoop.fs.s3a.bucket.example-repo.endpoint=https://example-org.us-east-1.lakefscloud.io, \\ --conf,spark.hadoop.fs.s3a.path.style.access=true, \\ s3a://&lt;lakefs-repo&gt;/&lt;lakefs-branch&gt;/path/to/jar]\" . With this configuration set, you read S3A paths with example-repo as the bucket will use lakeFS, while all other buckets will use AWS S3. Usage . Here’s an example for reading a Parquet file from lakeFS to a Spark DataFrame: . val repo = \"example-repo\" val branch = \"main\" val df = spark.read.parquet(s\"s3a://${repo}/${branch}/example-path/example-file.parquet\") . Here’s how to write some results back to a lakeFS path: . df.write.partitionBy(\"example-column\").parquet(s\"s3a://${repo}/${branch}/output-path/\") . The data is now created in lakeFS as new changes in your branch. You can now commit these changes or revert them. Configuring Azure Databricks with the S3-compatible API . If you use Azure Databricks, you can take advantage of the lakeFS S3-compatible API with your Azure account and the S3A FileSystem. This will require installing the hadoop-aws package (with the same version as your hadoop-azure package) to your Databricks cluster. Define your FileSystem configurations in the following way: . spark.hadoop.fs.lakefs.impl=org.apache.hadoop.fs.s3a.S3AFileSystem spark.hadoop.fs.lakefs.access.key=‘AKIAlakefs12345EXAMPLE’ // The access key to your lakeFS server spark.hadoop.fs.lakefs.secret.key=‘abc/lakefs/1234567bPxRfiCYEXAMPLEKEY’ // The secret key to your lakeFS server spark.hadoop.fs.lakefs.path.style.access=true spark.hadoop.fs.lakefs.endpoint=‘https://example-org.us-east-1.lakefscloud.io’ // The endpoint of your lakeFS server . For more details about Mounting cloud object storage on Databricks. Configuring Databricks SQL Warehouse with the S3-compatible API . A SQL warehouse is a compute resource that lets you run SQL commands on data objects within Databricks SQL. If you use Databricks SQL warehouse, you can take advantage of the lakeFS S3-compatible API with the S3A FileSystem. Define your SQL Warehouse configurations in the following way: . | In the top right, select Admin Settings and then SQL warehouse settings. | Under Data Access Configuration add the following key-value pairs for each lakeFS repository you want to access: . | . spark.hadoop.fs.s3a.impl shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem spark.hadoop.fs.s3a.bucket.example-repo.access.key AKIAIOSFODNN7EXAMPLE // The access key to your lakeFS server spark.hadoop.fs.s3a.bucket.example-repo.secret.key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY // The secret key to your lakeFS server spark.hadoop.fs.s3a.bucket.example-repo.endpoint https://example-org.us-east-1.lakefscloud.io // The endpoint of your lakeFS server spark.hadoop.fs.s3a.bucket.example-repo.path.style.access true . | Changes are applied automatically after the SQL Warehouse restarts. | You can now use the lakeFS S3-compatible API with your SQL Warehouse, e.g.: | . SELECT * FROM delta.`s3a://example-repo/main/datasets/delta-table/` LIMIT 100 . ",
    "url": "/v1.4/integrations/spark.html#s3-compatible-api",
    
    "relUrl": "/integrations/spark.html#s3-compatible-api"
  },"456": {
    "doc": "Apache Spark",
    "title": "lakeFS Hadoop FileSystem",
    "content": "If you’re using lakeFS on top of S3, this mode will enhance your application’s performance. In this mode, Spark will read and write objects directly from S3, reducing the load on the lakeFS server. It will still access the lakeFS server for metadata operations. After configuring the lakeFS Hadoop FileSystem below, use URIs of the form lakefs://example-repo/ref/path/to/data to interact with your data on lakeFS. Installation . | Spark Standalone | Databricks | Cloudera Spark | . Add the package to your spark-submit command: . --packages io.lakefs:hadoop-lakefs-assembly:0.2.1 . In your cluster settings, under the Libraries tab, add the following Maven package: . io.lakefs:hadoop-lakefs-assembly:0.2.1 . Once installed, it should look something like this: . Add the package to your pyspark or spark-submit command: . --packages io.lakefs:hadoop-lakefs-assembly:0.2.1 . Add the configuration to access the S3 bucket used by lakeFS to your pyspark or spark-submit command or add this configuration at the Cloudera cluster level (see below): . --conf spark.yarn.access.hadoopFileSystems=s3a://bucket-name . Add the configuration to access the S3 bucket used by lakeFS at the Cloudera cluster level: . | Log in to the CDP (Cloudera Data Platform) web interface. | From the CDP home screen, click the Management Console icon. | In the Management Console, select Data Hub Clusters from the navigation pane. | Select the cluster you want to configure. Click on CM-UI link under Services: | In Cloudera Manager web interface, click on Clusters from the navigation pane and click on spark_on_yarn option: | Click on Configuration tab and search for spark.yarn.access.hadoopFileSystems in the search box: | Add S3 bucket used by lakeFS s3a://bucket-name in the spark.yarn.access.hadoopFileSystems list: | . Configuration . Set the fs.lakefs.* Hadoop configurations to point to your lakeFS installation: . | fs.lakefs.impl: io.lakefs.LakeFSFileSystem | fs.lakefs.access.key: lakeFS access key | fs.lakefs.secret.key: lakeFS secret key | fs.lakefs.endpoint: lakeFS API URL (e.g. https://example-org.us-east-1.lakefscloud.io/api/v1) | . Configure the S3A FileSystem to access your S3 storage, for example using the fs.s3a.* configurations (these are not your lakeFS credentials): . | fs.s3a.access.key: AWS S3 access key | fs.s3a.secret.key: AWS S3 secret key | . Here are some configuration examples: . | CLI | Scala | PySpark | XML Configuration | Databricks | . spark-shell --conf spark.hadoop.fs.s3a.access.key='AKIAIOSFODNN7EXAMPLE' \\ --conf spark.hadoop.fs.s3a.secret.key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' \\ --conf spark.hadoop.fs.s3a.endpoint='https://s3.eu-central-1.amazonaws.com' \\ --conf spark.hadoop.fs.lakefs.impl=io.lakefs.LakeFSFileSystem \\ --conf spark.hadoop.fs.lakefs.access.key=AKIAlakefs12345EXAMPLE \\ --conf spark.hadoop.fs.lakefs.secret.key=abc/lakefs/1234567bPxRfiCYEXAMPLEKEY \\ --conf spark.hadoop.fs.lakefs.endpoint=https://example-org.us-east-1.lakefscloud.io/api/v1 \\ --packages io.lakefs:hadoop-lakefs-assembly:0.2.1 \\ io.example.ExampleClass . spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", \"AKIAIOSFODNN7EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.endpoint\", \"https://s3.eu-central-1.amazonaws.com\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.access.key\", \"AKIAlakefs12345EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.endpoint\", \"https://example-org.us-east-1.lakefscloud.io/api/v1\") . sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIAIOSFODNN7EXAMPLE\") sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\") sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"https://s3.eu-central-1.amazonaws.com\") sc._jsc.hadoopConfiguration().set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\") sc._jsc.hadoopConfiguration().set(\"fs.lakefs.access.key\", \"AKIAlakefs12345EXAMPLE\") sc._jsc.hadoopConfiguration().set(\"fs.lakefs.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") sc._jsc.hadoopConfiguration().set(\"fs.lakefs.endpoint\", \"https://example-org.us-east-1.lakefscloud.io/api/v1\") . Make sure that you load the lakeFS FileSystem into Spark by running it with --packages or --jars, and then add these into a configuration file, e.g., $SPARK_HOME/conf/hdfs-site.xml: . &lt;?xml version=\"1.0\"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.s3a.access.key&lt;/name&gt; &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.secret.key&lt;/name&gt; &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;https://s3.eu-central-1.amazonaws.com&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.impl&lt;/name&gt; &lt;value&gt;io.lakefs.LakeFSFileSystem&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.access.key&lt;/name&gt; &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.secret.key&lt;/name&gt; &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.endpoint&lt;/name&gt; &lt;value&gt;https://example-org.us-east-1.lakefscloud.io/api/v1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . Add the following the cluster’s configuration under Configuration ➡️ Advanced options: . spark.hadoop.fs.lakefs.impl io.lakefs.LakeFSFileSystem spark.hadoop.fs.lakefs.access.key AKIAlakefs12345EXAMPLE spark.hadoop.fs.lakefs.secret.key abc/lakefs/1234567bPxRfiCYEXAMPLEKEY spark.hadoop.fs.s3a.access.key AKIAIOSFODNN7EXAMPLE spark.hadoop.fs.s3a.secret.key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY spark.hadoop.fs.s3a.impl shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem spark.hadoop.fs.lakefs.endpoint https://example-org.us-east-1.lakefscloud.io/api/v1 . Alternatively, follow this step by step Databricks integration tutorial, including lakeFS Hadoop File System, Python client and lakeFS SPARK client. ⚠️ If your bucket is on a region other than us-east-1, you may also need to configure fs.s3a.endpoint with the correct region. Amazon provides S3 endpoints you can use. Usage . Hadoop FileSystem paths use the lakefs:// protocol, with paths taking the form lakefs://&lt;repository&gt;/&lt;ref&gt;/path/to/object. &lt;ref&gt; can be a branch, tag, or commit ID in lakeFS. Here’s an example for reading a Parquet file from lakeFS to a Spark DataFrame: . val repo = \"example-repo\" val branch = \"main\" val df = spark.read.parquet(s\"lakefs://${repo}/${branch}/example-path/example-file.parquet\") . Here’s how to write some results back to a lakeFS path: . df.write.partitionBy(\"example-column\").parquet(s\"lakefs://${repo}/${branch}/output-path/\") . The data is now created in lakeFS as new changes in your branch. You can now commit these changes or revert them. ",
    "url": "/v1.4/integrations/spark.html#lakefs-hadoop-filesystem",
    
    "relUrl": "/integrations/spark.html#lakefs-hadoop-filesystem"
  },"457": {
    "doc": "Apache Spark",
    "title": "Hadoop FileSystem in Presigned mode",
    "content": "Available starting version 0.1.13 of the FileSystem . In this mode, the lakeFS server is responsible for authenticating with your storage. The client will still perform data operations directly on the storage. To do so, it will use pre-signed storage URLs provided by the lakeFS server. When using this mode, you don’t need to configure the client with access to your storage: . | CLI | Scala | PySpark | XML Configuration | Databricks | . spark-shell --conf spark.hadoop.fs.lakefs.access.mode=presigned \\ --conf spark.hadoop.fs.lakefs.impl=io.lakefs.LakeFSFileSystem \\ --conf spark.hadoop.fs.lakefs.access.key=AKIAlakefs12345EXAMPLE \\ --conf spark.hadoop.fs.lakefs.secret.key=abc/lakefs/1234567bPxRfiCYEXAMPLEKEY \\ --conf spark.hadoop.fs.lakefs.endpoint=https://example-org.us-east-1.lakefscloud.io/api/v1 \\ --packages io.lakefs:hadoop-lakefs-assembly:0.2.1 . spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.access.mode\", \"presigned\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.access.key\", \"AKIAlakefs12345EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.endpoint\", \"https://example-org.us-east-1.lakefscloud.io/api/v1\") . sc._jsc.hadoopConfiguration().set(\"fs.lakefs.access.mode\", \"presigned\") sc._jsc.hadoopConfiguration().set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\") sc._jsc.hadoopConfiguration().set(\"fs.lakefs.access.key\", \"AKIAlakefs12345EXAMPLE\") sc._jsc.hadoopConfiguration().set(\"fs.lakefs.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") sc._jsc.hadoopConfiguration().set(\"fs.lakefs.endpoint\", \"https://example-org.us-east-1.lakefscloud.io/api/v1\") . Make sure that you load the lakeFS FileSystem into Spark by running it with --packages or --jars, and then add these into a configuration file, e.g., $SPARK_HOME/conf/hdfs-site.xml: . &lt;?xml version=\"1.0\"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.access.mode&lt;/name&gt; &lt;value&gt;presigned&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.impl&lt;/name&gt; &lt;value&gt;io.lakefs.LakeFSFileSystem&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.access.key&lt;/name&gt; &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.secret.key&lt;/name&gt; &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.endpoint&lt;/name&gt; &lt;value&gt;https://example-org.us-east-1.lakefscloud.io/api/v1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . Add the following the cluster’s configuration under Configuration ➡️ Advanced options: . spark.hadoop.fs.lakefs.access.mode presigned spark.hadoop.fs.lakefs.impl io.lakefs.LakeFSFileSystem spark.hadoop.fs.lakefs.access.key AKIAlakefs12345EXAMPLE spark.hadoop.fs.lakefs.secret.key abc/lakefs/1234567bPxRfiCYEXAMPLEKEY spark.hadoop.fs.lakefs.endpoint https://example-org.us-east-1.lakefscloud.io/api/v1 . ",
    "url": "/v1.4/integrations/spark.html#hadoop-filesystem-in-presigned-mode",
    
    "relUrl": "/integrations/spark.html#hadoop-filesystem-in-presigned-mode"
  },"458": {
    "doc": "Apache Spark",
    "title": "Apache Spark",
    "content": " ",
    "url": "/v1.4/integrations/spark.html",
    
    "relUrl": "/integrations/spark.html"
  },"459": {
    "doc": "Single Sign On (SSO)",
    "title": "Single Sign On (SSO)",
    "content": "lakeFS Cloud . lakeFS Enterprise . SSO is available for lakeFS Cloud and lakeFS Enterprise. If you’re using the open-source version of lakeFS you can read more about the authentication options available. ",
    "url": "/v1.4/reference/security/sso.html",
    
    "relUrl": "/reference/security/sso.html"
  },"460": {
    "doc": "Single Sign On (SSO)",
    "title": "SSO for lakeFS Cloud",
    "content": "lakeFS Cloud uses Auth0 for authentication and thus support the same identity providers as Auth0 including Active Directory/LDAP, ADFS, Azure Active Directory Native, Google Workspace, OpenID Connect, Okta, PingFederate, SAML, and Azure Active Directory. | Okta | AD FS | Azure AD | . ",
    "url": "/v1.4/reference/security/sso.html#sso-for-lakefs-cloud",
    
    "relUrl": "/reference/security/sso.html#sso-for-lakefs-cloud"
  },"461": {
    "doc": "Single Sign On (SSO)",
    "title": "Okta",
    "content": "This guide is based on Okta’s Create OIDC app integrations guide. Steps: . | Login to your Okta account | Select Applications &gt; Applications, then Create App Integration. | Select Create New App and enter the following: . | For Sign-in method, choose OIDC. | Under Application type, choose Web app. | Select Next. | . | Under General Settings: . | App integration name, enter a name for your application. (i.e lakeFS Cloud) | . | In the Sign-in redirect URIs field, enter https://lakefs-cloud.us.auth0.com/login (United States) or https://lakefs-cloud.eu.auth0.com/login (Europe). | Under Sign-in redirect URIs, click Add URI, enter https://lakefs-cloud.us.auth0.com/login/callback (United States) or https://lakefs-cloud.eu.auth0.com/login/callback (Europe). | Under Assignments, choose the wanted Controlled access. (i.e Allow everyone in your organization to access) | Uncheck Enable immediate access with Federation Broker Mode. | Select Save. | . Once you finish registering your application with Okta, save the Client ID, Client Secret and your Okta Domain, send this to Treeverse’s team to finish the integration. ",
    "url": "/v1.4/reference/security/sso.html#okta",
    
    "relUrl": "/reference/security/sso.html#okta"
  },"462": {
    "doc": "Single Sign On (SSO)",
    "title": "Active Directory Federation Services (AD FS)",
    "content": "Prerequisites: . | Client’s AD FS server should be exposed publicly or to Auth0’s IP ranges (either directly or using Web Application Proxy) | . Steps: . | Connect to the AD FS server | Open AD FS’ PowerShell CLI as Administrator through the server manager | Execute the following: . (new-object Net.WebClient -property @{Encoding = [Text.Encoding]::UTF8}).DownloadString(\"https://raw.github.com/auth0/adfs-auth0/master/adfs.ps1\") | iex AddRelyingParty \"urn:auth0:lakefs-cloud\" \"https://lakefs-cloud.us.auth0.com/login/callback\" . Note: If your organization data is located in Europe, use lakefs-cloud.eu.auth0.com instead of lakefs-cloud.us.auth0.com. | . Once you finish registering lakeFS Cloud with AD FS, save the AD FS URL and send this to Treeverse’s team to finish the integration. ",
    "url": "/v1.4/reference/security/sso.html#active-directory-federation-services-ad-fs",
    
    "relUrl": "/reference/security/sso.html#active-directory-federation-services-ad-fs"
  },"463": {
    "doc": "Single Sign On (SSO)",
    "title": "Azure Active Directory (AD)",
    "content": "Prerequisites: . | Azure account with permissions to manage applications in Azure Active Directory | . Note: If you’ve already set up lakeFS Cloud with your Azure account, you can skip the Register lakeFS Cloud with Azure and Add client secret and go directly to Add a redirect URI. Register lakeFS Cloud with Azure . Steps: . | Sign in to the Azure portal. | If you have access to multiple tenants, use the Directories + subscriptions filter in the top menu to switch to the tenant in which you want to register the application. | Search for and select Azure Active Directory. | Under Manage, select App registrations &gt; New registration. | Enter a display Name for your application. Users of your application might see the display name when they use the app, for example during sign-in. You can change the display name at any time and multiple app registrations can share the same name. The app registration’s automatically generated Application (client) ID, not its display name, uniquely identifies your app within the identity platform. | Specify who can use the application, sometimes called its sign-in audience. Note: don’t enter anything for Redirect URI (optional). You’ll configure a redirect URI in the next section. | Select Register to complete the initial app registration. | . When registration finishes, the Azure portal displays the app registration’s Overview pane. You see the Application (client) ID. Also called the client ID, this value uniquely identifies your application in the Microsoft identity platform. Important: new app registrations are hidden to users by default. When you are ready for users to see the app on their My Apps page you can enable it. To enable the app, in the Azure portal navigate to Azure Active Directory &gt; Enterprise applications and select the app. Then on the Properties page toggle Visible to users? to Yes. Add a secret . Sometimes called an application password, a client secret is a string value your app can use in place of a certificate to identity itself. Steps: . | In the Azure portal, in App registrations, select your application. | Select Certificates &amp; secrets &gt; Client secrets &gt; New client secret. | Add a description for your client secret. | Select an expiration for the secret or specify a custom lifetime. | Client secret lifetime is limited to two years (24 months) or less. You can’t specify a custom lifetime longer than 24 months. | Microsoft recommends that you set an expiration value of less than 12 months. | . | Select Add. | Record the secret’s value for use in your client application code. This secret value is never displayed again after you leave this page. | . Add a redirect URI . A redirect URI is the location where the Microsoft identity platform redirects a user’s client and sends security tokens after authentication. You add and modify redirect URIs for your registered applications by configuring their platform settings. Enter https://lakefs-cloud.us.auth0.com/login/callback as your redirect URI. Settings for each application type, including redirect URIs, are configured in Platform configurations in the Azure portal. Some platforms, like Web and Single-page applications, require you to manually specify a redirect URI. For other platforms, like mobile and desktop, you can select from redirect URIs generated for you when you configure their other settings. Steps: . | In the Azure portal, in App registrations, select your application. | Under Manage, select Authentication. | Under Platform configurations, select Add a platform. | Under Configure platforms, select the web option. | Select Configure to complete the platform configuration. | . Once you finish registering lakeFS Cloud with Azure AD send the following items to the Treeverse’s team: . | Client ID | Client Secret | Azure AD Domain | Identity API Version (v1 for Azure AD or v2 for Microsoft Identity Platform/Entra) | . ",
    "url": "/v1.4/reference/security/sso.html#azure-active-directory-ad",
    
    "relUrl": "/reference/security/sso.html#azure-active-directory-ad"
  },"464": {
    "doc": "Single Sign On (SSO)",
    "title": "SSO for lakeFS Enterprise",
    "content": "Authentication in lakeFS Enterprise is handled by a secondary service which runs side-by-side with lakeFS. With a nod to Hogwarts and their security system, we’ve named this service Fluffy. Details for configuring the supported identity providers with Fluffy are shown below. In addition, please review the necessary Helm configuration to configure Fluffy. | Active Directory Federation Services (AD FS) (using SAML) | OpenID Connect | LDAP | . If you’re using an authentication provider that is not listed please contact us for further assistance. | AD FS | OpenID Connect | LDAP | . ",
    "url": "/v1.4/reference/security/sso.html#sso-for-lakefs-enterprise",
    
    "relUrl": "/reference/security/sso.html#sso-for-lakefs-enterprise"
  },"465": {
    "doc": "Single Sign On (SSO)",
    "title": "Active Directory Federation Services (AD FS) (using SAML)",
    "content": "AD FS integration uses certificates to sign &amp; encrypt requests going out from Fluffy and decrypt incoming requests from AD FS server. In order for Fluffy to work, the following values must be configured. Update (or override) the following attributes in the chart’s values.yaml file. | Replace fluffy.saml_rsa_public_cert and fluffy.saml_rsa_private_key with real certificate values | Replace fluffyConfig.auth.saml.idp_metadata_url with the metadata URL of the AD FS provider (e.g adfs-auth.company.com) | Replace fluffyConfig.auth.saml.external_user_id_claim_name with the claim name representing user id name in AD FS | Replace lakefs.company.com with your lakeFS server URL. | . If you’d like to generate the certificates using OpenSSL, you can take a look at the following example: . openssl req -x509 -newkey rsa:2048 -keyout myservice.key -out myservice.cert -days 365 -nodes -subj \"/CN=lakefs.company.com\" - . lakeFS Server Configuration (Update in helm’s values.yaml file): . auth: cookie_auth_verification: auth_source: saml friendly_name_claim_name: displayName external_user_id_claim_name: samName default_initial_groups: - \"Developers\" logout_redirect_url: \"https://lakefs.company.com/logout-saml\" encrypt: secret_key: shared-secrey-key ui_config: login_url: \"https://lakefs.company.com/sso/login-saml\" logout_url: \"https://lakefs.company.com/sso/logout-saml\" login_cookie_names: - internal_auth_session - saml_auth_session . Fluffy Configuration (Update in helm’s values.yaml file): . logging: format: \"json\" level: \"INFO\" audit_log_level: \"INFO\" output: \"=\" auth: encrypt: secret_key: shared-secrey-key logout_redirect_url: https://lakefs.company.com post_login_redirect_url: https://lakefs.company.com saml: enabled: true sp_root_url: https://lakefs.company.com sp_x509_key_path: '/etc/saml_certs/rsa_saml_private.cert' sp_x509_cert_path: '/etc/saml_certs/rsa_saml_public.pem' sp_sign_request: true sp_signature_method: \"http://www.w3.org/2001/04/xmldsig-more#rsa-sha256\" idp_metadata_url: \"https://adfs-auth.company.com/federationmetadata/2007-06/federationmetadata.xml\" # idp_authn_name_id_format: \"urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified\" external_user_id_claim_name: samName # idp_metadata_file_path: # idp_skip_verify_tls_cert: true . OpenID Connect . In order for Fluffy to work, the following values must be configured. Update (or override) the following attributes in the chart’s values.yaml file. | Replace lakefsConfig.friendly_name_claim_name with the right claim name. | Replace lakefsConfig.default_initial_groups with desired claim name (See pre-configured groups for enterprise) | Replace fluffyConfig.auth.logout_redirect_url with your full OIDC logout URL (e.g https://oidc-provider-url.com/logout/path) | Replace fluffyConfig.auth.oidc.url with your OIDC provider URL (e.g https://oidc-provider-url.com) | Replace fluffyConfig.auth.oidc.logout_endpoint_query_parameters with parameters you’d like to pass to the OIDC provider for logout. | Replace fluffyConfig.auth.oidc.client_id and fluffyConfig.auth.oidc.client_secret with the client ID &amp; secret for OIDC. | Replace fluffyConfig.auth.oidc.logout_client_id_query_parameter with the query parameter that represent the client_id, note that it should match the the key/query param that represents the client id and required by the specific OIDC provider. | Replace lakefs.company.com with the lakeFS server URL. | . lakeFS Server Configuration (Update in helm’s values.yaml file): . # Important: make sure to include the rest of your lakeFS Configuration here! auth: encrypt: secret_key: shared-secrey-key oidc: friendly_name_claim_name: \"name\" default_initial_groups: [\"Developers\"] ui_config: login_url: /oidc/login logout_url: /oidc/logout login_cookie_names: - internal_auth_session - oidc_auth_session . Fluffy Configuration (Update in helm’s values.yaml file): . logging: format: \"json\" level: \"INFO\" audit_log_level: \"INFO\" output: \"=\" installation: fixed_id: fluffy-authenticator auth: post_login_redirect_url: / logout_redirect_url: https://oidc-provider-url.com/logout/url oidc: enabled: true url: https://oidc-provider-url.com/ client_id: &lt;oidc-client-id&gt; client_secret: &lt;oidc-client-secret&gt; callback_base_url: https://lakefs.company.com is_default_login: true logout_client_id_query_parameter: client_id logout_endpoint_query_parameters: - returnTo - https://lakefs.company.com/oidc/login encrypt: secret_key: shared-secrey-key . ",
    "url": "/v1.4/reference/security/sso.html#active-directory-federation-services-ad-fs-using-saml",
    
    "relUrl": "/reference/security/sso.html#active-directory-federation-services-ad-fs-using-saml"
  },"466": {
    "doc": "Single Sign On (SSO)",
    "title": "LDAP",
    "content": "In order for Fluffy to work, the following values must be configured. Update (or override) the following attributes in the chart’s values.yaml file. | Replace lakefsConfig.auth.remote_authenticator.endpoint with the lakeFS server URL combined with the api/v1/ldap/login suffix (e.g http://lakefs.company.com/api/v1/ldap/login) | Repalce fluffyConfig.auth.ldap.remote_authenticator.server_endpoint with your LDAP server endpoint (e.g ldaps://ldap.ldap-address.com:636) | Replace fluffyConfig.auth.ldap.remote_authenticator.bind_dn with the LDAP bind user/permissions to query your LDAP server. | Replace fluffyConfig.auth.ldap.remote_authenticator.user_base_dn with the user base to search users in. | . lakeFS Server Configuration (Update in helm’s values.yaml file): . # Important: make sure to include the rest of your lakeFS Configuration here! auth: remote_authenticator: enabled: true endpoint: https://lakefs.company.com/api/v1/ldap/login default_user_group: \"Developers\" ui_config: logout_url: /logout login_cookie_names: - internal_auth_session . Fluffy Configuration (Update in helm’s values.yaml file): . logging: format: \"json\" level: \"INFO\" audit_log_level: \"INFO\" output: \"=\" installation: fixed_id: fluffy-authenticator auth: post_login_redirect_url: / ldap: server_endpoint: 'ldaps://ldap.company.com:636' bind_dn: uid=&lt;bind-user-name&gt;,ou=Users,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com bind_password: '&lt;ldap pwd&gt;' username_attribute: uid user_base_dn: ou=Users,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com user_filter: (objectClass=inetOrgPerson) connection_timeout_seconds: 15 request_timeout_seconds: 7 . Helm . In order to use lakeFS Enterprise and Fluffy, we provided out of the box setup, see lakeFS Helm chart configuration. Notes: . | Check the examples on GitHub we provide for each authentication method (oidc/adfs/ldap + rbac). | The examples are provisioned with a Postgres pod for quick-start, make sure to replace that to a stable database once ready. | The encrypt secret key secrets.authEncryptSecretKey is shared between fluffy and lakeFS for authentication. | The lakeFS image.tag must be &gt;= 0.100.0 | The fluffy image.tag must be &gt;= 0.2.0 | Change the ingress.hosts[0] from lakefs.company.com to a real host (usually same as lakeFS), also update additional references in the file (note: URL path after host if provided should stay unchanged). | Update the ingress configuration with other optional fields if used | Fluffy docker image: replace the fluffy.image.privateRegistry.secretToken with real token to dockerhub for the fluffy docker image. | . ",
    "url": "/v1.4/reference/security/sso.html#ldap",
    
    "relUrl": "/reference/security/sso.html#ldap"
  },"467": {
    "doc": "Unity Delta Sharing",
    "title": "Unity Delta Sharing",
    "content": "lakeFS Cloud . ",
    "url": "/v1.4/howto/unity-delta-sharing.html",
    
    "relUrl": "/howto/unity-delta-sharing.html"
  },"468": {
    "doc": "Unity Delta Sharing",
    "title": "Introduction",
    "content": "lakeFS Unity Delta Sharing provides a read-only experience from Unity Catalog for lakeFS customers. Currently, this is available as a private preview. It provides full read-only functionality for Unity Catalog. It does not provide a “self-service” experience to set up the service. ",
    "url": "/v1.4/howto/unity-delta-sharing.html#introduction",
    
    "relUrl": "/howto/unity-delta-sharing.html#introduction"
  },"469": {
    "doc": "Unity Delta Sharing",
    "title": "Setup",
    "content": "This guide explains how to set up and use lakeFS Delta Sharing. Currently, you will have to configure lakeFS Delta Sharing in collaboration with Treeverse Customer Success. Once set up is complete, you will of course be able to use lakeFS Delta Sharing on existing and on new tables without further assistance. 1. Collect data and initial setup . | Select a Delta Sharing configuration URL. This is a single location on lakeFS to hold the top-level configuration of lakeFS Delta Sharing across all repositories of your organization. Typically, it will have the form lakefs://REPO/main/lakefs_delta_sharing.yaml for one of your repositories. A longer path may be supplied - however, we do recommend keeping it on the main branch, as this object represents state for the entire installation. | Create a user lakefs-delta-sharing-service for lakeFS Delta Sharing, and an access key for that user. It should have at least read permissions for the configuration URL and for all repositories and all data accesses by Unity. lakeFS Delta Sharing will these credentials to communicate with lakeFS. | . Communicate these items to Customer Success: . | Configuration URL | Access key ID and secret access key for user lakefs-delta-sharing-service. | . Note: All YAML files extensions used in this guide must be yaml. Do not use a yml extension instead. 2. Initial configuration . Select a secret authorization token to share Unity catalog. Unity catalog will use to authenticate to the lakeFS Delta Sharing server. You might use this command on Linux: . head -c 50 /dev/random | base64 . Create a file lakefs_delta_sharing.yaml and place it at the config URL selected above. It should look like this: . authorization_token: \"GENERATED TOKEN\" # Map lakeFS repositories to Unity shares repositories: - id: sample-repo share_name: undev # List branches and prefixes to export. Each of these branches (and only # these branches) will be available as a schema on Unity. branches: - main - staging - dev_* - id: repo2 share_name: share_two branches: - \"*\" . Note that a plain “*” line must be quoted in YAML. Upload it to your config URL. For instance if the config URL is lakefs://repo/main/lakefs_delta_sharing.yaml, you might use: . lakectl fs upload -s ./lakefs_delta_sharing.yaml lakefs://repo/main/lakefs_delta_sharing.yaml . 3. Connect Unity to lakeFS Delta Sharing! . You now need to configure Unity to use the lakeFS Delta Sharing server. Create a share provider file config.share.json; see the Delta Sharing manual: . { \"shareCredentialsVersion\": 1, \"endpoint\": \"https://ORG_ID.REGION.lakefscloud.io/service/delta-sharing/v1\", \"bearerToken\": \"GENERATED TOKEN\", \"expirationTime\": \"2030-01-01T00:00:00.0Z\" } . “GENERATED TOKEN” is the secret authorization token use above. Install the databricks cli. We will use it to create Delta Share on Unity. Follow the instructions to configure it. Run the provider creation command: . databricks unity-catalog providers create \\ --name lakefs-cloud \\ --recipient-profile-json-file config.share.json . Go to “Data » Delta Sharing” on the DataBricks environment. Once Treeverse have configured lakeFS Delta Sharing on your account with your config URL, the “lakefs-cloud” provider should appear under “Shared with me”. Click the provider to see its shares. You can now create a catalog from these shares. And you can see schemas for each of the branches that you configured in the share. Here branch name dev_experiment1 matches the pattern dev_* that we defined in the configuration object lakefs-delta-sharing.yaml, so it appears as a schema. At this point you have configured Delta Sharing on lakeFS, and DataBricks to communicate with lakeFS delta sharing. No further Treeverse involvement is required. Updates to lakefs_delta_sharing.yaml will update within a minute of uploading a new version. 4. Configure tables . Everything is ready: lakeFS repositories are configured as shares, and branches are configured as schemas. Now you can define tables! Once a repository is shared, its tables are configured as a table descriptor object on the repository on the path _lakefs_tables/TABLE.yaml. Delta Lake tables . Delta Lake format includes full metadata, so you only need to configure the prefix: . name: users type: delta path: path/to/users/ . Note: The filename of the ‘yaml’ file containing the table definition must match the ‘name’ of the table itself. In the example above, ‘_lakefs_tables/users.yaml’. When placed inside _lakefs_tables/users.yaml this defines a table users on the prefix path/to/users/ (so path/to/users/ holds the prefix _delta_log). Hive tables . Hive metadata server tables are essentially just a set of objects that share a prefix, with no table metadata stored on the object store. You need to configure prefix, partitions, and schema. name: clicks type: hive path: path/to/clicks/ partition_columns: ['year'] schema: type: struct fields: - name: year type: integer nullable: false metadata: {} - name: page type: string nullable: false metadata: {} - name: site type: string nullable: true metadata: comment: a comment about this column . Useful types recognized by DataBricks Photon include integer, long, short, string, double, float, date, and timestamp. For exact type mappings, and whether to specify a field as nullable: false, refer to DataBricks Photon documentation. ",
    "url": "/v1.4/howto/unity-delta-sharing.html#setup",
    
    "relUrl": "/howto/unity-delta-sharing.html#setup"
  },"470": {
    "doc": "Unity Catalog",
    "title": "Using lakeFS with the Unity Catalog",
    "content": " ",
    "url": "/v1.4/integrations/unity_catalog.html#using-lakefs-with-the-unity-catalog",
    
    "relUrl": "/integrations/unity_catalog.html#using-lakefs-with-the-unity-catalog"
  },"471": {
    "doc": "Unity Catalog",
    "title": "Table of contents",
    "content": ". | Overview | Prerequisites . | Databricks authentication | . | Guide . | Table descriptor definition | Write some data | The Unity Catalog exporter script | Action configuration | Databricks Integration | . | . ",
    "url": "/v1.4/integrations/unity_catalog.html#table-of-contents",
    
    "relUrl": "/integrations/unity_catalog.html#table-of-contents"
  },"472": {
    "doc": "Unity Catalog",
    "title": "Overview",
    "content": "Databricks Unity Catalog serves as a centralized data governance platform for your data lakes. Through the Unity Catalog, you can search for and locate data assets across workspaces via a unified catalog. Leveraging the external tables feature within Unity Catalog, you can register a Delta Lake table exported from lakeFS and access it through the unified catalog. The subsequent step-by-step guide will lead you through the process of configuring a Lua hook that exports Delta Lake tables from lakeFS, and subsequently registers them in Unity Catalog. Currently, Unity Catalog export feature exclusively supports AWS S3 as the underlying storage solution. It’s planned to support other cloud providers soon. ",
    "url": "/v1.4/integrations/unity_catalog.html#overview",
    
    "relUrl": "/integrations/unity_catalog.html#overview"
  },"473": {
    "doc": "Unity Catalog",
    "title": "Prerequisites",
    "content": "Before starting, ensure you have the following: . | Access to Unity Catalog | An active lakeFS installation with S3 as the backing storage, and a repository in this installation. | A Databricks SQL warehouse. | AWS Credentials with S3 access. | lakeFS credentials with access to your Delta Tables. | . Databricks authentication . Given that the hook will ultimately register a table in Unity Catalog, authentication with Databricks is imperative. Make sure that: . | You have a Databricks Service Principal. | The Service principal has token usage permissions, and an associated token configured. | The service principal has the Service principal: Manager privilege over itself (Workspace: Admin console -&gt; Service principals -&gt; &lt;service principal&gt; -&gt; Permissions -&gt; Grant access (&lt;service principal&gt;: Service principal: Manager), with Workspace access and Databricks SQL access checked (Admin console -&gt; Service principals -&gt; &lt;service principal&gt; -&gt; Configurations). | Your SQL warehouse allows the service principal to use it (SQL Warehouses -&gt; &lt;SQL warehouse&gt; -&gt; Permissions -&gt; &lt;service principal&gt;: Can use). | The catalog grants the USE CATALOG, USE SCHEMA, CREATE SCHEMA permissions to the service principal(Catalog -&gt; &lt;catalog name&gt; -&gt; Permissions -&gt; Grant -&gt; &lt;service principal&gt;: USE CATALOG, USE SCHEMA, CREATE SCHEMA). | You have an External Location configured, and the service principal has the CREATE EXTERNAL TABLE permission over it (Catalog -&gt; External Data -&gt; External Locations -&gt; Create location). | . ",
    "url": "/v1.4/integrations/unity_catalog.html#prerequisites",
    
    "relUrl": "/integrations/unity_catalog.html#prerequisites"
  },"474": {
    "doc": "Unity Catalog",
    "title": "Guide",
    "content": "Table descriptor definition . To guide the Unity Catalog exporter in configuring the table in the catalog, define its properties in the Delta Lake table descriptor. The table descriptor should include (at minimum) the following fields: . | name: The table name. | type: Should be delta. | catalog: The name of the catalog in which the table will be created. | path: The path in lakeFS (starting from the root of the branch) in which the Delta Lake table’s data is found. | . Let’s define the table descriptor and upload it to lakeFS: . Save the following as famous-people-td.yaml: . --- name: famous_people type: delta catalog: my-catalog-name path: tables/famous-people . It’s recommended to create a Unity catalog with the same name as your repository . Upload the table descriptor to _lakefs_tables/famous-people-td.yaml and commit: . lakectl fs upload lakefs://repo/main/_lakefs_tables/famous-people-td.yaml -s ./famous-people-td.yaml &amp;&amp; \\ lakectl commit lakefs://repo/main -m \"add famous people table descriptor\" . Write some data . Insert data into the table path, using your preferred method (e.g. Spark), and commit upon completion. We shall use Spark and lakeFS’s S3 gateway to write some data as a Delta table: . pyspark --packages \"io.delta:delta-spark_2.12:3.0.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262\" \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\ --conf spark.hadoop.fs.s3a.aws.credentials.provider='org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider' \\ --conf spark.hadoop.fs.s3a.endpoint='&lt;LAKEFS_SERVER_URL&gt;' \\ --conf spark.hadoop.fs.s3a.access.key='&lt;LAKEFS_ACCESS_KEY&gt;' \\ --conf spark.hadoop.fs.s3a.secret.key='&lt;LAKEFS_SECRET_ACCESS_KEY&gt;' \\ --conf spark.hadoop.fs.s3a.path.style.access=true . data = [ ('James','Bond','England','intelligence'), ('Robbie','Williams','England','music'), ('Hulk','Hogan','USA','entertainment'), ('Mister','T','USA','entertainment'), ('Rafael','Nadal','Spain','professional athlete'), ('Paul','Haver','Belgium','music'), ] columns = [\"firstname\",\"lastname\",\"country\",\"category\"] df = spark.createDataFrame(data=data, schema = columns) df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"category\", \"country\").save(\"s3a://repo/main/tables/famous-people\") . The Unity Catalog exporter script . For code references check delta_exporter and unity_exporter docs. Create unity_exporter.lua: . local aws = require(\"aws\") local formats = require(\"formats\") local databricks = require(\"databricks\") local delta_export = require(\"lakefs/catalogexport/delta_exporter\") local unity_export = require(\"lakefs/catalogexport/unity_exporter\") local sc = aws.s3_client(args.aws.access_key_id, args.aws.secret_access_key, args.aws.region) -- Export Delta Lake tables export: local delta_client = formats.delta_client(args.lakefs.access_key_id, args.lakefs.secret_access_key, args.aws.region) local delta_table_locations = delta_export.export_delta_log(action, args.table_defs, sc.put_object, delta_client, \"_lakefs_tables\") -- Register the exported table in Unity Catalog: local databricks_client = databricks.client(args.databricks_host, args.databricks_token) local registration_statuses = unity_export.register_tables(action, \"_lakefs_tables\", delta_table_locations, databricks_client, args.warehouse_id) for t, status in pairs(registration_statuses) do print(\"Unity catalog registration for table \\\"\" .. t .. \"\\\" completed with commit schema status : \" .. status .. \"\\n\") end . Upload the lua script to the main branch under scripts/unity_exporter.lua and commit: . lakectl fs upload lakefs://repo/main/scripts/unity_exporter.lua -s ./unity_exporter.lua &amp;&amp; \\ lakectl commit lakefs://repo/main -m \"upload unity exporter script\" . Action configuration . Define an action configuration that will run the above script after a commit is completed (post-commit) over the main branch. Create unity_exports_action.yaml: . --- name: unity_exports on: post-commit: branches: [\"main\"] hooks: - id: unity_export type: lua properties: script_path: scripts/unity_exporter.lua args: aws: access_key_id: &lt;AWS_ACCESS_KEY_ID&gt; secret_access_key: &lt;AWS_SECRET_ACCESS_KEY&gt; region: &lt;AWS_REGION&gt; lakefs: # provide credentials of a user that has access to the script and Delta Table access_key_id: &lt;LAKEFS_ACCESS_KEY_ID&gt; secret_access_key: &lt;LAKEFS_SECRET_ACCESS_KEY&gt; table_defs: # an array of table descriptors used to be defined in Unity Catalog - famous-people-td databricks_host: &lt;DATABRICKS_HOST_URL&gt; databricks_token: &lt;DATABRICKS_SERVICE_PRINCIPAL_TOKEN&gt; warehouse_id: &lt;WAREHOUSE_ID&gt; . Upload the action configurations to _lakefs_actions/unity_exports_action.yaml and commit: . Once the commit will finish its run, the action will start running since we’ve configured it to run on post-commit events on the main branch. lakectl fs upload lakefs://repo/main/_lakefs_actions/unity_exports_action.yaml -s ./unity_exports_action.yaml &amp;&amp; \\ lakectl commit lakefs://repo/main -m \"upload action and run it\" . The action has run and exported the famous_people Delta Lake table to the repo’s storage namespace, and has register the table as an external table in Unity Catalog under the catalog my-catalog-name, schema main (as the branch’s name) and table name famous_people: my-catalog-name.main.famous_people. Databricks Integration . After registering the table in Unity, you can leverage your preferred method to query the data from the exported table under my-catalog-name.main.famous_people, and view it in the Databricks’s Catalog Explorer, or retrieve it using the Databricks CLI with the following command: . databricks tables get my-catalog-name.main.famous_people . ",
    "url": "/v1.4/integrations/unity_catalog.html#guide",
    
    "relUrl": "/integrations/unity_catalog.html#guide"
  },"475": {
    "doc": "Unity Catalog",
    "title": "Unity Catalog",
    "content": " ",
    "url": "/v1.4/integrations/unity_catalog.html",
    
    "relUrl": "/integrations/unity_catalog.html"
  },"476": {
    "doc": "Upgrade lakeFS",
    "title": "Upgrading lakeFS",
    "content": "Note: For a fully managed lakeFS service with guaranteed SLAs, try lakeFS Cloud . Upgrading lakeFS from a previous version usually just requires re-deploying with the latest image (or downloading the latest version if you’re using the binary). If you’re upgrading, check whether the release requires a migration. ",
    "url": "/v1.4/howto/deploy/upgrade.html#upgrading-lakefs",
    
    "relUrl": "/howto/deploy/upgrade.html#upgrading-lakefs"
  },"477": {
    "doc": "Upgrade lakeFS",
    "title": "When DB migrations are required",
    "content": "lakeFS 0.103.0 or greater . Version 0.103.0 added support for rolling KV upgrade. This means that users who already migrated to the KV ref-store (versions 0.80.0 and above) no longer have to pass through specific versions for migration. This includes ACL migration which was introduced in lakeFS version 0.98.0. Running lakefs migrate up on the latest lakeFS version will perform all the necessary migrations up to that point. lakeFS 0.80.0 or greater (KV Migration) . Starting with version 0.80.2, lakeFS has transitioned from using a PostgreSQL based database implementation to a Key-Value datastore interface supporting multiple database implementations. More information can be found here. Users upgrading from a previous version of lakeFS must pass through the KV migration version (0.80.2) before upgrading to newer versions of lakeFS. IMPORTANT: Pre Migrate Requirements . | Users using OS environment variables for database configuration must define the connection_string explicitly or as environment variable before proceeding with the migration. | Database storage free capacity of at least twice the amount of the currently used capacity | It is strongly recommended to perform these additional steps: . | Commit all uncommitted data on branches | Create a snapshot of your database | . | By default, old database tables are not being deleted by the migration process, and should be removed manually after a successful migration. To enable table drop as part of the migration, set the database.drop_tables configuration param to true | . Migration Steps . For each lakeFS instance currently running with the database . | Modify the database section under lakeFS configuration yaml: . | Add type field with \"postgres\" as value | Copy the current configuration parameters to a new section called postgres | . --- database: type: \"postgres\" connection_string: \"postgres://localhost:5432/postgres?sslmode=disable\" max_open_connections: 20 postgres: connection_string: \"postgres://localhost:5432/postgres?sslmode=disable\" max_open_connections: 20 . | Stop all lakeFS instances | Using the lakefs binary for the new version (0.80.2), run the following: . lakefs migrate up . | lakeFS will run the migration process, which in the end should display the following message with no errors: . time=\"2022-08-10T14:46:25Z\" level=info msg=\"KV Migration took 717.629563ms\" func=\"pkg/logging.(*logrusEntryWrapper).Infof\" file=\"build/pkg/logging/logger.go:246\" TempDir=/tmp/kv_migrate_2913402680 . | It is now possible to remove the old database configuration. The updated configuration should look as such: . --- database: type: \"postgres\" postgres: connection_string: \"postgres://localhost:5432/postgres?sslmode=disable\" max_open_connections: 20 . | Deploy (or run) the new version of lakeFS. | . lakeFS 0.30.0 or greater . In case migration is required, you first need to stop the running lakeFS service. Using the lakefs binary for the new version, run the following: . lakefs migrate up . Deploy (or run) the new version of lakeFS. Note that an older version of lakeFS cannot run on a migrated database. Prior to lakeFS 0.30.0 . Note: with lakeFS &lt; 0.30.0, you should first upgrade to 0.30.0 following this guide. Then, proceed to upgrade to the newest version. Starting version 0.30.0, lakeFS handles your committed metadata in a new way, which is more robust and has better performance. To move your existing data, you will need to run the following upgrade commands. Verify lakeFS version == 0.30.0 (can skip if using Docker) . lakefs --version . Migrate data from the previous format: . lakefs migrate db . Or migrate using Docker image: . docker run --rm -it -e LAKEFS_DATABASE_CONNECTION_STRING=&lt;database connection string&gt; treeverse/lakefs:rocks-migrate migrate db . Once migrated, it is possible to now use more recent lakeFS versions. Please refer to their release notes for more information on ugrading and usage). If you want to start over, discarding your existing data, you need to explicitly state this in your lakeFS configuration file. To do so, add the following to your configuration (relevant only for 0.30.0): . cataloger: type: rocks . ",
    "url": "/v1.4/howto/deploy/upgrade.html#when-db-migrations-are-required",
    
    "relUrl": "/howto/deploy/upgrade.html#when-db-migrations-are-required"
  },"478": {
    "doc": "Upgrade lakeFS",
    "title": "Data Migration for Version v0.50.0",
    "content": "If you are using a version before 0.50.0, you must first perform the previous upgrade to that version. {: note: .note-warning } . ",
    "url": "/v1.4/howto/deploy/upgrade.html#data-migration-for-version-v0500",
    
    "relUrl": "/howto/deploy/upgrade.html#data-migration-for-version-v0500"
  },"479": {
    "doc": "Upgrade lakeFS",
    "title": "Upgrade lakeFS",
    "content": " ",
    "url": "/v1.4/howto/deploy/upgrade.html",
    
    "relUrl": "/howto/deploy/upgrade.html"
  },"480": {
    "doc": "Versioning Internals",
    "title": "Versioning Internals",
    "content": " ",
    "url": "/v1.4/understand/how/versioning-internals.html",
    
    "relUrl": "/understand/how/versioning-internals.html"
  },"481": {
    "doc": "Versioning Internals",
    "title": "Table of contents",
    "content": ". | Overview | SSTable File Format (“Graveler File”) | Constructing a consistent view of the keyspace (i.e., a commit) | Representing references and uncommitted metadata | . ",
    "url": "/v1.4/understand/how/versioning-internals.html#table-of-contents",
    
    "relUrl": "/understand/how/versioning-internals.html#table-of-contents"
  },"482": {
    "doc": "Versioning Internals",
    "title": "Overview",
    "content": "Since commits in lakeFS are immutable, they are easy to store on an immutable object store. Older commits are rarely accessed, while newer commits are accessed very frequently, a tiered storage approach can work very well - the object store is the source of truth, while local disk and even RAM can be used to cache the more frequently accessed ones. Since they are immutable - once cached, you only need to evict them when space is running out. There’s no complex invalidation that needs to happen. In terms of storage format, commits are be stored as SSTables, compatible with RocksDB. SSTables were chosen as a storage format for 3 major reasons: . | Extremely high read throughput on modern hardware: using commits representing a 200m object repository (modeled after the S3 inventory of one of our design partners), we were able to achieve close to 500k random GetObject calls / second. This provides a very high throughput/cost ratio, probably as high as can be achieved on public clouds. | Being a known storage format means it’s relatively easy to generate and consume. Storing it in the object store makes it accessible to data engineering tools for analysis and distributed computation, effectively reducing the silo effect of storing it in an operational database. | The SSTable format supports delta encoding for keys which makes them very space efficient for data lakes where many keys share the same common prefixes. | . Each lakeFS commit is represented as a set of contiguous, non-overlapping SSTables that make up the entire keyspace of a repository at that commit. ",
    "url": "/v1.4/understand/how/versioning-internals.html#overview",
    
    "relUrl": "/understand/how/versioning-internals.html#overview"
  },"483": {
    "doc": "Versioning Internals",
    "title": "SSTable File Format (“Graveler File”)",
    "content": "lakeFS metadata is encoded into a format called “Graveler” - a standardized way to encode content-addressable key value pairs. This is what a Graveler file looks like: . Each Key/Value pair (“ValueRecord”) is constructed of a key, identity, and value. A simple identity could be, for example, a sha256 hash of the value’s bytes. It could be any sequence of bytes that uniquely identifies the value. As far as the Graveler is concerned, two ValueRecords are considered identical if their key and identity fields are equal. A Graveler file itself is content-addressable, i.e., similarly to Git, the name of the file is its identity. File identity is calculated based on the identity of the ValueRecords the file contains: . valueRecordID = h(h(valueRecord.key) || h(valueRecord.Identity)) fileID = h(valueRecordID1 + … + valueRecordIDN) . ",
    "url": "/v1.4/understand/how/versioning-internals.html#sstable-file-format-graveler-file",
    
    "relUrl": "/understand/how/versioning-internals.html#sstable-file-format-graveler-file"
  },"484": {
    "doc": "Versioning Internals",
    "title": "Constructing a consistent view of the keyspace (i.e., a commit)",
    "content": "We have two additional requirements for the storage format: . | Be space and time efficient when creating a commit - assuming a commit changes a single object out of a billion, we don’t want to write a full snapshot of the entire repository. Ideally, we’ll be able to reuse some data files that haven’t changed to make the commit operations (in both space and time) proportional to the size of the difference as opposed to the total size of the repository. | Allow an efficient diff between commits which runs in time proportional to the size of their difference and not their absolute sizes. | . To support these requirements, we decided to essentially build a 2-layer Merkle tree composed of a set of leaf nodes (“Range”) addressed by their content address, and a “Meta Range”, which is a special range containing all ranges, thus representing an entire consistent view of the keyspace: . Assuming commit B is derived from commit A, and only changed files in range e-f, it can reuse all ranges except for SSTable #N (the one containing the modified range of keys), which will be recreated with a new hash representing the state as exists after applying commit B’s changes. This will, in turn, also create a new Metarange since its hash is now changed as well (as it is derived from the hash of all contained ranges). Assuming most commits usually change related objects (i.e., that are likely to share some common prefix), the reuse ratio could be very high. We tested this assumption using S3 inventory from 2 design partners - we partitioned the keyspace to an arbitrary number of simulated blocks and measured their change over time. We saw a daily change rate of about 5-20%. Given the size of the repositories, it’s safe to assume that a single day would translate into multiple commits. At a modest 20 commits per day, a commit is expected to reuse &gt;= 99% of the previous commit blocks, so acceptable in terms of write amplification generated on commit. On the object store, ranges are stored in the following hierarchy: . &lt;lakefs root&gt; _lakefs/ &lt;range hash1&gt; &lt;range hash2&gt; &lt;range hashN&gt; ... &lt;metarange hash1&gt; &lt;metarange hash2&gt; &lt;metarange hashN&gt; ... &lt;data object hash1&gt; &lt;data object hash2&gt; &lt;data object hashN&gt; ... Note: This relatively flat structure could be modified in the future. Looking at the diagram above, it imposes no real limitations on the depth of the tree. A tree could easily be made recursive by having Meta Ranges point to other Meta Ranges - and still provide all the same characteristics. For simplicity, we decided to start with a fixed 2-level hierarchy. ",
    "url": "/v1.4/understand/how/versioning-internals.html#constructing-a-consistent-view-of-the-keyspace-ie-a-commit",
    
    "relUrl": "/understand/how/versioning-internals.html#constructing-a-consistent-view-of-the-keyspace-ie-a-commit"
  },"485": {
    "doc": "Versioning Internals",
    "title": "Representing references and uncommitted metadata",
    "content": "lakeFS always stores the object data in the storage namespace in the user’s object store, committed and uncommitted data alike. However, the lakeFS object metadata might be stored in either the object store or a key-value store. Unlike committed metadata which is immutable, uncommitted (or “staged”) metadata experiences frequent random writes and is very mutable in nature. This is also true for “refs” - in particular, branches, which are simply pointers to an underlying commit, are modified frequently: on every commit or merge operation. Both these types of metadata are not only mutable, but also require strong consistency guarantees while also being fault tolerant. If we can’t access the current pointer of the main branch, a big portion of the system is essentially down. Luckily, this is also much smaller set of metadata compared to the committed metadata. References and uncommitted metadata are currently stored on a key-value store (See supported databases) for consistency guarantees. ",
    "url": "/v1.4/understand/how/versioning-internals.html#representing-references-and-uncommitted-metadata",
    
    "relUrl": "/understand/how/versioning-internals.html#representing-references-and-uncommitted-metadata"
  },"486": {
    "doc": "Vertex AI",
    "title": "Using Vertex AI with lakeFS",
    "content": "Vertex AI lets Google Cloud users Build, deploy, and scale machine learning (ML) models faster, with fully managed ML tools for any use case. lakeFS Works with Vertex AI by allowing users to create repositories on GCS Buckets, then use either the Dataset API to create managed Datasets on top of lakeFS version, or by automatically exporting lakeFS object versions in a way readable by Cloud Storage Mounts. ",
    "url": "/v1.4/integrations/vertex_ai.html#using-vertex-ai-with-lakefs",
    
    "relUrl": "/integrations/vertex_ai.html#using-vertex-ai-with-lakefs"
  },"487": {
    "doc": "Vertex AI",
    "title": "Table of contents",
    "content": ". | Using lakeFS with Vertex Managed Datasets | Using lakeFS with Cloud Storage Fuse | . ",
    "url": "/v1.4/integrations/vertex_ai.html#table-of-contents",
    
    "relUrl": "/integrations/vertex_ai.html#table-of-contents"
  },"488": {
    "doc": "Vertex AI",
    "title": "Using lakeFS with Vertex Managed Datasets",
    "content": "Vertex’s ImageDataset and VideoDataset allow creating a dataset by importing a CSV file from gcs (see gcs_source). This CSV file contains GCS addresses of image files and their corresponding labels. Since the lakeFS API supports exporting the underlying GCS address of versioned objects, we can generate such a CSV file when creating the dataset: . #!/usr/bin/env python # Requirements: # google-cloud-aiplatform&gt;=1.31.0 # lakefs-client&gt;=0.107.0 import csv from pathlib import PosixPath from io import StringIO import lakefs_client from lakefs_client.client import LakeFSClient from google.cloud import storage from google.cloud import aiplatform # lakeFS connection details configuration = lakefs_client.Configuration() configuration.username = 'AKIAIOSFODNN7EXAMPLE' configuration.password = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' configuration.host = 'https://lakefs.example.com/' client = LakeFSClient(configuration) # Dataset configuration lakefs_repo = 'my-repository' lakefs_ref = 'main' img_dataset = 'datasets/my-images/' # Vertex configuration import_bucket = 'underlying-gcs-bucket' # produce import file for Vertex's SDK buf = StringIO() csv_writer = csv.writer(buf) has_more = True next_offset = \"\" while has_more: files = client.objects_api.list_objects( lakefs_repo, lakefs_ref, prefix=img_dataset, after=next_offset) for r in files.get('results'): p = PosixPath(r.path) csv_writer.writerow((r.physical_address, p.parent.name)) has_more = files.get('pagination').get('has_more') next_offset = files.get('pagination').get('next_offset') # spit out CSV print('generated path and labels CSV') buf.seek(0) # Write it to storage storage_client = storage.Client() bucket = storage_client.bucket(import_bucket) blob = bucket.blob(f'vertex/imports/{lakefs_repo}/{lakefs_ref}/labels.csv') with blob.open('w') as out: out.write(buf.read()) print(f'wrote csv to gs: gs://{import_bucket}/vertex/imports/{lakefs_repo}/{lakefs_ref}/labels.csv') # import in vertex, as dataset print('Importing dataset...') ds = aiplatform.ImageDataset.create( display_name=f'{lakefs_repo}_{lakefs_ref}_imgs', gcs_source=f'gs://{import_bucket}/vertex/imports/{lakefs_repo}/{lakefs_ref}/labels.csv', import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification, sync=True ) ds.wait() print(f'Done! {ds.display_name} ({ds.resource_name})') . ",
    "url": "/v1.4/integrations/vertex_ai.html#using-lakefs-with-vertex-managed-datasets",
    
    "relUrl": "/integrations/vertex_ai.html#using-lakefs-with-vertex-managed-datasets"
  },"489": {
    "doc": "Vertex AI",
    "title": "Using lakeFS with Cloud Storage Fuse",
    "content": "Vertex allows using Google Cloud Storage mounted as a Fuse Filesystem as custom input for training jobs. Instead of having to copy lakeFS files for each version we want to consume, we can create symlinks by using gcsfuse’s native symlink inodes. This process can be fully automated by using the example gcsfuse_symlink_exporter.lua Lua hook. Here’s what we need to do: . | Upload the example .lua file into our lakeFS repository. For this example, we’ll put it under scripts/gcsfuse_symlink_exporter.lua. | Create a new hook definition file and upload to _lakefs_actions/export_images.yaml: | . --- # Example hook declaration: (_lakefs_actions/export_images.yaml): name: export_images on: post-commit: branches: [\"main\"] post-merge: branches: [\"main\"] post-create-tag: hooks: - id: gcsfuse_export_images type: lua properties: script_path: scripts/export_gcs_fuse.lua # Path to the script we uploaded in the previous step args: prefix: \"datasets/images/\" # Path we want to export every commit destination: \"gs://my-bucket/exports/my-repo/\" # Where should we create the symlinks? mount: from: \"gs://my-bucket/repos/my-repo/\" # Symlinks are to a unix-mounted file to: \"/gcs/my-bucket/repos/my-repo/\" # This will ensure they point to a location that exists. # Should be the contents of a valid credentials.json file # See: https://developers.google.com/workspace/guides/create-credentials # Will be used to write the symlink files gcs_credentials_json_string: | { \"client_id\": \"...\", \"client_secret\": \"...\", \"refresh_token\": \"...\", \"type\": \"...\" } . Done! On the next tag creation or update to the main branch, we’ll automatically export the lakeFS version of datasets/images/ to a mountable location. To consume the symlink-ed files, we can read them normally from the mount: . with open('/gcs/my-bucket/exports/my-repo/branches/main/datasets/images/001.jpg') as f: image_data = f.read() . Previously exported commits are also readable, if we exported them in the past: . commit_id = 'abcdef123deadbeef567' with open(f'/gcs/my-bucket/exports/my-repo/commits/{commit_id}/datasets/images/001.jpg') as f: image_data = f.read() . Considerations when using lakeFS with Cloud Storage Fuse . For lakeFS paths to be readable by gcsfuse, the mount option --implicit-dirs must be specified. ",
    "url": "/v1.4/integrations/vertex_ai.html#using-lakefs-with-cloud-storage-fuse",
    
    "relUrl": "/integrations/vertex_ai.html#using-lakefs-with-cloud-storage-fuse"
  },"490": {
    "doc": "Vertex AI",
    "title": "Vertex AI",
    "content": " ",
    "url": "/v1.4/integrations/vertex_ai.html",
    
    "relUrl": "/integrations/vertex_ai.html"
  },"491": {
    "doc": "Webhooks",
    "title": "Webhooks",
    "content": " ",
    "url": "/v1.4/howto/hooks/webhooks.html",
    
    "relUrl": "/howto/hooks/webhooks.html"
  },"492": {
    "doc": "Webhooks",
    "title": "Table of contents",
    "content": ". | Action File Webhook properties | Request body schema | . A Webhook is a Hook type that sends an HTTP POST request to the configured URL. Any non 2XX response by the responding endpoint will fail the Hook, cancel the execution of the following Hooks under the same Action. For pre-* hooks, the triggering operation will also be aborted. Warning: You should not use pre-* webhooks for long-running tasks, since they block the performed operation. Moreover, the branch is locked during the execution of pre-* hooks, so the webhook server cannot perform any write operations on the branch (like uploading or commits). ",
    "url": "/v1.4/howto/hooks/webhooks.html#table-of-contents",
    
    "relUrl": "/howto/hooks/webhooks.html#table-of-contents"
  },"493": {
    "doc": "Webhooks",
    "title": "Action File Webhook properties",
    "content": "See the Action configuration for overall configuration schema and details. | Property | Description | Data Type | Required | Default Value | Env Vars Support | . | url | The URL address of the request | String | true |   | no | . | timeout | Time to wait for response before failing the hook | String (golang’s Duration representation) | false | 1 minute | no | . | query_params | List of query params that will be added to the request | Dictionary(String:String or String:List(String) | false |   | yes | . | headers | Headers to add to the request | Dictionary(String:String) | false |   | yes | . Secrets &amp; Environment Variables lakeFS Actions supports secrets by using environment variables. The format {{ ENV.SOME_ENV_VAR }} will be replaced with the value of $SOME_ENV_VAR during the execution of the action. If that environment variable doesn’t exist in the lakeFS server environment, the action run will fail. For security purposes, any environment variable whose name begins with “LAKEFS” will be blocked. In this case, the variable will be evaluated to an empty string, effectively making it inaccessible. Additionally, the actions.env.enabled configuration parameter can be set to false to block access to all environment variables. Example: ... hooks: - id: prevent_user_columns type: webhook description: Ensure no user_* columns under public/ properties: url: \"http://&lt;host:port&gt;/webhooks/schema\" timeout: 1m30s query_params: disallow: [\"user_\", \"private_\"] prefix: public/ headers: secret_header: \"{{ ENV.MY_SECRET }}\" ... ",
    "url": "/v1.4/howto/hooks/webhooks.html#action-file-webhook-properties",
    
    "relUrl": "/howto/hooks/webhooks.html#action-file-webhook-properties"
  },"494": {
    "doc": "Webhooks",
    "title": "Request body schema",
    "content": "Upon execution, a webhook will send a request containing a JSON object with the following fields: . | Field | Description | Type | . | event_type | Type of the event that triggered the Action | string | . | event_time | Time of the event that triggered the Action (RFC3339 formatted) | string | . | action_name | Containing Hook Action’s Name | string | . | hook_id | ID of the Hook | string | . | repository_id | ID of the Repository | string | . | branch_id1 | ID of the Branch | string | . | source_ref | Reference to the source on which the event was triggered | string | . | commit_message2 | The message for the commit (or merge) that is taking place | string | . | committer2 | Name of the committer | string | . | commit_metadata2 | The metadata for the commit that is taking place | string | . | tag_id3 | The ID of the created/deleted tag | string | . Example: . { \"event_type\": \"pre-merge\", \"event_time\": \"2021-02-28T14:03:31Z\", \"action_name\": \"test action\", \"hook_id\": \"prevent_user_columns\", \"repository_id\": \"repo1\", \"branch_id\": \"feature-1\", \"source_ref\": \"feature-1\", \"commit_message\": \"merge commit message\", \"committer\": \"committer\", \"commit_metadata\": { \"key\": \"value\" } } . | N\\A for Tag events &#8617; . | N\\A for Tag and Create/Delete Branch events &#8617; &#8617;2 &#8617;3 . | Applicable only for Tag events &#8617; . | . ",
    "url": "/v1.4/howto/hooks/webhooks.html#request-body-schema",
    
    "relUrl": "/howto/hooks/webhooks.html#request-body-schema"
  }
}
