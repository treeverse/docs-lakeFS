{"0": {
    "doc": "Page not found",
    "title": "Page not found",
    "content": " ",
    "url": "/v0.70/404.html",
    "relUrl": "/404.html"
  },"1": {
    "doc": "Page not found",
    "title": "We can’t seem to find the page you’re looking for.",
    "content": "Error code: 404 . Back to Docs . ",
    "url": "/v0.70/404.html#we-cant-seem-to-find-the-page-youre-looking-for",
    "relUrl": "/404.html#we-cant-seem-to-find-the-page-youre-looking-for"
  },"2": {
    "doc": "Add Data",
    "title": "Add Data",
    "content": "In this section, you’ll learn how to copy a file into lakeFS. ",
    "url": "/v0.70/quickstart/add_data.html",
    "relUrl": "/quickstart/add_data.html"
  },"3": {
    "doc": "Add Data",
    "title": "Configuring the AWS CLI",
    "content": "Since lakeFS exposes an S3-compatible API, you can use the AWS CLI to operate on it. | If you don’t have the AWS CLI installed, follow the instructions here. | Configure a new connection profile using the lakeFS credentials you have generated earlier: . aws configure --profile local # fill in the lakeFS credentials generated earlier: # AWS Access Key ID [None]: AKIAJVHTOKZWGCD2QQYQ # AWS Secret Access Key [None]: **************************************** # Default region name [None]: # Default output format [None]: . | Let’s test it to check whether it works. You can do that by calling s3 ls, which should list your repositories: . aws --endpoint-url=http://localhost:8000 --profile local s3 ls # output: # 2021-06-15 13:43:03 example-repo . Note the usage of the --endpoint-url flag, which tells the AWS CLI to connect to lakeFS instead of AWS S3. | Great, now let’s copy some files. You will write to the main branch. This is done by prefixing the path with the name of the branch we’d like to read/write from: . aws --endpoint-url=http://localhost:8000 --profile local s3 cp ./foo.txt s3://example-repo/main/ # output: # upload: ./foo.txt to s3://example-repo/main/foo.txt . | Back in the lakeFS UI, you can see your file in the Uncommitted Changes tab: . | . Next steps . It’s time to commit your changes using the lakeFS CLI. ",
    "url": "/v0.70/quickstart/add_data.html#configuring-the-aws-cli",
    "relUrl": "/quickstart/add_data.html#configuring-the-aws-cli"
  },"4": {
    "doc": "Airbyte",
    "title": "Table of contents",
    "content": ". | Using lakeFS with Airbyte | Use cases | S3 Connector . | Configuring lakeFS using the connector | . | . ",
    "url": "/v0.70/integrations/airbyte.html#table-of-contents",
    "relUrl": "/integrations/airbyte.html#table-of-contents"
  },"5": {
    "doc": "Airbyte",
    "title": "Using lakeFS with Airbyte",
    "content": "The integration between the two open-source projects brings resilience and manageability when you use Airbyte connectors to sync data to your S3 buckets by leveraging lakeFS branches and atomic commits and merges. ",
    "url": "/v0.70/integrations/airbyte.html#using-lakefs-with-airbyte",
    "relUrl": "/integrations/airbyte.html#using-lakefs-with-airbyte"
  },"6": {
    "doc": "Airbyte",
    "title": "Use cases",
    "content": "You can take advantage of lakeFS consistency guarantees and CI/CD capabilities when ingesting data to S3 using lakeFS: . | Consolidate many data sources to a single branch and expose them to consumers simultaneously when merging to the main branch. | Test incoming data for breaking schema changes using lakeFS hooks. | Prevent consumers from reading partial data from connectors which failed half-way through sync. | Experiment with ingested data on a branch before exposing it. | . ",
    "url": "/v0.70/integrations/airbyte.html#use-cases",
    "relUrl": "/integrations/airbyte.html#use-cases"
  },"7": {
    "doc": "Airbyte",
    "title": "S3 Connector",
    "content": "lakeFS exposes an S3 Gateway that enables applications to communicate with lakeFS the same way they would with Amazon S3. You can use Airbyte’s S3 Destination to upload data to lakeFS. Configuring lakeFS using the connector . Set the following parameters when creating a new Destination of type S3: . | Name | Value | Example | . | Endpoint | The lakeFS S3 gateway URL | https://cute-axolotol.lakefs-demo.io | . | S3 Bucket Name | The lakeFS repository where the data will be written | example-repo | . | S3 Bucket Path | The branch and the path where the data will be written | main/data/from/airbyte Where main is the branch name, and data/from/airbyte is the path under the branch. | . | S3 Bucket Region | Not applicable to lakeFS, use us-east-1 | us-east-1 | . | S3 Key ID | The lakeFS access key id used to authenticate to lakeFS. | AKIAlakefs12345EXAMPLE | . | S3 Access Key | The lakeFS secret access key used to authenticate to lakeFS. | abc/lakefs/1234567bPxRfiCYEXAMPLEKEY | . Note The S3 Destination connector supports custom S3 endpoints starting with Airbyte’s version v0.26.0-alpha released on Jun 17th 2021 . The UI configuration will look as follows: . ",
    "url": "/v0.70/integrations/airbyte.html#s3-connector",
    "relUrl": "/integrations/airbyte.html#s3-connector"
  },"8": {
    "doc": "Airbyte",
    "title": "Airbyte",
    "content": "Airbyte is an open-source platform for syncing data from applications, APIs, and databases to warehouses, lakes, and other destinations. You can use Airbyte’s connectors to get your data pipelines to consolidate many input sources. ",
    "url": "/v0.70/integrations/airbyte.html",
    "relUrl": "/integrations/airbyte.html"
  },"9": {
    "doc": "Airflow",
    "title": "Using lakeFS with Airflow",
    "content": "Apache Airflow is a platform that allows users to programmatically author, schedule, and monitor workflows. To run Airflow with lakeFS, you need to follow a few steps. ",
    "url": "/v0.70/integrations/airflow.html#using-lakefs-with-airflow",
    "relUrl": "/integrations/airflow.html#using-lakefs-with-airflow"
  },"10": {
    "doc": "Airflow",
    "title": "Create a lakeFS connection on Airflow",
    "content": "To access the lakeFS server and authenticate with it, create a new Airflow Connection of type HTTP and add it to your DAG. You can do that using the Airflow UI or the CLI. Here’s an example Airflow command that does just that: . airflow connections add conn_lakefs --conn-type=HTTP --conn-host=http://&lt;LAKEFS_ENDPOINT&gt; \\ --conn-extra='{\"access_key_id\":\"&lt;LAKEFS_ACCESS_KEY_ID&gt;\",\"secret_access_key\":\"&lt;LAKEFS_SECRET_ACCESS_KEY&gt;\"}' . ",
    "url": "/v0.70/integrations/airflow.html#create-a-lakefs-connection-on-airflow",
    "relUrl": "/integrations/airflow.html#create-a-lakefs-connection-on-airflow"
  },"11": {
    "doc": "Airflow",
    "title": "Install the lakeFS Airflow package",
    "content": "You can use pip to install the package . pip install airflow-provider-lakefs . ",
    "url": "/v0.70/integrations/airflow.html#install-the-lakefs-airflow-package",
    "relUrl": "/integrations/airflow.html#install-the-lakefs-airflow-package"
  },"12": {
    "doc": "Airflow",
    "title": "Use the package",
    "content": "Operators . The package exposes several operations to interact with a lakeFS server: . | CreateBranchOperator creates a new lakeFS branch from the source branch (main by default). task_create_branch = CreateBranchOperator( task_id='create_branch', repo='example-repo', branch='example-branch', source_branch='main' ) . | CommitOperator commits uncommitted changes to a branch. task_commit = CommitOperator( task_id='commit', repo='example-repo', branch='example-branch', msg='committing to lakeFS using airflow!', metadata={'committed_from\": \"airflow-operator'} ) . | MergeOperator merges 2 lakeFS branches. task_merge = MergeOperator( task_id='merge_branches', source_ref='example-branch', destination_branch='main', msg='merging job outputs', metadata={'committer': 'airflow-operator'} ) . | . Sensors . Sensors are also available that allow synchronizing a running DAG with external operations: . | CommitSensor waits until a commit has been applied to the branch . task_sense_commit = CommitSensor( repo='example-repo', branch='example-branch', task_id='sense_commit' ) . | FileSensor waits until a given file is present on a branch. task_sense_file = FileSensor( task_id='sense_file', repo='example-repo', branch='example-branch', path=\"file/to/sense\" ) . | . Example . This example DAG in the airflow-provider-lakeFS repository shows how to use all of these. Performing other operations . Sometimes an operator might not be supported by airflow-provider-lakeFS yet. You can access lakeFS directly by using: . | SimpleHttpOperator to send API requests to lakeFS. | BashOperator with lakeCTL commands. For example, deleting a branch using BashOperator: commit_extract = BashOperator( task_id='delete_branch', bash_command='lakectl branch delete lakefs://example-repo/example-branch', dag=dag, ) . | . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. ",
    "url": "/v0.70/integrations/airflow.html#use-the-package",
    "relUrl": "/integrations/airflow.html#use-the-package"
  },"13": {
    "doc": "Airflow",
    "title": "Airflow",
    "content": " ",
    "url": "/v0.70/integrations/airflow.html",
    "relUrl": "/integrations/airflow.html"
  },"14": {
    "doc": "API Reference",
    "title": "API Reference",
    "content": "| ",
    "url": "/v0.70/reference/api.html",
    "relUrl": "/reference/api.html"
  },"15": {
    "doc": "Architecture",
    "title": "Architecture Overview",
    "content": " ",
    "url": "/v0.70/understand/architecture.html#architecture-overview",
    "relUrl": "/understand/architecture.html#architecture-overview"
  },"16": {
    "doc": "Architecture",
    "title": "Table of contents",
    "content": ". | Overview | Ways to deploy lakeFS . | Load Balancing | . | lakeFS Components . | S3 Gateway | OpenAPI Server | Storage Adapter | Graveler | Authentication &amp; Authorization Service | Hooks Engine | UI | . | Applications | lakeFS Clients . | OpenAPI Generated SDKs | lakectl | Spark Metadata Client | lakeFS Hadoop FileSystem | . | . ",
    "url": "/v0.70/understand/architecture.html#table-of-contents",
    "relUrl": "/understand/architecture.html#table-of-contents"
  },"17": {
    "doc": "Architecture",
    "title": "Overview",
    "content": "lakeFS is distributed as a single binary encapsulating several logical services: . The server itself is stateless, meaning you can easily add more instances to handle a bigger load. lakeFS stores data in an underlying object store (GCS, ABS, S3, or any S3-compatible stores like MinIO or Ceph), with some of its metadata stored in PostgreSQL (see Versioning internals). ",
    "url": "/v0.70/understand/architecture.html#overview",
    "relUrl": "/understand/architecture.html#overview"
  },"18": {
    "doc": "Architecture",
    "title": "Ways to deploy lakeFS",
    "content": "lakeFS releases include binaries for common operating systems, a containerized option or a Helm chart. Check out our guides for running lakeFS on K8S, ECS, Google Compute Engine and more. Load Balancing . Accessing lakeFS is done using HTTP. lakeFS exposes a frontend UI, an OpenAPI server, as well as an S3-compatible service (see S3 Gateway below). lakeFS uses a single port that serves all three endpoints, so for most use cases a single load balancer pointing to lakeFS server(s) would do. ",
    "url": "/v0.70/understand/architecture.html#ways-to-deploy-lakefs",
    "relUrl": "/understand/architecture.html#ways-to-deploy-lakefs"
  },"19": {
    "doc": "Architecture",
    "title": "lakeFS Components",
    "content": "S3 Gateway . The S3 Gateway implements lakeFS’s compatibility with S3. It implements a compatible subset of the S3 API to ensure most data systems can use lakeFS as a drop-in replacement for S3. See the S3 API Reference section for information on supported API operations. OpenAPI Server . The Swagger (OpenAPI) server exposes the full set of lakeFS operations (see Reference). This includes basic CRUD operations against repositories and objects, as well as versioning related operations such as branching, merging, committing, and reverting changes to data. Storage Adapter . The Storage Adapter is an abstraction layer for communicating with any underlying object store. Its implementations allow compatibility with many types of underlying storage such as S3, GCS, Azure Blob Storage, or non-production usages such as the local storage adapter. See the roadmap for information on the future plans for storage compatibility. Graveler . The Graveler handles lakeFS versioning by translating lakeFS addresses to the actual stored objects. To learn about the data model used to store lakeFS metadata, see the data model section. Authentication &amp; Authorization Service . The Auth service handles the creation, management, and validation of user credentials and RBAC policies. The credential scheme, along with the request signing logic, are compatible with AWS IAM (both SIGv2 and SIGv4). Currently, the Auth service manages its own database of users and credentials and doesn’t use IAM in any way. Hooks Engine . The Hooks Engine enables CI/CD for data by triggering user defined Actions that will run during commit/merge. UI . The UI layer is a simple browser-based client that uses the OpenAPI server. It allows management, exploration, and data access to repositories, branches, commits and objects in the system. ",
    "url": "/v0.70/understand/architecture.html#lakefs-components",
    "relUrl": "/understand/architecture.html#lakefs-components"
  },"20": {
    "doc": "Architecture",
    "title": "Applications",
    "content": "As a rule of thumb, lakeFS supports any S3-compatible application. This means that many common data applications work with lakeFS out-of-the-box. Check out our integrations to learn more. ",
    "url": "/v0.70/understand/architecture.html#applications",
    "relUrl": "/understand/architecture.html#applications"
  },"21": {
    "doc": "Architecture",
    "title": "lakeFS Clients",
    "content": "Some data applications benefit from deeper integrations with lakeFS to support different use cases or enhanced functionality provided by lakeFS clients. OpenAPI Generated SDKs . OpenAPI specification can be used to generate lakeFS clients for many programming languages. For example, the Python lakefs-client or the Java client are published with every new lakeFS release. lakectl . lakectl is a CLI tool that enables lakeFS operations using the lakeFS API from your preferred terminal. Spark Metadata Client . The lakeFS Spark Metadata Client makes it easy to perform operations related to lakeFS metadata, at scale. Examples include garbage collection or exporting data from lakeFS. lakeFS Hadoop FileSystem . Thanks to the S3 Gateway, it’s possible to interact with lakeFS using Hadoop’s S3AFIleSystem, but due to limitations of the S3 API, doing so requires reading and writing data objects through the lakeFS server. Using lakeFSFileSystem increases Spark ETL jobs performance by executing the metadata operations on the lakeFS server, and all data operations directly through the same underlying object store that lakeFS uses. ",
    "url": "/v0.70/understand/architecture.html#lakefs-clients",
    "relUrl": "/understand/architecture.html#lakefs-clients"
  },"22": {
    "doc": "Architecture",
    "title": "Architecture",
    "content": " ",
    "url": "/v0.70/understand/architecture.html",
    "relUrl": "/understand/architecture.html"
  },"23": {
    "doc": "Amazon Athena",
    "title": "Using lakeFS with Amazon Athena",
    "content": "Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Amazon Athena works directly above S3 and can’t access lakeFS. Tables created using Athena aren’t readable by lakeFS. However, tables stored in lakeFS (that were created with glue/hive) can be queried by Athena. To support querying data from lakeFS with Amazon Athena, we will use create-symlink, one of the metastore commands in lakectl. create-symlink receives a source table, destination table, and the table location. It performs two actions: . | It creates partitioned directories with symlink files in the underlying S3 bucket. | It creates a table in Glue catalog with symlink format type and location pointing to the created symlinks. | . Note .lakectl.yaml file should be configured with the proper hive/glue credentials. For more information . create-symlink receives a table in glue or hive pointing to lakeFS and creates a copy of the table in glue. The table data will use the SymlinkTextInputFormat, which will point to the lakeFS repository storage namespace. You will be able to query your data with Athena without copying any data. However, the symlinks table will only show the data that existed during the copy. If the table changed in lakeFS, you need to run create-symlink again for your changed to be reflected in Athena. Example: . Let’s assume that some time ago, we created a hive table my_table that is stored in lakeFS repo example under branch main, using the command: . CREATE EXTERNAL TABLE `my_table`( `id` bigint, `key` string ) PARTITIONED BY (YEAR INT, MONTH INT) LOCATION 's3://example/main/my_table'; WITH (format = 'PARQUET', external_location 's3a://example/main/my_table' ); . The repository example has the S3 storage space s3://my-bucket/my-repo-prefix/. After inserting some data into it, the object structure under lakefs://example/main/my_table looks as follows: . To query that table with Athena, you need to use the create-symlink command as follows: . lakectl metastore create-symlink \\ --repo example \\ --branch main \\ --path my_table \\ --from-client-type hive \\ --from-schema default \\ --from-table my_table \\ --to-schema default \\ --to-table my_table . The command will generate two notable outputs: . | For each partition, the command will create a symlink file: | . ➜ aws s3 ls s3://my-bucket/my-repo-prefix/my_table/ --recursive 2021-11-23 17:46:29 0 my-repo-prefix/my_table/symlinks/example/main/my_table/year=2021/month=11/symlink.txt 2021-11-23 17:46:29 60 my-repo-prefix/my_table/symlinks/example/main/my_table/year=2021/month=12/symlink.txt 2021-11-23 17:46:30 60 my-repo-prefix/my_table/symlinks/example/main/my_table/year=2022/month=1/symlink.txt . An example content of a symlink file, where each line represents a single object of the specific partition: . s3://my-bucket/my-repo-prefix/5bdc62da516944b49889770d98274227 s3://my-bucket/my-repo-prefix/64262fbf3d6347a79ead641d2b2baee6 s3://my-bucket/my-repo-prefix/64486c8de6484de69f12d7d26804c93e s3://my-bucket/my-repo-prefix/b0165d5c5b13473d8a0f460eece9eb26 . | A glue table pointing to the symlink directories structure: | . aws glue get-table --name my_table --database-name default { \"Table\": { \"Name\": \"my_table\", \"DatabaseName\": \"default\", \"Owner\": \"anonymous\", \"CreateTime\": \"2021-11-23T17:46:30+02:00\", \"UpdateTime\": \"2021-11-23T17:46:30+02:00\", \"LastAccessTime\": \"1970-01-01T02:00:00+02:00\", \"Retention\": 0, \"StorageDescriptor\": { \"Columns\": [ { \"Name\": \"id\", \"Type\": \"bigint\", \"Comment\": \"\" }, { \"Name\": \"key\", \"Type\": \"string\", \"Comment\": \"\" } ], \"Location\": \"s3://my-bucket/my-repo-prefix/symlinks/example/main/my_table\", \"InputFormat\": \"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat\", \"OutputFormat\": \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\", \"Compressed\": false, \"NumberOfBuckets\": -1, \"SerdeInfo\": { \"Name\": \"default\", \"SerializationLibrary\": \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\", \"Parameters\": { \"serialization.format\": \"1\" } }, \"StoredAsSubDirectories\": false }, \"PartitionKeys\": [ { \"Name\": \"year\", \"Type\": \"int\", \"Comment\": \"\" }, { \"Name\": \"month\", \"Type\": \"int\", \"Comment\": \"\" } ], \"ViewOriginalText\": \"\", \"ViewExpandedText\": \"\", \"TableType\": \"EXTERNAL_TABLE\", \"Parameters\": { \"EXTERNAL\": \"TRUE\", \"bucketing_version\": \"2\", \"transient_lastDdlTime\": \"1637681750\" }, \"CreatedBy\": \"arn:aws:iam::************:user/********\", \"IsRegisteredWithLakeFormation\": false, \"CatalogId\": \"*********\" } } . You can now safely use Athena to query my_table. ",
    "url": "/v0.70/integrations/athena.html#using-lakefs-with-amazon-athena",
    "relUrl": "/integrations/athena.html#using-lakefs-with-amazon-athena"
  },"24": {
    "doc": "Amazon Athena",
    "title": "Amazon Athena",
    "content": " ",
    "url": "/v0.70/integrations/athena.html",
    "relUrl": "/integrations/athena.html"
  },"25": {
    "doc": "Authentication",
    "title": "Authentication",
    "content": " ",
    "url": "/v0.70/reference/authentication.html",
    "relUrl": "/reference/authentication.html"
  },"26": {
    "doc": "Authentication",
    "title": "Table of contents",
    "content": ". | Authentication . | Authentication . | User Authentication . | Built-in database | LDAP server | . | API Server Authentication | S3 Gateway Authentication | . | . | . ",
    "url": "/v0.70/reference/authentication.html#table-of-contents",
    "relUrl": "/reference/authentication.html#table-of-contents"
  },"27": {
    "doc": "Authentication",
    "title": "Authentication",
    "content": "User Authentication . lakeFS authenticates users from a built-in authentication database, or, optionally, from a configured LDAP server. Built-in database . The built-in authentication database is always present and active. You can use the Web UI at Administration / Users to create users. Users have an access key AKIA... and an associated secret access key. These credentials are valid for logging into the Web UI or authenticating programmatic requests to the API Server or the S3 Gateway. LDAP server . Configure lakeFS to authenticate users on an LDAP server. Once configured, users can additionally log into lakeFS using their credentials LDAP. These users may then generate an access key and a secret access key on the Web UI at Administration / My Credentials. lakeFS generates an internal user once logged in via the LDAP server. Adding this internal user to a group allows assigning them a different policy. Configure the LDAP server using these fields in auth.ldap: . | server_endpoint: the ldaps: (or ldap:) URL of the LDAP server. | bind_dn, bind_password: Credentials for lakeFS to use to query the LDAP server for users. They must identify a user with Basic Authentication, and are used to convert a user ID attribute to a full user DN. | default_user_group: A group to add users the first time they log in using LDAP. Typically “Viewers” or “Developers”. Once logged in, LDAP users may be added as normal to any other group. | username_attribute: Attribute on LDAP user to identify user when logging in. Typically “uid” or “cn”. | user_base_dn: DN of root of DAP tree containing users, e.g. ou=Users,dc=treeverse,dc=io. | user_filter: An additional filter for users allowed to login, e.g. (objectClass=person). | . LDAP users log in using the following flow: . | Bind the lakeFS control connection to server_endpoint using bind_dn, bind_password. | Receive an LDAP user-ID (e.g. “joebloggs”) and a password entered on the Web UI login page. | Attempt to log in as internally-defined users; fail. | Search the LDAP server using the control connection for the user: out of all users under user_base_dn that satisfy user_filter, there must be a single user whose username_attribute was specified by the user. Get their DN. In our example, this may be uid=joebloggs,ou=Users,dc=treeverse,dc=io (this entry must have objectClass: person because of user_filter). | Attempt to bind the received DN on the LDAP server using the password. | On success, the user is authenticated. | Create a new internal user with that DN if needed. When creating a user, add them to the internal group named default_user_group. | . API Server Authentication . Authenticating against the API server is done using a key-pair, passed via Basic Access Authentication. All HTTP requests must carry an Authorization header with the following structure: . Authorization: Basic &lt;base64 encoded access_key_id:secret_access_key&gt; . For example, assuming my access_key_id is my_access_key_id and my secret_access_key is my_secret_access_key, we’d send the following header with every request: . Authorization: Basic bXlfYWNjZXNzX2tleV9pZDpteV9hY2Nlc3Nfc2VjcmV0X2tleQ== . S3 Gateway Authentication . To provide API compatibility with Amazon S3, authentication with the S3 Gateway supports both SIGv2 and SIGv4. Clients such as the AWS SDK that implement these authentication methods should work without modification. See this example for authenticating with the AWS CLI. ",
    "url": "/v0.70/reference/authentication.html",
    "relUrl": "/reference/authentication.html"
  },"28": {
    "doc": "Authorization",
    "title": "Authorization",
    "content": " ",
    "url": "/v0.70/reference/authorization.html",
    "relUrl": "/reference/authorization.html"
  },"29": {
    "doc": "Authorization",
    "title": "Table of contents",
    "content": ". | Authorization . | Authorization Model | Authorization process | Policy Precedence | Resource naming - ARNs | Actions and Permissions | Preconfigured Policies . | FSFullAccess | FSReadAll | FSReadWriteAll | AuthFullAccess | AuthManageOwnCredentials | RepoManagementFullAccess | RepoManagementReadAll | ExportSetConfiguration | . | Additional Policies . | Read/write access for a specific repository | . | Preconfigured Groups . | Admins | SuperUsers | Developers | Viewers | . | . | . ",
    "url": "/v0.70/reference/authorization.html#table-of-contents",
    "relUrl": "/reference/authorization.html#table-of-contents"
  },"30": {
    "doc": "Authorization",
    "title": "Authorization",
    "content": "Authorization Model . Access to resources is managed very much like AWS IAM. There are five basic components to the system: . | Users - Representing entities that access and use the system. A user is given one or more Access Credentials for authentication. | Actions - Representing a logical action within the system - reading a file, creating a repository, etc. | Resources - A unique identifier representing a specific resource in the system - a repository, an object, a user, etc. | Policies - Representing a set of Actions, a Resource and an effect: whether or not these actions are allowed or denied for the given resource(s). | Groups - A named collection of users. Users can belong to multiple groups. | . Controlling access is done by attaching Policies, either directly to Users, or to Groups they belong to. Authorization process . Every action in the system - be it an API request, UI interaction, S3 Gateway call, or CLI command - requires a set of actions to be allowed for one or more resources. When a user makes a request to perform that action, the following process takes place: . | Authentication - the credentials passed in the request are evaluated and the user’s identity is extracted. | Action permission resolution - lakeFS then calculates the set of allowed actions and resources that this request requires. | Effective policy resolution - the user’s policies (either attached directly or through group memberships) are calculated. | Policy/Permission evaluation - lakeFS will compare the given user policies with the request actions and determine whether or not the request is allowed to continue. | . Policy Precedence . Each policy attached to a user or a group has an Effect - either allow or deny. During evaluation of a request, deny would take precedence over any other allow policy. This helps us compose policies together. For example, we could attach a very permissive policy to a user and use deny rules to then selectively restrict what that user can do. Resource naming - ARNs . lakeFS uses ARN identifier - very similar in structure to those used by AWS. The resource segment of the ARN supports wildcards: use * to match 0 or more characters, or ? to match exactly one character. Additionally, the current user’s ID is interpolated in runtime into the ARN using the ${user} placeholder. Here are a few examples of valid ARNs within lakeFS: . arn:lakefs:auth:::user/jane.doe arn:lakefs:auth:::user/* arn:lakefs:fs:::repository/myrepo/* arn:lakefs:fs:::repository/myrepo/object/foo/bar/baz arn:lakefs:fs:::repository/myrepo/object/* arn:lakefs:fs:::repository/* arn:lakefs:fs:::* . this allows us to create fine-grained policies affecting only a specific subset of resources. See below for a full reference of ARNs and actions. Actions and Permissions . For the full list of actions and their required permissions see the following table: . | Action name | required action | Resource | API endpoint | S3 gateway operation | . | List Repositories | fs:ListRepositories | * | GET /repositories | ListBuckets | . | Get Repository | fs:ReadRepository | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId} | HeadBucket | . | Get Commit | fs:ReadCommit | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/commits/{commitId} | - | . | Create Commit | fs:CreateCommit | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | POST /repositories/{repositoryId}/branches/{branchId}/commits | - | . | Get Commit log | fs:ReadBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | GET /repositories/{repositoryId}/branches/{branchId}/commits | - | . | Create Repository | fs:CreateRepository | arn:lakefs:fs:::repository/{repositoryId} | POST /repositories | - | . | Namespace Attach to Repository | fs:AttachStorageNamespace | arn:lakefs:fs:::namespace/{storageNamespace} | POST /repositories | - | . | Delete Repository | fs:DeleteRepository | arn:lakefs:fs:::repository/{repositoryId} | DELETE /repositories/{repositoryId} | - | . | List Branches | fs:ListBranches | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/branches | ListObjects/ListObjectsV2 (with delimiter = / and empty prefix) | . | Get Branch | fs:ReadBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | GET /repositories/{repositoryId}/branches/{branchId} | - | . | Create Branch | fs:CreateBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | POST /repositories/{repositoryId}/branches | - | . | Delete Branch | fs:DeleteBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | DELETE /repositories/{repositoryId}/branches/{branchId} | - | . | Merge branches | fs:CreateCommit | arn:lakefs:fs:::repository/{repositoryId}/branch/{destinationBranchId} | POST /repositories/{repositoryId}/refs/{sourceBranchId}/merge/{destinationBranchId} | - | . | Diff branch uncommitted changes | fs:ListObjects | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/branches/{branchId}/diff | - | . | Diff refs | fs:ListObjects | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/refs/{leftRef}/diff/{rightRef} | - | . | Stat object | fs:ReadObject | arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey} | GET /repositories/{repositoryId}/refs/{ref}/objects/stat | HeadObject | . | Get Object | fs:ReadObject | arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey} | GET /repositories/{repositoryId}/refs/{ref}/objects | GetObject | . | List Objects | fs:ListObjects | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/refs/{ref}/objects/ls | ListObjects, ListObjectsV2 (no delimiter, or “/” + non-empty prefix) | . | Upload Object | fs:WriteObject | arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey} | POST /repositories/{repositoryId}/branches/{branchId}/objects | PutObject, CreateMultipartUpload, UploadPart, CompleteMultipartUpload | . | Delete Object | fs:DeleteObject | arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey} | DELETE /repositories/{repositoryId}/branches/{branchId}/objects | DeleteObject, DeleteObjects, AbortMultipartUpload | . | Revert Branch | fs:RevertBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | PUT /repositories/{repositoryId}/branches/{branchId} | - | . | Create User | auth:CreateUser | arn:lakefs:auth:::user/{userId} | POST /auth/users | - | . | List Users | auth:ListUsers | * | GET /auth/users | - | . | Get User | auth:ReadUser | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId} | - | . | Delete User | auth:DeleteUser | arn:lakefs:auth:::user/{userId} | DELETE /auth/users/{userId} | - | . | Get Group | auth:ReadGroup | arn:lakefs:auth:::group/{groupId} | GET /auth/groups/{groupId} | - | . | List Groups | auth:ListGroups | * | GET /auth/groups | - | . | Create Group | auth:CreateGroup | arn:lakefs:auth:::group/{groupId} | POST /auth/groups | - | . | Delete Group | auth:DeleteGroup | arn:lakefs:auth:::group/{groupId} | DELETE /auth/groups/{groupId} | - | . | List Policies | auth:ListPolicies | * | GET /auth/policies | - | . | Create Policy | auth:CreatePolicy | arn:lakefs:auth:::policy/{policyId} | POST /auth/policies | - | . | Update Policy | auth:UpdatePolicy | arn:lakefs:auth:::policy/{policyId} | POST /auth/policies | - | . | Delete Policy | auth:DeletePolicy | arn:lakefs:auth:::policy/{policyId} | DELETE /auth/policies/{policyId} | - | . | Get Policy | auth:ReadPolicy | arn:lakefs:auth:::policy/{policyId} | GET /auth/policies/{policyId} | - | . | List Group Members | auth:ReadGroup | arn:lakefs:auth:::group/{groupId} | GET /auth/groups/{groupId}/members | - | . | Add Group Member | auth:AddGroupMember | arn:lakefs:auth:::group/{groupId} | PUT /auth/groups/{groupId}/members/{userId} | - | . | Remove Group Member | auth:RemoveGroupMember | arn:lakefs:auth:::group/{groupId} | DELETE /auth/groups/{groupId}/members/{userId} | - | . | List User Credentials | auth:ListCredentials | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId}/credentials | - | . | Create User Credentials | auth:CreateCredentials | arn:lakefs:auth:::user/{userId} | POST /auth/users/{userId}/credentials | - | . | Delete User Credentials | auth:DeleteCredentials | arn:lakefs:auth:::user/{userId} | DELETE /auth/users/{userId}/credentials/{accessKeyId} | - | . | Get User Credentials | auth:ReadCredentials | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId}/credentials/{accessKeyId} | - | . | List User Groups | auth:ReadUser | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId}/groups | - | . | List User Policies | auth:ReadUser | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId}/policies | - | . | Attach Policy To User | auth:AttachPolicy | arn:lakefs:auth:::user/{userId} | PUT /auth/users/{userId}/policies/{policyId} | - | . | Detach Policy From User | auth:DetachPolicy | arn:lakefs:auth:::user/{userId} | DELETE /auth/users/{userId}/policies/{policyId} | - | . | List Group Policies | auth:ReadGroup | arn:lakefs:auth:::group/{groupId} | GET /auth/groups/{groupId}/policies | - | . | Attach Policy To Group | auth:AttachPolicy | arn:lakefs:auth:::group/{groupId} | PUT /auth/groups/{groupId}/policies/{policyId} | - | . | Detach Policy From Group | auth:DetachPolicy | arn:lakefs:auth:::group/{groupId} | DELETE /auth/groups/{groupId}/policies/{policyId} | - | . | Read Storage Config | fs:ReadConfig | * | GET /config/storage | - | . | Get Garbage Collection Rules | retention:GetGarbageCollectionRules | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/gc/rules | - | . | Set Garbage Collection Rules | retention:SetGarbageCollectionRules | arn:lakefs:fs:::repository/{repositoryId} | POST /repositories/{repositoryId}/gc/rules | - | . | Prepare Garbage Collection Commits | retention:PrepareGarbageCollectionCommits | arn:lakefs:fs:::repository/{repositoryId} | POST /repositories/{repositoryId}/gc/prepare_commits | - | . | List Repository Action Runs | ci:ReadAction | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repository}/actions/runs | - | . | Get Action Run | ci:ReadAction | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repository}/actions/runs/{run_id} | - | . | List Action Run Hooks | ci:ReadAction | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repository}/actions/runs/{run_id}/hooks | - | . | Get Action Run Hook Output | ci:ReadAction | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repository}/actions/runs/{run_id}/hooks/{hook_run_id}/output | - | . Some APIs may require more than one action.For instance, in order to create a repository (POST /repositories), you need permission to fs:CreateRepository for the name of the repository and also fs:AttachStorageNamespace for the storage namespace used. Preconfigured Policies . The following Policies are created during initial setup: . FSFullAccess . Policy: . { \"statement\": [ { \"action\": [ \"fs:*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . FSReadAll . Policy: . { \"statement\": [ { \"action\": [ \"fs:List*\", \"fs:Read*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . FSReadWriteAll . Policy: . { \"statement\": [ { \"action\": [ \"fs:ListRepositories\", \"fs:ReadRepository\", \"fs:ReadCommit\", \"fs:ListBranches\", \"fs:ListObjects\", \"fs:ReadObject\", \"fs:WriteObject\", \"fs:DeleteObject\", \"fs:RevertBranch\", \"fs:ReadBranch\", \"fs:CreateBranch\", \"fs:DeleteBranch\", \"fs:CreateCommit\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . AuthFullAccess . Policy: . { \"statement\": [ { \"action\": [ \"auth:*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . AuthManageOwnCredentials . Policy: . { \"statement\": [ { \"action\": [ \"auth:CreateCredentials\", \"auth:DeleteCredentials\", \"auth:ListCredentials\", \"auth:ReadCredentials\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:auth:::user/${user}\" } ] } . RepoManagementFullAccess . Policy: . { \"statement\": [ { \"action\": [ \"ci:*\" ], \"effect\": \"allow\", \"resource\": \"*\" }, { \"action\": [ \"retention:*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . RepoManagementReadAll . Policy: . { \"statement\": [ { \"action\": [ \"ci:Read*\" ], \"effect\": \"allow\", \"resource\": \"*\" }, { \"action\": [ \"retention:Get*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . ExportSetConfiguration . Additional Policies . The following examples can be used to create additional policies to further limit user access. Use the web UI or the lakectl auth command to create policies. Read/write access for a specific repository . Policy: . { \"statement\": [ { \"action\": [ \"fs:ReadRepository\", \"fs:ReadCommit\", \"fs:ListBranches\", \"fs:ListTags\", \"fs:ListObjects\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;\" }, { \"action\": [ \"fs:RevertBranch\", \"fs:ReadBranch\", \"fs:CreateBranch\", \"fs:DeleteBranch\", \"fs:CreateCommit\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;/branch/*\" }, { \"action\": [ \"fs:ListObjects\", \"fs:ReadObject\", \"fs:WriteObject\", \"fs:DeleteObject\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;/object/*\" }, { \"action\": [ \"fs:ReadTag\", \"fs:CreateTag\", \"fs:DeleteTag\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;/tag/*\" }, { \"action\": [\"fs:ReadConfig\"], \"effect\": \"allow\", \"resource\": \"*\" } ] } . Preconfigured Groups . Admins . Policies: [\"FSFullAccess\", \"AuthFullAccess\", \"RepoManagementFullAccess\", \"ExportSetConfiguration\"] . SuperUsers . Policies: [\"FSFullAccess\", \"AuthManageOwnCredentials\", \"RepoManagementReadAll\"] . Developers . Policies: [\"FSReadWriteAll\", \"AuthManageOwnCredentials\", \"RepoManagementReadAll\"] . Viewers . Policies: [\"FSReadAll\", \"AuthManageOwnCredentials\"] . ",
    "url": "/v0.70/reference/authorization.html",
    "relUrl": "/reference/authorization.html"
  },"31": {
    "doc": "On AWS",
    "title": "Deploy lakeFS on AWS",
    "content": "Expected deployment time: 25 min . ",
    "url": "/v0.70/deploy/aws.html#deploy-lakefs-on-aws",
    "relUrl": "/deploy/aws.html#deploy-lakefs-on-aws"
  },"32": {
    "doc": "On AWS",
    "title": "Table of contents",
    "content": ". | Prerequisites | Creating the Database on AWS RDS | Installation Options . | On EC2 | On ECS | On EKS | . | Load balancing | Next Steps | . ",
    "url": "/v0.70/deploy/aws.html#table-of-contents",
    "relUrl": "/deploy/aws.html#table-of-contents"
  },"33": {
    "doc": "On AWS",
    "title": "Prerequisites",
    "content": "Users that require S3 access using virtual host addressing should configure an S3 Gateway domain. ",
    "url": "/v0.70/deploy/aws.html#prerequisites",
    "relUrl": "/deploy/aws.html#prerequisites"
  },"34": {
    "doc": "On AWS",
    "title": "Creating the Database on AWS RDS",
    "content": "lakeFS requires a PostgreSQL database to synchronize actions on your repositories. We will show you how to create a database on AWS RDS but you can use any PostgreSQL database as long as it’s accessible by your lakeFS installation. If you already have a database, take note of the connection string and skip to the next step . | Follow the official AWS documentation on how to create a PostgreSQL instance and connect to it. You may use the default PostgreSQL engine, or Aurora PostgreSQL. Make sure that you’re using PostgreSQL version &gt;= 11. | Once your RDS is set up and the server is in Available state, take note of the endpoint and port. | Make sure your security group rules allow you to connect to the database instance. | . ",
    "url": "/v0.70/deploy/aws.html#creating-the-database-on-aws-rds",
    "relUrl": "/deploy/aws.html#creating-the-database-on-aws-rds"
  },"35": {
    "doc": "On AWS",
    "title": "Installation Options",
    "content": "On EC2 . | Save the following configuration file as config.yaml: . --- database: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: # replace this with a randomly-generated string: secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: s3 s3: region: us-east-1 # optional, fallback in case discover from bucket is not supported . | Download the binary to the EC2 instance. | Run the lakefs binary on the EC2 instance: lakefs --config config.yaml run . Note: It’s preferable to run the binary as a service using systemd or your operating system’s facilities. | . On ECS . To support container-based environments like AWS ECS, lakeFS can be configured using environment variables. Here is a docker run command to demonstrate starting lakeFS using Docker: . docker run \\ --name lakefs \\ -p 8000:8000 \\ -e LAKEFS_DATABASE_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\ -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\ -e LAKEFS_BLOCKSTORE_TYPE=\"s3\" \\ treeverse/lakefs:latest run . See the reference for a complete list of environment variables. On EKS . See Kubernetes Deployment. ",
    "url": "/v0.70/deploy/aws.html#installation-options",
    "relUrl": "/deploy/aws.html#installation-options"
  },"36": {
    "doc": "On AWS",
    "title": "Load balancing",
    "content": "Depending on how you chose to install lakeFS, you should have a load balancer direct requests to the lakeFS server. By default, lakeFS operates on port 8000, and exposes a /_health endpoint which you can use for health checks. Notes for using an AWS Application Load Balancer . | Your security groups should allow the load balancer to access the lakeFS server. | Create a target group with a listener for port 8000. | Setup TLS termination using the domain names you wish to use (e.g., lakefs.example.com and potentially s3.lakefs.example.com, *.s3.lakefs.example.com if using virtual-host addressing). | Configure the health-check to use the exposed /_health URL | . ",
    "url": "/v0.70/deploy/aws.html#load-balancing",
    "relUrl": "/deploy/aws.html#load-balancing"
  },"37": {
    "doc": "On AWS",
    "title": "Next Steps",
    "content": "Your next step is to prepare your storage. If you already have a storage bucket/container, you’re ready to create your first lakeFS repository. ",
    "url": "/v0.70/deploy/aws.html#next-steps",
    "relUrl": "/deploy/aws.html#next-steps"
  },"38": {
    "doc": "On AWS",
    "title": "On AWS",
    "content": " ",
    "url": "/v0.70/deploy/aws.html",
    "relUrl": "/deploy/aws.html"
  },"39": {
    "doc": "AWS CLI",
    "title": "Using lakeFS with AWS CLI",
    "content": "The AWS Command Line Interface (CLI) is a unified tool for managing your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts. You can use the file commands for S3 to access lakeFS. ",
    "url": "/v0.70/integrations/aws_cli.html#using-lakefs-with-aws-cli",
    "relUrl": "/integrations/aws_cli.html#using-lakefs-with-aws-cli"
  },"40": {
    "doc": "AWS CLI",
    "title": "Table of contents",
    "content": ". | Configuration | Path convention | Usage | Examples . | List directory | Copy from lakeFS to lakeFS | Copy from lakeFS to a local path | Copy from a local path to lakeFS | Delete file | Delete directory | . | Adding an alias | . ",
    "url": "/v0.70/integrations/aws_cli.html#table-of-contents",
    "relUrl": "/integrations/aws_cli.html#table-of-contents"
  },"41": {
    "doc": "AWS CLI",
    "title": "Configuration",
    "content": "You would like to configure an AWS profile for lakeFS. To configure the lakeFS credentials, run: . aws configure --profile lakefs . You will be prompted to enter AWS Access Key ID and AWS Secret Access Key. It should look like this: . aws configure --profile lakefs # output: # AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE # AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # Default region name [None]: # Default output format [None]: . ",
    "url": "/v0.70/integrations/aws_cli.html#configuration",
    "relUrl": "/integrations/aws_cli.html#configuration"
  },"42": {
    "doc": "AWS CLI",
    "title": "Path convention",
    "content": "When accessing objects in S3, you will need to use the lakeFS path convention: s3://[REPOSITORY]/[BRANCH]/PATH/TO/OBJECT . ",
    "url": "/v0.70/integrations/aws_cli.html#path-convention",
    "relUrl": "/integrations/aws_cli.html#path-convention"
  },"43": {
    "doc": "AWS CLI",
    "title": "Usage",
    "content": "After configuring the credentials, this is what a command should look: . aws s3 --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ ls s3://example-repo/main/example-directory . You can use an alias to make it shorter and more convenient. ",
    "url": "/v0.70/integrations/aws_cli.html#usage",
    "relUrl": "/integrations/aws_cli.html#usage"
  },"44": {
    "doc": "AWS CLI",
    "title": "Examples",
    "content": "List directory . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 ls s3://example-repo/main/example-directory . Copy from lakeFS to lakeFS . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 cp s3://example-repo/main/example-file-1 s3://example-repo/main/example-file-2 . Copy from lakeFS to a local path . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 cp s3://example-repo/main/example-file-1 /path/to/local/file . Copy from a local path to lakeFS . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 cp /path/to/local/file s3://example-repo/main/example-file-1 . Delete file . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 rm s3://example-repo/main/example-directory/example-file . Delete directory . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 rm s3://example-repo/main/example-directory/ --recursive . ",
    "url": "/v0.70/integrations/aws_cli.html#examples",
    "relUrl": "/integrations/aws_cli.html#examples"
  },"45": {
    "doc": "AWS CLI",
    "title": "Adding an alias",
    "content": "To make the command shorter and more convenient, you can create an alias: . alias awslfs='aws --endpoint https://lakefs.example.com --profile lakefs' . Now, the ls command using the alias will be as follows: . awslfs s3 ls s3://example-repo/main/example-directory . ",
    "url": "/v0.70/integrations/aws_cli.html#adding-an-alias",
    "relUrl": "/integrations/aws_cli.html#adding-an-alias"
  },"46": {
    "doc": "AWS CLI",
    "title": "AWS CLI",
    "content": " ",
    "url": "/v0.70/integrations/aws_cli.html",
    "relUrl": "/integrations/aws_cli.html"
  },"47": {
    "doc": "On Azure",
    "title": "Deploy lakeFS on Azure",
    "content": "Expected deployment time: 25 min . ",
    "url": "/v0.70/deploy/azure.html#deploy-lakefs-on-azure",
    "relUrl": "/deploy/azure.html#deploy-lakefs-on-azure"
  },"48": {
    "doc": "On Azure",
    "title": "Table of contents",
    "content": ". | Prerequisites | Creating the Database on Azure Database | Installation Options . | On Azure VM | On Azure Container instances | On AKS | . | Load balancing | Next Steps | . ",
    "url": "/v0.70/deploy/azure.html#table-of-contents",
    "relUrl": "/deploy/azure.html#table-of-contents"
  },"49": {
    "doc": "On Azure",
    "title": "Prerequisites",
    "content": "Users that require S3 access using virtual host addressing should configure an S3 Gateway domain. ",
    "url": "/v0.70/deploy/azure.html#prerequisites",
    "relUrl": "/deploy/azure.html#prerequisites"
  },"50": {
    "doc": "On Azure",
    "title": "Creating the Database on Azure Database",
    "content": "lakeFS requires a PostgreSQL database to synchronize actions in your repositories. We will show you how to create a database on Azure Database, but you can use any PostgreSQL database as long as it’s accessible by your lakeFS installation. If you already have a database, take note of the connection string and skip to the next step . | Follow the official Azure documentation on how to create a PostgreSQL instance and connect to it. Make sure that you’re using PostgreSQL version &gt;= 11. | Once your Azure Database for PostgreSQL server is set up and the server is in the Available state, take note of the endpoint and username. | Make sure your Access control roles allow you to connect to the database instance. | . ",
    "url": "/v0.70/deploy/azure.html#creating-the-database-on-azure-database",
    "relUrl": "/deploy/azure.html#creating-the-database-on-azure-database"
  },"51": {
    "doc": "On Azure",
    "title": "Installation Options",
    "content": "On Azure VM . | Save the following configuration file as config.yaml: . --- database: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: # replace this with a randomly-generated string: secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: azure azure: auth_method: msi # msi for active directory, access-key for access key # In case you chose to authenticate via access key, unmark the following rows and insert the values from the previous step # storage_account: [your storage account] # storage_access_key: [your access key] . | Download the binary to the Azure Virtual Machine. | Run the lakefs binary on the machine: lakefs --config config.yaml run . Note: It is preferable to run the binary as a service using systemd or your operating system’s facilities. | To support Azure AD authentication go to Identity tab and switch Status toggle to on, then add the `Storage Blob Data Contributor’ role on the container you created. | . On Azure Container instances . To support container-based environments like Azure Container Instances, you can configure lakeFS using environment variables. Here is a docker run command to demonstrate starting lakeFS using Docker: . docker run \\ --name lakefs \\ -p 8000:8000 \\ -e LAKEFS_DATABASE_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\ -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\ -e LAKEFS_BLOCKSTORE_TYPE=\"azure\" \\ -e LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCOUNT=\"[YOUR_STORAGE_ACCOUNT]\" \\ -e LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCESS_KEY=\"[YOUR_ACCESS_KEY]\" \\ treeverse/lakefs:latest run . See the reference for a complete list of environment variables. On AKS . See Kubernetes Deployment. ",
    "url": "/v0.70/deploy/azure.html#installation-options",
    "relUrl": "/deploy/azure.html#installation-options"
  },"52": {
    "doc": "On Azure",
    "title": "Load balancing",
    "content": "Depending on how you chose to install lakeFS, you should have a load balancer direct requests to the lakeFS server. By default, lakeFS operates on port 8000, and exposes a /_health endpoint which you can use for health checks. ",
    "url": "/v0.70/deploy/azure.html#load-balancing",
    "relUrl": "/deploy/azure.html#load-balancing"
  },"53": {
    "doc": "On Azure",
    "title": "Next Steps",
    "content": "Your next step is to prepare your storage. If you already have a storage bucket/container, you are ready to create your first lakeFS repository. ",
    "url": "/v0.70/deploy/azure.html#next-steps",
    "relUrl": "/deploy/azure.html#next-steps"
  },"54": {
    "doc": "On Azure",
    "title": "On Azure",
    "content": " ",
    "url": "/v0.70/deploy/azure.html",
    "relUrl": "/deploy/azure.html"
  },"55": {
    "doc": "Azure Blob Storage",
    "title": "Prepare Your Blob Storage Container",
    "content": "Create a container in the Azure portal: . | From the Azure portal, go to Storage Accounts and choose your account. Next, click + Container in the container tab. | Make sure you block public access | . ",
    "url": "/v0.70/setup/storage/blob.html#prepare-your-blob-storage-container",
    "relUrl": "/setup/storage/blob.html#prepare-your-blob-storage-container"
  },"56": {
    "doc": "Azure Blob Storage",
    "title": "Authenticate with a Secret Key",
    "content": "If you want lakeFS to authenticate with your storage using the storage account key, go to the Access Keys tab and click Show Keys. Use the values under Storage account name and Key in the lakeFS configuration. ",
    "url": "/v0.70/setup/storage/blob.html#authenticate-with-a-secret-key",
    "relUrl": "/setup/storage/blob.html#authenticate-with-a-secret-key"
  },"57": {
    "doc": "Azure Blob Storage",
    "title": "Authenticate with Active Directory",
    "content": "In case you want your lakeFS installation to access this Container using Active Directory authentication, first go to the container you created in step 1. | Go to Access Control (IAM) | Go to the Role assignments tab | Add the Storage Blob Data Contributor role to the installation running lakeFS. | . You’re now ready to create your first lakeFS repository. ",
    "url": "/v0.70/setup/storage/blob.html#authenticate-with-active-directory",
    "relUrl": "/setup/storage/blob.html#authenticate-with-active-directory"
  },"58": {
    "doc": "Azure Blob Storage",
    "title": "Azure Blob Storage",
    "content": " ",
    "url": "/v0.70/setup/storage/blob.html",
    "relUrl": "/setup/storage/blob.html"
  },"59": {
    "doc": "Boto (Python)",
    "title": "Using lakeFS with Boto (Python)",
    "content": "To use Boto with lakeFS alongside S3, check out Boto S3 Router. It will route requests to either S3 or lakeFS according to the provided bucket name. lakeFS exposes an S3-compatible API, so you can use Boto to interact with your objects on lakeFS. ",
    "url": "/v0.70/integrations/boto.html#using-lakefs-with-boto-python",
    "relUrl": "/integrations/boto.html#using-lakefs-with-boto-python"
  },"60": {
    "doc": "Boto (Python)",
    "title": "Table of contents",
    "content": ". | Creating a Boto client | Usage Examples . | Put an object into lakeFS | List objects | Get object metadata | . | . ",
    "url": "/v0.70/integrations/boto.html#table-of-contents",
    "relUrl": "/integrations/boto.html#table-of-contents"
  },"61": {
    "doc": "Boto (Python)",
    "title": "Creating a Boto client",
    "content": "Create a Boto3 S3 client with your lakeFS endpoint and key-pair: . import boto3 s3 = boto3.client('s3', endpoint_url='https://lakefs.example.com', aws_access_key_id='AKIAIOSFODNN7EXAMPLE', aws_secret_access_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY') . The client is now configured to operate on your lakeFS installation. ",
    "url": "/v0.70/integrations/boto.html#creating-a-boto-client",
    "relUrl": "/integrations/boto.html#creating-a-boto-client"
  },"62": {
    "doc": "Boto (Python)",
    "title": "Usage Examples",
    "content": "Put an object into lakeFS . Use a branch name and a path to put an object in lakeFS: . with open('/local/path/to/file_0', 'rb') as f: s3.put_object(Body=f, Bucket='example-repo', Key='main/example-file.parquet') . You can now commit this change using the lakeFS UI or CLI. List objects . List the branch objects starting with a prefix: . list_resp = s3.list_objects_v2(Bucket='example-repo', Prefix='main/example-prefix') for obj in list_resp['Contents']: print(obj['Key']) . Or, use a lakeFS commit ID to list objects for a specific commit: . list_resp = s3.list_objects_v2(Bucket='example-repo', Prefix='c7a632d74f/example-prefix') for obj in list_resp['Contents']: print(obj['Key']) . Get object metadata . Get object metadata using branch and path: . s3.head_object(Bucket='example-repo', Key='main/example-file.parquet') # output: # {'ResponseMetadata': {'RequestId': '72A9EBD1210E90FA', # 'HostId': '', # 'HTTPStatusCode': 200, # 'HTTPHeaders': {'accept-ranges': 'bytes', # 'content-length': '1024', # 'etag': '\"2398bc5880e535c61f7624ad6f138d62\"', # 'last-modified': 'Sun, 24 May 2020 10:42:24 GMT', # 'x-amz-request-id': '72A9EBD1210E90FA', # 'date': 'Sun, 24 May 2020 10:45:42 GMT'}, # 'RetryAttempts': 0}, # 'AcceptRanges': 'bytes', # 'LastModified': datetime.datetime(2020, 5, 24, 10, 42, 24, tzinfo=tzutc()), # 'ContentLength': 1024, # 'ETag': '\"2398bc5880e535c61f7624ad6f138d62\"', # 'Metadata': {}} . ",
    "url": "/v0.70/integrations/boto.html#usage-examples",
    "relUrl": "/integrations/boto.html#usage-examples"
  },"63": {
    "doc": "Boto (Python)",
    "title": "Boto (Python)",
    "content": " ",
    "url": "/v0.70/integrations/boto.html",
    "relUrl": "/integrations/boto.html"
  },"64": {
    "doc": "During Deployment",
    "title": "During Deployment",
    "content": "Every day we introduce new data to the lake. And even if the code and infra doesn’t change, the data might, and those changes introduce potential quality issues. This is one of the complexities of a data product; the data we consume changes over the course of a month, a week, day, hour, or even minute-to-minute. Examples of changes to data that may occur: . | A client-side bug in the data collection of website events | A new Android version that interferes with the collecting events from your App | COVID-19 abrupt impact on consumers’ behavior, and its effect on the accuracy of ML models. | During a change to Salesforce interface, the validation requirement from a certain field had been lost | . lakeFS enable CI/CD-inspired workflows to help validate expectations and assumptions about the data before it goes live in production or lands in the data environment. Example 1: Data update safety . Continuous deployment of existing data we expect to consume, flowing from ingest-pipelines into the lake. We merge data from an ingest branch (“events-data”), which allows us to create tests using data analysis tools or data quality services (e.g. Great Expectations, Monte Carlo) to ensure reliability of the data we merge to the main branch. Since merge is atomic, no performance issue will be introduced by using lakeFS, but your main branch will only include quality data. Each merge to the main branch creates a new commit on the main branch, which serves as a new version of the data. This allows us to easily revert to previous states of the data if a newer change introduces data issues. Example 2: Test - Validate new data . Examples of common validation checks enforced in organizations: . | No user_* columns except under /private/… | Only (*.parquet | *.orc | _delta_log/*.json) files allowed | Under /production, only backward-compatible schema changes are allowed | New tables on main must be registered in our metadata repository first, with owner and SLA | . lakeFS will assist in enforcing best practices by giving you a designated branch to ingest new data (“new-data-1” in the drawing). You may run automated tests to validate predefined best practices as pre-merge hooks. If the validation passes, the new data will be automatically and atomically merged to the main branch. However, if the validation fails, you will be alerted and the new data will not be exposed to consumers. By using this branching model and implementing best practices as pre merge hooks, you ensure the main lake is never compromised. ",
    "url": "/v0.70/using_lakefs/ci.html",
    "relUrl": "/using_lakefs/ci.html"
  },"65": {
    "doc": "Command (CLI) Reference",
    "title": "Commands (CLI) Reference",
    "content": " ",
    "url": "/v0.70/reference/commands.html#commands-cli-reference",
    "relUrl": "/reference/commands.html#commands-cli-reference"
  },"66": {
    "doc": "Command (CLI) Reference",
    "title": "Table of contents",
    "content": ". | Installing the lakectl command locally . | Configuring credentials and API endpoint | lakectl | lakectl abuse | lakectl abuse commit | lakectl abuse create-branches | lakectl abuse help | lakectl abuse link-same-object | lakectl abuse list | lakectl abuse random-read | lakectl abuse random-write | lakectl actions | lakectl actions help | lakectl actions runs | lakectl actions runs describe | lakectl actions runs help | lakectl actions runs list | lakectl actions validate | lakectl annotate | lakectl auth | lakectl auth groups | lakectl auth groups create | lakectl auth groups delete | lakectl auth groups help | lakectl auth groups list | lakectl auth groups members | lakectl auth groups members add | lakectl auth groups members help | lakectl auth groups members list | lakectl auth groups members remove | lakectl auth groups policies | lakectl auth groups policies attach | lakectl auth groups policies detach | lakectl auth groups policies help | lakectl auth groups policies list | lakectl auth help | lakectl auth policies | lakectl auth policies create | lakectl auth policies delete | lakectl auth policies help | lakectl auth policies list | lakectl auth policies show | lakectl auth users | lakectl auth users create | lakectl auth users credentials | lakectl auth users credentials create | lakectl auth users credentials delete | lakectl auth users credentials help | lakectl auth users credentials list | lakectl auth users delete | lakectl auth users groups | lakectl auth users groups help | lakectl auth users groups list | lakectl auth users help | lakectl auth users list | lakectl auth users policies | lakectl auth users policies attach | lakectl auth users policies detach | lakectl auth users policies help | lakectl auth users policies list | lakectl branch | lakectl branch create | lakectl branch delete | lakectl branch help | lakectl branch list | lakectl branch reset | lakectl branch revert | lakectl branch show | lakectl branch-protect | lakectl branch-protect add | lakectl branch-protect delete | lakectl branch-protect help | lakectl branch-protect list | lakectl cat-hook-output | lakectl cat-sst | lakectl commit | lakectl completion | lakectl config | lakectl dbt | lakectl dbt create-branch-schema | lakectl dbt generate-schema-macro | lakectl dbt help | lakectl diff | lakectl docs | lakectl doctor | lakectl fs | lakectl fs cat | lakectl fs help | lakectl fs ls | lakectl fs rm | lakectl fs stage | lakectl fs stat | lakectl fs upload | lakectl gc | lakectl gc get-config | lakectl gc help | lakectl gc set-config | lakectl help | lakectl ingest | lakectl log | lakectl merge | lakectl metastore | lakectl metastore copy | lakectl metastore copy-all | lakectl metastore copy-schema | lakectl metastore create-symlink | lakectl metastore diff | lakectl metastore help | lakectl metastore import-all | lakectl refs-dump | lakectl refs-restore | lakectl repo | lakectl repo create | lakectl repo create-bare | lakectl repo delete | lakectl repo help | lakectl repo list | lakectl show | lakectl tag | lakectl tag create | lakectl tag delete | lakectl tag help | lakectl tag list | lakectl tag show | . | . ",
    "url": "/v0.70/reference/commands.html#table-of-contents",
    "relUrl": "/reference/commands.html#table-of-contents"
  },"67": {
    "doc": "Command (CLI) Reference",
    "title": "Installing the lakectl command locally",
    "content": "lakectl is distributed as a single binary, with no external dependencies - and is available for MacOS, Windows and Linux. Download lakectl . Configuring credentials and API endpoint . Once you’ve installed the lakectl command, run: . lakectl config # output: # Config file /home/janedoe/.lakectl.yaml will be used # Access key ID: AKIAIOSFODNN7EXAMPLE # Secret access key: **************************************** # Server endpoint URL: http://localhost:8000/api/v1 . This will setup a $HOME/.lakectl.yaml file with the credentials and API endpoint you’ve supplied. When setting up a new installation and creating initial credentials (see Quick start), the UI will provide a link to download a preconfigured configuration file for you. lakectl configuration items can each be controlled by an environment variable. The variable name will have a prefix of LAKECTL_, followed by the name of the configuration, replacing every ‘.’ with a ‘_’. Example: LAKECTL_SERVER_ENDPOINT_URL controls server.endpoint_url. lakectl . A cli tool to explore manage and work with lakeFS . Synopsis . lakeFS is data lake management solution, allowing Git-like semantics over common object stores . lakectl is a CLI tool allowing exploration and manipulation of a lakeFS environment . Options . --base-uri string base URI used for lakeFS address parse -c, --config string config file (default is $HOME/.lakectl.yaml) -h, --help help for lakectl --log-format string set logging output format --log-level string set logging level (default \"none\") --log-output strings set logging output(s) --no-color don't use fancy output colors (default when not attached to an interactive terminal) --verbose run in verbose mode . note: The base-uri option can be controlled with the LAKECTL_BASE_URI environment variable. Example usage . $ export LAKECTL_BASE_URI=\"lakefs://my-repo/my-branch\" # Once set, use relative lakefs uri's: $ lakectl fs ls /path . lakectl abuse . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Abuse a running lakeFS instance. See sub commands for more info. Options . -h, --help help for abuse . lakectl abuse commit . Commits to the source ref repeatedly . lakectl abuse commit &lt;source ref uri&gt; [flags] . Options . --amount int amount of commits to do (default 100) --gap duration duration to wait between commits (default 2s) -h, --help help for commit . lakectl abuse create-branches . Create a lot of branches very quickly. lakectl abuse create-branches &lt;source ref uri&gt; [flags] . Options . --amount int amount of things to do (default 1000000) --branch-prefix string prefix to create branches under (default \"abuse-\") --clean-only only clean up past runs -h, --help help for create-branches --parallelism int amount of things to do in parallel (default 100) . lakectl abuse help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type abuse help [path to command] for full details. lakectl abuse help [command] [flags] . Options . -h, --help help for help . lakectl abuse link-same-object . Link the same object in parallel. lakectl abuse link-same-object &lt;source ref uri&gt; [flags] . Options . --amount int amount of link object to do (default 1000000) -h, --help help for link-same-object --key string key used for the test (default \"linked-object\") --parallelism int amount of link object to do in parallel (default 100) . lakectl abuse list . List from the source ref . lakectl abuse list &lt;source ref uri&gt; [flags] . Options . --amount int amount of lists to do (default 1000000) -h, --help help for list --parallelism int amount of lists to do in parallel (default 100) --prefix string prefix to list under (default \"abuse/\") . lakectl abuse random-read . Read keys from a file and generate random reads from the source ref for those keys. lakectl abuse random-read &lt;source ref uri&gt; [flags] . Options . --amount int amount of reads to do (default 1000000) --from-file string read keys from this file (\"-\" for stdin) -h, --help help for random-read --parallelism int amount of reads to do in parallel (default 100) . lakectl abuse random-write . Generate random writes to the source branch . lakectl abuse random-write &lt;source branch uri&gt; [flags] . Options . --amount int amount of writes to do (default 1000000) -h, --help help for random-write --parallelism int amount of writes to do in parallel (default 100) --prefix string prefix to create paths under (default \"abuse/\") . lakectl actions . Manage Actions commands . Options . -h, --help help for actions . lakectl actions help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type actions help [path to command] for full details. lakectl actions help [command] [flags] . Options . -h, --help help for help . lakectl actions runs . Explore runs information . Options . -h, --help help for runs . lakectl actions runs describe . Describe run results . Synopsis . Show information about the run and all the hooks that were executed as part of the run . lakectl actions runs describe [flags] . Examples . lakectl actions runs describe lakefs://&lt;repository&gt; &lt;run_id&gt; . Options . --after string show results after this value (used for pagination) --amount int number of results to return. By default, all results are returned. -h, --help help for describe . lakectl actions runs help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type runs help [path to command] for full details. lakectl actions runs help [command] [flags] . Options . -h, --help help for help . lakectl actions runs list . List runs . Synopsis . List all runs on a repository optional filter by branch or commit . lakectl actions runs list [flags] . Examples . lakectl actions runs list lakefs://&lt;repository&gt; [--branch &lt;branch&gt;] [--commit &lt;commit_id&gt;] . Options . --after string show results after this value (used for pagination) --amount int number of results to return (default 100) --branch string show results for specific branch --commit string show results for specific commit ID -h, --help help for list . lakectl actions validate . Validate action file . Synopsis . Tries to parse the input action file as lakeFS action file . lakectl actions validate [flags] . Examples . lakectl actions validate &lt;path&gt; . Options . -h, --help help for validate . lakectl annotate . List entries under a given path, annotating each with the latest modifying commit . lakectl annotate &lt;path uri&gt; [flags] . Options . -h, --help help for annotate -r, --recursive recursively annotate all entries under a given path or prefix . lakectl auth . Manage authentication and authorization . Synopsis . manage authentication and authorization including users, groups and policies . Options . -h, --help help for auth . lakectl auth groups . Manage groups . Options . -h, --help help for groups . lakectl auth groups create . Create a group . lakectl auth groups create [flags] . Options . -h, --help help for create --id string Group identifier . lakectl auth groups delete . Delete a group . lakectl auth groups delete [flags] . Options . -h, --help help for delete --id string Group identifier . lakectl auth groups help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type groups help [path to command] for full details. lakectl auth groups help [command] [flags] . Options . -h, --help help for help . lakectl auth groups list . List groups . lakectl auth groups list [flags] . Options . --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth groups members . Manage group user memberships . Options . -h, --help help for members . lakectl auth groups members add . Add a user to a group . lakectl auth groups members add [flags] . Options . -h, --help help for add --id string Group identifier --user string Username (email for password-based users, default: current user) . lakectl auth groups members help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type members help [path to command] for full details. lakectl auth groups members help [command] [flags] . Options . -h, --help help for help . lakectl auth groups members list . List users in a group . lakectl auth groups members list [flags] . Options . --id string Group identifier --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth groups members remove . Remove a user from a group . lakectl auth groups members remove [flags] . Options . -h, --help help for remove --id string Group identifier --user string Username (email for password-based users, default: current user) . lakectl auth groups policies . Manage group policies . Options . -h, --help help for policies . lakectl auth groups policies attach . Attach a policy to a group . lakectl auth groups policies attach [flags] . Options . -h, --help help for attach --id string User identifier --policy string Policy identifier . lakectl auth groups policies detach . Detach a policy from a group . lakectl auth groups policies detach [flags] . Options . -h, --help help for detach --id string User identifier --policy string Policy identifier . lakectl auth groups policies help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type policies help [path to command] for full details. lakectl auth groups policies help [command] [flags] . Options . -h, --help help for help . lakectl auth groups policies list . List policies for the given group . lakectl auth groups policies list [flags] . Options . --id string Group identifier --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type auth help [path to command] for full details. lakectl auth help [command] [flags] . Options . -h, --help help for help . lakectl auth policies . Manage policies . Options . -h, --help help for policies . lakectl auth policies create . Create a policy . lakectl auth policies create [flags] . Options . -h, --help help for create --id string Policy identifier --statement-document string JSON statement document path (or \"-\" for stdin) . lakectl auth policies delete . Delete a policy . lakectl auth policies delete [flags] . Options . -h, --help help for delete --id string Policy identifier . lakectl auth policies help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type policies help [path to command] for full details. lakectl auth policies help [command] [flags] . Options . -h, --help help for help . lakectl auth policies list . List policies . lakectl auth policies list [flags] . Options . --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth policies show . Show a policy . lakectl auth policies show [flags] . Options . -h, --help help for show --id string Policy identifier . lakectl auth users . Manage users . Options . -h, --help help for users . lakectl auth users create . Create a user . lakectl auth users create [flags] . Options . -h, --help help for create --id string Username . lakectl auth users credentials . Manage user credentials . Options . -h, --help help for credentials . lakectl auth users credentials create . Create user credentials . lakectl auth users credentials create [flags] . Options . -h, --help help for create --id string Username (email for password-based users, default: current user) . lakectl auth users credentials delete . Delete user credentials . lakectl auth users credentials delete [flags] . Options . --access-key-id string Access key ID to delete -h, --help help for delete --id string Username (email for password-based users, default: current user) . lakectl auth users credentials help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type credentials help [path to command] for full details. lakectl auth users credentials help [command] [flags] . Options . -h, --help help for help . lakectl auth users credentials list . List user credentials . lakectl auth users credentials list [flags] . Options . --id string Username (email for password-based users, default: current user) --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth users delete . Delete a user . lakectl auth users delete [flags] . Options . -h, --help help for delete --id string Username (email for password-based users) . lakectl auth users groups . Manage user groups . Options . -h, --help help for groups . lakectl auth users groups help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type groups help [path to command] for full details. lakectl auth users groups help [command] [flags] . Options . -h, --help help for help . lakectl auth users groups list . List groups for the given user . lakectl auth users groups list [flags] . Options . --id string Username (email for password-based users) --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth users help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type users help [path to command] for full details. lakectl auth users help [command] [flags] . Options . -h, --help help for help . lakectl auth users list . List users . lakectl auth users list [flags] . Options . --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth users policies . Manage user policies . Options . -h, --help help for policies . lakectl auth users policies attach . Attach a policy to a user . lakectl auth users policies attach [flags] . Options . -h, --help help for attach --id string Username (email for password-based users) --policy string Policy identifier . lakectl auth users policies detach . Detach a policy from a user . lakectl auth users policies detach [flags] . Options . -h, --help help for detach --id string Username (email for password-based users) --policy string Policy identifier . lakectl auth users policies help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type policies help [path to command] for full details. lakectl auth users policies help [command] [flags] . Options . -h, --help help for help . lakectl auth users policies list . List policies for the given user . lakectl auth users policies list [flags] . Options . --effective List all distinct policies attached to the user, including by group memberships --id string Username (email for password-based users) --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl branch . Create and manage branches within a repository . Synopsis . Create delete and list branches within a lakeFS repository . Options . -h, --help help for branch . lakectl branch create . Create a new branch in a repository . lakectl branch create &lt;branch uri&gt; -s &lt;source ref uri&gt; [flags] . Examples . lakectl branch create lakefs://example-repo/new-branch -s lakefs://example-repo/main . Options . -h, --help help for create -s, --source string source branch uri . lakectl branch delete . Delete a branch in a repository, along with its uncommitted changes (CAREFUL) . lakectl branch delete &lt;branch uri&gt; [flags] . Examples . lakectl branch delete lakefs://example-repo/example-branch . Options . -h, --help help for delete -y, --yes Automatically say yes to all confirmations . lakectl branch help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type branch help [path to command] for full details. lakectl branch help [command] [flags] . Options . -h, --help help for help . lakectl branch list . List branches in a repository . lakectl branch list &lt;repository uri&gt; [flags] . Examples . lakectl branch list lakefs://&lt;repository&gt; . Options . --after string show results after this value (used for pagination) --amount int number of results to return (default 100) -h, --help help for list . lakectl branch reset . Reset uncommitted changes - all of them, or by path . Synopsis . reset changes. There are four different ways to reset changes: . | reset all uncommitted changes - reset lakefs://myrepo/main | reset uncommitted changes under specific path - reset lakefs://myrepo/main –prefix path | reset uncommitted changes for specific object - reset lakefs://myrepo/main –object path | . lakectl branch reset &lt;branch uri&gt; [--prefix|--object] [flags] . Examples . lakectl branch reset lakefs://example-repo/example-branch . Options . -h, --help help for reset --object string path to object to be reset --prefix string prefix of the objects to be reset -y, --yes Automatically say yes to all confirmations . lakectl branch revert . Given a commit, record a new commit to reverse the effect of this commit . Synopsis . The commits will be reverted in left-to-right order . lakectl branch revert &lt;branch uri&gt; &lt;commit ref to revert&gt; [&lt;more commits&gt;...] [flags] . Examples . lakectl branch revert lakefs://example-repo/example-branch commitA Revert the changes done by commitA in example-branch branch revert lakefs://example-repo/example-branch HEAD~1 HEAD~2 HEAD~3 Revert the changes done by the second last commit to the fourth last commit in example-branch . Options . -h, --help help for revert -m, --parent-number int the parent number (starting from 1) of the mainline. The revert will reverse the change relative to the specified parent. -y, --yes Automatically say yes to all confirmations . lakectl branch show . Show branch latest commit reference . lakectl branch show &lt;branch uri&gt; [flags] . Examples . lakectl branch show lakefs://example-repo/example-branch . Options . -h, --help help for show . lakectl branch-protect . Create and manage branch protection rules . Synopsis . Define branch protection rules to prevent direct changes. Changes to protected branches can only be done by merging from other branches. Options . -h, --help help for branch-protect . lakectl branch-protect add . Add a branch protection rule . Synopsis . Add a branch protection rule for a given branch name pattern . lakectl branch-protect add &lt;repo uri&gt; &lt;pattern&gt; [flags] . Examples . lakectl branch-protect add lakefs://&lt;repository&gt; 'stable_*' . Options . -h, --help help for add . lakectl branch-protect delete . Delete a branch protection rule . Synopsis . Delete a branch protection rule for a given branch name pattern . lakectl branch-protect delete &lt;repo uri&gt; &lt;pattern&gt; [flags] . Examples . lakectl branch-protect delete lakefs://&lt;repository&gt; stable_* . Options . -h, --help help for delete . lakectl branch-protect help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type branch-protect help [path to command] for full details. lakectl branch-protect help [command] [flags] . Options . -h, --help help for help . lakectl branch-protect list . List all branch protection rules . lakectl branch-protect list &lt;repo uri&gt; [flags] . Examples . lakectl branch-protect list lakefs://&lt;repository&gt; . Options . -h, --help help for list . lakectl cat-hook-output . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Cat actions hook output . lakectl cat-hook-output [flags] . Examples . lakectl cat-hook-output lakefs://&lt;repository&gt; &lt;run_id&gt; &lt;run_hook_id&gt; . Options . -h, --help help for cat-hook-output . lakectl cat-sst . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Explore lakeFS .sst files . lakectl cat-sst &lt;sst-file&gt; [flags] . Options . --amount int how many records to return, or -1 for all records (default -1) -f, --file string path to an sstable file, or \"-\" for stdin -h, --help help for cat-sst . lakectl commit . Commit changes on a given branch . lakectl commit &lt;branch uri&gt; [flags] . Options . --allow-empty-message allow an empty commit message -h, --help help for commit -m, --message string commit message --meta strings key value pair in the form of key=value . lakectl completion . Generate completion script . Synopsis . To load completions: . Bash: . $ source &lt;(lakectl completion bash) . To load completions for each session, execute once: Linux: . $ lakectl completion bash &gt; /etc/bash_completion.d/lakectl . MacOS: . $ lakectl completion bash &gt; /usr/local/etc/bash_completion.d/lakectl . Zsh: . If shell completion is not already enabled in your environment you will need to enable it. You can execute the following once: . $ echo \"autoload -U compinit; compinit\" &gt;&gt; ~/.zshrc . To load completions for each session, execute once: . $ lakectl completion zsh &gt; \"${fpath[1]}/_lakectl\" . You will need to start a new shell for this setup to take effect. Fish: . $ lakectl completion fish | source . To load completions for each session, execute once: . $ lakectl completion fish &gt; ~/.config/fish/completions/lakectl.fish . lakectl completion &lt;bash|zsh|fish&gt; . Options . -h, --help help for completion . lakectl config . Create/update local lakeFS configuration . lakectl config [flags] . Options . -h, --help help for config . lakectl dbt . Integration with dbt commands . Options . -h, --help help for dbt . lakectl dbt create-branch-schema . Creates a new schema dedicated for branch and clones all dbt models to new schema . lakectl dbt create-branch-schema [flags] . Examples . lakectl dbt create-branch-schema --branch &lt;branch-name&gt; . Options . --branch string requested branch --continue-on-error prevent command from failing when a single table fails --continue-on-schema-exists allow running on existing schema --create-branch create a new branch for the schema --dbfs-location string -h, --help help for create-branch-schema --project-root string location of dbt project (default \".\") --skip-views --to-schema string destination schema name [default is branch] . lakectl dbt generate-schema-macro . generates the a macro allowing lakectl to run dbt on dynamic schemas . lakectl dbt generate-schema-macro [flags] . Examples . lakectl dbt generate-schema-macro . Options . -h, --help help for generate-schema-macro --project-root string location of dbt project (default \".\") . lakectl dbt help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type dbt help [path to command] for full details. lakectl dbt help [command] [flags] . Options . -h, --help help for help . lakectl diff . Show changes between two commits, or the currently uncommitted changes . lakectl diff &lt;ref uri&gt; [ref uri] [flags] . Examples . lakectl diff lakefs://example-repo/example-branch Show uncommitted changes in example-branch. lakectl diff lakefs://example-repo/main lakefs://example-repo/dev This shows the differences between master and dev starting at the last common commit. This is similar to the three-dot (...) syntax in git. Uncommitted changes are not shown. lakectl diff --two-way lakefs://example-repo/main lakefs://example-repo/dev Show changes between the tips of the main and dev branches. This is similar to the two-dot (..) syntax in git. Uncommitted changes are not shown. lakectl diff --two-way lakefs://example-repo/main lakefs://example-repo/dev$ Show changes between the tip of the main and the dev branch, including uncommitted changes on dev. Options . -h, --help help for diff --two-way Use two-way diff: show difference between the given refs, regardless of a common ancestor. lakectl docs . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. lakectl docs [outfile] [flags] . Options . -h, --help help for docs . lakectl doctor . Run a basic diagnosis of the LakeFS configuration . lakectl doctor [flags] . Options . -h, --help help for doctor . lakectl fs . View and manipulate objects . Options . -h, --help help for fs . lakectl fs cat . Dump content of object to stdout . lakectl fs cat &lt;path uri&gt; [flags] . Options . -d, --direct read directly from backing store (faster but requires more credentials) -h, --help help for cat . lakectl fs help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type fs help [path to command] for full details. lakectl fs help [command] [flags] . Options . -h, --help help for help . lakectl fs ls . List entries under a given tree . lakectl fs ls &lt;path uri&gt; [flags] . Options . -h, --help help for ls --recursive list all objects under the specified prefix . lakectl fs rm . Delete object . lakectl fs rm &lt;path uri&gt; [flags] . Options . -C, --concurrency int max concurrent single delete operations to send to the lakeFS server (default 50) -h, --help help for rm -r, --recursive recursively delete all objects under the specified path . lakectl fs stage . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Stage a reference to an existing object, to be managed in lakeFS . lakectl fs stage &lt;path uri&gt; [flags] . Options . --checksum string Object MD5 checksum as a hexadecimal string --content-type string MIME type of contents -h, --help help for stage --location string fully qualified storage location (i.e. \"s3://bucket/path/to/object\") --meta strings key value pairs in the form of key=value --mtime int Object modified time (Unix Epoch in seconds). Defaults to current time --size int Object size in bytes . lakectl fs stat . View object metadata . lakectl fs stat &lt;path uri&gt; [flags] . Options . -h, --help help for stat . lakectl fs upload . Upload a local file to the specified URI . lakectl fs upload &lt;path uri&gt; [flags] . Options . --content-type string MIME type of contents -d, --direct write directly to backing store (faster but requires more credentials) -h, --help help for upload -r, --recursive recursively copy all files under local source -s, --source string local file to upload, or \"-\" for stdin . lakectl gc . Manage garbage collection configuration . Options . -h, --help help for gc . lakectl gc get-config . Show garbage collection configuration JSON . lakectl gc get-config [flags] . Examples . lakectl gc get-config &lt;repository uri&gt; . Options . -h, --help help for get-config -p, --json get rules as JSON . lakectl gc help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type gc help [path to command] for full details. lakectl gc help [command] [flags] . Options . -h, --help help for help . lakectl gc set-config . Set garbage collection configuration JSON . Synopsis . Sets the garbage collection configuration JSON. Example configuration file: { “default_retention_days”: 21, “branches”: [ { “branch_id”: “main”, “retention_days”: 28 }, { “branch_id”: “dev”, “retention_days”: 14 } ] } . lakectl gc set-config [flags] . Examples . lakectl gc set-config &lt;repository uri&gt; -f config.json . Options . -f, --filename string file containing the GC configuration -h, --help help for set-config . lakectl help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type lakectl help [path to command] for full details. lakectl help [command] [flags] . Options . -h, --help help for help . lakectl ingest . Ingest objects from an external source into a lakeFS branch (without actually copying them) . lakectl ingest --from &lt;object store URI&gt; --to &lt;lakeFS path URI&gt; [--dry-run] [flags] . Options . -C, --concurrency int max concurrent API calls to make to the lakeFS server (default 64) --dry-run only print the paths to be ingested --from string prefix to read from (e.g. \"s3://bucket/sub/path/\"). must not be in a storage namespace -h, --help help for ingest --s3-endpoint-url string URL to access S3 storage API (by default, use regular AWS S3 endpoint --to string lakeFS path to load objects into (e.g. \"lakefs://repo/branch/sub/path/\") . lakectl log . Show log of commits . Synopsis . Show log of commits for a given branch . lakectl log &lt;branch uri&gt; [flags] . Options . --after string show results after this value (used for pagination) --amount int number of results to return. By default, all results are returned -h, --help help for log --objects strings show results that contains changes to at least one path in that list of objects. Use comma separator to pass all objects together --prefixes strings show results that contains changes to at least one path in that list of prefixes. Use comma separator to pass all prefixes together --show-meta-range-id also show meta range ID . lakectl merge . Merge &amp; commit changes from source branch into destination branch . Synopsis . Merge &amp; commit changes from source branch into destination branch . lakectl merge &lt;source ref&gt; &lt;destination ref&gt; [flags] . Options . -h, --help help for merge --strategy string In case of a merge conflict, this option will force the merge process to automatically favor changes from the dest branch (\"dest-wins\") or from the source branch(\"source-wins\"). In case no selection is made, the merge process will fail in case of a conflict . lakectl metastore . Manage metastore commands . Options . -h, --help help for metastore . lakectl metastore copy . Copy or merge table . Synopsis . Copy or merge table. the destination table will point to the selected branch . lakectl metastore copy [flags] . Options . --catalog-id string Glue catalog ID --dbfs-root dbfs:/ dbfs location root will replace dbfs:/ in the location before transforming --from-client-type string metastore type [hive, glue] --from-schema string source schema name --from-table string source table name -h, --help help for copy --metastore-uri string Hive metastore URI -p, --partition strings partition to copy --serde string serde to set copy to [default is to-table] --to-branch string lakeFS branch name --to-client-type string metastore type [hive, glue] --to-schema string destination schema name [default is from-branch] --to-table string destination table name [default is from-table] . lakectl metastore copy-all . Copy from one metastore to another . Synopsis . copy or merge requested tables between hive metastores. the destination tables will point to the selected branch . lakectl metastore copy-all [flags] . Options . --branch string lakeFS branch name --continue-on-error prevent copy-all from failing when a single table fails --dbfs-root dbfs:/ dbfs location root will replace dbfs:/ in the location before transforming --from-address string source metastore address --from-client-type string metastore type [hive, glue] -h, --help help for copy-all --schema-filter string filter for schemas to copy in metastore pattern (default \".*\") --table-filter string filter for tables to copy in metastore pattern (default \".*\") --to-address string destination metastore address --to-client-type string metastore type [hive, glue] . lakectl metastore copy-schema . Copy schema . Synopsis . Copy schema (without tables). the destination schema will point to the selected branch . lakectl metastore copy-schema [flags] . Options . --catalog-id string Glue catalog ID --dbfs-root dbfs:/ dbfs location root will replace dbfs:/ in the location before transforming --from-client-type string metastore type [hive, glue] --from-schema string source schema name -h, --help help for copy-schema --metastore-uri string Hive metastore URI --to-branch string lakeFS branch name --to-client-type string metastore type [hive, glue] --to-schema string destination schema name [default is from-branch] . lakectl metastore create-symlink . Create symlink table and data . Synopsis . create table with symlinks, and create the symlinks in s3 in order to access from external services that could only access s3 directly (e.g athena) . lakectl metastore create-symlink [flags] . Options . --branch string lakeFS branch name --catalog-id string Glue catalog ID --from-client-type string metastore type [hive, glue] --from-schema string source schema name --from-table string source table name -h, --help help for create-symlink --path string path to table on lakeFS --repo string lakeFS repository name --to-schema string destination schema name --to-table string destination table name . lakectl metastore diff . Show column and partition differences between two tables . lakectl metastore diff [flags] . Options . --catalog-id string Glue catalog ID --from-address string source metastore address --from-client-type string metastore type [hive, glue] --from-schema string source schema name --from-table string source table name -h, --help help for diff --metastore-uri string Hive metastore URI --to-address string destination metastore address --to-client-type string metastore type [hive, glue] --to-schema string destination schema name --to-table string destination table name [default is from-table] . lakectl metastore help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type metastore help [path to command] for full details. lakectl metastore help [command] [flags] . Options . -h, --help help for help . lakectl metastore import-all . Import from one metastore to another . Synopsis . import requested tables between hive metastores. the destination tables will point to the selected repository and branch table with location s3://my-s3-bucket/path/to/table will be transformed to location s3://repo-param/bucket-param/path/to/table . lakectl metastore import-all [flags] . Options . --branch string lakeFS branch name --continue-on-error prevent import-all from failing when a single table fails --dbfs-root dbfs:/ dbfs location root will replace dbfs:/ in the location before transforming --from-address string source metastore address --from-client-type string metastore type [hive, glue] -h, --help help for import-all --repo string lakeFS repo name --schema-filter string filter for schemas to copy in metastore pattern (default \".*\") --table-filter string filter for tables to copy in metastore pattern (default \".*\") --to-address string destination metastore address --to-client-type string metastore type [hive, glue] . lakectl refs-dump . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Dumps refs (branches, commits, tags) to the underlying object store . lakectl refs-dump &lt;repository uri&gt; [flags] . Options . -h, --help help for refs-dump . lakectl refs-restore . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Restores refs (branches, commits, tags) from the underlying object store to a bare repository . Synopsis . restores refs (branches, commits, tags) from the underlying object store to a bare repository. This command is expected to run on a bare repository (i.e. one created with ‘lakectl repo create-bare’). Since a bare repo is expected, in case of transient failure, delete the repository and recreate it as bare and retry. lakectl refs-restore &lt;repository uri&gt; [flags] . Examples . aws s3 cp s3://bucket/_lakefs/refs_manifest.json - | lakectl refs-restore lakefs://my-bare-repository --manifest - . Options . -h, --help help for refs-restore --manifest refs-dump path to a refs manifest json file (as generated by refs-dump). Alternatively, use \"-\" to read from stdin . lakectl repo . Manage and explore repos . Options . -h, --help help for repo . lakectl repo create . Create a new repository . lakectl repo create &lt;repository uri&gt; &lt;storage namespace&gt; [flags] . Examples . lakectl repo create lakefs://some-repo-name s3://some-bucket-name . Options . -d, --default-branch string the default branch of this repository (default \"main\") -h, --help help for create . lakectl repo create-bare . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Create a new repository with no initial branch or commit . lakectl repo create-bare &lt;repository uri&gt; &lt;storage namespace&gt; [flags] . Examples . lakectl create-bare lakefs://some-repo-name s3://some-bucket-name . Options . -d, --default-branch string the default branch name of this repository (will not be created) (default \"main\") -h, --help help for create-bare . lakectl repo delete . Delete existing repository . lakectl repo delete &lt;repository uri&gt; [flags] . Options . -h, --help help for delete -y, --yes Automatically say yes to all confirmations . lakectl repo help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type repo help [path to command] for full details. lakectl repo help [command] [flags] . Options . -h, --help help for help . lakectl repo list . List repositories . lakectl repo list [flags] . Options . --after string show results after this value (used for pagination) --amount int number of results to return (default 100) -h, --help help for list . lakectl show . See detailed information about an entity by ID (commit, user, etc) . lakectl show &lt;repository uri&gt; [flags] . Options . --commit string commit ID to show -h, --help help for show --show-meta-range-id when showing commits, also show meta range ID . lakectl tag . Create and manage tags within a repository . Synopsis . Create delete and list tags within a lakeFS repository . Options . -h, --help help for tag . lakectl tag create . Create a new tag in a repository . lakectl tag create &lt;tag uri&gt; &lt;commit uri&gt; [flags] . Examples . lakectl tag create lakefs://example-repo/example-tag lakefs://example-repo/2397cc9a9d04c20a4e5739b42c1dd3d8ba655c0b3a3b974850895a13d8bf9917 . Options . -f, --force override the tag if it exists -h, --help help for create . lakectl tag delete . Delete a tag from a repository . lakectl tag delete &lt;tag uri&gt; [flags] . Options . -h, --help help for delete . lakectl tag help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type tag help [path to command] for full details. lakectl tag help [command] [flags] . Options . -h, --help help for help . lakectl tag list . List tags in a repository . lakectl tag list &lt;repository uri&gt; [flags] . Examples . lakectl tag list lakefs://&lt;repository&gt; . Options . --after string show results after this value (used for pagination) --amount int number of results to return (default 100) -h, --help help for list . lakectl tag show . Show tag’s commit reference . lakectl tag show &lt;tag uri&gt; [flags] . Options . -h, --help help for show . ",
    "url": "/v0.70/reference/commands.html#installing-the-lakectl-command-locally",
    "relUrl": "/reference/commands.html#installing-the-lakectl-command-locally"
  },"68": {
    "doc": "Command (CLI) Reference",
    "title": "Command (CLI) Reference",
    "content": " ",
    "url": "/v0.70/reference/commands.html",
    "relUrl": "/reference/commands.html"
  },"69": {
    "doc": "Our commitment to open source",
    "title": "Our commitment to open source",
    "content": "lakeFS is an open-source project under the Apache 2.0 license. The project was created and is supported by Treeverse, a commercial company founded by engineers passionate about providing solutions to the evolving world of data engineering. Why did we choose to open the source of our core capabilities? . We believe in the bottom-up adoption of technologies. We believe collaborative communities have the power to bring the best solutions to the community. We believe that every engineer should be able to use, contribute to, and influence cutting edge technologies, so they can innovate in their domain. What is our commitment to open source? . We created lakeFS, our open-source project, to provide a Git-like interface on top of object stores - so that you can fully take advantage of with any data application at any scale. For that reason, we commit that the following capabilities are and will remain open-source as part of lakeFS: . | All versioning capabilities, | Git-Like interface for the versioning operations, | Support for public object store APIs, | Integrations with publicly available applications accessing an object store, | CLI, API, and GUI interfaces. | . We also commit to keeping lakeFS scalable in throughput and performance. We are deeply committed to our community of engineers who use and contribute to the project. We are and will continue to be highly responsive and shape lakeFS together to provide the data versioning capabilities we are all looking for. What is lakeFS Cloud? . Treeverse offers lakeFS Cloud, which provides all the same benefits of the Git-like interface on top of object stores as a fully-managed service. The vision behind lakeFS Cloud is to provide a managed data versioning and management solution for data practitioners. lakeFS Cloud will leverage the lakeFS open-source technology, integrate capabilities and unique features, and lead its users to implement best practices. As part of our commitment to the open source values of transparency and interoperability, we believe everyone should be able to enjoy these benefits, regardless of whether or not they choose to use the managed offering. Because of that, we will not intentionally make it harder to build these features independently on top of the open source solution. ",
    "url": "/v0.70/commitment.html",
    "relUrl": "/commitment.html"
  },"70": {
    "doc": "Configuration Reference",
    "title": "Configuration Reference",
    "content": " ",
    "url": "/v0.70/reference/configuration.html",
    "relUrl": "/reference/configuration.html"
  },"71": {
    "doc": "Configuration Reference",
    "title": "Table of contents",
    "content": ". | Reference | Using Environment Variables | Example: Local Development | Example: AWS Deployment | Example: Google Storage | Example: MinIO | Example: Azure blob storage | . Configuring lakeFS is done using a YAML configuration file and/or environment variable. The configuration file’s location can be set with the ‘–config’ flag. If not specified, the first file found in the following order will be used: . | ./config.yaml | $HOME/lakefs/config.yaml | /etc/lakefs/config.yaml | $HOME/.lakefs.yaml | . Configuration items can each be controlled by an environment variable. The variable name will have a prefix of LAKEFS_, followed by the name of the configuration, replacing every ‘.’ with a ‘_’. Example: LAKEFS_LOGGING_LEVEL controls logging.level. This reference uses . to denote the nesting of values. ",
    "url": "/v0.70/reference/configuration.html#table-of-contents",
    "relUrl": "/reference/configuration.html#table-of-contents"
  },"72": {
    "doc": "Configuration Reference",
    "title": "Reference",
    "content": ". | logging.format (one of [\"json\", \"text\"] : \"text\") - Format to output log message in | logging.level (one of [\"TRACE\", \"DEBUG\", \"INFO\", \"WARN\", \"ERROR\", \"NONE\"] : \"DEBUG\") - Logging level to output | logging.audit_log_level (one of [\"TRACE\", \"DEBUG\", \"INFO\", \"WARN\", \"ERROR\", \"NONE\"] : \"DEBUG\") - Audit logs level to output. Please notice that in case you configure this field to be lower than the main logger level, you won’t be able to get the audit logs | logging.output (string : \"-\") - A path or paths to write logs to. A - means the standard output, = means the standard error. | logging.file_max_size_mb (int : 100) - Output file maximum size in megabytes. | logging.files_keep (int : 0) - Number of log files to keep, default is all. | actions.enabled (bool : true) - Setting this to false will block hooks from being executed | database.connection_string (string : \"postgres://localhost:5432/postgres?sslmode=disable\") - PostgreSQL connection string to use | database.max_open_connections (int : 25) - Maximum number of open connections to the database | database.max_idle_connections (int : 25) - Sets the maximum number of connections in the idle connection pool | database.connection_max_lifetime (duration : 5m) - Sets the maximum amount of time a connection may be reused | listen_address (string : \"0.0.0.0:8000\") - A &lt;host&gt;:&lt;port&gt; structured string representing the address to listen on | auth.cache.enabled (bool : true) - Whether to cache access credentials and user policies in-memory. Can greatly improve throughput when enabled. | auth.cache.size (int : 1024) - How many items to store in the auth cache. Systems with a very high user count should use a larger value at the expense of ~1kb of memory per cached user. | auth.cache.ttl (time duration : \"20s\") - How long to store an item in the auth cache. Using a higher value reduces load on the database, but will cause changes longer to take effect for cached users. | auth.cache.jitter (time duration : \"3s\") - A random amount of time between 0 and this value is added to each item’s TTL. This is done to avoid a large bulk of keys expiring at once and overwhelming the database. | auth.encrypt.secret_key (string : required) - A random (cryptographically safe) generated string that is used for encryption and HMAC signing | auth.cookie_domain (string : \"\") - Domain attribute to set the access_token cookie on (the default is an empty string which defaults to the same host that sets the cookie) | auth.api.endpoint (string: https://external.service/api/v1) - URL to external Authorization Service described at authorization.yml; | auth.api.token (string: eyJhbGciOiJIUzI1NiIsInR5...) - API token used to authenticate requests to api endpoint . Note: It is best to keep this somewhere safe such as KMS or Hashicorp Vault, and provide it to the system at run time . | auth.ldap.server_endpoint (string : required) - If specified, also authenticate users via this LDAP server. | auth.ldap.bind_dn (string : required) - Use this DN to bind lakeFS on the LDAP server for searching for users. | auth.ldap.bind_password (string : ) - If set, use this password for binding bind_dn. | auth.ldap.username_attribute (string : required) - Attribute holding login username on LDAP users, e.g. cn or uid. | auth.ldap.user_base_dn (string : required) - Base DN for searching for users. Search looks for users in the subtree below this. | auth.ldap.default_user_group (string : ) - Create all LDAP users in this group. Defaults to Viewers. | auth.ldap.user_filter (string : ) - Additional filter for users. | auth.oidc.enabled (boolean : false) - Set to true to enable authentication with an external OIDC provider. | auth.oidc.is_default_login (boolean : false) - If true, the lakeFS login page will redirect to the external provider by default. | auth.oidc.client_id (string : ) - OIDC client ID. | auth.oidc.client_secret (string : ) - OIDC client secret. | auth.oidc.url (string : ) - The base URL of your OIDC compatible identity provider. | auth.oidc.callback_base_url (string : ) - The scheme, host and port of your lakeFS installation. After authenticating, your identity provider will redirect you to a URL under this base. | auth.oidc.default_initial_groups (string[] : []) - By default, OIDC users will be assigned to these groups | auth.oidc.initial_groups_claim_name (string[] : []) - Use this claim from the ID token to provide the initial group for new users. | auth.oidc.friendly_name_claim_name (string[] : ) - If specified, the value from the claim with this name will be used as the user’s display name. | auth.oidc.authorize_endpoint_query_parameters (map[string]string : ) - Add these parameters when calling the provider’s /authorize endpoint | auth.oidc.validate_id_token_claims (map[string]string : ) - When a user tries to access lakeFS, validate that the ID token contains these claims with the corresponding values. | blockstore.type (one of [\"local\", \"s3\", \"gs\", \"azure\", \"mem\"] : required). Block adapter to use. This controls where the underlying data will be stored | blockstore.default_namespace_prefix (string : ) - Use this to help your users choose a storage namespace for their repositories. If specified, the storage namespace will be filled with this default value as a prefix when creating a repository from the UI. The user may still change it to something else. | blockstore.local.path (string: \"~/lakefs/data\") - When using the local Block Adapter, which directory to store files in | blockstore.gs.credentials_file (string : ) - If specified will be used as a file path of the JSON file that contains your Google service account key | blockstore.gs.credentials_json (string : ) - If specified will be used as JSON string that contains your Google service account key (when credentials_file is not set) | blockstore.azure.storage_account (string : ) - If specified, will be used as the Azure storage account | blockstore.azure.storage_access_key (string : ) - If specified, will be used as the Azure storage access key | blockstore.azure.auth_method (one of [\"msi\", \"access-key\"]: \"access-key\" ) - Authentication method to use (msi is used for Azure AD authentication). | blockstore.s3.region (string : \"us-east-1\") - Default region for lakeFS to use when interacting with S3. | blockstore.s3.profile (string : ) - If specified, will be used as a named credentials profile | blockstore.s3.credentials_file (string : ) - If specified, will be used as a credentials file | blockstore.s3.credentials.access_key_id (string : ) - If specified, will be used as a static set of credential | blockstore.s3.credentials.secret_access_key (string : ) - If specified, will be used as a static set of credential | blockstore.s3.credentials.session_token (string : ) - If specified, will be used as a static session token | blockstore.s3.endpoint (string : ) - If specified, custom endpoint for the AWS S3 API (https://s3_compatible_service_endpoint:port) | blockstore.s3.force_path_style (boolean : false) - When true, use path-style S3 URLs (https:/// instead of https://.) . | blockstore.s3.streaming_chunk_size (int : 1048576) - Object chunk size to buffer before streaming to blockstore (use a lower value for less reliable networks). Minimum is 8192. | blockstore.s3.streaming_chunk_timeout (time duration : \"60s\") - Per object chunk timeout for blockstore streaming operations (use a larger value for less reliable networks). | blockstore.s3.discover_bucket_region (boolean : true) - (Can be turned off if the underlying S3 bucket doesn’t support the GetBucketRegion API). | committed.local_cache - an object describing the local (on-disk) cache of metadata from permanent storage: . | committed.local_cache.size_bytes (int : 1073741824) - bytes for local cache to use on disk. The cache may use more storage for short periods of time. | committed.local_cache.dir (string, ~/lakefs/local_tier) - directory to store local cache. | committed.local_cache.range_proportion (float : 0.9) - proportion of local cache to use for storing ranges (leaves of committed metadata storage). | committed.local_cache.range.open_readers (int : 500) - maximal number of unused open SSTable readers to keep for ranges. | committed.local_cache.range.num_shards (int : 30) - sharding factor for open SSTable readers for ranges. Should be at least sqrt(committed.local_cache.range.open_readers). | committed.local_cache.metarange_proportion (float : 0.1) - proportion of local cache to use for storing metaranges (roots of committed metadata storage). | committed.local_cache.metarange.open_readers (int : 50) - maximal number of unused open SSTable readers to keep for metaranges. | committed.local_cache.metarange.num_shards (int : 10) - sharding factor for open SSTable readers for metaranges. Should be at least sqrt(committed.local_cache.metarange.open_readers). | . | committed.block_storage_prefix (string : _lakefs) - Prefix for metadata file storage in each repository’s storage namespace | committed.permanent.min_range_size_bytes (int : 0) - Smallest allowable range in metadata. Increase to somewhat reduce random access time on committed metadata, at the cost of increased committed metadata storage cost. | committed.permanent.max_range_size_bytes (int : 20971520) - Largest allowable range in metadata. Should be close to the size at which fetching from remote storage becomes linear. | committed.permanent.range_raggedness_entries (int : 50_000) - Average number of object pointers to store in each range (subject to min_range_size_bytes and max_range_size_bytes). | committed.sstable.memory.cache_size_bytes (int : 200_000_000) - maximal size of in-memory cache used for each SSTable reader. | email.smtp_host (string) - A string representing the URL of the SMTP host. | email.smtp_port (int) - An integer representing the port of the SMTP service (465, 587, 993, 25 are some standard ports) | email.use_ssl (bool : false) - Use SSL connection with SMTP host. | email.username (string) - A string representing the username of the specific account at the SMTP. It’s recommended to provide this value at runtime from a secret vault of some sort. | email.password (string) - A string representing the password of the account. It’s recommended to provide this value at runtime from a secret vault of some sort. | email.local_name (string) - A string representing the hostname sent to the SMTP server with the HELO command. By default, “localhost” is sent. | email.sender (string) - A string representing the email account which is set as the sender. | email.limit_every_duration (duration : 1m) - The average time between sending emails. If zero is entered, there is no limit to the amount of emails that can be sent. | email.burst (int: 10) - Maximal burst of emails before applying limit_every_duration. The zero value means no burst and therefore no emails can be sent. | email.lakefs_base_url (string : \"http://localhost:8000\") - A string representing the base lakefs endpoint to be directed to when emails are sent inviting users, reseting passwords etc. | gateways.s3.domain_name (string : \"s3.local.lakefs.io\") - a FQDN representing the S3 endpoint used by S3 clients to call this server (*.s3.local.lakefs.io always resolves to 127.0.0.1, useful for local development, if using virtual-host addressing. | gateways.s3.region (string : \"us-east-1\") - AWS region we’re pretending to be in, it should match the region configuration used in AWS SDK clients | gateways.s3.fallback_url (string) - If specified, requests with a non-existing repository will be forwarded to this URL. This can be useful for using lakeFS side-by-side with S3, with the URL pointing at an S3Proxy instance. | stats.enabled (boolean : true) - Whether to periodically collect anonymous usage statistics | security.audit_check_interval (duration : 12h) - Duration in which we check for security audit. | ui.enable (boolean: true) - Whether to server the embedded UI from the binary | . ",
    "url": "/v0.70/reference/configuration.html#reference",
    "relUrl": "/reference/configuration.html#reference"
  },"73": {
    "doc": "Configuration Reference",
    "title": "Using Environment Variables",
    "content": "All the configuration variables can be set or overridden using environment variables. To set an environment variable, prepend LAKEFS_ to its name, convert it to upper case, and replace . with _: . For example, logging.format becomes LAKEFS_LOGGING_FORMAT, blockstore.s3.region becomes LAKEFS_BLOCKSTORE_S3_REGION, etc. ",
    "url": "/v0.70/reference/configuration.html#using-environment-variables",
    "relUrl": "/reference/configuration.html#using-environment-variables"
  },"74": {
    "doc": "Configuration Reference",
    "title": "Example: Local Development",
    "content": "--- listen_address: \"0.0.0.0:8000\" database: connection_string: \"postgres://localhost:5432/postgres?sslmode=disable\" logging: format: text level: DEBUG output: \"-\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc09e90b6641\" blockstore: type: local local: path: \"~/lakefs/dev/data\" gateways: s3: region: us-east-1 . ",
    "url": "/v0.70/reference/configuration.html#example-local-development",
    "relUrl": "/reference/configuration.html#example-local-development"
  },"75": {
    "doc": "Configuration Reference",
    "title": "Example: AWS Deployment",
    "content": "--- logging: format: json level: WARN output: \"-\" database: connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: s3 s3: region: us-east-1 # optional, fallback in case discover from bucket is not supported credentials_file: /secrets/aws/credentials profile: default . ",
    "url": "/v0.70/reference/configuration.html#example-aws-deployment",
    "relUrl": "/reference/configuration.html#example-aws-deployment"
  },"76": {
    "doc": "Configuration Reference",
    "title": "Example: Google Storage",
    "content": "--- logging: format: json level: WARN output: \"-\" database: connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: gs gs: credentials_file: /secrets/lakefs-service-account.json . ",
    "url": "/v0.70/reference/configuration.html#example-google-storage",
    "relUrl": "/reference/configuration.html#example-google-storage"
  },"77": {
    "doc": "Configuration Reference",
    "title": "Example: MinIO",
    "content": "--- logging: format: json level: WARN output: \"-\" database: connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: s3 s3: force_path_style: true endpoint: http://localhost:9000 discover_bucket_region: false credentials: access_key_id: minioadmin secret_access_key: minioadmin . ",
    "url": "/v0.70/reference/configuration.html#example-minio",
    "relUrl": "/reference/configuration.html#example-minio"
  },"78": {
    "doc": "Configuration Reference",
    "title": "Example: Azure blob storage",
    "content": "--- logging: format: json level: WARN output: \"-\" database: connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: azure azure: auth_method: access-key storage_account: exampleStorageAcount storage_access_key: ExampleAcessKeyMD7nkPOWgV7d4BUjzLw== . ",
    "url": "/v0.70/reference/configuration.html#example-azure-blob-storage",
    "relUrl": "/reference/configuration.html#example-azure-blob-storage"
  },"79": {
    "doc": "Contributing",
    "title": "Contributing to lakeFS",
    "content": "Thank you for your interest in contributing to our project. Whether it’s a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure that we have all the necessary information to effectively respond to your bug report or contribution. Don’t know where to start? Reach out on the #dev channel on our Slack and we will help you get started. We also recommend this free series about contributing to OSS projects. ",
    "url": "/v0.70/contributing.html#contributing-to-lakefs",
    "relUrl": "/contributing.html#contributing-to-lakefs"
  },"80": {
    "doc": "Contributing",
    "title": "Getting Started",
    "content": "Before you get started, we kindly ask that you: . | Check out the code of conduct. | Sign the lakeFS CLA when making your first pull request (individual / corporate) | Submit any security issues directly to security@treeverse.io. | Contributions should have an associated GitHub issue. | Before making major contributions, please reach out to us on the #dev channel on Slack. We will make sure no one else is working on the same feature. | . ",
    "url": "/v0.70/contributing.html#getting-started",
    "relUrl": "/contributing.html#getting-started"
  },"81": {
    "doc": "Contributing",
    "title": "Setting up an Environment",
    "content": "This section was tested on macOS and Linux (Fedora 32, Ubuntu 20.04) - Your mileage may vary . Our Go release workflow holds the Go and Node.js versions we currently use under go-version and node-version compatibly. The Java workflows use Maven 3.8.1 (but any recent version of Maven should work). | Install the required dependencies for your OS: . | Git | GNU make (probably best to install from your OS package manager such as apt or brew) | Docker | Go | Node.js &amp; npm | Maven to build and test Spark client codes. | Optional - PostgreSQL 11 (useful for running and debugging locally) | . | With Apple M1, you can install Java from Azul Zulu Builds for Java JDK. | . | Clone the repository from https://github.com/treeverse/lakeFS (gives you read-only access to the repository. To contribute, see the next section). | Build the project: . make build . | . Note: Make build won’t work for Windows user for building lakeFS. | Make sure tests are passing: . make test . | . ",
    "url": "/v0.70/contributing.html#setting-up-an-environment",
    "relUrl": "/contributing.html#setting-up-an-environment"
  },"82": {
    "doc": "Contributing",
    "title": "Before creating a pull request",
    "content": ". | Review this document in full. | Make sure there’s an open issue on GitHub that this pull request addresses, and that it isn’t labeled x/wontfix. | Fork the lakeFS repository. | If you’re adding new functionality, create a new branch named feature/&lt;DESCRIPTIVE NAME&gt;. | If you’re fixing a bug, create a new branch named fix/&lt;DESCRIPTIVE NAME&gt;-&lt;ISSUE NUMBER&gt;. | . ",
    "url": "/v0.70/contributing.html#before-creating-a-pull-request",
    "relUrl": "/contributing.html#before-creating-a-pull-request"
  },"83": {
    "doc": "Contributing",
    "title": "Testing your change",
    "content": "Once you’ve made the necessary changes to the code, make sure the tests pass: . Run unit tests: . make test . Check that linting rules are passing: . make checks-validator . lakeFS uses go fmt as a style guide for Go code. Run system-tests: . make system-tests . Want to dive deeper into our system tests infrastructure? Need to debug the tests? Follow this documentation. ",
    "url": "/v0.70/contributing.html#testing-your-change",
    "relUrl": "/contributing.html#testing-your-change"
  },"84": {
    "doc": "Contributing",
    "title": "Submitting a pull request",
    "content": "Open a GitHub pull request with your change. The PR description should include a brief explanation of your change. You should also mention the related GitHub issue. If the issue should be automatically closed after the merge, please link it to the PR. After submitting your pull request, GitHub Actions will automatically run tests on your changes and make sure that your updated code builds and runs on Go 1.17.x. Check back shortly after submitting your pull request to make sure that your code passes these checks. If any of the checks come back with a red X, then do your best to address the errors. A developer from our team will review your pull request, and may request some changes to it. After the request is approved, it will be merged to our main branch. ",
    "url": "/v0.70/contributing.html#submitting-a-pull-request",
    "relUrl": "/contributing.html#submitting-a-pull-request"
  },"85": {
    "doc": "Contributing",
    "title": "Documentation",
    "content": "Documentation of features and changes in behaviour should be included in the pull request. You can create separate pull requests for documentation changes only. Documentation site customizations should be performed in accordance with the Just The Docs Customization guide, which is applied during the site creation process. To render the documentation locally and preview changes, use the following command and browse the documentation locally: . cd docs docker run --rm -p 4000:4000 --volume=\"$PWD:/srv/jekyll:Z\" -it jekyll/jekyll:3.8 jekyll serve . CHANGELOG.md . Any user-facing change should be labeled with include-changelog. The PR title should contain a concise summary of the feature or fix and the description should have the GitHub issue number. When we publish a new version of lakeFS, we will add this to the relevant version section of the changelog. If the change should not be included in the changelog, label it with exclude-changelog. ",
    "url": "/v0.70/contributing.html#documentation",
    "relUrl": "/contributing.html#documentation"
  },"86": {
    "doc": "Contributing",
    "title": "Contributing",
    "content": " ",
    "url": "/v0.70/contributing.html",
    "relUrl": "/contributing.html"
  },"87": {
    "doc": "Create a Repository",
    "title": "Create a Repository",
    "content": "A repository contains all of your objects, including the revision history. It can be considered the lakeFS analog of a bucket in an object store. Since it has version control characteristics, it’s also analogous to a repository in Git. ",
    "url": "/v0.70/setup/create-repo.html",
    "relUrl": "/setup/create-repo.html"
  },"88": {
    "doc": "Create a Repository",
    "title": "Create the first user",
    "content": "When you first open the lakeFS UI, you will be asked to create an initial admin user. | In your browser, open the address of your lakeFS server. Depending on how you deployed lakeFS, this can be a custom address pointing at your server (e.g., https://lakefs.example.com), the address of a load balancer, or something else. You should see the following page, prompting you to set up an admin user. Note: If you already have lakeFS credentials, log in and skip to creating the repository. | Follow the steps to create an initial administrator user. Save the credentials you’ve received somewhere safe, you won’t be able to see them again! . | Follow the link and go to the login screen. | Use the credentials to login as an administrator. | . ",
    "url": "/v0.70/setup/create-repo.html#create-the-first-user",
    "relUrl": "/setup/create-repo.html#create-the-first-user"
  },"89": {
    "doc": "Create a Repository",
    "title": "Create the repository",
    "content": ". | Click Create Repository. | Fill in a repository name. | Set the Storage Namespace to a location in the bucket you’ve configured in a previous step. The storage namespace is a location in the underlying storage where data for this repository will be stored. | Click Create Repository. | . ",
    "url": "/v0.70/setup/create-repo.html#create-the-repository",
    "relUrl": "/setup/create-repo.html#create-the-repository"
  },"90": {
    "doc": "Create a Repository",
    "title": "Next steps",
    "content": "You’ve just created your first lakeFS repository! . | You may now want to import data into your repository. | Check out how lakeFS easily integrates with your other tools. | Join us on Slack to introduce yourself, discover best practices and share your own! | . ",
    "url": "/v0.70/setup/create-repo.html#next-steps",
    "relUrl": "/setup/create-repo.html#next-steps"
  },"91": {
    "doc": "In Development",
    "title": "In Development",
    "content": "As part of our routine work with data we develop new code, improve and upgrade old code, upgrade infrastructures, and test new technologies. lakeFS enables a safe development environment on your data lake without the need to copy or mock data, work on the pipelines or involve DevOps. Creating a branch provides you an isolated environment with a snapshot of your repository (any part of your data lake you chose to manage on lakeFS). While working on your own branch in isolation, all other data users will be looking at the repository’s main branch. They can’t see your changes, and you don’t see changes to main done after you created the branch. No worries, no data duplication is done, it’s all metadata management behind the scenes. Let’s look at 3 examples of a development environment and their branching models. Example 1: Upgrading Spark and using Reset action . You installed the latest version of Apache Spark. As a first step you’ll test your Spark jobs to see that the upgrade doesn’t have any undesired side effects. For this purpose, you may create a branch (testing-spark-3.0) which will only be used to test the Spark upgrade, and discarded later. Jobs may run smoothly (the theoretical possibility exists!), or they may fail halfway through, leaving you with some intermediate partitions, data and metadata. In this case, you can simply reset the branch to its original state, without worrying about the intermediate results of your last experiment, and perform another (hopefully successful) test in an isolated branch. Reset actions are atomic and immediate, so no manual cleanup is required. Once testing is completed, and you have achieved the desired result, you can delete this experimental branch, and all data not used on any other branch will be deleted with it. Creating a testing branch: . lakectl branch create \\ lakefs://example-repo/testing-spark-3 \\ --source lakefs://example-repo/main # output: # created branch 'testing-spark-3' . Resetting changes to a branch: . lakectl branch reset lakefs://example-repo/testing-spark-3 # are you sure you want to reset all uncommitted changes?: y█ . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. Example 2: Collaborate &amp; Compare - Which option is better? . Easily compare by testing which one performs better on your data set. Examples may be: . | Different computation tools, e.g Spark vs. Presto | Different compression algorithms | Different Spark configurations | Different code versions of an ETL | . Run each experiment on its own independent branch, while the main remains untouched. Once both experiments are done, create a comparison query (using Hive or Presto or any other tool of your choice) to compare data characteristics, performance or any other metric you see fit. With lakeFS you don’t need to worry about creating data paths for the experiments, copying data, and remembering to delete it. It’s substantially easier to avoid errors and maintain a clean lake after. Reading from and comparing branches using Spark: . val dfExperiment1 = sc.read.parquet(\"s3a://example-repo/experiment-1/events/by-date\") val dfExperiment2 = sc.read.parquet(\"s3a://example-repo/experiment-2/events/by-date\") dfExperiment1.groupBy(\"...\").count() dfExperiment2.groupBy(\"...\").count() // now we can compare the properties of the data itself . ",
    "url": "/v0.70/using_lakefs/data-devenv.html",
    "relUrl": "/using_lakefs/data-devenv.html"
  },"92": {
    "doc": "Databricks",
    "title": "Using lakeFS with Databricks",
    "content": "Databricks is an Apache Spark-based analytics platform. ",
    "url": "/v0.70/integrations/databricks.html#using-lakefs-with-databricks",
    "relUrl": "/integrations/databricks.html#using-lakefs-with-databricks"
  },"93": {
    "doc": "Databricks",
    "title": "Table of contents",
    "content": ". | Configuration . | When running lakeFS inside your VPC . | Using multi-cluster writes | . | . | Reading Data | Writing Data | Case Study: SimilarWeb | . ",
    "url": "/v0.70/integrations/databricks.html#table-of-contents",
    "relUrl": "/integrations/databricks.html#table-of-contents"
  },"94": {
    "doc": "Databricks",
    "title": "Configuration",
    "content": "For Databricks to work with lakeFS, set the S3 Hadoop configuration to the lakeFS endpoint and credentials: . | In Databricks, go to your cluster configuration page. | Click Edit. | Expand Advanced Options. | Under the Spark tab, add the following configuration, replacing &lt;repo-name&gt; with your lakeFS repository name. Also, replace the credentials and endpoint with those of your lakeFS installation. | . spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.access.key AKIAIOSFODNN7EXAMPLE spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.secret.key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.endpoint https://lakefs.example.com spark.hadoop.fs.s3a.path.style.access true . When using DeltaLake tables, the following may also be required in some versions of Databricks: . spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.aws.credentials.provider shaded.databricks.org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.session.token lakefs . For more information, please see the documentation from Databricks. When running lakeFS inside your VPC . When lakeFS runs inside your private network, your Databricks cluster needs to be able to access it. This can be done by setting up a VPC peering between the two VPCs (the one where lakeFS runs and the one where Databricks runs). For this to work on DeltaLake tables, you would also have to disable multi-cluster writes with: . spark.databricks.delta.multiClusterWrites.enabled false . Using multi-cluster writes . When using multi-cluster writes, Databricks overrides Delta’s S3-commit action. The new action tries to contact lakeFS from servers on Databricks’ own AWS account, which of course won’t be able to access your private network. So, if you must use multi-cluster writes, you’ll have to allow access from Databricks’ AWS account to lakeFS. If you are trying to achieve that, please reach out on Slack and the community will try to assist. ",
    "url": "/v0.70/integrations/databricks.html#configuration",
    "relUrl": "/integrations/databricks.html#configuration"
  },"95": {
    "doc": "Databricks",
    "title": "Reading Data",
    "content": "To access objects in lakeFS. you’ll need to use the lakeFS path conventions: s3a://[REPOSITORY]/[BRANCH]/PATH/TO/OBJECT . Here is an example for reading a parquet file from lakeFS to a Spark DataFrame: . val repo = \"example-repo\" val branch = \"main\" val dataPath = s\"s3a://${repo}/${branch}/example-path/example-file.parquet\" val df = spark.read.parquet(dataPath) . You can now use this DataFrame like you would normally do. ",
    "url": "/v0.70/integrations/databricks.html#reading-data",
    "relUrl": "/integrations/databricks.html#reading-data"
  },"96": {
    "doc": "Databricks",
    "title": "Writing Data",
    "content": "Now simply write your results back to a lakeFS path: . df.write .partitionBy(\"example-column\") .parquet(s\"s3a://${repo}/${branch}/output-path/\") . The data is now created in lakeFS as new changes in your branch. You can now commit these changes or revert them. ",
    "url": "/v0.70/integrations/databricks.html#writing-data",
    "relUrl": "/integrations/databricks.html#writing-data"
  },"97": {
    "doc": "Databricks",
    "title": "Case Study: SimilarWeb",
    "content": "See how SimilarWeb integrated lakeFS with Databricks. ",
    "url": "/v0.70/integrations/databricks.html#case-study-similarweb",
    "relUrl": "/integrations/databricks.html#case-study-similarweb"
  },"98": {
    "doc": "Databricks",
    "title": "Databricks",
    "content": " ",
    "url": "/v0.70/integrations/databricks.html",
    "relUrl": "/integrations/databricks.html"
  },"99": {
    "doc": "dbt",
    "title": "Maintaining environments with dbt and lakeFS",
    "content": "dbt can run on lakeFS with a Spark adapter or Presto/Trino adapter. Both Spark and Presto use Hive metastore or Glue to manage tables and views. When creating a branch in lakeFS, you receive a logical copy of the data that can be accessed by s3://my-repo/branch/... To run a dbt project on a newly created branch, you need to have a copy of the metadata as well. The lakectl dbt command generates all the metadata needed in order to work on the newly created branch, continuing from the last state in the source branch. The dbt lakectl command does this using dbt commands and lakectl metastore commands. ",
    "url": "/v0.70/integrations/dbt.html#maintaining-environments-with-dbt-and-lakefs",
    "relUrl": "/integrations/dbt.html#maintaining-environments-with-dbt-and-lakefs"
  },"100": {
    "doc": "dbt",
    "title": "Table of contents",
    "content": ". | Configuration . | Hive metastore | Glue | . | Views . | Using lakectl | Manual configuration | . | Create Schema | . ",
    "url": "/v0.70/integrations/dbt.html#table-of-contents",
    "relUrl": "/integrations/dbt.html#table-of-contents"
  },"101": {
    "doc": "dbt",
    "title": "Configuration",
    "content": "To run the lakectl-dbt commands you need to configure both dbt and lakectl. Assuming dbt is already configured, using either a Spark or Presto/Trino target you’ll need to add configurations to give lakeFS access to your catalog (metastore). This is done by adding the following configurations to the lakectl configuration file (by default ~/.lakectl.yaml) . Hive metastore . metastore: type: hive hive: uri: hive-metastore:9083 . Glue . metastore: type: glue glue: catalog-id: 123456789012 region: us-east-1 profile: default # optional, implies using a credentials file credentials: access_key_id: AKIAIOSFODNN7EXAMPLE secret_access_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY . ",
    "url": "/v0.70/integrations/dbt.html#configuration",
    "relUrl": "/integrations/dbt.html#configuration"
  },"102": {
    "doc": "dbt",
    "title": "Views",
    "content": "lakectl copies all the models materialized as tables and incremental directly on your metastore. However, copying views should be done manually or with lakectl. Using lakectl . The generate_schema_name macro could be used by lakectl to create models using dbt on a dynamic schema. The following command will add a macro to your project, allowing lakectl to run dbt on the destination schema using an environment variable. lakectl dbt generate-schema-macro . Manual configuration . If you don’t want to add the generate_schema_name macro to your project, you can create the views on the destination schema manually. For every run: . | use the --skip-views flag, | change the default schema to be the branch schema in your dbt configuration file, | run dbt on all views. | . dbt run --select config.materialized:view . ",
    "url": "/v0.70/integrations/dbt.html#views",
    "relUrl": "/integrations/dbt.html#views"
  },"103": {
    "doc": "dbt",
    "title": "Create Schema",
    "content": "Creating the schema From your dbt project run: . lakectl dbt create-branch-schema --branch my-branch --to-schema my_branch . You can find more advanced options here. ",
    "url": "/v0.70/integrations/dbt.html#create-schema",
    "relUrl": "/integrations/dbt.html#create-schema"
  },"104": {
    "doc": "dbt",
    "title": "dbt",
    "content": " ",
    "url": "/v0.70/integrations/dbt.html",
    "relUrl": "/integrations/dbt.html"
  },"105": {
    "doc": "Delta Lake",
    "title": "Using lakeFS with Delta Lake",
    "content": "Delta Lake is an open file format designed to improve performance and provide transactional guarantees to data lake tables. lakeFS is format-agnostic, so you can save data in Delta format within a lakeFS repository to reap the benefits of both technologies. Specifically: . | ACID operations can now span across many Delta tables. | CI/CD hooks can validate Delta table contents, schema, or even referential integrity. | lakeFS supports zero-copy branching for quick experimentation with full isolation. | . ",
    "url": "/v0.70/integrations/delta.html#using-lakefs-with-delta-lake",
    "relUrl": "/integrations/delta.html#using-lakefs-with-delta-lake"
  },"106": {
    "doc": "Delta Lake",
    "title": "Table of contents",
    "content": ". | Configuration | Limitations | Read more | . ",
    "url": "/v0.70/integrations/delta.html#table-of-contents",
    "relUrl": "/integrations/delta.html#table-of-contents"
  },"107": {
    "doc": "Delta Lake",
    "title": "Configuration",
    "content": "Most commonly, you interact with Delta tables in a Spark environment - given the native integration between Delta Lake and Spark. To configure a Spark environment to read from and write to a Delta table within a lakeFS repository, you need to set the proper credentials and endpoint in the S3 Hadoop configuration, like you’d do with any Spark script. sc.hadoopConfiguration.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") sc.hadoopConfiguration.set(\"spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.access.key\", \"AKIAIOSFODNN7EXAMPLE\") sc.hadoopConfiguration.set(\"spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\") sc.hadoopConfiguration.set(\"spark.hadoop.fs.s3a.bucket.&lt;repo-name&gt;.endpoint\", \"https://lakefs.example.com\") . Once set, you can interact with Delta tables using regular Spark path URIs. Make sure that you include the lakeFS repository and branch name: . data.write.format(\"delta\").save(\"s3a://&lt;repo-name&gt;/&lt;branch-name&gt;/path/to/delta-table\") . Note: If using the Databricks Analytics Platform, see the integration guide for configuring a Databricks cluster to use lakeFS. ",
    "url": "/v0.70/integrations/delta.html#configuration",
    "relUrl": "/integrations/delta.html#configuration"
  },"108": {
    "doc": "Delta Lake",
    "title": "Limitations",
    "content": "The Delta log is an auto-generated sequence of text files used to keep track of transactions on a Delta table sequentially. Writing to one Delta table from multiple lakeFS branches is possible, but note that it would result in conflicts if later attempting to merge one branch into the other. For this reason, production workflows should ideally write to a single lakeFS branch that could then be safely merged into main. ",
    "url": "/v0.70/integrations/delta.html#limitations",
    "relUrl": "/integrations/delta.html#limitations"
  },"109": {
    "doc": "Delta Lake",
    "title": "Read more",
    "content": "See this post on the lakeFS blog that shows how to guarantee data quality in a Delta table by utilizing lakeFS branches. ",
    "url": "/v0.70/integrations/delta.html#read-more",
    "relUrl": "/integrations/delta.html#read-more"
  },"110": {
    "doc": "Delta Lake",
    "title": "Delta Lake",
    "content": " ",
    "url": "/v0.70/integrations/delta.html",
    "relUrl": "/integrations/delta.html"
  },"111": {
    "doc": "Copying Data with DistCp",
    "title": "Copying Data to/from lakeFS with DistCp",
    "content": "Apache Hadoop DistCp (distributed copy) is a tool used for large inter/intra-cluster copying. You can easily use it with your lakeFS repositories. ",
    "url": "/v0.70/integrations/distcp.html#copying-data-tofrom-lakefs-with-distcp",
    "relUrl": "/integrations/distcp.html#copying-data-tofrom-lakefs-with-distcp"
  },"112": {
    "doc": "Copying Data with DistCp",
    "title": "Table of contents",
    "content": ". | Copying from lakeFS to lakeFS | Copying between S3 and lakeFS . | From S3 to lakeFs | From lakeFS to S3 | . | . Note . In the following examples, we set AWS credentials on the command line for clarity. In production, you should set these properties using one of Hadoop’s standard ways of Authenticating with S3. ",
    "url": "/v0.70/integrations/distcp.html#table-of-contents",
    "relUrl": "/integrations/distcp.html#table-of-contents"
  },"113": {
    "doc": "Copying Data with DistCp",
    "title": "Copying from lakeFS to lakeFS",
    "content": "You can use DistCP to copy between two different lakeFS repositories. Replace the access key pair with your lakeFS access key pair: . hadoop distcp \\ -Dfs.s3a.path.style.access=true \\ -Dfs.s3a.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\ -Dfs.s3a.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\ -Dfs.s3a.endpoint=\"https://lakefs.example.com\" \\ \"s3a://example-repo-1/main/example-file.parquet\" \\ \"s3a://example-repo-2/main/example-file.parquet\" . val workDir = s”s3a://${repo}/${branch}/collection/shows” val dataPath = s”$workDir/title.basics.parquet” . ",
    "url": "/v0.70/integrations/distcp.html#copying-from-lakefs-to-lakefs",
    "relUrl": "/integrations/distcp.html#copying-from-lakefs-to-lakefs"
  },"114": {
    "doc": "Copying Data with DistCp",
    "title": "Copying between S3 and lakeFS",
    "content": "To copy between an S3 bucket and lakeFS repository, use Hadoop’s per-bucket configuration. In the following examples, replace the first access key pair with your lakeFS key pair, and the second one with your AWS IAM key pair: . From S3 to lakeFs . hadoop distcp \\ -Dfs.s3a.path.style.access=true \\ -Dfs.s3a.bucket.example-repo.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\ -Dfs.s3a.bucket.example-repo.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\ -Dfs.s3a.bucket.example-repo.endpoint=\"https://lakefs.example.com\" \\ -Dfs.s3a.bucket.example-bucket.access.key=\"AKIAIOSFODNN3EXAMPLE\" \\ -Dfs.s3a.bucket.example-bucket.secret.key=\"wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY\" \\ \"s3a://example-bucket/example-file.parquet\" \\ \"s3a://example-repo/main/example-file.parquet\" . From lakeFS to S3 . hadoop distcp \\ -Dfs.s3a.path.style.access=true \\ -Dfs.s3a.bucket.example-repo.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\ -Dfs.s3a.bucket.example-repo.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\ -Dfs.s3a.bucket.example-repo.endpoint=\"https://lakefs.example.com\" \\ -Dfs.s3a.bucket.example-bucket.access.key=\"AKIAIOSFODNN3EXAMPLE\" \\ -Dfs.s3a.bucket.example-bucket.secret.key=\"wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY\" \\ \"s3a://example-repo/main/myfile\" \\ \"s3a://example-bucket/myfile\" . ",
    "url": "/v0.70/integrations/distcp.html#copying-between-s3-and-lakefs",
    "relUrl": "/integrations/distcp.html#copying-between-s3-and-lakefs"
  },"115": {
    "doc": "Copying Data with DistCp",
    "title": "Copying Data with DistCp",
    "content": " ",
    "url": "/v0.70/integrations/distcp.html",
    "relUrl": "/integrations/distcp.html"
  },"116": {
    "doc": "With Docker",
    "title": "Deploy lakeFS on Docker",
    "content": " ",
    "url": "/v0.70/deploy/docker.html#deploy-lakefs-on-docker",
    "relUrl": "/deploy/docker.html#deploy-lakefs-on-docker"
  },"117": {
    "doc": "With Docker",
    "title": "Database",
    "content": "lakeFS requires a PostgreSQL database to synchronize actions on your repositories. This section assumes that you already have a PostgreSQL database accessible from where you intend to install lakeFS. You can find instructions for creating the database in the deployment instructions for AWS, Azure and GCP. ",
    "url": "/v0.70/deploy/docker.html#database",
    "relUrl": "/deploy/docker.html#database"
  },"118": {
    "doc": "With Docker",
    "title": "Table of contents",
    "content": ". | Prerequisites | Installing on Docker | Load balancing | Next Steps | . ",
    "url": "/v0.70/deploy/docker.html#table-of-contents",
    "relUrl": "/deploy/docker.html#table-of-contents"
  },"119": {
    "doc": "With Docker",
    "title": "Prerequisites",
    "content": "Users that require S3 access using virtual host addressing should configure an S3 Gateway domain. ",
    "url": "/v0.70/deploy/docker.html#prerequisites",
    "relUrl": "/deploy/docker.html#prerequisites"
  },"120": {
    "doc": "With Docker",
    "title": "Installing on Docker",
    "content": "To deploy using Docker, create a yaml configuration file. Here is a minimal example, but you can see the reference for the full list of configurations. | AWS | Google Cloud | Microsoft Azure | . database: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: s3 . database: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: gs # Uncomment the following lines to give lakeFS access to your buckets using a service account: # gs: # credentials_json: [YOUR SERVICE ACCOUNT JSON STRING] . database: connection_string: \"postgres://user:pass@&lt;AZURE_POSTGRES_SERVER_NAME&gt;...\" auth: encrypt: secret_key: \"&lt;RANDOM_GENERATED_STRING&gt;\" blockstore: type: azure azure: auth_method: msi # msi for active directory, access-key for access key # In case you chose to authenticate via access key replace unmark the following rows and insert the values from the previous step # storage_account: &lt;your storage account&gt; # storage_access_key: &lt;your access key&gt; . Save the configuration file locally as lakefs-config.yaml and run the following command: . docker run \\ --name lakefs \\ -p 8000:8000 \\ -v $(pwd)/lakefs-config.yaml:/etc/lakefs/config.yaml \\ treeverse/lakefs:latest run --config /etc/lakefs/config.yaml . ",
    "url": "/v0.70/deploy/docker.html#installing-on-docker",
    "relUrl": "/deploy/docker.html#installing-on-docker"
  },"121": {
    "doc": "With Docker",
    "title": "Load balancing",
    "content": "You should have a load balancer direct requests to the lakeFS server. By default, lakeFS operates on port 8000, and exposes a /_health endpoint which you can use for health checks. ",
    "url": "/v0.70/deploy/docker.html#load-balancing",
    "relUrl": "/deploy/docker.html#load-balancing"
  },"122": {
    "doc": "With Docker",
    "title": "Next Steps",
    "content": "Your next step is to prepare your storage. If you already have a storage bucket/container, you are ready to create your first lakeFS repository. ",
    "url": "/v0.70/deploy/docker.html#next-steps",
    "relUrl": "/deploy/docker.html#next-steps"
  },"123": {
    "doc": "With Docker",
    "title": "With Docker",
    "content": " ",
    "url": "/v0.70/deploy/docker.html",
    "relUrl": "/deploy/docker.html"
  },"124": {
    "doc": "Dremio",
    "title": "Using lakeFS with Dremio",
    "content": "Dremio is a next-generation data lake engine that liberates your data with live, interactive queries directly on cloud data lake storage, including S3 and lakeFS. ",
    "url": "/v0.70/integrations/dremio.html#using-lakefs-with-dremio",
    "relUrl": "/integrations/dremio.html#using-lakefs-with-dremio"
  },"125": {
    "doc": "Dremio",
    "title": "Configuration",
    "content": "Starting from version 3.2.3, Dremio supports Minio as an experimental S3-compatible plugin. Similarly, you can connect lakeFS with Dremio. Suppose you already have both lakeFS and Dremio deployed, and want to use Dremio to query your data in the lakeFS repositories. You can follow the steps listed below to configure on Dremio UI: . | click Add Data Lake. | Under File Stores, choose Amazon S3. | Under Advanced Options, check Enable compatibility mode (experimental). | Under Advanced Options &gt; Connection Properties, add fs.s3a.path.style.access and set the value to true. | Under Advanced Options &gt; Connection Properties, add fs.s3a.endpoint and set lakeFS S3 endpoint to the value. | Under the General tab, specify the access_key_id and secret_access_key provided by lakeFS server. | Click Save, and now you should be able to browse lakeFS repositories on Dremio. | . ",
    "url": "/v0.70/integrations/dremio.html#configuration",
    "relUrl": "/integrations/dremio.html#configuration"
  },"126": {
    "doc": "Dremio",
    "title": "Dremio",
    "content": " ",
    "url": "/v0.70/integrations/dremio.html",
    "relUrl": "/integrations/dremio.html"
  },"127": {
    "doc": "EMR",
    "title": "Using lakeFS with EMR",
    "content": "Amazon EMR is a managed cluster platform that simplifies running Big Data frameworks, such as Apache Hadoop and Apache Spark. ",
    "url": "/v0.70/integrations/emr.html#using-lakefs-with-emr",
    "relUrl": "/integrations/emr.html#using-lakefs-with-emr"
  },"128": {
    "doc": "EMR",
    "title": "Configuration",
    "content": "To configure Spark on EMR to work with lakeFS, you need to set the lakeFS credentials and endpoint in the appropriate fields. The exact configuration keys depends on the application running in EMR, but their format follows this form: . lakeFS endpoint: *.fs.s3a.endpoint . lakeFS access key: *.fs.s3a.access.key . lakeFS secret key: *.fs.s3a.secret.key . EMR will encourage users to use s3:// with Spark as it will use EMR’s proprietary driver. Users need to use s3a:// for this guide to work. The Spark job reads and writes will be directed to the lakeFS instance, using the S3 gateway. You can choose from two options for configuring an EMR cluster to work with lakeFS: . | When you create a cluster - All the steps will use the cluster configuration. No specific configuration needed when adding a step. | Configuring on each step - A cluster is created with the default S3 configuration. Each step using lakeFS should pass the appropriate config params. | . ",
    "url": "/v0.70/integrations/emr.html#configuration",
    "relUrl": "/integrations/emr.html#configuration"
  },"129": {
    "doc": "EMR",
    "title": "Configuration on cluster creation",
    "content": "Use the below configuration when creating the cluster. You may delete any app configuration that is not suitable for your use case. [{ \"Classification\": \"presto-connector-hive\", \"Properties\": { \"hive.s3.aws-access-key\": \"AKIAIOSFODNN7EXAMPLE\", \"hive.s3.aws-secret-key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"hive.s3.endpoint\": \"https://lakefs.example.com\", \"hive.s3.path-style-access\": \"true\", \"hive.s3-file-system-type\": \"PRESTO\" } }, { \"Classification\": \"hive-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://lakefs.example.com\", \"fs.s3.path.style.access\": \"true\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://lakefs.example.com\", \"fs.s3a.path.style.access\": \"true\" } }, { \"Classification\": \"hdfs-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://lakefs.example.com\", \"fs.s3.path.style.access\": \"true\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://lakefs.example.com\", \"fs.s3a.path.style.access\": \"true\" } }, { \"Classification\": \"core-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://lakefs.example.com\", \"fs.s3.path.style.access\": \"true\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://lakefs.example.com\", \"fs.s3a.path.style.access\": \"true\" } }, { \"Classification\": \"emrfs-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://lakefs.example.com\", \"fs.s3.path.style.access\": \"true\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://lakefs.example.com\", \"fs.s3a.path.style.access\": \"true\" } }, { \"Classification\": \"mapred-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://lakefs.example.com\", \"fs.s3.path.style.access\": \"true\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://lakefs.example.com\", \"fs.s3a.path.style.access\": \"true\" } }, { \"Classification\": \"spark-defaults\", \"Properties\": { \"spark.sql.catalogImplementation\": \"hive\" } } ] . ",
    "url": "/v0.70/integrations/emr.html#configuration-on-cluster-creation",
    "relUrl": "/integrations/emr.html#configuration-on-cluster-creation"
  },"130": {
    "doc": "EMR",
    "title": "Configuration on adding a step",
    "content": "When a cluster was created without the above configuration, you can still use lakeFS when adding a step. For example, when creating a Spark job: . aws emr add-steps --cluster-id j-197B3AEGQ9XE4 \\ --steps=\"Type=Spark,Name=SparkApplication,ActionOnFailure=CONTINUE, \\ Args=[--conf,spark.hadoop.fs.s3a.access.key=AKIAIOSFODNN7EXAMPLE, \\ --conf,spark.hadoop.fs.s3a.secret.key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY, \\ --conf,spark.hadoop.fs.s3a.endpoint=https://lakefs.example.com, \\ --conf,spark.hadoop.fs.s3a.path.style.access=true, \\ s3a://&lt;lakefs-repo&gt;/&lt;lakefs-branch&gt;/path/to/jar]\" . The Spark context in the running job will already be initialized to use the provided lakeFS configuration. There’s no need to repeat the configuration steps mentioned in Using lakeFS with Spark . ",
    "url": "/v0.70/integrations/emr.html#configuration-on-adding-a-step",
    "relUrl": "/integrations/emr.html#configuration-on-adding-a-step"
  },"131": {
    "doc": "EMR",
    "title": "EMR",
    "content": " ",
    "url": "/v0.70/integrations/emr.html",
    "relUrl": "/integrations/emr.html"
  },"132": {
    "doc": "Exporting Data",
    "title": "Exporting Data",
    "content": "The export operation copies all data from a given lakeFS commit to a designated object store location. For instance, the contents lakefs://example/main might be exported on s3://company-bucket/example/latest. Clients entirely unaware of lakeFS could use that base URL to access latest files on main. Clients aware of lakeFS can continue to use the lakeFS S3 endpoint to access repository files on s3://example/main, as well as other versions and uncommitted versions. Possible use-cases: . | External consumers of data don’t have access to your lakeFS installation. | Some data pipelines in the organization are not fully migrated to lakeFS. | You want to experiment with lakeFS as a side-by-side installation first. | Create copies of your data lake in other regions (taking into account read pricing). | . ",
    "url": "/v0.70/reference/export.html",
    "relUrl": "/reference/export.html"
  },"133": {
    "doc": "Exporting Data",
    "title": "Table of contents",
    "content": ". | Exporting Data With Spark . | Using spark-submit | Using custom code (Notebook/Spark) | . | Success/Failure Indications | Export Rounds (Spark success files) | Example | Exporting Data with Docker | . ",
    "url": "/v0.70/reference/export.html#table-of-contents",
    "relUrl": "/reference/export.html#table-of-contents"
  },"134": {
    "doc": "Exporting Data",
    "title": "Exporting Data With Spark",
    "content": "Using spark-submit . You can use the export main in three different modes: . | Export all the objects from branch example-branch on example-repo repository to S3 location s3://example-bucket/prefix/: .... example-repo s3://example-bucket/prefix/ --branch=example-branch . | Export all the objects from a commit c805e49bafb841a0875f49cd555b397340bbd9b8 on example-repo repository to S3 location s3://example-bucket/prefix/: .... example-repo s3://example-bucket/prefix/ --commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8 . | Export only the diff between branch example-branch and commit c805e49bafb841a0875f49cd555b397340bbd9b8 on example-repo repository to S3 location s3://example-bucket/prefix/: .... example-repo s3://example-bucket/prefix/ --branch=example-branch --prev_commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8 . | . The complete spark-submit command would look as follows: . spark-submit --conf spark.hadoop.lakefs.api.url=https://&lt;LAKEFS_ENDPOINT&gt;/api/v1 \\ --conf spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY_ID&gt; \\ --conf spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_ACCESS_KEY&gt; \\ --packages io.lakefs:lakefs-spark-client-301_2.12:0.1.6 \\ --class io.treeverse.clients.Main export-app example-repo s3://example-bucket/prefix \\ --branch=example-branch . The command assumes that the Spark cluster has permissions to write to s3://example-bucket/prefix. Otherwise, add spark.hadoop.fs.s3a.access.key and spark.hadoop.fs.s3a.secret.key with the proper credentials. Using custom code (Notebook/Spark) . Set up lakeFS Spark metadata client with the endpoint and credentials as instructed in the previous page. The client exposes the Exporter object with three export options: . | Export all the objects at the HEAD of a given branch. Does not include files that were added to that branch but were not committed. | . exportAllFromBranch(branch: String) . | Export ALL objects from a commit: | . exportAllFromCommit(commitID: String) . | Export just the diff between a commit and the HEAD of a branch. This is an ideal option for continuous exports of a branch since it will change only the files that have been changed since the previous commit. exportFrom(branch: String, prevCommitID: String) . | . ",
    "url": "/v0.70/reference/export.html#exporting-data-with-spark",
    "relUrl": "/reference/export.html#exporting-data-with-spark"
  },"135": {
    "doc": "Exporting Data",
    "title": "Success/Failure Indications",
    "content": "When the Spark export operation ends, an additional status file will be added to the root object storage destination. If all files were exported successfully, the file path will be of the form: EXPORT_&lt;commitID&gt;_&lt;ISO-8601-time-UTC&gt;_SUCCESS. For failures: the form will beEXPORT_&lt;commitID&gt;_&lt;ISO-8601-time-UTC&gt;_FAILURE, and the file will include a log of the failed files operations. ",
    "url": "/v0.70/reference/export.html#successfailure-indications",
    "relUrl": "/reference/export.html#successfailure-indications"
  },"136": {
    "doc": "Exporting Data",
    "title": "Export Rounds (Spark success files)",
    "content": "Some files should be exported before others, e.g., a Spark _SUCCESS file exported before other files under the same prefix might send the wrong indication. The export operation may contain several rounds within the same export. A failing round will stop the export of all the files of the next rounds. By default, lakeFS will use the SparkFilter and have 2 rounds for each export. The first round will export any non-Spark _SUCCESS files. Second round will export all Spark’s _SUCCESS files. You may override the default behavior by passing a custom filter to the Exporter. ",
    "url": "/v0.70/reference/export.html#export-rounds-spark-success-files",
    "relUrl": "/reference/export.html#export-rounds-spark-success-files"
  },"137": {
    "doc": "Exporting Data",
    "title": "Example",
    "content": ". | First configure the Exporter instance: . import io.treeverse.clients.{ApiClient, Exporter} import org.apache.spark.sql.SparkSession val endpoint = \"http://&lt;LAKEFS_ENDPOINT&gt;/api/v1\" val accessKey = \"&lt;LAKEFS_ACCESS_KEY_ID&gt;\" val secretKey = \"&lt;LAKEFS_SECRET_ACCESS_KEY&gt;\" val repo = \"example-repo\" val spark = SparkSession.builder().appName(\"I can export\").master(\"local\").getOrCreate() val sc = spark.sparkContext sc.hadoopConfiguration.set(\"lakefs.api.url\", endpoint) sc.hadoopConfiguration.set(\"lakefs.api.access_key\", accessKey) sc.hadoopConfiguration.set(\"lakefs.api.secret_key\", secretKey) // Add any required spark context configuration for s3 val rootLocation = \"s3://company-bucket/example/latest\" val apiClient = new ApiClient(endpoint, accessKey, secretKey) val exporter = new Exporter(spark, apiClient, repo, rootLocation) . | Now you can export all objects from main branch to s3://company-bucket/example/latest: . val branch = \"main\" exporter.exportAllFromBranch(branch) . | Assuming a previous successful export on commit f3c450d8cd0e84ac67e7bc1c5dcde9bef82d8ba7, you can alternatively export just the difference between main branch and the commit: . val branch = \"main\" val commit = \"f3c450d8cd0e84ac67e7bc1c5dcde9bef82d8ba7\" exporter.exportFrom(branch, commit) . | . ",
    "url": "/v0.70/reference/export.html#example",
    "relUrl": "/reference/export.html#example"
  },"138": {
    "doc": "Exporting Data",
    "title": "Exporting Data with Docker",
    "content": "This option is recommended if you don’t have Spark at your tool-set. It doesn’t support distribution across machines, therefore may have a lower performance. Using this method, you can export data from lakeFS to S3 using the export options (in a similar way to the Spark export): . | Export all objects from a branch example-branch on example-repo repository to S3 location s3://destination-bucket/prefix/: .... example-repo s3://destination-bucket/prefix/ --branch=\"example-branch\" . | Export all objects from a commit c805e49bafb841a0875f49cd555b397340bbd9b8 on example-repo repository to S3 location s3://destination-bucket/prefix/: .... example-repo s3://destination-bucket/prefix/ --commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8 . | Export only the diff between branch example-branch and commit c805e49bafb841a0875f49cd555b397340bbd9b8 on example-repo repository to S3 location s3://destination-bucket/prefix/: .... example-repo s3://destination-bucket/prefix/ --branch=\"example-branch\" --prev_commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8 . | . You will need to add the relevant environment variables. The complete docker run command would look like: . docker run \\ -e LAKEFS_ACCESS_KEY=XXX -e LAKEFS_SECRET_KEY=YYY -e LAKEFS_ENDPOINT=https://&lt;LAKEFS_ENDPOINT&gt;/ \\ -e S3_ACCESS_KEY=XXX -e S3_SECRET_KEY=YYY \\ -it treeverse/lakefs-rclone-export:latest example-repo s3://destination-bucket/prefix/ --branch=\"example-branch\" . Note: This feature uses rclone, and specifically rclone sync. This can change the destination path, therefore the s3 destination location must be designated to lakeFS export. ",
    "url": "/v0.70/reference/export.html#exporting-data-with-docker",
    "relUrl": "/reference/export.html#exporting-data-with-docker"
  },"139": {
    "doc": "FAQ",
    "title": "FAQ",
    "content": "1. Is lakeFS open-source? . lakeFS is completely free, open-source, and licensed under the Apache 2.0 License. We maintain a public product roadmap and a Slack channel for open discussions. 2. How does lakeFS data versioning work? . lakeFS uses a copy-on-write mechanism to avoid data duplication. For example, creating a new branch is a metadata-only operation: no objects are actually copied. Only when an object changes does lakeFS create another version of the data in the storage. For more information, see Versioning internals. 3. How do I get support for my lakeFS installation? . We are extremely responsive on our Slack channel, and we make sure to prioritize the most pressing issues for the community. For SLA-based support, please contact us at support@treeverse.io. 4. Do you collect data from your active installations? . We collect anonymous usage statistics to understand the patterns of use and to detect product gaps we may have so we can fix them. This is optional and may be turned off by setting stats.enabled to false. See the configuration reference for more details. The data we gather is limited to the following: . | A UUID which is generated when setting up lakeFS for the first time and contains no personal or otherwise identifiable information, | The lakeFS version currently running, | The OS and architecture lakeFS is running on, | Metadata regarding the database used (version, installed extensions and parameters such as DB Timezone and work memory), | Periodic aggregated action counters (e.g. how many “get_object” operations occurred). | . 5. How is lakeFS different from Delta Lake / Hudi / Iceberg? . Delta Lake, Hudi, and Iceberg all define dedicated, structured data formats that allow deletes and upserts. lakeFS is format-agnostic and enables consistent cross-collection versioning of your data using Git-like operations. Read our comparison for a more detailed comparison. 6. What inspired the lakeFS logo? . The Axolotl – a species of salamander, also known as the Mexican Lake Monster or the Peter Pan of the animal kingdom. It’s a magical creature, living in a lake - just like us! :) . copyright . ",
    "url": "/v0.70/faq.html",
    "relUrl": "/faq.html"
  },"140": {
    "doc": "Commit the Changes",
    "title": "Commit the Changes",
    "content": " ",
    "url": "/v0.70/quickstart/first_commit.html",
    "relUrl": "/quickstart/first_commit.html"
  },"141": {
    "doc": "Commit the Changes",
    "title": "Install lakectl",
    "content": "lakeFS comes with its own native CLI client, lakectl. You can use it to perform Git-like operations like committing, reverting, and merging. Follow the tutorial video below to get started with the CLI, or follow the instructions on this page. Here’s how to get started with the CLI: . | Download the CLI binary: . You can download the CLI binary from the official lakeFS releases published on GitHub under “Assets”. Unless you need a specific previous version of the CLI, it’s recommended to download the most recently released one. The Operating System of the computer you’re using determines whether you should pick the binary Asset compiled for Windows, Linux, or Mac (Darwin). For Mac and Linux Operating Systems, the processor determines whether you should download the x64 or arm binary. | Once unzipped, inside the downloaded asset you’ll see a file named lakectl. It’s recommended that you place this file somewhere in your PATH (this is OS dependant but for *NIX systems , /usr/local/bin is usually a safe bet). Once in your PATH, you’ll be able to open a Terminal program and run lakectl commands! . | We recommend starting with lakectl config to configure the CLI to use the credentials created earlier: . lakectl config # output: # Config file /home/janedoe/.lakectl.yaml will be used # Access key ID: AKIAJVHTOKZWGCD2QQYQ # Secret access key: **************************************** # Server endpoint URL: http://localhost:8000 . Note The first time you run a lakectl command you may need to respond to a prompt to allow the program to run (depending on your computer’s security settings). | To verify that lakectl is properly configured, you can list the branches in your repository: . lakectl branch list lakefs://example-repo # output: # +----------+------------------------------------------------------------------+ # | REF NAME | COMMIT ID | # +----------+------------------------------------------------------------------+ # | main | a91f56a7e11be1348fc405053e5234e4af7d6da01ed02f3d9a8ba7b1f71499c8 | # +----------+------------------------------------------------------------------+ . | . ",
    "url": "/v0.70/quickstart/first_commit.html#install-lakectl",
    "relUrl": "/quickstart/first_commit.html#install-lakectl"
  },"142": {
    "doc": "Commit the Changes",
    "title": "Perform your first commit",
    "content": "Now you can commit the file you’ve added in the previous section: . lakectl commit lakefs://example-repo/main -m 'added my first file!' # output: # Commit for branch \"main\" done. # # ID: 901f7b21e1508e761642b142aea0ccf28451675199655381f65101ea230ebb87 # Timestamp: 2021-06-15 13:48:37 +0300 IDT # Parents: a91f56a7e11be1348fc405053e5234e4af7d6da01ed02f3d9a8ba7b1f71499c8 . Note: lakeFS versions &lt;= v0.33.1 used ‘@’ (instead of ‘/’) as separator between repository and branch. And finally, you can view the log to see the new commit: . lakectl log lakefs://example-repo/main # output: # commit 901f7b21e1508e761642b142aea0ccf28451675199655381f65101ea230ebb87 # Author: Example User &lt;user@example.com&gt; # Date: 2021-06-15 13:48:37 +0300 IDT added my first file! . Congratulations! You’ve completed your first commit in lakeFS. Next steps . | Learn how to deploy lakeFS lakeFS on your cloud. | Join us on Slack to introduce yourself, discover best practices and share your own! | . ",
    "url": "/v0.70/quickstart/first_commit.html#perform-your-first-commit",
    "relUrl": "/quickstart/first_commit.html#perform-your-first-commit"
  },"143": {
    "doc": "Garbage Collection",
    "title": "Garbage Collection",
    "content": "By default, lakeFS keeps all your objects forever. This allows you to travel back in time to previous versions of your data. However, sometimes you may want to hard-delete your objects - namely, delete them from the underlying storage. Reasons for this include cost-reduction and privacy policies. Garbage collection rules in lakeFS define for how long to retain objects after they have been deleted (see more information below). lakeFS provides a Spark program to hard-delete objects that have been deleted and whose retention period has ended according to the GC rules. The GC job does not remove any commits: you will still be able to use commits containing hard-deleted objects, but trying to read these objects from lakeFS will result in a 410 Gone HTTP status. Note At this point, lakeFS supports Garbage Collection only on S3 and Azure. We have concrete plans to extend the support to GCP. | Understanding Garbage Collection . | What gets collected | What does not get collected | . | Configuring GC rules | Running the GC job | Considerations | . ",
    "url": "/v0.70/reference/garbage-collection.html",
    "relUrl": "/reference/garbage-collection.html"
  },"144": {
    "doc": "Garbage Collection",
    "title": "Understanding Garbage Collection",
    "content": "For every branch, the GC job retains deleted objects for the number of days defined for the branch. In the absence of a branch-specific rule, the default rule for the repository is used. If an object is present in more than one branch ancestry, it’s retained according to the rule with the largest number of days between those branches. That is, it’s hard-deleted only after the retention period has ended for all relevant branches. Example GC rules for a repository: . { \"default_retention_days\": 14, \"branches\": [ {\"branch_id\": \"main\", \"retention_days\": 21}, {\"branch_id\": \"dev\", \"retention_days\": 7} ] } . In the above example, objects are retained for 14 days after deletion by default. However, if they are present in the branch main, they are retained for 21 days. Objects present in the dev branch (but not in any other branch) are retained for 7 days after they are deleted. What gets collected . Because each object in lakeFS may be accessible from multiple branches, it might not be obvious which objects will be considered garbage and collected. Garbage collection is configured by specifying the number of days to retain objects on each branch. If a branch is configured to retain objects for a given number of days, any object that was accessible from the HEAD of a branch in that past number of days will be retained. The garbage collection process proceeds in two main phases: . | Discover which commits will retain their objects. For every branch, the garbage collection job looks at the HEAD of the branch that many days ago; every commit at or since that HEAD must be retained. Continuing the example, branch main retains for 21 days and branch dev for 7. When running GC on 2022-03-31: . | 7 days ago, on 2022-03-24 the head of branch dev was d: 2022-03-23. So, that commit is retained (along with all more recent commits on dev) but all older commits d: * will be collected. | 21 days ago, on 2022-03-10, the head of branch main was 2022-03-09. So that commit is retained (along with all more recent commits on main) but commits 2022-02-27 and 2022-03-01 will be collected. | . | Discover which objects need to be garbage collected. Hold (only) objects accessible on some retained commits. In the example, all objects of commit 2022-03-12, for instance, are retained. This includes objects added in previous commits. However, objects added in commit d: 2022-03-14 which were overwritten or committed in commit d: 2022-03-20 are not visible in any retained commit and will be garbage collected. | Garbage collect those objects by deleting them. The data of any deleted object will no longer be accessible. lakeFS retains all metadata about the object, but attempting to read it via the lakeFS API or the S3 gateway will return HTTP status 410 (“Gone”). | . What does not get collected . From the above definition of what gets collected, some objects will not be collected regardless of configured GC rules: . | Any object that was uploaded but never committed cannot be collected. See #1933 for more details. | Any object that is present on a branch HEAD is visible on that branch. Commits at the HEAD of a branch are retained, so such an object will not be collected. | . ",
    "url": "/v0.70/reference/garbage-collection.html#understanding-garbage-collection",
    "relUrl": "/reference/garbage-collection.html#understanding-garbage-collection"
  },"145": {
    "doc": "Garbage Collection",
    "title": "Configuring GC rules",
    "content": "Using lakectl . Use the lakectl CLI to define the GC rules: . cat &lt;&lt;EOT &gt;&gt; example_repo_gc_rules.json { \"default_retention_days\": 14, \"branches\": [ {\"branch_id\": \"main\", \"retention_days\": 21}, {\"branch_id\": \"dev\", \"retention_days\": 7} ] } EOT lakectl gc set-config lakefs://example-repo -f example_repo_gc_rules.json . From the lakeFS UI . | Navigate to the main page of your repository. | Go to Settings -&gt; Retention. | Click Edit policy and paste your GC rule into the text box as a JSON. | Save your changes. | . ",
    "url": "/v0.70/reference/garbage-collection.html#configuring-gc-rules",
    "relUrl": "/reference/garbage-collection.html#configuring-gc-rules"
  },"146": {
    "doc": "Garbage Collection",
    "title": "Running the GC job",
    "content": "The GC job is a Spark program that can be run using spark-submit (or using your preferred method of running Spark programs). The job will hard-delete objects that were deleted and whose retention period has ended according to the GC rules. First, you’ll have to download the lakeFS Spark client Uber-jar. The Uber-jar can be found on a public S3 location: . For Spark 2.4.7: http://treeverse-clients-us-east.s3-website-us-east-1.amazonaws.com/lakefs-spark-client-247/${CLIENT_VERSION}/lakefs-spark-client-247-assembly-${CLIENT_VERSION}.jar . For Spark 3.0.1: http://treeverse-clients-us-east.s3-website-us-east-1.amazonaws.com/lakefs-spark-client-301/${CLIENT_VERSION}/lakefs-spark-client-301-assembly-${CLIENT_VERSION}.jar . CLIENT_VERSIONs for Spark 2.4.7 can be found here, and for Spark 3.0.1 they can be found here. Running options: . | On AWS | On Azure | . You should specify the Uber-jar path instead of &lt;APPLICATION-JAR-PATH&gt; and run the following command to make the garbage collector start running: . spark-submit --class io.treeverse.clients.GarbageCollector \\ -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1 \\ -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\ -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\ -c spark.hadoop.fs.s3a.access.key=&lt;S3_ACCESS_KEY&gt; \\ -c spark.hadoop.fs.s3a.secret.key=&lt;S3_SECRET_KEY&gt; \\ &lt;APPLICATION-JAR-PATH&gt; \\ example-repo us-east-1 . You should run the following command to make the garbage collector start running: . spark-submit --class io.treeverse.clients.GarbageCollector \\ -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1 \\ -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\ -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\ -c spark.hadoop.fs.azure.account.key.&lt;AZURE_STORAGE_ACCOUNT&gt;.dfs.core.windows.net=&lt;AZURE_STORAGE_ACCESS_KEY&gt; \\ s3://treeverse-clients-us-east/lakefs-spark-client-312-hadoop3/0.2.0/lakefs-spark-client-312-hadoop3-assembly-0.2.0.jar \\ example-repo . Notes: . | To run GC on Azure, use lakefs-spark-client-312-hadoop3 only. This client is compiled for Spark 3.1.2 with Hadoop 3.2.1, but may work with other Spark versions and higher Hadoop versions. Specifically, this client was tested on Databricks runtime DBR 11.0 (Spark 3.3.0, 3.3.2). | GC on Azure is supported from Spark client version &gt;= v0.2.0. | In case you don’t have hadoop-azure package as part of your environment, you should add the package to your spark-submit with --packages org.apache.hadoop:hadoop-azure:3.2.1 | For GC to work on Azure blob, soft delete should be disabled. | . ",
    "url": "/v0.70/reference/garbage-collection.html#running-the-gc-job",
    "relUrl": "/reference/garbage-collection.html#running-the-gc-job"
  },"147": {
    "doc": "Garbage Collection",
    "title": "Considerations",
    "content": ". | In order for an object to be hard-deleted, it must be deleted from all branches. You should remove stale branches to prevent them from retaining old objects. For example, consider a branch that has been merged to main and has become stale. An object which is later deleted from main will always be present in the stale branch, preventing it from being hard-deleted. | lakeFS will never delete objects outside your repository’s storage namespace. In particular, objects that were imported using lakefs import or lakectl ingest will not be affected by GC jobs. | In cases where deleted objects are brought back to life while a GC job is running, said objects may or may not be deleted. Such actions include: . | Reverting a commit in which a file was deleted. | Branching out from an old commit. | Expanding the retention period of a branch. | Creating a branch from an existing branch, where the new branch has a longer retention period. | . | . ",
    "url": "/v0.70/reference/garbage-collection.html#considerations",
    "relUrl": "/reference/garbage-collection.html#considerations"
  },"148": {
    "doc": "On GCP",
    "title": "Deploy lakeFS on GCP",
    "content": "Expected deployment time: 25 min . ",
    "url": "/v0.70/deploy/gcp.html#deploy-lakefs-on-gcp",
    "relUrl": "/deploy/gcp.html#deploy-lakefs-on-gcp"
  },"149": {
    "doc": "On GCP",
    "title": "Table of contents",
    "content": ". | Prerequisites | Creating the Database on GCP SQL | Installation Options . | On Google Compute Engine | On Google Cloud Run | On GKE | . | Load balancing | Next Steps | . ",
    "url": "/v0.70/deploy/gcp.html#table-of-contents",
    "relUrl": "/deploy/gcp.html#table-of-contents"
  },"150": {
    "doc": "On GCP",
    "title": "Prerequisites",
    "content": "Users that require S3 access using virtual host addressing should configure an S3 Gateway domain. ",
    "url": "/v0.70/deploy/gcp.html#prerequisites",
    "relUrl": "/deploy/gcp.html#prerequisites"
  },"151": {
    "doc": "On GCP",
    "title": "Creating the Database on GCP SQL",
    "content": "lakeFS requires a PostgreSQL database to synchronize actions on your repositories. We will show you how to create a database on Google Cloud SQL, but you can use any PostgreSQL database as long as it’s accessible by your lakeFS installation. If you already have a database, take note of the connection string and skip to the next step . | Follow the official Google documentation on how to create a PostgreSQL instance. Make sure you’re using PostgreSQL version &gt;= 11. | On the Users tab in the console, create a user. The lakeFS installation will use it to connect to your database. | Choose the method by which lakeFS will connect to your database. Google recommends using the SQL Auth Proxy. | . Depending on the chosen lakeFS installation method, you will need to make sure lakeFS can access your database. For example, if you install lakeFS on GKE, you need to deploy the SQL Auth Proxy from this Helm chart, or as a sidecar container in your lakeFS pod. ",
    "url": "/v0.70/deploy/gcp.html#creating-the-database-on-gcp-sql",
    "relUrl": "/deploy/gcp.html#creating-the-database-on-gcp-sql"
  },"152": {
    "doc": "On GCP",
    "title": "Installation Options",
    "content": "On Google Compute Engine . | Save the following configuration file as config.yaml: . --- database: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: # replace this with a randomly-generated string: secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: gs # Uncomment the following lines to give lakeFS access to your buckets using a service account: # gs: # credentials_json: [YOUR SERVICE ACCOUNT JSON STRING] . | Download the binary to the GCE instance. | Run the lakefs binary on the GCE machine: lakefs --config config.yaml run . Note: it is preferable to run the binary as a service using systemd or your operating system’s facilities. | . On Google Cloud Run . To support container-based environments like Google Cloud Run, lakeFS can be configured using environment variables. Here is a docker run command to demonstrate starting lakeFS using Docker: . docker run \\ --name lakefs \\ -p 8000:8000 \\ -e LAKEFS_DATABASE_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\ -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\ -e LAKEFS_BLOCKSTORE_TYPE=\"gs\" \\ treeverse/lakefs:latest run . See the reference for a complete list of environment variables. On GKE . See Kubernetes Deployment. ",
    "url": "/v0.70/deploy/gcp.html#installation-options",
    "relUrl": "/deploy/gcp.html#installation-options"
  },"153": {
    "doc": "On GCP",
    "title": "Load balancing",
    "content": "Depending on how you chose to install lakeFS, you should have a load balancer direct requests to the lakeFS server. By default, lakeFS operates on port 8000, and exposes a /_health endpoint that you can use for health checks. ",
    "url": "/v0.70/deploy/gcp.html#load-balancing",
    "relUrl": "/deploy/gcp.html#load-balancing"
  },"154": {
    "doc": "On GCP",
    "title": "Next Steps",
    "content": "Your next step is to prepare your storage. If you already have a storage bucket/container, you are ready to create your first lakeFS repository. ",
    "url": "/v0.70/deploy/gcp.html#next-steps",
    "relUrl": "/deploy/gcp.html#next-steps"
  },"155": {
    "doc": "On GCP",
    "title": "On GCP",
    "content": " ",
    "url": "/v0.70/deploy/gcp.html",
    "relUrl": "/deploy/gcp.html"
  },"156": {
    "doc": "Google Cloud Storage",
    "title": "Prepare Your GCS Bucket",
    "content": ". | On the Google Cloud Storage console, click Create Bucket. Follow the instructions. | In the Permissions tab, add the service account with which you intend to use lakeFS. Give it a role that allows reading and writing to the bucket, e.g., Storage Object Creator. | . You’re now ready to create your first lakeFS repository. ",
    "url": "/v0.70/setup/storage/gcs.html#prepare-your-gcs-bucket",
    "relUrl": "/setup/storage/gcs.html#prepare-your-gcs-bucket"
  },"157": {
    "doc": "Google Cloud Storage",
    "title": "Google Cloud Storage",
    "content": " ",
    "url": "/v0.70/setup/storage/gcs.html",
    "relUrl": "/setup/storage/gcs.html"
  },"158": {
    "doc": "lakeFS Glossary",
    "title": "Glossary",
    "content": "This page has definition and explanations of all terms related to lakeFS technical internals and the architecture. ",
    "url": "/v0.70/glossary.html#glossary",
    "relUrl": "/glossary.html#glossary"
  },"159": {
    "doc": "lakeFS Glossary",
    "title": "Table of contents",
    "content": ". | Glossary . | Auditing | Branch | Collection | Commit | Cross-collection Consistency | Data Lifecycle Management | Data Pipeline Reproducibility | Data Quality Testing | Data Versioning | Git-like Operations | Graveler | Hooks | Isolated Data Snapshot | Main Branch | Metadata Management | Merge | Repository | Rollback | Storage Namespace | Underlying Storage | Tag | . | . Auditing . Data auditing is data assessment to ensure its accuracy, security, and efficacy for specific usage. It also involves assessing data quality through its lifecycle and understanding the impact of poor quality data on the organization’s performance and revenue. Ensuring data reproducibility, auditability, and governance is one of the key concerns of data engineers today. lakeFS commit history helps the data teams to keep track of all changes to the data, supporting data auditing. Branch . Branches are similar in concept to Git branches. When creating a new branch, we are actually creating a consistent snapshot of the entire repository, which is isolated from other branches and their changes. A branch is a mutable pointer to a commit and uncommitted changes in its staging area (i.e., mutable storage where users can create, update, and delete objects). When a user creates a commit from a branch, all the files from the staging area will be merged into the contents of the current branch, generating a new set of objects. The pointer is updated to reference the new set of objects. The new branch tip is set to the latest commit and the previous branch tip serves as the commit’s parent. Just like in git, a branch spans a repository. Learn more about the lakeFS branching model. Collection . A collection, roughly speaking, is a set of data. Collections may be structured or unstructured; a structured collection is often referred to as a table. Commit . A commit is a point-in-time immutable snapshot of a branch. It’s a collection of object metadata and data, including paths and the object contents and metadata. Commits have their own commit metadata. Every repository has one initial commit with no parent commits. If a commit has more than one parent, it is a merge commit. lakeFS supports only merge commits with two parents. Cross-collection Consistency . It is unfortunate that the word ‘consistency’ has multiple meanings, at least four of them according to Martin Kleppmann. Consistency in the context of lakeFS and data versioning is, the guarantee that operations in a transaction are performed accurately, correctly and most important, atomically. A repository (and thus a branch) in lakeFS, can span multiple tables or collections. By providing branch, commit, merge and revert operations atomically on a branch, lakeFS achieves consistency guarantees across different logical collections. That is, data versioning is consistent across multiple collections within a repository. It is sometimes referred as multi-table transactions. That is, lakeFS offers transactional guarantees across multiple tables. Data Lifecycle Management . In data-intensive applications, data should be managed through its entire lifecycle similar to how teams manage code. By doing so, we could leverage the best practices and tools from application lifecycle management (like CI/CD operations) and apply them to data. lakeFS offers data lifecycle management via isolated data development environments instead of shared buckets. Data Pipeline Reproducibility . Reproducibility in data pipelines is the ability to repeat a process. An example of this is recreating an issue that occurred in the production pipeline. Reproducibility allows for the controlled manufacture of an error to debug and troubleshoot it at a later point in time. Reproducing a data pipeline issue is a challenge that most data engineers face on a daily basis. Learn more about how lakeFS supports data pipeline reproducibility. Other use cases include running ad-hoc queries (useful for data science), review, and backfill. Data Quality Testing . This term describes ways to test data for its accuracy, completeness, consistency, timeliness, validity, and integrity. lakeFS hooks can be used to implement and run data quality tests before promoting staging data into production. Data Versioning . To version data means creating a unique point-in-time reference for data that can be accessed later. This reference can take the form of a query, an ID, or also commonly, a DateTime identifier. Data versioning may also include saving an entire copy of the data under a new name or file path every time you want to create a version of it. More advanced versioning solutions like lakeFS perform versioning through zero-copy data operations. lakeFS also optimizes storage usage between versions and exposes special operations to manage them. Git-like Operations . lakeFS allows teams to treat their data lake as a Git repository. Git is used for code versioning, whereas lakeFS is used for data versioning. lakeFS provides Git-like operations such as branch, commit, merge and revert. Graveler . Graveler is the core versioning engine of lakeFS. It handles versioning by translating lakeFS addresses to the actual stored objects. See the versioning internals section to learn how lakeFS stores metadata. Hooks . lakeFS hooks allow you to automate and ensure that a given set of checks and validations happens before important lifecycle events. They are similar conceptually to Git Hooks, but in contrast, they run remotely on a server. Currently, lakeFS allows executing hooks when two types of events occur: pre-commit events that run before a commit is acknowledged and pre-merge events that trigger right before a merge operation. Isolated Data Snapshot . Creating a branch in lakeFS provides an isolated environment containing a snapshot of your repository. While working on your branch in isolation, all other data users will be looking at the repository’s main branch. So they won’t see your changes, and you also won’t see the changes applied to the main branch. All of this happens without any data duplication but metadata management. Main Branch . Every Git repository has the main branch (unless you take explicit steps to remove it) and it plays a key role in the software development process. In most projects, it represents the source of truth - all the code that works has been tested and is ready to be pushed to production. Similarly, main branch in lakeFS could be used as the single source of truth. For example, the live production data can be on the main branch. Metadata Management . Where there is data, there is also metadata. lakeFS uses metadata to define schema, data types, data versions, relations to other datasets, etc. This helps to improve discoverability and manageability. lakeFS performs data versioning through metadata operations. Merge . lakeFS merge command, similar to git merge functionality, allows you to merge data branches. Once you commit data, you can review it and then merge the committed data into the target branch. A merge generates a commit on the target branch with all your changes. lakeFS guarantees atomic merges that are fast, given they don’t involve copying data. Repository . A repository is a collection of objects with common history tracking. lakeFS manages versions of the repository identified by their commits. Rollback . A rollback is an atomic operation reversing the effects of a previous commit. If a developer introduces a new code version to production and discovers that it has a critical bug, they can simply roll back to the previous version. In lakeFS, a rollback is an atomic action that prevents the data consumers from receiving low-quality data until the issue is resolved. Learn more about how lakeFS supports the rollback operation. Storage Namespace . The storage namespace is a location in the underlying storage dedicated to a specific repository. lakeFS uses it to store the repository’s objects and some of its metadata. Underlying Storage . The underlying storage is a location in some object store where lakeFS keeps your objects and some metadata. Tag . A tag is an immutable pointer to a single commit. Tags have readable names. Because tags are commits, a repository can be read from any tag. Example tags: . | v2.3 to mark a release | dev:jane-before-v2.3-merge to mark Jane’s private temporary point. | . ",
    "url": "/v0.70/glossary.html#table-of-contents",
    "relUrl": "/glossary.html#table-of-contents"
  },"160": {
    "doc": "lakeFS Glossary",
    "title": "lakeFS Glossary",
    "content": " ",
    "url": "/v0.70/glossary.html",
    "relUrl": "/glossary.html"
  },"161": {
    "doc": "Glue ETL",
    "title": "Using lakeFS with Glue ETL",
    "content": "AWS Glue is a fully managed extract, transform, and load (ETL) service. With AWS Glue ETL, you can run your ETL jobs as soon as new data becomes available in Amazon S3 by invoking your AWS Glue ETL jobs from an AWS Lambda function. ",
    "url": "/v0.70/integrations/glue_etl.html#using-lakefs-with-glue-etl",
    "relUrl": "/integrations/glue_etl.html#using-lakefs-with-glue-etl"
  },"162": {
    "doc": "Glue ETL",
    "title": "Configuration",
    "content": "Since Glue ETL is essentially running Spark jobs, to configure Glue ETL to work with lakeFS, all you need to do it apply the lakeFS Spark configuration to your Glue ETL script. ",
    "url": "/v0.70/integrations/glue_etl.html#configuration",
    "relUrl": "/integrations/glue_etl.html#configuration"
  },"163": {
    "doc": "Glue ETL",
    "title": "Glue ETL",
    "content": " ",
    "url": "/v0.70/integrations/glue_etl.html",
    "relUrl": "/integrations/glue_etl.html"
  },"164": {
    "doc": "Glue / Hive metastore",
    "title": "Table of contents",
    "content": ". | About Glue / Hive Metastore | Managing Tables With lakeFS Branches . | Motivation | Configurations | Suggested Model | Commands . | Copy | Diff | . | . | . ",
    "url": "/v0.70/integrations/glue_hive_metastore.html#table-of-contents",
    "relUrl": "/integrations/glue_hive_metastore.html#table-of-contents"
  },"165": {
    "doc": "Glue / Hive metastore",
    "title": "About Glue / Hive Metastore",
    "content": "This part explains about how Glue/Hive Metastore work with lakeFS. Glue and Hive Metastore store metadata related to Hive and other services (such as Spark and Trino). They contain metadata such as the location of the table, information about columns, partitions and many more. ",
    "url": "/v0.70/integrations/glue_hive_metastore.html#about-glue--hive-metastore",
    "relUrl": "/integrations/glue_hive_metastore.html#about-glue--hive-metastore"
  },"166": {
    "doc": "Glue / Hive metastore",
    "title": "Without lakeFS",
    "content": "To query the table my_table, Spark will: . | Request the metadata from Hive metastore (steps 1,2), | Use the location from the metadata to access the data in S3 (steps 3,4). | . ",
    "url": "/v0.70/integrations/glue_hive_metastore.html#without-lakefs",
    "relUrl": "/integrations/glue_hive_metastore.html#without-lakefs"
  },"167": {
    "doc": "Glue / Hive metastore",
    "title": "With lakeFS",
    "content": "When using lakeFS, the flow stays exactly the same. Note that the location of the table my_table now contains the branch s3://example/main/path/to/table . ",
    "url": "/v0.70/integrations/glue_hive_metastore.html#with-lakefs",
    "relUrl": "/integrations/glue_hive_metastore.html#with-lakefs"
  },"168": {
    "doc": "Glue / Hive metastore",
    "title": "Managing Tables With lakeFS Branches",
    "content": " ",
    "url": "/v0.70/integrations/glue_hive_metastore.html#managing-tables-with-lakefs-branches",
    "relUrl": "/integrations/glue_hive_metastore.html#managing-tables-with-lakefs-branches"
  },"169": {
    "doc": "Glue / Hive metastore",
    "title": "Motivation",
    "content": "When creating a table in Glue/Hive metastore (using a client such as Spark, Hive, Presto), we specify the table location. Consider the table my_table that was created with the location s3://example/main/path/to/table. Suppose you created a new branch called DEV with main as the source branch. The data from s3://example/main/path/to/table is now accessible in s3://example/DEV/path/to/table. The metadata is not managed in lakeFS, meaning you don’t have any table pointing to s3://example/DEV/path/to/table. To address this, lakeFS introduces lakectl metastore commands. The case above can be handled using the copy command: it creates a copy of my_table with data located in s3://example/DEV/path/to/table. Note that this is a fast, metadata-only operation. ",
    "url": "/v0.70/integrations/glue_hive_metastore.html#motivation",
    "relUrl": "/integrations/glue_hive_metastore.html#motivation"
  },"170": {
    "doc": "Glue / Hive metastore",
    "title": "Configurations",
    "content": "The lakectl metastore commands can run on Glue or Hive metastore. Add the following to the lakectl configuration file (by default ~/.lakectl.yaml): . Hive . metastore: type: hive hive: uri: hive-metastore:9083 . Glue . metastore: type: glue glue: catalog-id: 123456789012 region: us-east-1 profile: default # optional, implies using a credentials file credentials: access_key_id: AKIAIOSFODNN7EXAMPLE secret_access_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY . Note: It’s recommended to set type and catalog-id/metastore-uri in the lakectl configuration file. ",
    "url": "/v0.70/integrations/glue_hive_metastore.html#configurations",
    "relUrl": "/integrations/glue_hive_metastore.html#configurations"
  },"171": {
    "doc": "Glue / Hive metastore",
    "title": "Suggested Model",
    "content": "For simplicity, we recommend creating a schema for each branch. That way, you can use the same table name across different schemas. For example: after creating branch example_branch, also create a schema named example_branch. For a table named my_table under the schema main, create a new table under the same name and under the schema example_branch. You now have two my_table, one in the main schema and one in the branch schema. ",
    "url": "/v0.70/integrations/glue_hive_metastore.html#suggested-model",
    "relUrl": "/integrations/glue_hive_metastore.html#suggested-model"
  },"172": {
    "doc": "Glue / Hive metastore",
    "title": "Commands",
    "content": "Metastore tools support three commands: copy, diff, and create-symlink. copy and diff can work both on Glue and on Hive. create-symlink works only on Glue. Note: If to-schema or to-table are not specified, the destination branch and source table names will be used as per the suggested model. Notr: Metastore commands can only run on tables located in lakeFS. You should not use tables that aren’t located in lakeFS. Copy . The copy command creates a copy of a table pointing to the defined branch. In case the destination table already exists, the command will only merge the changes. Example: . Suppose we created the table inventory on branch main on schema default. CREATE EXTERNAL TABLE `inventory`( `inv_item_sk` int, `inv_warehouse_sk` int, `inv_quantity_on_hand` int) PARTITIONED BY ( `inv_date_sk` int) STORED AS ORC LOCATION 's3a://my_repo/main/path/to/table'; . We create a new lakeFS branch example_branch: . lakectl branch create lakefs://my_repo/example_branch --source lakefs://my_repo/main . The data from s3://my_repo/main/path/to/table is now accessible in s3://my_repo/DEV/path/to/table. To query the data in s3://my_repo/DEV/path/to/table, you would like to create a copy of the table inventory in schema example_branch pointing to the new branch. lakectl metastore copy --from-schema default --from-table inventory --to-schema example_branch --to-table inventory --to-branch example_branch . After running this command, query the table example_branch.inventory to get the data from s3://my_repo/DEV/path/to/table . Copy Partition . After adding a partition to the branch table, you may want to copy the partition to the main table. For example, for the new partition 2020-08-01, run the following to copy the partition to the main table: . lakectl metastore copy --type hive --from-schema example_branch --from-table inventory --to-schema default --to-table inventory --to-branch main -p 2020-08-01 . For a table partitioned by more than one column, specify the partition flag for every column. For example, for the partition (year='2020',month='08',day='01'): . lakectl metastore copy --from-schema example_branch --from-table branch_inventory --to-schema default --to-branch main -p 2020 -p 08 -p 01 . Diff . Provides a two-way diff between two tables. Shows added+ , removed- and changed~ partitions and columns. Example: . Suppose you made some changes on the copied table inventory on schema example_branch and now want to view the changes before merging back to inventory on schema default. Hive: . lakectl metastore diff --type hive --address thrift://hive-metastore:9083 --from-schema example_branch --from-table branch --to-schema default --to-table inventory . The output will look like this: . Columns are identical Partitions - 2020-07-04 + 2020-07-05 + 2020-07-06 ~ 2020-07-08 . ",
    "url": "/v0.70/integrations/glue_hive_metastore.html#commands",
    "relUrl": "/integrations/glue_hive_metastore.html#commands"
  },"173": {
    "doc": "Glue / Hive metastore",
    "title": "Glue / Hive metastore",
    "content": " ",
    "url": "/v0.70/integrations/glue_hive_metastore.html",
    "relUrl": "/integrations/glue_hive_metastore.html"
  },"174": {
    "doc": "Hive",
    "title": "Using lakeFS with Hive",
    "content": "The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive. ",
    "url": "/v0.70/integrations/hive.html#using-lakefs-with-hive",
    "relUrl": "/integrations/hive.html#using-lakefs-with-hive"
  },"175": {
    "doc": "Hive",
    "title": "Table of contents",
    "content": ". | Configuration | Examples . | Example with schema | Example with an external table | . | . ",
    "url": "/v0.70/integrations/hive.html#table-of-contents",
    "relUrl": "/integrations/hive.html#table-of-contents"
  },"176": {
    "doc": "Hive",
    "title": "Configuration",
    "content": "To configure Hive to work with lakeFS, you need to set the lakeFS credentials in the corresponding S3 credential fields. lakeFS endpoint: fs.s3a.endpoint . lakeFS access key: fs.s3a.access.key . lakeFS secret key: fs.s3a.secret.key . Note In the following examples, we set AWS credentials at runtime for clarity. In production, these properties should be set using one of Hadoop’s standard ways of Authenticating with S3. For example, you can add the configurations to the file hdfs-site.xml: . &lt;configuration&gt; ... &lt;property&gt; &lt;name&gt;fs.s3a.secret.key&lt;/name&gt; &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.access.key&lt;/name&gt; &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;https://lakefs.example.com&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . Note In this example, we set fs.s3a.path.style.access to true to remove the need for additional DNS records for virtual hosting fs.s3a.path.style.access that was introduced in Hadoop 2.8.0 . ",
    "url": "/v0.70/integrations/hive.html#configuration",
    "relUrl": "/integrations/hive.html#configuration"
  },"177": {
    "doc": "Hive",
    "title": "Examples",
    "content": "Example with schema . CREATE SCHEMA example LOCATION 's3a://example/main/' ; CREATE TABLE example.request_logs ( request_time timestamp, url string, ip string, user_agent string ); . Example with an external table . CREATE EXTERNAL TABLE request_logs ( request_time timestamp, url string, ip string, user_agent string ) LOCATION 's3a://example/main/request_logs' ; . ",
    "url": "/v0.70/integrations/hive.html#examples",
    "relUrl": "/integrations/hive.html#examples"
  },"178": {
    "doc": "Hive",
    "title": "Hive",
    "content": " ",
    "url": "/v0.70/integrations/hive.html",
    "relUrl": "/integrations/hive.html"
  },"179": {
    "doc": "Hooks",
    "title": "Configurable Hooks",
    "content": " ",
    "url": "/v0.70/setup/hooks.html#configurable-hooks",
    "relUrl": "/setup/hooks.html#configurable-hooks"
  },"180": {
    "doc": "Hooks",
    "title": "Table of contents",
    "content": ". | Example use cases | Terminology | Uploading Action files | Runs API &amp; CLI | Hook types . | Webhooks . | Action file Webhook properties | Request body schema | . | Airflow Hooks . | Action file Airflow hook properties | Hook Record in configuration field | . | . | Experimentation | . Like other version control systems, lakeFS allows you to configure Actions to trigger when predefined events occur. Supported Events: . | Event | Description | . | pre_commit | Runs when the commit occurs, before the commit is finalized | . | post_commit | Runs after the commit is finalized | . | pre_merge | Runs on the source branch when the merge occurs, before the merge is finalized | . | post_merge | Runs on the merge result, after the merge is finalized | . | pre_create_branch | Runs on the source branch prior to creating a new branch | . | post_create_branch | Runs on the new branch after the branch was created | . | pre_delete_branch | Runs prior to deleting a branch | . | post_delete_branch | Runs after the branch was deleted | . | pre_create_tag | Runs prior to creating a new tag | . | post_create_tag | Runs after the tag was created | . | pre_delete_tag | Runs prior to deleting a tag | . | post_delete_tag | Runs after the tag was deleted | . lakeFS Actions are handled per repository and cannot be shared between repositories. A failure of any Hook under any Action of a pre_* event will result in aborting the lakeFS operation that is taking place. Hook failures under any Action of a post_* event will not revert the operation. Hooks are managed by Action files that are written to a prefix in the lakeFS repository. This allows configuration-as-code inside lakeFS, where Action files are declarative and written in YAML. ",
    "url": "/v0.70/setup/hooks.html#table-of-contents",
    "relUrl": "/setup/hooks.html#table-of-contents"
  },"181": {
    "doc": "Hooks",
    "title": "Example use cases",
    "content": ". | Format Validator: A webhook that checks new files to ensure they are of a set of allowed data format. | Schema Validator: A webhook that reads new Parquet and ORC files to ensure they don’t contain a block list of column names (or name prefixes). This is useful for avoiding accidental PII exposure. | . For more examples and configuration samples, check out lakeFS-hooks example repo. ",
    "url": "/v0.70/setup/hooks.html#example-use-cases",
    "relUrl": "/setup/hooks.html#example-use-cases"
  },"182": {
    "doc": "Hooks",
    "title": "Terminology",
    "content": "Action . An Action is a list of Hooks with the same trigger configuration, i.e. an event will trigger all Hooks under an Action or none at all. The Hooks under an Action are ordered and so is their execution. A Hook will only be executed if all the previous Hooks that were triggered with it had passed. Hook . A Hook is the basic building block of an Action. The failure of a single Hook will stop the execution of the containing Action and fail the Run. Action file . Schema of the Action file: . | Property | Description | Data Type | Required | Default Value | . | name | Identify the Action file | String | false | If missing, filename is used instead | . | on | List of events that will trigger the hooks | List | true |   | . | on.branches | Glob pattern list of branches that triggers the hooks | List | false | Not applicable to Tag events. If empty, Action runs on all branches | . | hooks | List of hooks to be executed | List | true |   | . | hook.id | ID of the hook, must be unique within the Action | String | true |   | . | hook.type | Type of the hook (types) | String | true |   | . | hook.properties | Hook’s specific configuration | Dictionary | true |   | . Example: . name: Good files check description: set of checks to verify that branch is good on: pre-commit: pre-merge: branches: - main hooks: - id: no_temp type: webhook description: checking no temporary files found properties: url: \"https://your.domain.io/webhook?notmp=true?t=1za2PbkZK1bd4prMuTDr6BeEQwWYcX2R\" - id: no_freeze type: webhook description: check production is not in dev freeze properties: url: \"https://your.domain.io/webhook?nofreeze=true?t=1za2PbkZK1bd4prMuTDr6BeEQwWYcX2R\" . Note: lakeFS will validate action files only when an Event has occurred. Use lakectl actions validate &lt;path&gt; to validate your action files locally. Run . A Run is an instantiation of the repository’s Action files when the triggering event occurs. For example, if your repository contains a pre-commit hook, every commit would generate a Run for that specific commit. lakeFS will fetch, parse and filter the repository Action files and start to execute the Hooks under each Action. All executed Hooks (each with hook_run_id) exist in the context of that Run (run_id). ",
    "url": "/v0.70/setup/hooks.html#terminology",
    "relUrl": "/setup/hooks.html#terminology"
  },"183": {
    "doc": "Hooks",
    "title": "Uploading Action files",
    "content": "Action files should be uploaded with the prefix _lakefs_actions/ to the lakeFS repository. When an actionable event (see Supported Events above) takes place, lakeFS will read all files with prefix _lakefs_actions/ in the repository branch where the action occurred. A failure to parse an Action file will result with a failing Run. For example, lakeFS will search and execute all the matching Action files with the prefix lakefs://repo1/feature-1/_lakefs_actions/ on: . | Commit to feature-1 branch on repo1 repository. | Merge to main branch from feature-1 branch on repo1 repository. | . ",
    "url": "/v0.70/setup/hooks.html#uploading-action-files",
    "relUrl": "/setup/hooks.html#uploading-action-files"
  },"184": {
    "doc": "Hooks",
    "title": "Runs API &amp; CLI",
    "content": "The lakeFS API and lakectl expose the results of executions per repository, branch, commit, and specific Action. The endpoint also allows to download the execution log of any executed Hook under each Run for observability. Result Files . The metadata section of lakeFS repository with each Run contains two types of files: . | _lakefs/actions/log/&lt;runID&gt;/&lt;hookRunID&gt;.log - Execution log of the specific Hook run. | _lakefs/actions/log/&lt;runID&gt;/run.manifest - Manifest with all Hooks execution for the run with their results and additional metadata. | . Note: Metadata section of a lakeFS repository is where lakeFS keeps its metadata, like commits and metaranges. Metadata files stored in the metadata section aren’t accessible like user stored files. ",
    "url": "/v0.70/setup/hooks.html#runs-api--cli",
    "relUrl": "/setup/hooks.html#runs-api--cli"
  },"185": {
    "doc": "Hooks",
    "title": "Hook types",
    "content": "Currently, there are two types of Hooks that are supported by lakeFS: Webhook and Airflow. Webhooks . A Webhook is a Hook type that sends an HTTP POST request to the configured URL. Any non 2XX response by the responding endpoint will fail the Hook, cancel the execution of the following Hooks under the same Action. For pre_* hooks, the triggering operation will also be aborted. Warning: You should not use pre_* webhooks for long-running tasks, since they block the performed operation. Moreover, the branch is locked during the execution of pre_* hooks, so the webhook server cannot perform any write operations on the branch (like uploading or commits). Action file Webhook properties . | Property | Description | Data Type | Required | Default Value | Env Vars Support | . | url | The URL address of the request | String | true |   | no | . | timeout | Time to wait for response before failing the hook | String (golang’s Duration representation) | false | 1 minute | no | . | query_params | List of query params that will be added to the request | Dictionary(String:String or String:List(String) | false |   | yes | . | headers | Headers to add to the request | Dictionary(String:String) | false |   | yes | . Secrets &amp; Environment Variables lakeFS Actions supports secrets by using environment variables. The format {{ ENV.SOME_ENV_VAR }} will be replaced with the value of SOME_ENV_VAR during the execution of the action. If that environment variable doesn’t exist in the lakeFS server environment, the action run will fail. Example: ... hooks: - id: prevent_user_columns type: webhook description: Ensure no user_* columns under public/ properties: url: \"http://&lt;host:port&gt;/webhooks/schema\" timeout: 1m30s query_params: disallow: [\"user_\", \"private_\"] prefix: public/ headers: secret_header: \"{{ ENV.MY_SECRET }}\" ... Request body schema . Upon execution, a webhook will send a request containing a JSON object with the following fields: . | Field | Description | Type | . | event_type | Type of the event that triggered the Action | string | . | event_time | Time of the event that triggered the Action (RFC3339) | string | . | action_name | Containing Hook Action’s Name | string | . | hook_id | ID of the Hook | string | . | repository_id | ID of the Repository | string | . | branch_id1 | ID of the Branch | string | . | source_ref | Reference to the source on which the event was triggered | string | . | commit_message2 | The message for the commit (or merge) that is taking place | string | . | committer2 | Name of the committer | string | . | commit_metadata2 | The metadata for the commit that is taking place | string | . | tag_id3 | The ID of the created/deleted tag | string | . Example: . { \"event_type\": \"pre-merge\", \"event_time\": \"2021-02-28T14:03:31Z\", \"action_name\": \"test action\", \"hook_id\": \"prevent_user_columns\", \"repository_id\": \"repo1\", \"branch_id\": \"feature-1\", \"source_ref\": \"feature-1\", \"commit_message\": \"merge commit message\", \"committer\": \"committer\", \"commit_metadata\": { \"key\": \"value\" } } . Airflow Hooks . Airflow Hook triggers a DAG run in an Airflow installation using Airflow’s REST API. The hook run succeeds if the DAG was triggered, and fails otherwise. Action file Airflow hook properties . | Property | Description | Data Type | Example | Required | Env Vars Support | . | url | The URL of the Airflow instance | String | “http://localhost:8080” | true | no | . | dag_id | The DAG to trigger | String | “example_dag” | true | no | . | username | The name of the Airflow user performing the request | String | “admin” | true | no | . | password | The password of the Airflow user performing the request | String | “admin” | true | yes | . | dag_conf | DAG run configuration that will be passed as is | JSON |   | false | no | . | wait_for_dag | Wait for DAG run to complete and reflect state (default: false) | Boolean |   | false | no | . | timeout | Time to wait for the DAG run to complete (default: 1m) | String (golang’s Duration representation) |   | false | no | . Example: ... hooks: - id: trigger_my_dag type: airflow description: Trigger an example_dag properties: url: \"http://localhost:8000\" dag_id: \"example_dag\" username: \"admin\" password: \"{{ ENV.AIRFLOW_SECRET }}\" dag_conf: some: \"additional_conf\" ... Hook Record in configuration field . lakeFS will add an entry to the Airflow request configuration property (conf) with the event that triggered the action. The key of the record will be lakeFS_event and the value will match the one described here . ",
    "url": "/v0.70/setup/hooks.html#hook-types",
    "relUrl": "/setup/hooks.html#hook-types"
  },"186": {
    "doc": "Hooks",
    "title": "Experimentation",
    "content": "Sometimes it may be easier to start experimenting with lakeFS webhooks, even before you have a running server to receive the calls. There are a couple of online tools that can intercept and display the webhook requests, and one of them is Svix. | Go to play.svix.com and copy the URL address supplied by Svix. It should look like https://api.relay.svix.com/api/v1/play/receive/&lt;Random_Gen_String&gt;/ . | Upload the following action file to lakeFS under the path _lakefs_actions/test.yaml in the default branch: . name: Sending everything to Svix description: Experimenting with webhooks on: pre-commit: branches: pre-merge: branches: post-commit: branches: post-merge: branches: hooks: - id: svix type: webhook properties: url: \"https://api.relay.svix.com/api/v1/play/receive/&lt;Random_Gen_String&gt;/\" . by using: . lakectl fs upload lakefs://example-repo/main/_lakefs_actions/test.yaml -s path/to/action/file . or the UI. | Commit that file to the branch. lakectl commit lakefs://example-repo/main -m 'added webhook action file' . | Every time you commit or merge to a branch, the relevant pre_* and post_* requests will be available in the Svix endpoint you provided. You can also check the Actions tab in the lakeFS UI for more details. | . | N\\A for Tag events &#8617; . | N\\A for Tag and Create/Delete Branch events &#8617; &#8617;2 &#8617;3 . | Applicable only for Tag events &#8617; . | . ",
    "url": "/v0.70/setup/hooks.html#experimentation",
    "relUrl": "/setup/hooks.html#experimentation"
  },"187": {
    "doc": "Hooks",
    "title": "Hooks",
    "content": " ",
    "url": "/v0.70/setup/hooks.html",
    "relUrl": "/setup/hooks.html"
  },"188": {
    "doc": "Import data into lakeFS",
    "title": "Import data into lakeFS",
    "content": " ",
    "url": "/v0.70/setup/import.html",
    "relUrl": "/setup/import.html"
  },"189": {
    "doc": "Import data into lakeFS",
    "title": "Table of contents",
    "content": ". | Use external tools | Zero-copy import . | UI Import . | Prerequisite | Limitations | . | lakectl ingest | lakefs import | . | Importing from public buckets | . ",
    "url": "/v0.70/setup/import.html#table-of-contents",
    "relUrl": "/setup/import.html#table-of-contents"
  },"190": {
    "doc": "Import data into lakeFS",
    "title": "Use external tools",
    "content": "To import existing data to lakeFS, you may choose to copy it using S3 CLI or tools like Apache DistCp. This is the most straightforward way, and we recommend it if it’s applicable to you. ",
    "url": "/v0.70/setup/import.html#use-external-tools",
    "relUrl": "/setup/import.html#use-external-tools"
  },"191": {
    "doc": "Import data into lakeFS",
    "title": "Zero-copy import",
    "content": "lakeFS supports three ways to ingest objects from the object store without copying the data. They differ by the outcome, scale, and ease of use: . | Import from the UI - A UI dialog to trigger an import to a designated import branch. It creates a commit from all imported objects. Easy to use and scales well. | lakectl ingest - You can use a simple CLI command to create uncommitted objects in a branch. It will make sequential calls between the CLI and the server, which makes scaling difficult. | lakefs import - You can use the lakeFS binary to create a commit from an S3 inventory file. Supported only with lakeFS with S3 storage adapters. Requires the greatest setup effort - lakeFS binary with DB access and an S3 inventory file. The most scalable option. | . UI Import . Clicking the Import button from any branch will open the following dialog: . If it’s the first import to my-branch, it will create the import branch named _my-branch_imported. lakeFS will import all objects from the Source URI to the import branch under the given prefix. Hang tight! It might take several minutes for the operation to complete: . Once the import is completed, you can merge the changes from the import branch to the source branch. Prerequisite . lakeFS must have permissions to list the objects at the source object store, and in the same region of your destination bucket. Limitations . Importing is only possible from the object storage service in which your installation stores its data. For example, if lakeFS is configured on top of S3, you cannot import data from Azure. Although created by lakeFS, import branches are just like any other branch. Authorization policies, CI/CD triggering, branch protection rules and all other lakeFS concepts apply to them as they apply to any other branch. lakectl ingest . For cases where copying data is not feasible, the lakectl command supports ingesting objects from a source object store without actually copying the data itself. This is done by listing the source bucket (and optional prefix), and creating pointers to the returned objects in lakeFS. By doing this, you can take even large sets of objects and have them appear as objects in a lakeFS branch, as if they were written directly to it. For this to work, make sure that: . | The user calling lakectl ingest has permissions to list the objects at the source object store. | The lakeFS installation has read permissions to the objects being ingested. | The source path is not a storage namespace used by lakeFS. For example, if lakefs://my-repo created with storage namespace s3://my-bucket, then s3://my-bucket/* cannot be an ingestion source. | . | AWS S3 or S3 API Compatible storage | Azure Blob | Google Cloud Storage | . lakectl ingest \\ --from s3://bucket/optional/prefix/ \\ --to lakefs://my-repo/ingest-branch/optional/path/ . The lakectl ingest command will attempt to use the current user’s existing credentials and respect instance profiles, environment variables, and credential files (similarly to AWS CLI) Specify an endpoint to ingest from other S3 compatible storage solutions, e.g., add --s3-endpoint-url https://play.min.io. export AZURE_STORAGE_ACCOUNT=\"storageAccountName\" export AZURE_STORAGE_ACCESS_KEY=\"EXAMPLEroozoo2gaec9fooTieWah6Oshai5Sheofievohthapob0aidee5Shaekahw7loo1aishoonuuquahr3==\" lakectl ingest \\ --from https://storageAccountName.blob.core.windows.net/container/optional/prefix/ \\ --to lakefs://my-repo/ingest-branch/optional/path/ . The lakectl ingest command currently supports storage accounts configured through environment variables as shown above. Note: Currently, lakectl import supports the http:// and https:// schemes for Azure storage URIs. wasb, abfs or adls are currently not supported. export GOOGLE_APPLICATION_CREDENTIALS=\"$HOME/.gcs_credentials.json\" # Optional, will fallback to the default configured credentials lakectl ingest \\ --from gs://bucket/optional/prefix/ \\ --to lakefs://my-repo/ingest-branch/optional/path/ . The lakectl ingest command currently supports the standard GOOGLE_APPLICATION_CREDENTIALS environment variable as described in Google Cloud’s documentation. lakefs import . Importing a very large amount of objects (&gt; ~250M) might take some time using lakectl ingest as described above, since it has to paginate through all the objects in the source using API calls. For S3, we provide a utility as part of the lakefs binary called lakefs import. The lakeFS import tool will use the S3 Inventory feature to create lakeFS metadata. The imported metadata will be committed to a special branch, called import-from-inventory. You should not make any changes or commit anything to branch import-from-inventory: it will be operated on only by lakeFS. After importing, you will be able to merge this branch into your main branch. How it works . The imported data is not copied to the repository’s dedicated bucket. Rather, it will be read directly from your existing bucket when you access it through lakeFS. Files created or replaced through lakeFS will then be stored in the repository’s dedicated bucket. It is important to note that due to the deduplication feature of lakeFS, data will be read from your original bucket even when accessing it through other branches. In a sense, your original bucket becomes an initial snapshot of your data. Note: lakeFS will never make any changes to the import source bucket. Prerequisites . | Your bucket should have S3 Inventory enabled. | The inventory should be in Parquet or ORC format. | The inventory must contain (at least) the size, last-modified-at, and e-tag columns. | The S3 credentials you provided to lakeFS should have GetObject permissions on the source bucket and on the bucket where the inventory is stored. | If you want to use the tool for gradual import, you should not delete the data for the most recently imported inventory, until a more recent inventory is successfully imported. | . For a step-by-step walkthrough of this process, see the post 3 Ways to Add Data to lakeFS on our blog. Usage . Import is performed by the lakefs import command. Assuming your manifest.json is at s3://example-bucket/path/to/inventory/YYYY-MM-DDT00-00Z/manifest.json, and your lakeFS configuration YAML is at config.yaml (see notes below), run the following command to start the import: . lakefs import lakefs://example-repo -m s3://example-bucket/path/to/inventory/YYYY-MM-DDT00-00Z/manifest.json --config config.yaml . You will see the progress of your import as it’s performed. After the import is finished, a summary will be printed along with suggestions for commands to access your data. Added or changed objects: 565000 Deleted objects: 0 Commit ref: cf349ded0a0e65e20bd3b25ea8d9b656c2870b7f1f32f60eb1d90ca5873b6c03 Import to branch import-from-inventory finished successfully. To list imported objects, run: $ lakectl fs ls lakefs://example-repo/cf349ded0a0e65e20bd3b25ea8d9b656c2870b7f1f32f60eb1d90ca5873b6c03/ To merge the changes to your main branch, run: $ lakectl merge lakefs://example-repo/import-from-inventory lakefs://goo/main . Merging imported data to the main branch . As previously mentioned, the above command imports data to the dedicated import-from-inventory branch. By adding the --with-merge flag to the import command, this branch will be automatically merged to your main branch immediately after the import. lakefs import --with-merge lakefs://example-repo -m s3://example-bucket/path/to/inventory/YYYY-MM-DDT00-00Z/manifest.json --config config.yaml . Notes . | Perform the import from a machine with access to your database, and on the same region of your destination bucket. | You can download the lakefs binary from here. Make sure you choose one compatible with your installation of lakeFS. | Use a configuration file like the one used to start your lakeFS installation. This will be used to access your database. An example can be found here. | . Warning: the import-from-inventory branch should only be used by lakeFS. You should not make any operations on it. Gradual Import . Once you switch to using the lakeFS S3-compatible endpoint in all places, you can stop making changes to your original bucket. However, if your operation still requires that you work on the original bucket, you can repeat using the import API with up-to-date inventories every day, until you complete the onboarding process. You can specify only the prefixes that require import. lakeFS will merge those prefixes with the previous imported inventory. For example, a prefixes-file that contains only the prefix new/data/. The new commit to import-from-inventory branch will include all objects from the HEAD of that branch, except for objects with prefix new/data/ that is imported from the inventory. Limitations . Note that lakeFS cannot manage your metadata if you make changes to data in the original bucket. The following table describes the results of making changes in the original bucket, without importing it to lakeFS: . | Object action in the original bucket | ListObjects result in lakeFS | GetObject result in lakeFS | . | Create | Object not visible | Object not accessible | . | Overwrite | Object visible with outdated metadata | Updated object accessible | . | Delete | Object visible | Object not accessible | . ",
    "url": "/v0.70/setup/import.html#zero-copy-import",
    "relUrl": "/setup/import.html#zero-copy-import"
  },"192": {
    "doc": "Import data into lakeFS",
    "title": "Importing from public buckets",
    "content": "lakeFS needs access to the imported location to first list the files to import and later read the files upon users request. There are some use cases where the user would like to import from a destination which isn’t owned by the account running lakeFS. For example, importing public datasets to experiment with lakeFS and Spark. lakeFS will require additional permissions to read from public buckets. For example, for S3 public buckets, the following policy needs to be attached to the lakeFS S3 service-account to allow access to public buckets, while blocking access to other owned buckets: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"PubliclyAccessibleBuckets\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetBucketVersioning\", \"s3:ListBucket\", \"s3:GetBucketLocation\", \"s3:ListBucketMultipartUploads\", \"s3:ListBucketVersions\", \"s3:GetObject\", \"s3:GetObjectVersion\", \"s3:AbortMultipartUpload\", \"s3:ListMultipartUploadParts\" ], \"Resource\": [\"*\"], \"Condition\": { \"StringNotEquals\": { \"s3:ResourceAccount\": \"&lt;YourAccountID&gt;\" } } } ] } . ",
    "url": "/v0.70/setup/import.html#importing-from-public-buckets",
    "relUrl": "/setup/import.html#importing-from-public-buckets"
  },"193": {
    "doc": "Deploy lakeFS",
    "title": "Deploy lakeFS",
    "content": "This page contains a collection of practical step-by-step instructions to help you set up lakeFS on your preferred cloud environemnt. If you just want to try out lakeFS locally, see Quickstart. ",
    "url": "/v0.70/deploy/",
    "relUrl": "/deploy/"
  },"194": {
    "doc": "Slack",
    "title": "Slack",
    "content": " ",
    "url": "/v0.70/slack/",
    "relUrl": "/slack/"
  },"195": {
    "doc": "Prepare Your Storage",
    "title": "Prepare Your Storage",
    "content": "A production installation of lakeFS will usually use your cloud provider’s object storage as the underlying storage layer. You can create a new bucket/container (recommended) or use an existing one with a path prefix. The path under the existing bucket/container should be empty. If you already have a bucket/container configured, you’re ready to create your first lakeFS repository. Choose your storage provider to configure your storage: . ",
    "url": "/v0.70/setup/storage/",
    "relUrl": "/setup/storage/"
  },"196": {
    "doc": "Set up lakeFS",
    "title": "Set up lakeFS",
    "content": "Once you install lakeFS, you need to take a few simple steps to start working. The first one is to create a bucket/container in your underlying storage. If you already have one, you’re ready to create your first lakeFS repository. ",
    "url": "/v0.70/setup/",
    "relUrl": "/setup/"
  },"197": {
    "doc": "Understanding lakeFS",
    "title": "Understanding lakeFS",
    "content": "This section includes all the details about the lakeFS open source project. ",
    "url": "/v0.70/understand/",
    "relUrl": "/understand/"
  },"198": {
    "doc": "Use Cases",
    "title": "Use Cases",
    "content": " ",
    "url": "/v0.70/use_cases/",
    "relUrl": "/use_cases/"
  },"199": {
    "doc": "Reference",
    "title": "Reference",
    "content": " ",
    "url": "/v0.70/reference/",
    "relUrl": "/reference/"
  },"200": {
    "doc": "Quickstart",
    "title": "Quickstart",
    "content": " ",
    "url": "/v0.70/quickstart/",
    "relUrl": "/quickstart/"
  },"201": {
    "doc": "Quickstart",
    "title": "lakeFS Playground",
    "content": "Experience lakeFS first hand with your own isolated environment. You can easily integrate it with your existing tools, and feel lakeFS in action in an environment similar to your own. Try lakeFS now without installing . ",
    "url": "/v0.70/quickstart/#lakefs-playground",
    "relUrl": "/quickstart/#lakefs-playground"
  },"202": {
    "doc": "Quickstart",
    "title": "lakeFS Docker “Everything Bagel”",
    "content": "Get a local lakeFS instance running in a Docker container. This environment includes lakeFS and other common data tools like Spark, dbt, Trino, Hive, and Jupyter. As a prerequisite, Docker is required to be installed on your machine. For download instructions, click here . The following commands can be run in your terminal to get the Bagel running: . | Clone the lakeFS repo: git clone https://github.com/treeverse/lakeFS.git | Start the Docker containers: cd lakeFS/deployments/compose &amp;&amp; docker compose up -d | . Once you have your Docker environment running, it is helpful to pull up the UI for lakeFS. To do this navigate to http://localhost:8000 in your browser. The access key and secret to login are found in the docker_compose.yml file in the lakefs-setup section. Once you are logged in, you should see a page that looks like below. The first thing to notice is in this environment, lakeFS comes with a repository called example already created, and the repo’s default branch is main. If your lakeFS installation doesn’t have the example repo created, you can use the green Create Repository button to do so: . ",
    "url": "/v0.70/quickstart/#lakefs-docker-everything-bagel",
    "relUrl": "/quickstart/#lakefs-docker-everything-bagel"
  },"203": {
    "doc": "Quickstart",
    "title": "Next Steps",
    "content": "You can now install lakeFS on your computer or deploy it on your cloud account. ",
    "url": "/v0.70/quickstart/#next-steps",
    "relUrl": "/quickstart/#next-steps"
  },"204": {
    "doc": "Using lakeFS",
    "title": "Using lakeFS",
    "content": " ",
    "url": "/v0.70/using_lakefs/",
    "relUrl": "/using_lakefs/"
  },"205": {
    "doc": "Spark",
    "title": "lakeFS with Spark:",
    "content": " ",
    "url": "/v0.70/spark/#lakefs-with-spark",
    "relUrl": "/spark/#lakefs-with-spark"
  },"206": {
    "doc": "Spark",
    "title": "Looking to run lakeFS with Apache Spark 💫 ?",
    "content": "Visit Spark-lakeFS integration page. ",
    "url": "/v0.70/spark/#looking-to-run-lakefs-with-apache-spark--",
    "relUrl": "/spark/#looking-to-run-lakefs-with-apache-spark--"
  },"207": {
    "doc": "Spark",
    "title": "Want to integrate lakeFS with Spark and DeltaLake?",
    "content": "Check out Configuring DeltaLake with lakeFS. Have questions? want to contribute new features? 💻 Join the conversation!. ",
    "url": "/v0.70/spark/#want-to-integrate-lakefs-with-spark-and-deltalake",
    "relUrl": "/spark/#want-to-integrate-lakefs-with-spark-and-deltalake"
  },"208": {
    "doc": "Spark",
    "title": "Spark",
    "content": " ",
    "url": "/v0.70/spark/",
    "relUrl": "/spark/"
  },"209": {
    "doc": "Integrations",
    "title": "Integrations",
    "content": " ",
    "url": "/v0.70/integrations/",
    "relUrl": "/integrations/"
  },"210": {
    "doc": "What is lakeFS",
    "title": "What is lakeFS",
    "content": "lakeFS transforms object storage buckets into data lake repositories that expose a Git-like interface. By design, it works with data of any size. The Git-like interface means users of lakeFS can use the same development workflows for code and data. Git workflows greatly improved software development practices; we designed lakeFS to bring the same benefits to data. In this way, lakeFS brings a unique combination of performance and manageability to data lakes. To learn more about applying Git principles to data, see here. The open source lakeFS project supports AWS S3, Azure Blob Storage and Google Cloud Storage (GCS) as its underlying storage service. It is API compatible with S3 and integrates seamlessly with popular data frameworks such as Spark, Hive, dbt, Trino, and many others. ",
    "url": "/v0.70/",
    "relUrl": "/"
  },"211": {
    "doc": "What is lakeFS",
    "title": "New! lakeFS Playground",
    "content": "Experience lakeFS first hand with your own isolated environment. You can easily integrate it with your existing tools, and feel lakeFS in action in an environment similar to your own. Try lakeFS now without installing . ",
    "url": "/v0.70/#new-lakefs-playground",
    "relUrl": "/#new-lakefs-playground"
  },"212": {
    "doc": "What is lakeFS",
    "title": "How do I use lakeFS?",
    "content": "lakeFS maintains compatibility with the S3 API to minimize adoption friction. Use it as a drop-in replacement for S3 from the perspective of any tool interacting with a data lake. For example, take the common operation of reading a collection of data from object storage into a Spark DataFrame. For data outside a lakeFS repo, the code will look like: . df = spark.read.parquet(\"s3a://my-bucket/collections/foo/\") . After adding the data collections in my-bucket to a repository, the same operation becomes: . df = spark.read.parquet(\"s3a://my-repo/main-branch/collections/foo/\") . You can use the same methods and syntax you already use to read and write data when using a lakeFS repository. This simplifies adoption of lakeFS: minimal changes are needed to get started, making further changes an incremental process. ",
    "url": "/v0.70/#how-do-i-use-lakefs",
    "relUrl": "/#how-do-i-use-lakefs"
  },"213": {
    "doc": "What is lakeFS",
    "title": "Why is lakeFS the data solution you’ve been missing?",
    "content": "Working with data in a lakeFS repository — as opposed to a bucket — enables simplified workflows when developing data lake pipelines. lakeFS performs all these operations safely and efficiently: . | Copying objects between prefixes to promote new data | Deleting specific objects to recover from data errors | Maintaining auxilliary jobs that populate a development environment with data | . If today you spend time performing any of these actions, adopting lakeFS will speed up your development and deployment cycles, reduce the chance of incorrect data making it into production, and make recovery less painful if it does. Through its versioning engine, lakeFS enables the following built-in operations familiar from Git: . | branch: a consistent copy of a repository, isolated from other branches and their changes. Initial creation of a branch is a metadata operation that does not duplicate objects. | commit: an immutable checkpoint containing a complete snapshot of a repository. | merge: performed between two branches — merges atomically update one branch with the changes from another. | revert: return a repo to the exact state of a previous commit. | tag: a pointer to a single immutable commit with a readable, meaningful name. | . See the object model for an in-depth definition of these,and the CLI reference for the full list of commands. Incorporating these operations into your data lake pipelines provides the same collaboration and organizational benefits you get when managing application code with source control. The lakeFS promotion workflow . Here’s how lakeFS branches and merges improve the universal process of updating collections with the latest data. | First, create a new branch from main to instantly generate a complete “copy” of your production data. | Apply changes or make updates on the isolated branch to understand their impact prior to exposure. | And finally, perform a merge from the feature branch back to main to atomically promote the updates into production. | . Following this pattern, lakeFS facilitates a streamlined data deployment workflow that consistently produces data assets you can have total confidence in. ",
    "url": "/v0.70/#why-is-lakefs-the-data-solution-youve-been-missing",
    "relUrl": "/#why-is-lakefs-the-data-solution-youve-been-missing"
  },"214": {
    "doc": "What is lakeFS",
    "title": "What else does lakeFS do?",
    "content": "lakeFS helps you maintain a tidy data lake in several other ways, including: . Recovery from data errors . Human error, misconfiguration, or wide-ranging systematic effects are unavoidable. When they do happen, erroneous data may make it into production, or critical data assets might accidentally by deleted. By their nature, backups are the wrong tool for recovering from such events. Backups are periodic events that are usually not tied to performing erroneous operations. So they may be out of date, and they will require sifting through data at the object level. This process is inefficient and can take hours, days, or in some cases, weeks to complete. By quickly committing entire snapshots of data at well-defined times, recovering data in deletion or corruption events becomes an instant one-line operation with lakeFS: just identify a good historical commit, and then restore to it or copy from it. Reverting your data lake back to previous version using our command-line tool is explained here. Data reprocessing and backfills . Occasionally, we might need to reprocess historical data. This can be due to several reasons: . | Implementing new logic. | Late arriving data that wasn’t included in previous analysis, and need to be backfilled after the fact. | . This is tricky first and foremost because it often involves huge volumes of historical data. In addition, auditing requirements may necessitate keeping the old version of the data. lakeFS allows you to manage the reprocess on an isolated branch before merging to ensure the reprocessed data is exposed atomically. It also allows you to easily access the different versions of reprocessed data, using any tag or a historical commit ID. Cross-collection consistency guarantees . Data engineers typically need to implement custom logic in scripts to guarantee two or more data assets are updated synchronously. This logic often requires extensive rewrites or periods during which data is unavailable. The lakeFS merge operation from one branch into another removes the need to implement this logic yourself. Instead, make updates to the desired data assets on a branch, and then utilize a lakeFS merge to atomically expose the data to downstream consumers. To learn more about atomic cross-collection updates, this video describes the concept in more detail. Troubleshooting production problems . Data engineers are often asked to validate the data. A user might report inconsistencies, question the accuracy, or simply report it to be incorrect. Since the data continuously changes, it is challenging to understand its state at the time of the error. The best way to investigate, therefore, is to have a snapshot of the data as close as possible to the time of the error. Once implementing a regular commit cadence in lakeFS, each commit represents an accessible historical snapshot of the data. When needed, a branch may be created from a commit ID to debug an issue in isolation. To learn more on how to access a specific historical commit in a repository, see our seminal post on data reproducibility. Establishing data quality guarantees . The best way to deal with mistakes is to avoid them. A data source that is ingested into the lake introducing low-quality data should be blocked before exposure if possible. With lakeFS, you can achieve this by tying data quality tests to commit and merge operations via lakeFS hooks. Additional things you should know about lakeFS: . | It is format agnostic | Your data stays in place | It minimizes data duplication via a copy-on-write mechanism | It maintains high performance over data lakes of any size | It includes configurable garbage collection capabilities | It is highly available and production ready | . Downloads . Binary Releases . Binary packages are available for Linux/macOS/Windows on GitHub Releases . Docker Images . Official Docker images are available at https://hub.docker.com/r/treeverse/lakefs . Next steps . Get started and set up lakeFS on your preferred cloud environemnt . ",
    "url": "/v0.70/#what-else-does-lakefs-do",
    "relUrl": "/#what-else-does-lakefs-do"
  },"215": {
    "doc": "Install lakeFS",
    "title": "Install lakeFS",
    "content": "Note: The quickstart section is for learning purposes. The installations below will not persist your data. Instead, it will spin-up a database in a docker container, which will be discarded later. For a production suitable deployment, learn how to deploy lakeFS on your cloud. ",
    "url": "/v0.70/quickstart/installing.html",
    "relUrl": "/quickstart/installing.html"
  },"216": {
    "doc": "Install lakeFS",
    "title": "Using Docker Compose",
    "content": "To run a local lakeFS instance using Docker Compose: . | Ensure that you have Docker and Docker Compose installed on your computer, and that the Compose version is 1.25.04 or higher. For more information, please see this issue. | Run the following command in your terminal: . curl https://compose.lakefs.io | docker-compose -f - up . | Check your installation by opening http://127.0.0.1:8000/setup in your web browser. | Create your first repository in lakeFS. | . ",
    "url": "/v0.70/quickstart/installing.html#using-docker-compose",
    "relUrl": "/quickstart/installing.html#using-docker-compose"
  },"217": {
    "doc": "Install lakeFS",
    "title": "Other methods",
    "content": "You can try lakeFS: . | On Kubernetes. | With docker-compose on Windows. | By running the binary directly. | . ",
    "url": "/v0.70/quickstart/installing.html#other-methods",
    "relUrl": "/quickstart/installing.html#other-methods"
  },"218": {
    "doc": "Install lakeFS",
    "title": "Next steps",
    "content": "Now that your lakeFS is running, try creating a repository. ",
    "url": "/v0.70/quickstart/installing.html#next-steps",
    "relUrl": "/quickstart/installing.html#next-steps"
  },"219": {
    "doc": "Isolated Environment",
    "title": "Isolated Environments",
    "content": " ",
    "url": "/v0.70/use_cases/iso_env.html#isolated-environments",
    "relUrl": "/use_cases/iso_env.html#isolated-environments"
  },"220": {
    "doc": "Isolated Environment",
    "title": "Why do I need multiple environments?",
    "content": "When developing over a data lake, it’s useful to have replicas of your production environment. These replicas allow you to test and understand changes to your data without impacting the consumers of the production data. Running ETL and transformation jobs directly in production is a guaranteed way to have data issues flow into dashboards, ML models, and other consumers sooner or later. The most common approach to avoid making changes directly in production is to create and maintain a second data environment called development (or dev) where updates are implemented first. The issue with this approach is that it’s time-consuming and costly to maintain this separate dev environment. And for larger teams it forces multiple people to share a single environment, requiring significant co-ordination. ",
    "url": "/v0.70/use_cases/iso_env.html#why-do-i-need-multiple-environments",
    "relUrl": "/use_cases/iso_env.html#why-do-i-need-multiple-environments"
  },"221": {
    "doc": "Isolated Environment",
    "title": "How do I create isolated environments with lakeFS?",
    "content": "lakeFS makes creating isolated development environments instantaneous. This frees you from spending time on environment maintenance and makes it possible to create as many environments as needed. In a lakeFS repository, data is always located on a branch. You can think of each branch in lakeFS as its own environment. This is because branches are isolated, meaning changes on one branch have no effect other branches. Objects that remain unchanged between two branches are not copied, but rather shared to both branches via metadata pointers that lakeFS manages. If you make a change on one branch and want it reflected on another, you can perform a merge operation to update one branch with the changes from another. Let’s see an example of using multiple lakeFS branches for isolation. ",
    "url": "/v0.70/use_cases/iso_env.html#how-do-i-create-isolated-environments-with-lakefs",
    "relUrl": "/use_cases/iso_env.html#how-do-i-create-isolated-environments-with-lakefs"
  },"222": {
    "doc": "Isolated Environment",
    "title": "Using branches as environments",
    "content": "The key difference when using lakeFS for isolated data environments is that you can create them immediately before testing a change. And once new data is merged into production, you can delete the branch - effectively deleting the old environment. This is different from creating a long-living dev environment used as a staging area to test all the updates. With lakeFS, we create a new branch for each change to production that we want to make. One benefit of this is the ability to test multiple changes at one time. Prerequisites . This tutorial will use an exisitng lakeFS environment and an Apache Spark notebook. To read more about how to setup lakeFS environment, check out Quickstart. Once you have a live environment, it’ll be useful to add some data into the lakeFS repo. We’ll use an Amazon review dataset from a public S3 bucket. First, we’ll download the file to our local computer using the AWS CLI. Then we’ll upload it into lakeFS using the Upload Object button in the UI. To install the AWS CLI, follow these instructions . Download the file . aws s3 cp s3://amazon-reviews-pds/parquet/product_category=Sports/part-00000-495c48e6-96d6-4650-aa65-3c36a3516ddd.c000.snappy.parquet $HOME/ . Next, on the Objects tab of the example repo, click Upload Object and then Choose File and find it in the Finder window. Once it is uploaded, we’ll see the file in the repository on the main branch. Currently, it’s in an uncommitted state. Let’s commit it! . To do this, we can go to the Uncommitted Changes tab and click the green Commit Changes button in the top right. Add a commit message and the file is in the version history of our lakeFS repo. As the final setup step, we’re going to create a new branch called double-branch. To do this, we can use the lakeFS UI by going to the Branches tab and clicking Create Branch. Once we create it, we’ll see two branches: main and double-branch. This new branch serves as an isolated environment on which we can make changes that have no effect on main. Let’s see that in action by using… . Data manipulation with Jupyter &amp; Spark . This use case shows how manipulating data using Spark works, and we’re using Juputer. However, you can integrate lakeFS with your favorite Spark environment. Let’s use them to manipulate the data on one branch, showing how it has no effect on the other. Go to your Spark notebook UI, in our case this is the Jupyter UI. Inside the notebook, create a new notebook and start a spark context: . from pyspark.context import SparkContext from pyspark.sql.session import SparkSession sc = SparkContext('local') spark = SparkSession(sc) . Now we can use Spark to read in the Parquet file we added to the main branch of our lakeFS repo: . df = spark.read.parquet('s3a://example/main/') . To see the Dataframe, run display(df.show()). If we run display(df.count()) we’ll get returned that the Dataframe has 486k rows. Changing one branch . Let’s accidentally write the DataFrame back to the double-branch branch, creating a duplicate object on that branch. df.write.mode('append').parquet('s3a://example/double-branch/') . What happens if we re-read in the data on both branches and perform a count on the resulting DataFrames? . As expected, there are now twice as many rows, 972k, on the double-branch branch. That means we duplicated our data! oh no! . Data duplication introduce errors into our data analytics, BI and machine learning efforts, hence we would like to avoid duplicating our data. On the main branch however, there is still just the origin 486k rows. This shows the utility of branch-based isolated environments with lakeFS. You can safely continue working with the data from main which is unharmed due to lakeFS isolation capabilities. ",
    "url": "/v0.70/use_cases/iso_env.html#using-branches-as-environments",
    "relUrl": "/use_cases/iso_env.html#using-branches-as-environments"
  },"223": {
    "doc": "Isolated Environment",
    "title": "Isolated Environment",
    "content": " ",
    "url": "/v0.70/use_cases/iso_env.html",
    "relUrl": "/use_cases/iso_env.html"
  },"224": {
    "doc": "With Kubernetes",
    "title": "Deploy lakeFS on Kubernetes",
    "content": " ",
    "url": "/v0.70/deploy/k8s.html#deploy-lakefs-on-kubernetes",
    "relUrl": "/deploy/k8s.html#deploy-lakefs-on-kubernetes"
  },"225": {
    "doc": "With Kubernetes",
    "title": "Database",
    "content": "lakeFS requires a PostgreSQL database to synchronize actions on your repositories. This section assumes that you already have a PostgreSQL database accessible from your Kubernetes cluster. You can find the instructions for creating the database in the deployment guide for AWS, Azure and GCP. ",
    "url": "/v0.70/deploy/k8s.html#database",
    "relUrl": "/deploy/k8s.html#database"
  },"226": {
    "doc": "With Kubernetes",
    "title": "Table of contents",
    "content": ". | Prerequisites | Installing on Kubernetes | Load balancing | Next Steps | . ",
    "url": "/v0.70/deploy/k8s.html#table-of-contents",
    "relUrl": "/deploy/k8s.html#table-of-contents"
  },"227": {
    "doc": "With Kubernetes",
    "title": "Prerequisites",
    "content": "Users that require S3 access using virtual host addressing should configure an S3 Gateway domain. ",
    "url": "/v0.70/deploy/k8s.html#prerequisites",
    "relUrl": "/deploy/k8s.html#prerequisites"
  },"228": {
    "doc": "With Kubernetes",
    "title": "Installing on Kubernetes",
    "content": "You can easily install lakeFS on Kubernetes using a Helm chart. To install lakeFS with Helm: . | Copy the Helm values file relevant to your storage provider: | . | S3 | GCS | Azure Blob | . secrets: # replace DATABASE_CONNECTION_STRING with the connection string of the database you created in a previous step. # e.g. postgres://postgres:myPassword@my-lakefs-db.rds.amazonaws.com:5432/lakefs databaseConnectionString: [DATABASE_CONNECTION_STRING] # replace this with a randomly-generated string authEncryptSecretKey: [ENCRYPTION_SECRET_KEY] lakefsConfig: | blockstore: type: s3 s3: region: us-east-1 # optional, fallback in case discover from bucket is not supported . secrets: # replace DATABASE_CONNECTION_STRING with the connection string of the database you created in a previous step. # e.g.: postgres://postgres:myPassword@localhost/postgres:5432 databaseConnectionString: [DATABASE_CONNECTION_STRING] # replace this with a randomly-generated string authEncryptSecretKey: [ENCRYPTION_SECRET_KEY] lakefsConfig: | blockstore: type: gs # Uncomment the following lines to give lakeFS access to your buckets using a service account: # gs: # credentials_json: [YOUR SERVICE ACCOUNT JSON STRING] . Notes for running lakeFS on GKE . | To connect to your database, you need to use one of the ways of connecting GKE to Cloud SQL. | To give lakeFS access to your bucket, you can start the cluster in storage-rw mode. Alternatively, you can use a service account JSON string by uncommenting the gs.credentials_json property in the following yaml. | . secrets: # replace this with the connection string of the database you created in a previous step: databaseConnectionString: [DATABASE_CONNECTION_STRING] # replace this with a randomly-generated string authEncryptSecretKey: [ENCRYPTION_SECRET_KEY] lakefsConfig: | blockstore: type: azure azure: auth_method: msi # msi for active directory, access-key for access key # If you chose to authenticate via access key, unmark the following rows and insert the values from the previous step # storage_account: [your storage account] # storage_access_key: [your access key] . | Fill in the missing values and save the file as conf-values.yaml. For more configuration options, see our Helm chart README. The lakefsConfig parameter is the lakeFS configuration documented here but without sensitive information. Sensitive information like databaseConnectionString is given through separate parameters, and the chart will inject it into Kubernetes secrets. | In the directory where you created conf-values.yaml, run the following commands: . # Add the lakeFS repository helm repo add lakefs https://charts.lakefs.io # Deploy lakeFS helm install example-lakefs lakefs/lakefs -f conf-values.yaml . example-lakefs is the Helm Release name. | . You should give your Kubernetes nodes access to all buckets/containers with which you intend to use lakeFS. If you can’t provide such access, lakeFS can be configured to use an AWS key-pair, an Azure access key, or a Google Cloud credentials file to authenticate (part of the lakefsConfig YAML below). ",
    "url": "/v0.70/deploy/k8s.html#installing-on-kubernetes",
    "relUrl": "/deploy/k8s.html#installing-on-kubernetes"
  },"229": {
    "doc": "With Kubernetes",
    "title": "Load balancing",
    "content": "You should have a load balancer direct requests to the lakeFS server. Options to do so include a Kubernetes Service of type LoadBalancer or a Kubernetes Ingress. By default, lakeFS operates on port 8000 and exposes a /_health endpoint that you can use for health checks. The NGINX Ingress Controller by default limits the client body size to 1 MiB. Some clients use bigger chunks to upload objects - for example, multipart upload to lakeFS using the S3 Gateway or a simple PUT request using the OpenAPI Server. Checkout Nginx documentation for increasing the limit, or an example of Nginx configuration with MinIO. ",
    "url": "/v0.70/deploy/k8s.html#load-balancing",
    "relUrl": "/deploy/k8s.html#load-balancing"
  },"230": {
    "doc": "With Kubernetes",
    "title": "Next Steps",
    "content": "Your next step is to prepare your storage. If you already have a storage bucket/container, you are ready to create your first lakeFS repository. ",
    "url": "/v0.70/deploy/k8s.html#next-steps",
    "relUrl": "/deploy/k8s.html#next-steps"
  },"231": {
    "doc": "With Kubernetes",
    "title": "With Kubernetes",
    "content": " ",
    "url": "/v0.70/deploy/k8s.html",
    "relUrl": "/deploy/k8s.html"
  },"232": {
    "doc": "Kafka",
    "title": "Using lakeFS with Kafka",
    "content": "Apache Kafka provides a unified, high-throughput, low-latency platform for handling real-time data feeds. Different distributions of Kafka offer different methods for exporting data to S3 called Kafka Sink Connectors. The most commonly used Connector for S3 is Confluent’s S3 Sink Connector. Add the following to connector.properties file for lakeFS support: . # Your lakeFS repository s3.bucket.name=example-repo # Your lakeFS S3 endpoint and credentials store.url=https://lakefs.example.com aws.access.key.id=AKIAIOSFODNN7EXAMPLE aws.secret.access.key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # main being the branch we want to write to topics.dir=main/topics . ",
    "url": "/v0.70/integrations/kakfa.html#using-lakefs-with-kafka",
    "relUrl": "/integrations/kakfa.html#using-lakefs-with-kafka"
  },"233": {
    "doc": "Kafka",
    "title": "Kafka",
    "content": " ",
    "url": "/v0.70/integrations/kakfa.html",
    "relUrl": "/integrations/kakfa.html"
  },"234": {
    "doc": "Kubeflow",
    "title": "Using lakeFS with Kubeflow pipelines",
    "content": "Kubeflow is a project dedicated to making deployments of ML workflows on Kubernetes simple, portable, and scalable. A Kubeflow pipeline is a portable and scalable definition of an ML workflow composed of steps. Each step in the pipeline is an instance of a component represented as an instance of ContainerOp. ",
    "url": "/v0.70/integrations/kubeflow.html#using-lakefs-with-kubeflow-pipelines",
    "relUrl": "/integrations/kubeflow.html#using-lakefs-with-kubeflow-pipelines"
  },"235": {
    "doc": "Kubeflow",
    "title": "Table of contents",
    "content": ". | Add pipeline steps for lakeFS operations . | Function-based ContainerOps | Non-function-based ContainerOps | . | Add the lakeFS steps to your pipeline | . ",
    "url": "/v0.70/integrations/kubeflow.html#table-of-contents",
    "relUrl": "/integrations/kubeflow.html#table-of-contents"
  },"236": {
    "doc": "Kubeflow",
    "title": "Add pipeline steps for lakeFS operations",
    "content": "To integrate lakeFS into your Kubeflow pipeline, you need to create Kubeflow components that perform lakeFS operations. Currently, there are two methods to create lakeFS ContainerOps: . | Implement a function-based ContainerOp that uses lakeFS’s Python API to invoke lakeFS operations. | Implement a ContainerOp that uses the lakectl CLI docker image to invoke lakeFS operations. | . Function-based ContainerOps . To implement a function-based component that invokes lakeFS operations, you should use the Python OpenAPI client lakeFS provides. See the example below that demonstrates how to make the client’s package available to your ContainerOp. Example operations . Create a new branch: A function-based ContainerOp that creates a branch called example-branch based on the main branch of example-repo. from kfp import components def create_branch(repo_name, branch_name, source_branch): import lakefs_client from lakefs_client import models from lakefs_client.client import LakeFSClient # lakeFS credentials and endpoint configuration = lakefs_client.Configuration() configuration.username = 'AKIAIOSFODNN7EXAMPLE' configuration.password = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' configuration.host = 'https://lakefs.example.com' client = LakeFSClient(configuration) client.branches.create_branch(repository=repo_name, branch_creation=models.BranchCreation(name=branch_name, source=source_branch)) # Convert the function to a lakeFS pipeline step. create_branch_op = components.func_to_container_op( func=create_branch, packages_to_install=['lakefs_client==&lt;lakeFS version&gt;']) # Type in the lakeFS version you are using . You can invoke any lakeFS operation supported by lakeFS OpenAPI. For example, you could implement a commit and merge function-based ContainerOps. Check out the full API reference. Non-function-based ContainerOps . To implement a non-function based ContainerOp, you should use the treeverse/lakectl docker image. With this image, you can run lakeFS CLI commands to execute the desired lakeFS operation. For lakectl to work with Kubeflow, you will need to pass your lakeFS configurations as environment variables named: . | LAKECTL_CREDENTIALS_ACCESS_KEY_ID: AKIAIOSFODNN7EXAMPLE | LAKECTL_SECRET_ACCESS_KEY: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY | LAKECTL_SERVER_ENDPOINT_URL: https://lakefs.example.com | . Example operations . | Commit changes to a branch: A ContainerOp that commits uncommitted changes to example-branch on example-repo. from kubernetes.client.models import V1EnvVar def commit_op(): return dsl.ContainerOp( name='commit', image='treeverse/lakectl', arguments=['commit', 'lakefs://example-repo/example-branch', '-m', 'commit message']).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_ACCESS_KEY_ID',value='AKIAIOSFODNN7EXAMPLE')).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY',value='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')).add_env_variable(V1EnvVar(name='LAKECTL_SERVER_ENDPOINT_URL',value='https://lakefs.example.com')) . | Merge two lakeFS branches: A ContainerOp that merges example-branch into the main branch of example-repo. def merge_op(): return dsl.ContainerOp( name='merge', image='treeverse/lakectl', arguments=['merge', 'lakefs://example-repo/example-branch', 'lakefs://example-repo/main']).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_ACCESS_KEY_ID',value='AKIAIOSFODNN7EXAMPLE')).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY',value='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')).add_env_variable(V1EnvVar(name='LAKECTL_SERVER_ENDPOINT_URL',value='https://lakefs.example.com')) . | . You can invoke any lakeFS operation supported by lakectl by implementing it as a ContainerOp. Check out the complete CLI reference for the list of supported operations. Note The lakeFS Kubeflow integration that uses lakectl is supported on lakeFS version &gt;= v0.43.0. ",
    "url": "/v0.70/integrations/kubeflow.html#add-pipeline-steps-for-lakefs-operations",
    "relUrl": "/integrations/kubeflow.html#add-pipeline-steps-for-lakefs-operations"
  },"237": {
    "doc": "Kubeflow",
    "title": "Add the lakeFS steps to your pipeline",
    "content": "Add the steps created in the previous step to your pipeline before compiling it. Example pipeline . A pipeline that implements a simple ETL that has steps for branch creation and commits. def lakectl_pipeline(): create_branch_task = create_branch_op('example-repo', 'example-branch', 'main') # A function-based component extract_task = example_extract_op() commit_task = commit_op() transform_task = example_transform_op() commit_task = commit_op() load_task = example_load_op() . Note It’s recommended to store credentials as Kubernetes secrets and pass them as environment variables to Kubeflow operations using V1EnvVarSource. ",
    "url": "/v0.70/integrations/kubeflow.html#add-the-lakefs-steps-to-your-pipeline",
    "relUrl": "/integrations/kubeflow.html#add-the-lakefs-steps-to-your-pipeline"
  },"238": {
    "doc": "Kubeflow",
    "title": "Kubeflow",
    "content": " ",
    "url": "/v0.70/integrations/kubeflow.html",
    "relUrl": "/integrations/kubeflow.html"
  },"239": {
    "doc": "MapReduce",
    "title": "Using lakeFS with MapReduce",
    "content": " ",
    "url": "/v0.70/integrations/mapreduce.html#using-lakefs-with-mapreduce",
    "relUrl": "/integrations/mapreduce.html#using-lakefs-with-mapreduce"
  },"240": {
    "doc": "MapReduce",
    "title": "MapReduce",
    "content": " ",
    "url": "/v0.70/integrations/mapreduce.html",
    "relUrl": "/integrations/mapreduce.html"
  },"241": {
    "doc": "MinIO",
    "title": "Using lakeFS with MinIO",
    "content": "MinIO is a high-performance distributed object storage system. You can use lakeFS to add Git-like capabilities to it. For learning purposes, it’s recommended to follow our step-by-step guide on how to deploy lakeFS locally over MinIO. If you already know how to install lakeFS and want to configure it to use MinIO as the underlying storage, your lakeFS configuration should contain the following: . blockstore: type: s3 s3: force_path_style: true endpoint: http://&lt;minio_endpoint&gt;:9000 discover_bucket_region: false credentials: access_key_id: &lt;minio_access_key&gt; secret_access_key: &lt;minio_secret_key&gt; . The full example can be found here. Note that lakeFS can also be configured using environment variables. ",
    "url": "/v0.70/integrations/minio.html#using-lakefs-with-minio",
    "relUrl": "/integrations/minio.html#using-lakefs-with-minio"
  },"242": {
    "doc": "MinIO",
    "title": "MinIO",
    "content": " ",
    "url": "/v0.70/integrations/minio.html",
    "relUrl": "/integrations/minio.html"
  },"243": {
    "doc": "Model",
    "title": "Model",
    "content": " ",
    "url": "/v0.70/understand/model.html",
    "relUrl": "/understand/model.html"
  },"244": {
    "doc": "Model",
    "title": "Table of contents",
    "content": ". | Objects | Version Control . | Repository | Commits | Branches | Staging Area | Tags | Ref expressions | History | Merge | . | Concepts unique to lakeFS . | lakefs protocol URIs | . | . lakeFS blends concepts from object stores such as S3 with concepts from Git. This reference defines the common concepts of lakeFS. ",
    "url": "/v0.70/understand/model.html#table-of-contents",
    "relUrl": "/understand/model.html#table-of-contents"
  },"245": {
    "doc": "Model",
    "title": "Objects",
    "content": "lakeFS is an object store and borrows concepts from S3. An object store links objects to paths. An object holds: . | Some contents, with unlimited size and format. | Some metadata, including . | size in bytes, | the creation time, a timestamp with seconds resolution, | a checksum string which uniquely identifies the contents, | some user metadata, a small map of strings to strings. | . | . Similarly to many object stores, lakeFS objects are immutable and never rewritten. They can be entirely replaced or deleted, but not modified. The actual data itself is not stored inside lakeFS directly but in an underlying object store. lakeFS will manage these writes and store a pointer to the object in its metadata database. ",
    "url": "/v0.70/understand/model.html#objects",
    "relUrl": "/understand/model.html#objects"
  },"246": {
    "doc": "Model",
    "title": "Version Control",
    "content": "lakeFS borrows its concepts for version control from Git. Repository . In lakeFS, a repository is a logical namespace used to group together objects, branches, and commits. It can be considered the lakeFS analog of a bucket in an object store. Since it has version control qualities, it’s also analogous to a repository in Git. Commits . Commits are immutable “checkpoints” containing an entire snapshot of a repository at a given point in time. This is very similar to commits in Git. Each commit contains metadata - the committer, timestamp, a commit message, as well as arbitrary key/value pairs you can choose to add. Using commits, you can view your Data Lake at a certain point in its history and you’re guaranteed that the data you see is exactly is it was at the point of committing it. Every repository has exactly one initial commit with no parents. A commit with more than one parent is a merge commit. Currently lakeFS only supports merge commits with two parents. Identifying commits . A commit is identified by its commit ID, a digest of all contents of the commit. Commit IDs are by nature long, so you may use a unique prefix to abbreviate them. A commit may also be identified by using a textual definition, called a ref. Examples of refs include tags, branch names, and expressions. Branches . Branches are similar in concept to Git branches. It is a mutable pointer to a commit and its staging area (see below). A branch in lakeFS is a consistent snapshot of the entire repository, which is isolated from other branches and their changes. Example branches: . | main, the trunk. | staging, maybe ahead of main. | dev:joe-bugfix-1234 for Joe to fix issue 1234. | . Staging Area . The staging area is where your changes appear before they are committed. Unlike Git, every branch in lakeFS has its own staging area. Uncommitted changes are visible to all users with read permissions, for example using the Uncommitted Changes tab in the UI. When you commit these changes, a new commit is added to the commit history, and the changes disappear from the staging area. This operation is atomic: readers will either see all your committed changes or none at all. Tags . A tag is an immutable pointer to a single commit. Tags have readable names. Since tags are commits, a repository can be read from any tag. Example tags: . | v2.3 to mark a release. | dev:jane-before-v2.3-merge to mark Jane’s private temporary point. | . Ref expressions . lakeFS also supports expressions for creating a ref. These are similar to revisions in Git; indeed all ~ and ^ examples at the end of that section will work unchanged in lakeFS. | A branch or a tag are ref expressions. | If &lt;ref&gt; is a ref expression, then: . | &lt;ref&gt;^ is a ref expression referring to its first parent. | &lt;ref&gt;^N is a ref expression referring to its N’th parent; in particular &lt;ref&gt;^1 is the same as &lt;ref&gt;^. | &lt;ref&gt;~ is a ref expression referring to its first parent; in particular &lt;ref&gt;~ is the same as &lt;ref&gt;^ and &lt;ref&gt;~. | &lt;ref&gt;~N is a ref expression referring to its N’th parent, always traversing to the first parent. So &lt;ref&gt;~N is the same as &lt;ref&gt;^^...^ with N consecutive carets ^. | . | . History . The history of the branch is the list of commits from the branch tip through the first parent of each commit. Histories go back in time. Merge . Merging is the way to integrate changes from a branch into another branch. The result of a merge is a new commit, with the destination as the first parent and the source as the second. Unlike Git, lakeFS doesn’t apply a diff algorithm while merging. This is because lakeFS is used for unstructured data, where it makes little sense to merge multiple changes into a single object. How it works . To merge a merge source (a commit) into a merge destination (another commit), lakeFS first finds the merge base the nearest common parent of the two commits. It can now perform a three-way merge, by examining the presence and identity of files in each commit. In the table below, “A”, “B” and “C” are possible file contents, “X” is a missing file, and “conflict” (which only appears as a result) is a merge failure. | In base | In source | In destination | Result | Comment | . | A | A | A | A | Unchanged file | . | A | B | B | B | Files changed on both sides in same way | . | A | B | C | conflict | Files changed on both sides differently | . | A | A | B | B | File changed only on one branch | . | A | B | A | B | File changed only on one branch | . | A | X | X | X | Files deleted on both sides | . | A | B | X | conflict | File changed on one side, deleted on the other | . | A | X | B | conflict | File changed on one side, deleted on the other | . | A | A | X | X | File deleted on one side | . | A | X | A | X | File deleted on one side | . The API and lakectl allow passing an optional strategy flag with the following values: . | dest-wins - in case of a conflict, merge will pick the destination object. | source-wins - in case of a conflict, merge will pick the source object. If the strategy is set, it will affect all the objects in the merge, there is currently no way to treat each conflict differently. | . As a format-agnostic system, lakeFS currently merges by complete files. Format-specific and other user-defined merge strategies for handling conflicts are on the roadmap. ",
    "url": "/v0.70/understand/model.html#version-control",
    "relUrl": "/understand/model.html#version-control"
  },"247": {
    "doc": "Model",
    "title": "Concepts unique to lakeFS",
    "content": "The underlying storage is a location in an object store where lakeFS keeps your objects and some immutable metadata. When creating a lakeFS repository, you assign it with a storage namespace. The repository’s storage namespace is a location in the underlying storage where data for this repository will be stored. We sometimes refer to underlying storage as physical. The path used to store the contents of an object is then termed a physical path. Once lakeFS saves an object in the underlying storage it is never modified, except to remove it entirely during some cleanups. A lot of what lakeFS does is to manage how lakeFS paths translate to physical paths on the object store. This mapping is generally not straightforward. Importantly (and contrary to many object stores), lakeFS may map multiple paths to the same object on backing storage, and always does this for objects that are unchanged across versions. lakefs protocol URIs . lakeFS uses a specific format for path URIs. The URI lakefs://&lt;REPO&gt;/&lt;REF&gt;/&lt;KEY&gt; is a path to objects in the given repo and ref expression under key. This is used both for path prefixes and for full paths. In similar fashion, lakefs://&lt;REPO&gt;/&lt;REF&gt; identifies the repository at a ref expression, and lakefs://&lt;REPO&gt; identifes a repo. ",
    "url": "/v0.70/understand/model.html#concepts-unique-to-lakefs",
    "relUrl": "/understand/model.html#concepts-unique-to-lakefs"
  },"248": {
    "doc": "Monitoring using Prometheus",
    "title": "Monitoring using Prometheus",
    "content": " ",
    "url": "/v0.70/reference/monitor.html",
    "relUrl": "/reference/monitor.html"
  },"249": {
    "doc": "Monitoring using Prometheus",
    "title": "Table of contents",
    "content": ". | Example prometheus.yml | Metrics exposed by lakeFS | Example queries . | 99th percentile of API request latencies | 50th percentile of S3-compatible API latencies | Number of errors in outgoing S3 requests | Number of open connections to the database | Example Grafana dashboard | . | . ",
    "url": "/v0.70/reference/monitor.html#table-of-contents",
    "relUrl": "/reference/monitor.html#table-of-contents"
  },"250": {
    "doc": "Monitoring using Prometheus",
    "title": "Example prometheus.yml",
    "content": "lakeFS exposes metrics through the same port used by the lakeFS service, using the standard /metrics path. An example prometheus.yml could look like this: . scrape_configs: - job_name: lakeFS scrape_interval: 10s metrics_path: /metrics static_configs: - targets: - lakefs.example.com:8000 . ",
    "url": "/v0.70/reference/monitor.html#example-prometheusyml",
    "relUrl": "/reference/monitor.html#example-prometheusyml"
  },"251": {
    "doc": "Monitoring using Prometheus",
    "title": "Metrics exposed by lakeFS",
    "content": "By default, Prometheus exports metrics with OS process information like memory and CPU. It also includes Go-specific metrics such as details about GC and a number of goroutines. You can learn about these default metrics in this post. In addition, lakeFS exposes the following metrics to help monitor your deployment: . | Name in Prometheus | Description | Labels | . | api_requests_total | lakeFS API requests (counter) | code: http statusmethod: http method | . | api_request_duration_seconds | Durations of lakeFS API requests (histogram) | operation: name of API operationcode: http status | . | gateway_request_duration_seconds | lakeFS S3-compatible endpoint request (histogram) | operation: name of gateway operationcode: http status | . | s3_operation_duration_seconds | Outgoing S3 operations (histogram) | operation: operation nameerror: “true” if error, “false” otherwise | . | gs_operation_duration_seconds | Outgoing Google Storage operations (histogram) | operation: operation nameerror: “true” if error, “false” otherwise | . | azure_operation_duration_seconds | Outgoing Azure storage operations (histogram) | operation: operation nameerror: “true” if error, “false” otherwise | . | go_sql_stats_* | Go DB stats metrics have this prefix.dlmiddlecote/sqlstats is used to expose them. |   | . ",
    "url": "/v0.70/reference/monitor.html#metrics-exposed-by-lakefs",
    "relUrl": "/reference/monitor.html#metrics-exposed-by-lakefs"
  },"252": {
    "doc": "Monitoring using Prometheus",
    "title": "Example queries",
    "content": "Note: when using Prometheus functions like rate or increase, results are extrapolated and may not be exact. 99th percentile of API request latencies . sum by (operation)(histogram_quantile(0.99, rate(api_request_duration_seconds_bucket[1m]))) . 50th percentile of S3-compatible API latencies . sum by (operation)(histogram_quantile(0.5, rate(gateway_request_duration_seconds_bucket[1m]))) . Number of errors in outgoing S3 requests . sum by (operation) (increase(s3_operation_duration_seconds_count{error=\"true\"}[1m])) . Number of open connections to the database . go_sql_stats_connections_open . Example Grafana dashboard . ",
    "url": "/v0.70/reference/monitor.html#example-queries",
    "relUrl": "/reference/monitor.html#example-queries"
  },"253": {
    "doc": "More Quickstart Options",
    "title": "More Quickstart Options",
    "content": "Note: The quickstart section is for learning purposes. The installations below will not persist your data. Instead, it will spin-up a database in a docker container, which will be discarded later. For a production suitable deployment, learn how to deploy lakeFS on your cloud. ",
    "url": "/v0.70/quickstart/more_quickstart_options.html",
    "relUrl": "/quickstart/more_quickstart_options.html"
  },"254": {
    "doc": "More Quickstart Options",
    "title": "Table of contents",
    "content": ". | Docker on Windows | On Kubernetes with Helm | Using the Binary | . ",
    "url": "/v0.70/quickstart/more_quickstart_options.html#table-of-contents",
    "relUrl": "/quickstart/more_quickstart_options.html#table-of-contents"
  },"255": {
    "doc": "More Quickstart Options",
    "title": "Docker on Windows",
    "content": "To run a local lakeFS instance using Docker Compose: . | Ensure that you have Docker installed on your computer and that compose version is 1.25.04 or higher. For more information, please see this issue. | Run the following command in your terminal: . Invoke-WebRequest https://compose.lakefs.io | Select-Object -ExpandProperty Content | docker-compose -f - up . | Check your installation by opening http://127.0.0.1:8000/setup in your web browser. | You’re now ready to create your first repository in lakeFS. | . ",
    "url": "/v0.70/quickstart/more_quickstart_options.html#docker-on-windows",
    "relUrl": "/quickstart/more_quickstart_options.html#docker-on-windows"
  },"256": {
    "doc": "More Quickstart Options",
    "title": "On Kubernetes with Helm",
    "content": ". | Install lakeFS on a Kubernetes cluster using Helm: # Add the lakeFS Helm repository helm repo add lakefs https://charts.lakefs.io # Deploy lakeFS with helm release \"my-lakefs\" helm install my-lakefs lakefs/lakefs . | The printed output will help you forward a port to lakeFS, so you can access it from your browser at http://127.0.0.1:8000/setup. | Move on to create your first repository in lakeFS. | . ",
    "url": "/v0.70/quickstart/more_quickstart_options.html#on-kubernetes-with-helm",
    "relUrl": "/quickstart/more_quickstart_options.html#on-kubernetes-with-helm"
  },"257": {
    "doc": "More Quickstart Options",
    "title": "Using the Binary",
    "content": "Alternatively, you may opt to run the lakefs binary directly on your computer. | Download the lakeFS binary for your operating system: . Download lakefs . | Install and configure PostgreSQL . | Create a configuration file: . --- database: connection_string: \"postgres://localhost:5432/postgres?sslmode=disable\" blockstore: type: \"local\" local: path: \"~/lakefs_data\" auth: encrypt: secret_key: \"a random string that should be kept secret\" . | Create a local directory to store objects: . mkdir ~/lakefs_data . | Run the server: ./lakefs --config /path/to/config.yaml run . | Check your installation by opening http://127.0.0.1:8000/setup in your web browser. | You are now ready to create your first repository in lakeFS. | . ",
    "url": "/v0.70/quickstart/more_quickstart_options.html#using-the-binary",
    "relUrl": "/quickstart/more_quickstart_options.html#using-the-binary"
  },"258": {
    "doc": "Migrating away from lakeFS",
    "title": "Migrating away from lakeFS",
    "content": " ",
    "url": "/v0.70/reference/offboarding.html",
    "relUrl": "/reference/offboarding.html"
  },"259": {
    "doc": "Migrating away from lakeFS",
    "title": "Copying data from a lakeFS repository to an S3 bucket",
    "content": "The simplest way to migrate away from lakeFS is by copying data from a lakeFS repository to an S3 bucket (or any other object store). For smaller repositories, you can do this by using the AWS CLI or rclone. For larger repositories, running distcp with lakeFS as the source is also an option. ",
    "url": "/v0.70/reference/offboarding.html#copying-data-from-a-lakefs-repository-to-an-s3-bucket",
    "relUrl": "/reference/offboarding.html#copying-data-from-a-lakefs-repository-to-an-s3-bucket"
  },"260": {
    "doc": "OIDC support",
    "title": "OIDC support",
    "content": "You can manage lakeFS users externally using an identity provider compatible with OpenID Connect (OIDC). ",
    "url": "/v0.70/reference/oidc.html",
    "relUrl": "/reference/oidc.html"
  },"261": {
    "doc": "OIDC support",
    "title": "Table of contents",
    "content": ". | OIDC support . | Configuring lakeFS server for OIDC | User permissions . | Using a different claim name | . | . | . ",
    "url": "/v0.70/reference/oidc.html#table-of-contents",
    "relUrl": "/reference/oidc.html#table-of-contents"
  },"262": {
    "doc": "OIDC support",
    "title": "Configuring lakeFS server for OIDC",
    "content": "To support OIDC, add the following to your lakeFS configuration: . auth: oidc: enabled: true client_id: example-client-id client_secret: exampleSecretValue callback_base_url: https://lakefs.example.com # The scheme, domain (and port) of your lakeFS installation url: https://my-account.oidc-provider-example.com default_initial_groups: [\"Developers\"] friendly_name_claim_name: name # Optional: use the value from this claim as the user's display name . Your login page will now include a link to sign in using the OIDC provider. When a user first logs in through the provider, a corresponding user is created in lakeFS. Notes . | As always, you may choose to provide these configurations using environment variables. | You may already have other configuration values under the auth key, so make sure you combine them correctly. | . ",
    "url": "/v0.70/reference/oidc.html#configuring-lakefs-server-for-oidc",
    "relUrl": "/reference/oidc.html#configuring-lakefs-server-for-oidc"
  },"263": {
    "doc": "OIDC support",
    "title": "User permissions",
    "content": "Authorization is still managed via lakeFS groups and policies. By default, an externally managed user is assigned to the lakeFS groups configured in the default_initial_groups property above. For a user to be assigned to other groups, add the initial_groups claim to their ID token claims. The claim should contain a comma-separated list of group names. Once the user has been created, you can manage their permissions from the Administration pages in the lakeFS UI or using lakectl. Using a different claim name . To supply the initial groups using another claim from your ID token, you can use the auth.oidc.initial_groups_claim_name lakeFS configuration. For example, to take the initial groups from the roles claim, add: . auth: oidc: # ... Other OIDC configurations initial_groups_claim_name: roles . ",
    "url": "/v0.70/reference/oidc.html#user-permissions",
    "relUrl": "/reference/oidc.html#user-permissions"
  },"264": {
    "doc": "Presto/Trino",
    "title": "Using lakeFS with Presto/Trino",
    "content": "Presto and Trino are a distributed SQL query engines designed to query large data sets distributed over one or more heterogeneous data sources. ",
    "url": "/v0.70/integrations/presto_trino.html#using-lakefs-with-prestotrino",
    "relUrl": "/integrations/presto_trino.html#using-lakefs-with-prestotrino"
  },"265": {
    "doc": "Presto/Trino",
    "title": "Table of contents",
    "content": ". | Using lakeFS with Presto/Trino . | | Configuration . | Configure the Hive connector | Configure Hive | . | Examples . | Example with schema | Example with External table | Example of copying a table with metastore tools: | . | . | . Querying data in lakeFS from Presto/Trino is similar to querying data in S3 from Presto/Trino. It is done using the Presto Hive connector or Trino Hive connector. Note In the following examples, we set AWS credentials at runtime for clarity. In production, these properties should be set using one of Hadoop’s standard ways of Authenticating with S3. ",
    "url": "/v0.70/integrations/presto_trino.html#table-of-contents",
    "relUrl": "/integrations/presto_trino.html#table-of-contents"
  },"266": {
    "doc": "Presto/Trino",
    "title": "Configuration",
    "content": "Configure the Hive connector . Create /etc/catalog/hive.properties with the following contents to mount the hive-hadoop2 connector as the hive catalog, replacing example.net:9083 with the correct host and port for your Hive Metastore Thrift service: . connector.name=hive-hadoop2 hive.metastore.uri=thrift://example.net:9083 . Add the lakeFS configurations to /etc/catalog/hive.properties in the corresponding S3 configuration properties: . hive.s3.aws-access-key=AKIAIOSFODNN7EXAMPLE hive.s3.aws-secret-key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY hive.s3.endpoint=https://lakefs.example.com hive.s3.path-style-access=true . Configure Hive . Presto/Trino uses Hive Metastore Service (HMS) or a compatible implementation of the Hive Metastore such as AWS Glue Data Catalog to write data to S3. In case you are using Hive Metastore, you will need to configure Hive as well. In file hive-site.xml add to the configuration: . &lt;configuration&gt; ... &lt;property&gt; &lt;name&gt;fs.s3a.access.key&lt;/name&gt; &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.secret.key&lt;/name&gt; &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;https://lakefs.example.com&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . ",
    "url": "/v0.70/integrations/presto_trino.html#configuration",
    "relUrl": "/integrations/presto_trino.html#configuration"
  },"267": {
    "doc": "Presto/Trino",
    "title": "Examples",
    "content": "Here are some examples based on examples from the Presto Hive connector examples and Trino Hive connector examples . Example with schema . Create a new schema named main that will store tables in a lakeFS repository named example branch: master: . CREATE SCHEMA main WITH (location = 's3a://example/main') . Create a new Hive table named page_views in the web schema stored using the ORC file format, partitioned by date and country, and bucketed by user into 50 buckets (note that Hive requires the partition columns to be the last columns in the table): . CREATE TABLE main.page_views ( view_time timestamp, user_id bigint, page_url varchar, ds date, country varchar ) WITH ( format = 'ORC', partitioned_by = ARRAY['ds', 'country'], bucketed_by = ARRAY['user_id'], bucket_count = 50 ) . Example with External table . Create an external Hive table named request_logs that points at existing data in lakeFS: . CREATE TABLE main.request_logs ( request_time timestamp, url varchar, ip varchar, user_agent varchar ) WITH ( format = 'TEXTFILE', external_location = 's3a://example/main/data/logs/' ) . Example of copying a table with metastore tools: . Copy the created table page_views on schema main to schema example_branch with location s3a://example/example_branch/page_views/ . lakectl metastore copy --from-schema main --from-table page_views --to-branch example_branch . ",
    "url": "/v0.70/integrations/presto_trino.html#examples",
    "relUrl": "/integrations/presto_trino.html#examples"
  },"268": {
    "doc": "Presto/Trino",
    "title": "Presto/Trino",
    "content": " ",
    "url": "/v0.70/integrations/presto_trino.html",
    "relUrl": "/integrations/presto_trino.html"
  },"269": {
    "doc": "In Production",
    "title": "In Production",
    "content": "Errors with data in production inevitably occur. When they do, they best thing we can do is remove the erroneous data, understand why the issue happened, and deploy changes that prevent it from occurring again. Example 1: RollBack! - Data ingested from a Kafka stream . If you introduce a new code version to production and discover it has a critical bug, you can simply roll back to the previous version. But you also need to roll back the results of running it. Similar to Git, lakeFS allows you to revert your commits in case they introduced low-quality data. Revert in lakeFS is an atomic action that prevents the data consumers from receiving low quality data until the issue is resolved. As previously mentioned, with lakeFS the recommended branching schema is to ingest data to a dedicated branch. When streaming data, we can decide to merge the incoming data to main at a given time interval or checkpoint, depending on how we chose to write it from Kafka. You can run quality tests for each merge (as discussed in the During Deployment section). Alas, tests are not perfect and we might still introduce low quality data to our main branch at some point. In such a case, we can revert the bad commits from main to the last known high quality commit. This will record new commits reversing the effect of the bad commits. Reverting commits using the CLI . lakectl branch revert lakefs://example-repo/main 20c30c96 ababea32 . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. Example 2: Troubleshoot - Reproduce a bug in production . You upgraded spark and deployed changes in production. A few days or weeks later, you identify a data quality issue, a performance degradation, or an increase to your infra costs. Something that requires investigation and fixing (aka, a bug). lakeFS allows you to open a branch of your lake from the specific merge/commit that introduced the changes to production. Using the metadata saved on the merge/commit you can reproduce all aspects of the environment, then reproduce the issue on the branch and debug it. Meanwhile, you can revert the main to a previous point in time, or keep it as is, depending on the use case . Reading from a historic version (a previous commit) using Spark . // represents the data as existed at commit \"11eef40b\": spark.read.parquet(\"s3://example-repo/11eef40b/events/by-date\") . Example 3: Cross collection consistency . We often need consistency between different data collections. A few examples may be: . | To join different collections in order to create a unified view of an account, a user or another entity we measure. | To introduce the same data in different formats | To introduce the same data with a different leading index or sorting due to performance considerations | . lakeFS will help ensure you introduce only consistent data to your consumers by exposing the new collections and their join in one atomic action to main. Once you consumed the collections on a different branch, and only when both are synchronized, we calculated the join and merged to main. In this example you can see two data sets (Sales data and Marketing data) consumed each to its own independent branch, and after the write of both data sets is completed, they are merged to a different branch (leads branch) where the join ETL runs and creates a joined collection by account. The joined table is then merged to main. The same logic can apply if the data is ingested in streaming, using standard formats, or formats that allow upsert/delete such as Apache Hudi, Delta Lake or Iceberg. ",
    "url": "/v0.70/using_lakefs/production.html",
    "relUrl": "/using_lakefs/production.html"
  },"270": {
    "doc": "In Production",
    "title": "Case Study: Windward",
    "content": "See how Windward is using lakeFS’ isolation and atomic commits to achieve consistency on top of S3. ",
    "url": "/v0.70/using_lakefs/production.html#case-study-windward",
    "relUrl": "/using_lakefs/production.html#case-study-windward"
  },"271": {
    "doc": "Protected Branches",
    "title": "Branch Protection Rules",
    "content": "Define branch protection rules to prevent direct changes and commits to specific branches. Only merges are allowed into protected branches. Together with the power of pre-merge hooks, you can run validations on your data before it reaches your important branches and is exposed to consumers. You can create rules for a specific branch or any branch that matches a name pattern you specify with glob syntax (supporting ? and * wildcards). ",
    "url": "/v0.70/reference/protected_branches.html#branch-protection-rules",
    "relUrl": "/reference/protected_branches.html#branch-protection-rules"
  },"272": {
    "doc": "Protected Branches",
    "title": "How it works",
    "content": "When at least one protection rule applies to a branch, the branch is protected. The following operations will fail on protected branches: . | Object write operations: upload and delete objects. | Branch operations: commit and reset uncommitted changes. | . To operate on a protected branch, merge commits from other branches into it. Use pre-merge hooks to validate the changes before they are merged. Reverting a previous commit using lakectl branch revert is allowed on a protected branch. ",
    "url": "/v0.70/reference/protected_branches.html#how-it-works",
    "relUrl": "/reference/protected_branches.html#how-it-works"
  },"273": {
    "doc": "Protected Branches",
    "title": "Managing branch protection rules",
    "content": "This section explains how to use the lakeFS UI to manage rules. You can also use the command line. Reaching the branch protection rules page . | On lakeFS, navigate to the main page of the repository. | Click on the Settings tab. | In the left menu, click Branches. | . Adding a rule . To add a new rule, click the Add button. In the dialog, enter the branch name pattern and then click Create. Deleting a rule . To delete a rule, click the Delete button next to it. ",
    "url": "/v0.70/reference/protected_branches.html#managing-branch-protection-rules",
    "relUrl": "/reference/protected_branches.html#managing-branch-protection-rules"
  },"274": {
    "doc": "Protected Branches",
    "title": "Protected Branches",
    "content": " ",
    "url": "/v0.70/reference/protected_branches.html",
    "relUrl": "/reference/protected_branches.html"
  },"275": {
    "doc": "Python",
    "title": "Calling the lakeFS API from Python",
    "content": "The lakeFS API is OpenAPI 3.0-compliant, allowing the generation of clients from multiple languages or directly accessed by any HTTP client. For Python, this example uses lakeFS’s python package. The lakefs-client pacakge was created by OpenAPI Generator using our OpenAPI definition served by a lakeFS server. ",
    "url": "/v0.70/integrations/python.html#calling-the-lakefs-api-from-python",
    "relUrl": "/integrations/python.html#calling-the-lakefs-api-from-python"
  },"276": {
    "doc": "Python",
    "title": "Table of contents",
    "content": ". | Install lakeFS Python Client API | Working with the Client API | Using the generated client . | Creating a repository | Creating a branch, uploading files, committing changes | Merging changes from a branch into main | . | Python Client documentation | Full API reference | . ",
    "url": "/v0.70/integrations/python.html#table-of-contents",
    "relUrl": "/integrations/python.html#table-of-contents"
  },"277": {
    "doc": "Python",
    "title": "Install lakeFS Python Client API",
    "content": "Install the Python client using pip: . pip install 'lakefs_client==&lt;lakeFS version&gt;' . The package is available from version &gt;= 0.34.0. ",
    "url": "/v0.70/integrations/python.html#install-lakefs-python-client-api",
    "relUrl": "/integrations/python.html#install-lakefs-python-client-api"
  },"278": {
    "doc": "Python",
    "title": "Working with the Client API",
    "content": "Here’s how to instantiate a client: . import lakefs_client from lakefs_client import models from lakefs_client.client import LakeFSClient # lakeFS credentials and endpoint configuration = lakefs_client.Configuration() configuration.username = 'AKIAIOSFODNN7EXAMPLE' configuration.password = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' configuration.host = 'http://localhost:8000' client = LakeFSClient(configuration) . ",
    "url": "/v0.70/integrations/python.html#working-with-the-client-api",
    "relUrl": "/integrations/python.html#working-with-the-client-api"
  },"279": {
    "doc": "Python",
    "title": "Using the generated client",
    "content": "Now that you have a client object, you can use it to interact with the API. Creating a repository . repo = models.RepositoryCreation(name='example-repo', storage_namespace='s3://storage-bucket/repos/example-repo', default_branch='main') client.repositories.create_repository(repo) # output: # {'creation_date': 1617532175, # 'default_branch': 'main', # 'id': 'example-repo', # 'storage_namespace': 's3://storage-bucket/repos/example-repo'} . Creating a branch, uploading files, committing changes . List the repository branches: . client.branches.list_branches('example-repo') # output: # [{'commit_id': 'cdd673a4c5f42d33acdf3505ecce08e4d839775485990d231507f586ebe97656', 'id': 'main'}] . Create a new branch: . client.branches.create_branch(repository='example-repo', branch_creation=models.BranchCreation(name='experiment-aggregations1', source='main')) # output: # 'cdd673a4c5f42d33acdf3505ecce08e4d839775485990d231507f586ebe97656' . List again to see your newly created branch: . client.branches.list_branches('example-repo').results # output: # [{'commit_id': 'cdd673a4c5f42d33acdf3505ecce08e4d839775485990d231507f586ebe97656', 'id': 'experiment-aggregations1'}, {'commit_id': 'cdd673a4c5f42d33acdf3505ecce08e4d839775485990d231507f586ebe97656', 'id': 'main'}] . Great. Now, let’s upload a file into your new branch: . with open('file.csv', 'rb') as f: client.objects.upload_object(repository='example-repo', branch='experiment-aggregations1', path='path/to/file.csv', content=f) # output: # {'checksum': '0d3b39380e2500a0f60fb3c09796fdba', # 'mtime': 1617534834, # 'path': 'path/to/file.csv', # 'path_type': 'object', # 'physical_address': 'local://example-repo/1865650a296c42e28183ad08e9b068a3', # 'size_bytes': 18} . Diffing a single branch will show all the uncommitted changes on that branch: . client.branches.diff_branch(repository='example-repo', branch='experiment-aggregations1').results # output: # [{'path': 'path/to/file.csv', 'path_type': 'object', 'type': 'added'}] . As expected, our change appears here. Let’s commit it and attach some arbitrary metadata: . client.commits.commit( repository='example-repo', branch='experiment-aggregations1', commit_creation=models.CommitCreation(message='Added a CSV file!', metadata={'using': 'python_api'})) # output: # {'committer': 'barak', # 'creation_date': 1617535120, # 'id': 'e80899a5709509c2daf797c69a6118be14733099f5928c14d6b65c9ac2ac841b', # 'message': 'Added a CSV file!', # 'meta_range_id': '', # 'metadata': {'using': 'python_api'}, # 'parents': ['cdd673a4c5f42d33acdf3505ecce08e4d839775485990d231507f586ebe97656']} . Diffing again, this time there should be no uncommitted files: . client.branches.diff_branch(repository='example-repo', branch='experiment-aggregations1').results # output: # [] . Merging changes from a branch into main . Let’s diff between your branch and the main branch: . client.refs.diff_refs(repository='example-repo', left_ref='main', right_ref='experiment-aggregations1').results # output: # [{'path': 'path/to/file.csv', 'path_type': 'object', 'type': 'added'}] . Looks like you have a change. Let’s merge it: . client.refs.merge_into_branch(repository='example-repo', source_ref='experiment-aggregations1', destination_branch='main') # output: # {'reference': 'd0414a3311a8c1cef1ef355d6aca40db72abe545e216648fe853e25db788fa2e', # 'summary': {'added': 1, 'changed': 0, 'conflict': 0, 'removed': 0}} . Let’s diff again - there should be no changes as all changes are on our main branch already: . client.refs.diff_refs(repository='example-repo', left_ref='main', right_ref='experiment-aggregations1').results # output: # [] . ",
    "url": "/v0.70/integrations/python.html#using-the-generated-client",
    "relUrl": "/integrations/python.html#using-the-generated-client"
  },"280": {
    "doc": "Python",
    "title": "Python Client documentation",
    "content": "For the documentation of lakeFS’s Python package, see https://pydocs.lakefs.io . ",
    "url": "/v0.70/integrations/python.html#python-client-documentation",
    "relUrl": "/integrations/python.html#python-client-documentation"
  },"281": {
    "doc": "Python",
    "title": "Full API reference",
    "content": "For a full reference of the lakeFS API, see lakeFS API . ",
    "url": "/v0.70/integrations/python.html#full-api-reference",
    "relUrl": "/integrations/python.html#full-api-reference"
  },"282": {
    "doc": "Python",
    "title": "Python",
    "content": " ",
    "url": "/v0.70/integrations/python.html",
    "relUrl": "/integrations/python.html"
  },"283": {
    "doc": "Copying data with Rclone",
    "title": "Copying data with Rclone",
    "content": "Rclone is a command line program to sync files and directories between cloud providers. To use it with lakeFS, create an Rclone remote as describe below and then use it as you would any other Rclone remote. ",
    "url": "/v0.70/integrations/rclone.html",
    "relUrl": "/integrations/rclone.html"
  },"284": {
    "doc": "Copying data with Rclone",
    "title": "Table of contents",
    "content": ". | Creating a remote for lakeFS in Rclone . | Option 1: Add an entry in your Rclone configuration file | Option 2: Use the Rclone interactive config command | . | Examples . | Syncing your data from S3 to lakeFS | Syncing a local directory to lakeFS | . | . ",
    "url": "/v0.70/integrations/rclone.html#table-of-contents",
    "relUrl": "/integrations/rclone.html#table-of-contents"
  },"285": {
    "doc": "Copying data with Rclone",
    "title": "Creating a remote for lakeFS in Rclone",
    "content": "To add the remote to Rclone, choose one of the following options: . Option 1: Add an entry in your Rclone configuration file . | Find the path to your Rclone configuration file and copy it for the next step. rclone config file # output: # Configuration file is stored at: # /home/myuser/.config/rclone/rclone.conf . | If your lakeFS access key is already set in an AWS profile or environment variables, run the following command, replacing the endpoint property with your lakeFS endpoint: . cat &lt;&lt;EOT &gt;&gt; /home/myuser/.config/rclone/rclone.conf [lakefs] type = s3 provider = AWS endpoint = https://lakefs.example.com no_check_bucket = true EOT . | Otherwise, also include your lakeFS access key pair in the Rclone configuration file: . cat &lt;&lt;EOT &gt;&gt; /home/myuser/.config/rclone/rclone.conf [lakefs] type = s3 provider = AWS env_auth = false access_key_id = AKIAIOSFODNN7EXAMPLE secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY endpoint = https://lakefs.example.com no_check_bucket = true EOT . | . Option 2: Use the Rclone interactive config command . Run this command and follow the instructions: . rclone config . Choose AWS S3 as your type of storage, and enter your lakeFS endpoint as your S3 endpoint. You will have to choose whether you use your environment for authentication (recommended), or enter the lakeFS access key pair into the Rclone configuration. Select “Edit advanced config” and accept defaults for all values except no_check_bucket: . If set, don't attempt to check the bucket exists or create it. This can be useful when trying to minimize the number of transactions Rclone carries out, if you know the bucket exists already. This might also be needed if the user you're using doesn't have bucket creation permissions. Before v1.52.0, this would have passed silently due to a bug. Enter a boolean value (true or false). Press Enter for the default (\"false\"). no_check_bucket&gt; yes . ",
    "url": "/v0.70/integrations/rclone.html#creating-a-remote-for-lakefs-in-rclone",
    "relUrl": "/integrations/rclone.html#creating-a-remote-for-lakefs-in-rclone"
  },"286": {
    "doc": "Copying data with Rclone",
    "title": "Examples",
    "content": "Syncing your data from S3 to lakeFS . rclone sync mys3remote://mybucket/path/ lakefs:example-repo/main/path . Syncing a local directory to lakeFS . rclone sync /home/myuser/path/ lakefs:example-repo/main/path . ",
    "url": "/v0.70/integrations/rclone.html#examples",
    "relUrl": "/integrations/rclone.html#examples"
  },"287": {
    "doc": "Create a Repository",
    "title": "Create a Repository",
    "content": "A repository contains all of your objects, including the revision history. You can consider it as the lakeFS analog of a bucket in an object store. Since it has version control qualities, it is also analogous to a repository in Git. ",
    "url": "/v0.70/quickstart/repository.html",
    "relUrl": "/quickstart/repository.html"
  },"288": {
    "doc": "Create a Repository",
    "title": "Create the first user",
    "content": "When you first open the lakeFS UI, you will be asked to create an initial admin user. | Open http://127.0.0.1:8000/ in your web browser. | Follow the steps to create an initial administrator user. Save the credentials you’ve received somewhere safe, you won’t be able to see them again! . | Follow the link and go to the login screen: . | . ",
    "url": "/v0.70/quickstart/repository.html#create-the-first-user",
    "relUrl": "/quickstart/repository.html#create-the-first-user"
  },"289": {
    "doc": "Create a Repository",
    "title": "Create the repository",
    "content": ". | Use the credentials from the previous step to log in as an administrator. | Click Create Repository. | Fill in the repository name. | Under Storage Namespace, enter local://. In this tutorial, the underlying storage for lakeFS is the local disk. Accordingly, the value for Storage Namespace should simply be local://. For a deployment that uses an object store as the underlying storage, this would be a location in the store - for example, s3://example-bucket/prefix. | Click Create Repository. | . Next steps . You’ve just created your first lakeFS repository! You can now add some data to it. ",
    "url": "/v0.70/quickstart/repository.html#create-the-repository",
    "relUrl": "/quickstart/repository.html#create-the-repository"
  },"290": {
    "doc": "Reproducibility",
    "title": "The Benefits of Reproducible Data",
    "content": "Data changes frequently. This makes the task of keeping track of its exact state over time difficult. Oftentimes, people maintain only one state of their data––its current state. This has a negative impact on the work, as it becomes hard to: . | Debug a data issue. | Validate machine learning training accuracy (re-running a model over different data gives different results). | Comply with data audits. | . In comparison, lakeFS exposes a Git-like interface to data that allows keeping track of more than just the current state of data. This makes reproducing its state at any point in time straightforward. ",
    "url": "/v0.70/use_cases/reproducibility.html#the-benefits-of-reproducible-data",
    "relUrl": "/use_cases/reproducibility.html#the-benefits-of-reproducible-data"
  },"291": {
    "doc": "Reproducibility",
    "title": "Achieving Reproducibility with lakeFS",
    "content": "To make data reproducible, we recommend taking a new commit of your lakeFS repository every time the data in it changes. As long as there’s a commit taken, the process to reproduce a given state is as simple as reading the data from a path that includes the unique commit_id generated for each commit. To read data at it’s current state, we can use a static path containing the repository and branch names. To give an example, if you have a repository named example with a branch named main, reading the latest state of this data into a Spark Dataframe is always: . df = spark.read.parquet(‘s3://example/main/”) . Note: The code above assumes that all objects in the repository under this path are stored in parquet format. If a different format is used, the applicable Spark read method should be used. In a lakeFS repository, we are capable of taking many commits over the data, making many points in time reproducible. In the repository above, a new commit is taken each time a model training script is run, and the commit message includes the specific run number. If we wanted to re-run the model training script and reproduce the exact same results for a historical run, say run #435, we could copy the commit ID associated with the run and read the data into a dataframe like so: . df = spark.read.parquet(\"s3://example/296e54fbee5e176f3f4f4aeb7e087f9d57515750e8c3d033b8b841778613cb23/training_dataset/”) . The ability to reference a specific commit_id in code simplifies reproducing the specific state a data collection or even multiple collections. This has many applications that are common in data development, such as historical debugging, identifying deltas in a data collection, audit compliance, and more. ",
    "url": "/v0.70/use_cases/reproducibility.html#achieving-reproducibility-with-lakefs",
    "relUrl": "/use_cases/reproducibility.html#achieving-reproducibility-with-lakefs"
  },"292": {
    "doc": "Reproducibility",
    "title": "Reproducibility",
    "content": " ",
    "url": "/v0.70/use_cases/reproducibility.html",
    "relUrl": "/use_cases/reproducibility.html"
  },"293": {
    "doc": "Roadmap",
    "title": "Roadmap",
    "content": " ",
    "url": "/v0.70/understand/roadmap.html",
    "relUrl": "/understand/roadmap.html"
  },"294": {
    "doc": "Roadmap",
    "title": "Table of contents",
    "content": ". | Ecosystem . | Repository Templates: Easily use lakeFS with your data stack High Priority | Pluggable diff/merge operators . | Delta Lake merges and diffs across branches | Iceberg merges and diffs across branches High Priority | . | Hadoop 3 support in all lakeFS clients High Priority | Native Spark OutputCommitter | Native connector: Trino | Improved streaming support for Apache Kafka | User management using OIDC providers High Priority | Reproducible data images | . | Versioning Capabilities . | Support asynchronous hooks High Priority | Support Garbage Collection on Azure High Priority | Garbage Collection on Google Cloud Platform | Collaborate on your data | . | Architecture . | Decouple ref-store from PostgreSQL High Priority | Ref-store implementation for DynamoDB High Priority | Ref-store implementation for RocksDB (for testing and experimentation) | . | . ",
    "url": "/v0.70/understand/roadmap.html#table-of-contents",
    "relUrl": "/understand/roadmap.html#table-of-contents"
  },"295": {
    "doc": "Roadmap",
    "title": "Ecosystem",
    "content": "Repository Templates: Easily use lakeFS with your data stack High Priority . A wizard will walk you through launching a repository tailored for your use case. Integrate with Spark, Hive Metastore and your other tools. Track and discuss it on GitHub . Pluggable diff/merge operators . Currently, lakeFS supports merging and comparing references by doing an object-wise comparison. For unstructured data and some forms of tabluar data (namely, Hive structured tables), this works fine. However, in some cases simply creating a union of object modifications from both references isn’t good enough. Modern table formats such as Delta Lake, Hudi, and Iceberg rely on a set of manifest or log files that describe the logical structure of the table. In those cases, a merge operation might have to be aware of the structure of the data: generate a new manifest or re-order the log in order for the output to make sense. Additionally, the definition of a conflict is also a bit different: simply looking at object names to determine whether or not a conflict occured might not be good enough. With that in mind, we plan to make the diff and merge operations pluggable. lakeFS already supports injecting custom behavior using hooks. Ideally, we can support this by introducing on-diff and on-merge hooks that allow implementing hooks in different languages, possibly utilizing existing code and libraries to aid with understanding these formats. Once this is done, one may implement: . Delta Lake merges and diffs across branches . Delta lake stores metadata files that represent a logical transaction log that relies on numerical ordering. Currently, when trying to modify a Delta table from two different branches, lakeFS would correctly recognize a conflict: this log diverged into two different copies, representing different changes. Users would then have to forgo one of the change sets by either retaining the destination’s branch set of changes or the source’s branch. A much better user experience would be to allow merging this log into a new unified set of changes, representing changes made in both branches as a new set of log files (and potentially, data files too!). Track and discuss it on GitHub . Iceberg merges and diffs across branches High Priority . Iceberg stores metadata files (“manifests”) that represent a snapshot of a given Iceberg table. Currently, when trying to modify an Iceberg table from two different branches, lakeFS would not be able to create a logical snapshot that consists of the changes applied in both references. Users would then have to forgo one of the change sets by either retaining the destination’s branch set of changes or the source’s branch. A much better user experience would be to allow merging snapshots into a new unified set of changes, representing changes made in both refs as a new snapshot. Track and discuss it on GitHub . Hadoop 3 support in all lakeFS clients High Priority . We intend to test, verify and document the version support matrix for our Hadoop-based clients: . | lakeFS Hadoop Filesystem | Spark metadata client | RouterFS | . In particular, all features will support Hadoop versions 3.x. Native Spark OutputCommitter . We plan to add a Hadoop OutputCommitter that uses existing lakeFS operations for atomic commits that are efficient and safely concurrent. This comes with several benefits: . | Performance: This committer does metadata operations only and doesn’t rely on copying data. | Atomicity: A commit in lakeFS is guaranteed to either succeed or fail, but will not leave any intermediate state on failure. | Traceability: Attaching metadata to each commit means we get quite a lot of information on where data is coming from, how it’s generated, etc. This allows building reproducible pipelines in an easier way. | Resilience: Since every Spark write is a commit, it’s also undoable by reverting it. | . Track and discuss it on GitHub . Native connector: Trino . Currently, the Trino integration works well using the lakeFS S3 Gateway. While easy to integrate and useful out-of-the-box, due to the S3 protocol, it means that the data itself must pass through the lakeFS server. For larger installations, a native integration where lakeFS handles metadata and returns locations in the underlying object store that Trino can then access directly would allow reducing the operational overhead and increasing the scalability of lakeFS. This would be done in a similar way to the Native Spark integration using the Hadoop Filesystem implementation. Track and discuss it on GitHub . Improved streaming support for Apache Kafka . Committing (along with attaching useful information to the commit) makes a lot of sense for batch workloads: . | run a job or a pipeline on a separate branch and commit, | record information such as the git hash of the code executed, the versions of frameworks used, and information about the data artifacts, | once the pipeline has completed successfully, commit, and attach the recorded information as metadata. | . For streaming, however, this is currently less clear: There’s no obvious point in time to commit as things never actually “finish successfully”. The recommended pattern would be to ingest from a stream on a separate branch, periodically committing - storing not only the data added since last commit but also capturing the offset read from the stream, for reproducibility. These commits can then be merged into a main branch given they pass all relevant quality checks and other validations using hooks, exposing consumers to validated, clean data. In practice, implementing such a workflow is a little challenging. Users need to: . | Orchestrate the commits and merge operations. | Figure out how to attach the correct offset read from the stream broker. | Handle writes coming in while the commit is taking place. | . Ideally, lakeFS should provide tools to automate this, with native support for Apache Kafka. Track and discuss it on GitHub . User management using OIDC providers High Priority . Allow admins to manage their users externally using any OIDC-compatible provider. Reproducible data images . ",
    "url": "/v0.70/understand/roadmap.html#ecosystem",
    "relUrl": "/understand/roadmap.html#ecosystem"
  },"296": {
    "doc": "Roadmap",
    "title": "Versioning Capabilities",
    "content": "Support asynchronous hooks High Priority . Support running hooks that might possibly take many minutes to complete. This is useful for things such as data quality checks - where we might want to do big queries or scans to ensure the data being merged adheres to certain business rules. Currently, pre-commit and pre-merge hooks in lakeFS are tied to the lifecycle of the API request that triggers the said commit or merge operation. In order to support long running hooks, there are enhancements to make to lakeFS APIs in order to support an asynchronous commit and merge operations that are no longer tied to the HTTP request that triggered them. Support Garbage Collection on Azure High Priority . The lakeFS Garbage Collection capability hard-deletes objects deleted from branches, helping users reduce costs and comply with data privacy policies. Currently, lakeFS only supports Garbage Collection of S3 objects managed by lakeFS. Extending the support to Azure will allow lakeFS users that use Azure as their underlying storage to use this feature. Track and discuss it on GitHub . Garbage Collection on Google Cloud Platform . Same as above, but for GCP. Collaborate on your data . Use lakeFS to comment, review and request changes before your data reaches consumers. Track and discuss it on GitHub . ",
    "url": "/v0.70/understand/roadmap.html#versioning-capabilities",
    "relUrl": "/understand/roadmap.html#versioning-capabilities"
  },"297": {
    "doc": "Roadmap",
    "title": "Architecture",
    "content": "Decouple ref-store from PostgreSQL High Priority . Currently, lakeFS requires a PostgreSQL database. Internally, it’s used to store references (branches, tags, etc.), uncommitted objects metadata, and other metadata such as user management. Making this store a pluggable component would allow the following: . | Simpler quickstart using only an object store: allow running lakeFS without any dependencies. This ref-store will use the underlying object store to also store the references. For S3 (or any object store that doesn’t support any native transaction/compare-and-swap semantics), this will be available only when running in single-instance mode. This is still beneficial for running lakeFS in POC or development mode, removing the need to run and connect multiple Docker containers. | Flexible production setup: A PostgreSQL option will still be available, but additional implementations will also be possible. Using other RDBMS types such as MySQL &amp;emdash; or using managed services such as DynamoDB that lakeFS will be able to manage itself. | Easier scalability: Scaling RDBMS for very high throughput while keeping it predictable in performance for different loads and access patterns has a very high operational cost. | . This release will mark the completion of project “lakeFS on the Rocks” . Track and discuss it on GitHub . Ref-store implementation for DynamoDB High Priority . Once we’ve decoupled the ref-store from PostgreSQL, we’d like to create a ref-store implementation that supports DynamoDB. This has several advantages for users looking to run lakeFS on AWS: . | DynamoDB is fast to provision and requires very little configuration. | The operational overhead of maintaining a serverless database is very small. | Scaling according to usage is much more fine grained, which eliminates a lot of the cost for smaller installations (as opposed to RDS). | . Track and discuss it on GitHub . Ref-store implementation for RocksDB (for testing and experimentation) . Once we’ve decoupled the ref-store from PostgreSQL, we’d like to create a ref-store implementation that supports running with an embedded RocksDB database. While not fit for real world production use, it makes trying lakeFS when running locally easier - either by directly executing the binary or doing a single docker run with the right configuration (as opposed to having to use docker-compose or run PostgreSQL locally). Track and discuss it on GitHub . ",
    "url": "/v0.70/understand/roadmap.html#architecture",
    "relUrl": "/understand/roadmap.html#architecture"
  },"298": {
    "doc": "Rollback",
    "title": "Rollbacks",
    "content": " ",
    "url": "/v0.70/use_cases/rollback.html#rollbacks",
    "relUrl": "/use_cases/rollback.html#rollbacks"
  },"299": {
    "doc": "Rollback",
    "title": "What Is a Rollback?",
    "content": "A rollback operation is used to to fix critical data errors immediately. What is a critical data error? Think of a situation where erroneous or misformatted data causes a signficant issue with an important service or function. In such situations, the first thing to do is stop the bleeding. Rolling back returns data to a state in the past, before the error was present. You might not be showing all the latest data after a rollback, but at least you aren’t showing incorrect data or raising errors. ",
    "url": "/v0.70/use_cases/rollback.html#what-is-a-rollback",
    "relUrl": "/use_cases/rollback.html#what-is-a-rollback"
  },"300": {
    "doc": "Rollback",
    "title": "Why Rollbacks Are Useful",
    "content": "A Rollback is used as a stopgap measure to “put out the fire” as quickly as possible while RCA (root cause analysis) is performed to understand 1) exactly how the error happened, and 2) what can be done to prevent it from happening again. It can be a pressured, stressful situation to deal with a critical data error. Having the ability to employ a rollback relieves some of the pressure and makes it more likely you can figure out what happened without creating additional issues. As a real world example, the 14-day outage some Atlassian users experienced in May 2022 could have been an uninteresting minor incident had rolling back the deleted customer data been an option. ",
    "url": "/v0.70/use_cases/rollback.html#why-rollbacks-are-useful",
    "relUrl": "/use_cases/rollback.html#why-rollbacks-are-useful"
  },"301": {
    "doc": "Rollback",
    "title": "Performing Rollbacks with lakeFS",
    "content": "lakeFS lets you develop in your data lake in such a way that rollbacks are simple to perform. This starts by taking a commit over your lakeFS repository whenever a change to its state occurs. Using the lakeFS UI or CLI, you can set the current state, or HEAD, of a branch to any historical commit in seconds, effectively performing a rollback. To demonstrate how this works, let’s take the example of a lakeFS repo with the following commit history: . As can be inferred from the history, this repo is updated every minute with a data sync from some data source. An example data sync is a typical ETL job that replicates data from an internal database or any other data source. After each sync, a commit is taken in lakeFS to save a snapshot of data at that point in time. How to Rollback From a Bad Data Sync? . Say a situation occurs where one of the syncs had bad data and is causing downstream dashboards to fail to load. Since we took a commit of the repo right after the sync ran, we can use a revert operation to undo the changes introduced in that sync. Step 1: Copy the commit_id associated with the commit we want to revert. As the screenshot above shows, you can use the Copy ID to Clipboard button to do this. Step 2: Run the revert command using lakectl, the lakeFS CLI. In this example, the command will be as follows: . lakectl branch revert lakefs://example/main 9666d7c9daf37b3ba6964e733d08596ace2ec2c7bc3a4023ad8e80737a6c3e9d . This will undo the changes introduced by this commit, completing the rollback! . The rollback operation is that simple, even if many changes were introduced in a commit, spanning acrossmultiple data collections. In lakeFS, rolling back data is always a one-liner. ",
    "url": "/v0.70/use_cases/rollback.html#performing-rollbacks-with-lakefs",
    "relUrl": "/use_cases/rollback.html#performing-rollbacks-with-lakefs"
  },"302": {
    "doc": "Rollback",
    "title": "Rollback",
    "content": " ",
    "url": "/v0.70/use_cases/rollback.html",
    "relUrl": "/use_cases/rollback.html"
  },"303": {
    "doc": "AWS S3",
    "title": "Prepare Your AWS S3 Bucket",
    "content": ". | From the S3 Administration console, choose Create Bucket. | Make sure that you: . | Block public access. | Disable Object Locking. | . | lakeFS requires permissions to interact with your bucket. The following is a minimal bucket policy. To add it, go to the Permissions tab, and paste it as : . { \"Id\": \"Policy1590051531320\", \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Stmt1590051522178\", \"Action\": [ \"s3:GetObject\", \"s3:GetObjectVersion\", \"s3:PutObject\", \"s3:AbortMultipartUpload\", \"s3:ListMultipartUploadParts\", \"s3:GetBucketVersioning\", \"s3:ListBucket\", \"s3:GetBucketLocation\", \"s3:ListBucketMultipartUploads\", \"s3:ListBucketVersions\" ], \"Effect\": \"Allow\", \"Resource\": [\"arn:aws:s3:::&lt;BUCKET_NAME&gt;\", \"arn:aws:s3:::&lt;BUCKET_NAME_WITH_PATH_PREFIX&gt;/*\"], \"Principal\": { \"AWS\": [\"arn:aws:iam::&lt;ACCOUNT_ID&gt;:role/&lt;IAM_ROLE&gt;\"] } } ] } . Replace &lt;ACCOUNT_ID&gt;, &lt;BUCKET_NAME&gt; and &lt;IAM_ROLE&gt; with values relevant to your environment. IAM_ROLE should be the role assumed by your lakeFS installation. Alternatively, if you use an AWS user’s key-pair to authenticate lakeFS to AWS, change the policy’s Principal to be the user: . \"Principal\": { \"AWS\": [\"arn:aws:iam::&lt;ACCOUNT_ID&gt;:user/&lt;IAM_USER&gt;\"] } . | . You’re now ready to create your first lakeFS repository. ",
    "url": "/v0.70/setup/storage/s3.html#prepare-your-aws-s3-bucket",
    "relUrl": "/setup/storage/s3.html#prepare-your-aws-s3-bucket"
  },"304": {
    "doc": "AWS S3",
    "title": "AWS S3",
    "content": " ",
    "url": "/v0.70/setup/storage/s3.html",
    "relUrl": "/setup/storage/s3.html"
  },"305": {
    "doc": "S3 Supported API",
    "title": "S3-Supported API",
    "content": "The S3 Gateway emulates a subset of the API exposed by S3. This subset includes all API endpoints relevant to data systems. For more information, see architecture . lakeFS supports the following API operations: . | Identity and authorization . | SIGv2 | SIGv4 | . | Bucket operations: . | HEAD bucket | . | Object operations: . | DeleteObject | DeleteObjects | GetObject . | Support for caching headers, ETag | Support for range requests | No support for SSE | No support for SelectObject operations | . | HeadObject | PutObject . | Support multi-part uploads | No support for storage classes | No object level tagging | . | CopyObject | . | Object Listing: . | ListObjects | ListObjectsV2 | Delimiter support (for \"/\" only) | . | Multipart Uploads: . | AbortMultipartUpload | CompleteMultipartUpload | CreateMultipartUpload | ListParts | Upload Part | UploadPartCopy | . | . ",
    "url": "/v0.70/reference/s3.html#s3-supported-api",
    "relUrl": "/reference/s3.html#s3-supported-api"
  },"306": {
    "doc": "S3 Supported API",
    "title": "S3 Supported API",
    "content": " ",
    "url": "/v0.70/reference/s3.html",
    "relUrl": "/reference/s3.html"
  },"307": {
    "doc": "SageMaker",
    "title": "Using lakeFS with SageMaker",
    "content": "Amazon SageMaker helps to prepare, build, train and deploy ML models quickly by bringing together a broad set of capabilities purpose-built for ML. ",
    "url": "/v0.70/integrations/sagemaker.html#using-lakefs-with-sagemaker",
    "relUrl": "/integrations/sagemaker.html#using-lakefs-with-sagemaker"
  },"308": {
    "doc": "SageMaker",
    "title": "Table of contents",
    "content": ". | Initializing session and client | Usage Examples . | Upload train and test data | Download objects | . | . ",
    "url": "/v0.70/integrations/sagemaker.html#table-of-contents",
    "relUrl": "/integrations/sagemaker.html#table-of-contents"
  },"309": {
    "doc": "SageMaker",
    "title": "Initializing session and client",
    "content": "Initialize a Sagemaker session and an S3 client with lakeFS as the endpoint: . import sagemaker import boto3 endpoint_url = '&lt;LAKEFS_ENDPOINT&gt;' aws_access_key_id = '&lt;LAKEFS_ACCESS_KEY_ID&gt;' aws_secret_access_key = '&lt;LAKEFS_SECRET_ACCESS_KEY&gt;' repo = 'example-repo' sm = boto3.client('sagemaker', endpoint_url=endpoint_url, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key) s3_resource = boto3.resource('s3', endpoint_url=endpoint_url, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key) session = sagemaker.Session(boto3.Session(), sagemaker_client=sm, default_bucket=repo) session.s3_resource = s3_resource . ",
    "url": "/v0.70/integrations/sagemaker.html#initializing-session-and-client",
    "relUrl": "/integrations/sagemaker.html#initializing-session-and-client"
  },"310": {
    "doc": "SageMaker",
    "title": "Usage Examples",
    "content": "Upload train and test data . Let’s use the created session for uploading data to the ‘main’ branch: . prefix = \"/prefix-within-branch\" branch = 'main' train_file = 'train_data.csv'; train_data.to_csv(train_file, index=False, header=True) train_data_s3_path = session.upload_data(path=train_file, key_prefix=branch + prefix + \"/train\") test_file = 'test_data.csv'; test_data_no_target.to_csv(test_file, index=False, header=False) test_data_s3_path = session.upload_data(path=test_file, key_prefix=branch + prefix + \"/test\") . Download objects . You can use the integration with lakeFS to download a portion of the data you see fit: . repo = 'example-repo' prefix = \"/prefix-to-download\" branch = 'main' localpath = './' + branch session.download_data(path=localpath, bucket=repo, key_prefix = branch + prefix) . Note: Advanced AWS SageMaker features, like Autopilot jobs, are encapsulated and don’t have the option to override the S3 endpoint. However, it is possible to export the required inputs from lakeFS to S3. If you’re using SageMaker features that aren’t supported by lakeFS, we’d love to hear from you. ",
    "url": "/v0.70/integrations/sagemaker.html#usage-examples",
    "relUrl": "/integrations/sagemaker.html#usage-examples"
  },"311": {
    "doc": "SageMaker",
    "title": "SageMaker",
    "content": " ",
    "url": "/v0.70/integrations/sagemaker.html",
    "relUrl": "/integrations/sagemaker.html"
  },"312": {
    "doc": "Sizing Guide",
    "title": "Sizing guide",
    "content": " ",
    "url": "/v0.70/understand/sizing-guide.html#sizing-guide",
    "relUrl": "/understand/sizing-guide.html#sizing-guide"
  },"313": {
    "doc": "Sizing Guide",
    "title": "Table of contents",
    "content": ". | System Requirements . | Operating Systems and ISA | Memory and CPU requirements | Network | Disk | PostgreSQL database . | Storage | RAM and shared_buffers | CPU | . | . | Scaling factors . | Understanding latency and throughput considerations | . | Benchmarks . | Random reads | Random Writes | Branch creation | . | Important metrics | Reference architectures . | Reference Architecture: Data Science/Research environment | Reference Architecture: Automated Production Pipelines | . | . ",
    "url": "/v0.70/understand/sizing-guide.html#table-of-contents",
    "relUrl": "/understand/sizing-guide.html#table-of-contents"
  },"314": {
    "doc": "Sizing Guide",
    "title": "System Requirements",
    "content": "Operating Systems and ISA . lakeFS can run on MacOS and Linux. Windows binaries are available but not rigorously tested - we don’t recommend deploying lakeFS to production on Windows. x86_64 and arm64 architectures are supported for both MacOS and Linux. Memory and CPU requirements . lakeFS servers require a minimum of 512mb of RAM and 1 CPU core. For high throughput, additional CPUs help scale requests across different cores. “Expensive” operations such as large diff or commit operations can take advantage of multiple cores. Network . If using the data APIs such as the S3 Gateway, lakeFS will require enough network bandwidth to support the planned concurrent network upload/download operations. For most cloud providers, more powerful machines (i.e., more expensive and usually containing more CPU cores) also provide increased network bandwidth. If using only the metadata APIs (for example, only using the Hadoop/Spark clients), network bandwidth is minimal, at roughly 1Kb per request. Disk . lakeFS greatly benefits from fast local disks. A lakeFS instance doesn’t require any strong durability guarantees from the underlying storage, as the disk is only ever used as a local caching layer for lakeFS metadata and not for long-term storage. lakeFS is designed to work with ephemeral disks - these are usually based on NVMe and are tied to the machine’s lifecycle. Using ephemeral disks lakeFS can provide a very high throughput/cost ratio, probably the best that could be achieved on a public cloud, so we recommend those. A local cache of at least 512 MiB should be provided. For large installations (managing &gt;100 concurrently active branches, with &gt;100M objects per commit), we recommend allocating at least 10 GiB - since it’s a caching layer over a relatively slow storage (the object store), see Important metrics below to understand how to size this: it should be big enough to hold all commit metadata for actively referenced commits. PostgreSQL database . lakeFS uses a PostgreSQL instance to manage branch references, authentication and authorization information and to keep track of currently uncommitted data across branches. Storage . The dataset stored in PostgreSQL is relatively modest as most metadata is pushed down into the object store. Required storage is mostly a factor of the amount of uncommitted writes across all branches at any given point in time: in the range of 150 MiB per every 100,000 uncommitted writes. We recommend starting at 10 GiB for a production deployment, as it will likely be more than enough. RAM and shared_buffers . Since the data size is small, it’s recommended to provide enough memory to hold the vast majority of that data in RAM: Ideally, configure the shared_buffers of your PostgreSQL instances to be large enough to contain the currently active dataset. Pick a database instance with enough RAM to accommodate this buffer size at roughly x4 the size given for shared_buffers. For example, if an installation has ~500,000 uncommitted writes at any given time, it would require about 750 MiB of shared_buffers that would require about 3 GiB of RAM. Cloud providers will save you the need to tune this parameter - it will be set to a fixed percentage the chosen instance’s available RAM (25% on AWS RDS, 30% on Google Cloud SQL). CPU . PostgreSQL CPU cores help scale concurrent requests. 1 CPU core for every 5,000 requests/second is ideal. ",
    "url": "/v0.70/understand/sizing-guide.html#system-requirements",
    "relUrl": "/understand/sizing-guide.html#system-requirements"
  },"315": {
    "doc": "Sizing Guide",
    "title": "Scaling factors",
    "content": "Scaling lakeFS, like most data systems, moves across two axes: throughput of requests (amount per given timeframe) and latency (time to complete a single request). Understanding latency and throughput considerations . Most lakeFS operations are designed to be very low in latency. Assuming a well-tuned local disk cache (see Storage above), most critical path operations (writing objects, requesting objects, deleting objects) are designed to complete in &lt;25ms at p90. Listing objects obviously requires accessing more data, but should always be on-par with what the underlying object store can provide, and in most cases, it’s actually faster. At the worst case, for directory listing with 1,000 common prefixes returned, expect a latency of 75ms at p90. Managing branches (creating them, listing them and deleting them) are all constant-time operations, generally taking &lt;30ms at p90. Committing and merging can take longer, as they are proportional to the amount of changes introduced. This is what makes lakeFS optimal for large Data Lakes - the amount of changes introduced per commit usually stays relatively stable while the entire data set usually grows over time. This means lakeFS will provide predictable performance: committing 100 changes will take roughly the same amount of time whether the resulting commit contains 500 or 500 million objects. See Data Model for more information. Scaling throughput depends very much on the amount of CPU cores available to lakeFS. In many cases, it’s easier to scale lakeFS across a fleet of smaller cloud instances (or containers) than scale up with machines that have many cores. In fact, lakeFS works well in both cases. Most critical path operations scale very well across machines. ",
    "url": "/v0.70/understand/sizing-guide.html#scaling-factors",
    "relUrl": "/understand/sizing-guide.html#scaling-factors"
  },"316": {
    "doc": "Sizing Guide",
    "title": "Benchmarks",
    "content": "All benchmarks below were measured using 2 x c5ad.4xlarge instances on AWS us-east-1. Similar results can be achieved on Google Cloud using a c2-standard-16 machine type, with an attached local SSD. On Azure, you can use a Standard_F16s_v2 virtual machine. The PostgreSQL instance that was used is a db.m6g.2xlarge (8 vCPUs, 32 GB RAM). Equivalent machines on Google Cloud or Azure should yield similar results. The example repository we tested against contains the metadata of a large lakeFS installation, where each commit contains ~180,000,000 objects (representing ~7.5 Petabytes of data). All tests are reproducible using the lakectl abuse command, so use it to properly size and tune your setup. All tests are accompanied by the relevant lakectl abuse command that generated them. Random reads . This test generates random read requests to lakeFS, in a given commit. Paths are requested randomly from a file containing a set of preconfigured (and existing) paths. command executed: . lakectl abuse random-read \\ --from-file randomly_selected_paths.txt \\ --amount 500000 \\ --parallelism 128 \\ lakefs://example-repo/&lt;commit hash&gt; . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and commit hash. Result Histogram (raw): . Histogram (ms): 1 0 2 0 5 37945 7 179727 10 296964 15 399682 25 477502 50 499625 75 499998 100 499998 250 500000 350 500000 500 500000 750 500000 1000 500000 5000 500000 min 3 max 222 total 500000 . So 50% of all requests took &lt;10ms, while 99.9% of them took &lt;50ms . throughput: . Average throughput during the experiment was 10851.69 requests/second . Random Writes . This test generates random write requests to a given lakeFS branch. All the paths are pre-generated and don’t overwrite each other (as overwrites are relatively rare in a Data Lake setup). command executed: . lakectl abuse random-write \\ --amount 500000 \\ --parallelism 64 \\ lakefs://example-repo/main . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. Result Histogram (raw): . Histogram (ms): 1 0 2 0 5 30715 7 219647 10 455807 15 498144 25 499535 50 499742 75 499784 100 499802 250 500000 350 500000 500 500000 750 500000 1000 500000 5000 500000 min 3 max 233 total 500000 . So, 50% of all requests took &lt;10ms, while 99.9% of them took &lt;25ms. throughput: . The average throughput during the experiment was 7595.46 requests/second. Branch creation . This test creates branches from a given reference. command executed: . lakectl abuse create-branches \\ --amount 500000 \\ --branch-prefix \"benchmark-\" \\ --parallelism 256 \\ lakefs://example-repo/&lt;commit hash&gt; . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and commit hash. Result Histogram (raw): . Histogram (ms): 1 0 2 1 5 5901 7 39835 10 135863 15 270201 25 399895 50 484932 75 497180 100 499303 250 499996 350 500000 500 500000 750 500000 1000 500000 5000 500000 min 2 max 304 total 500000 . So, 50% of all requests took &lt;15ms, while 99.9% of them took &lt;100ms. throughput: . The average throughput during the experiment was 7069.03 requests/second. ",
    "url": "/v0.70/understand/sizing-guide.html#benchmarks",
    "relUrl": "/understand/sizing-guide.html#benchmarks"
  },"317": {
    "doc": "Sizing Guide",
    "title": "Important metrics",
    "content": "lakeFS exposes metrics using the Prometheus protocol. Every lakeFS instance exposes a /metrics endpoint that could be used to extract them. Here are a few notable metrics to keep track of when sizing lakeFS: . api_requests_total - Tracks throughput of API requests over time. api_request_duration_seconds - Histogram of latency per operation type. gateway_request_duration_seconds - Histogram of latency per S3 Gateway operation. go_sql_stats_* - Important client-side metrics collected from the PostgreSQL driver. See The full reference here. ",
    "url": "/v0.70/understand/sizing-guide.html#important-metrics",
    "relUrl": "/understand/sizing-guide.html#important-metrics"
  },"318": {
    "doc": "Sizing Guide",
    "title": "Reference architectures",
    "content": "Below are a few example architectures for lakeFS deployment. Reference Architecture: Data Science/Research environment . Use case: Manage Machine learning or algorithms development. Use lakeFS branches to achieve both isolation and reproducibility of experiments. Data being managed by lakeFS is both structured tabular data, as well as unstructured sensor and image data used for training. Assuming a team of 20-50 researchers, with a dataset size of 500 TiB across 20M objects. Environment: lakeFS will be deployed on Kubernetes managed by AWS EKS with PostgreSQL on AWS RDS Aurora . Sizing: Since most of the work is done by humans (vs. automated pipelines), most experiments tend to be small in scale, reading and writing 10s to 1000s of objects. The expected amount of branches active in parallel is relatively low, around 1-2 per user, each representing a small amount of uncommitted changes at any given point in time. Let’s assume 5,000 uncommitted writes per branch = ~500k. To support the expected throughput, a single moderate lakeFS instance should be more than enough, since requests per second would be on the order of 10s to 100s. For high availability, we’ll deploy 2 pods with 1 CPU core and 1 GiB of RAM each. Since the PostgreSQL instance is expected to hold a very small dataset (at 500k, expected dataset size is 150MiB (for 100k records) * 5 = 750MiB). To ensure we have enough RAM to hold this, we’ll need 3 GiB of RAM, so, a very moderate Aurora instance db.t3.large (2 vCPUs, 8 GB RAM) will be more than enough. An equivalent database instance on GCP or Azure should give similar results. Reference Architecture: Automated Production Pipelines . Use case: Manage multiple concurrent data pipelines using Apache Spark and Airflow. Airflow DAGs start by creating a branch for isolation and for CI/CD. Data being managed by lakeFS is structured, tabular data. The total dataset size is 10 PiB, spanning across 500M objects. The expected throughput is 10k reads/second + 2k writes per second across 100 concurrent branches. Environment: lakeFS will be deployed on Kubernetes managed by AWS EKS with PostgreSQL on AWS RDS . Sizing: Data pipelines tend to be bursty in nature: reading in a lot of objects concurrently, doing some calculation or aggregation, and then writing many objects concurrently. The expected amount of branches active in parallel is high, with many Airflow DAGs running per day, each representing a moderate amount of uncommitted changes at any given point in time. Let’s assume 1,000 uncommitted writes/branch * 2500 branches = ~2.5M records. To support the expected throughput, looking the benchmarking numbers above, we’re doing roughly 625 requests/core, so 24 cores should cover our peak traffic. We can deploy 6 * 4 CPU core pods. On to the PostgreSQL instance - at 500k, the expected dataset size is 150MiB (for 100k records) * 25 = 3750 MiB. To ensure we have enough RAM to hold this, we’ll need at least 15 GiB of RAM, so we’ll go with a db.r5.xlarge (4 vCPUs, 32GB RAM) Aurora instance. An equivalent database instance on GCP or Azure should give similar results. ",
    "url": "/v0.70/understand/sizing-guide.html#reference-architectures",
    "relUrl": "/understand/sizing-guide.html#reference-architectures"
  },"319": {
    "doc": "Sizing Guide",
    "title": "Sizing Guide",
    "content": " ",
    "url": "/v0.70/understand/sizing-guide.html",
    "relUrl": "/understand/sizing-guide.html"
  },"320": {
    "doc": "Spark Client",
    "title": "lakeFS Spark Client",
    "content": "Utilize the power of Spark to interact with the metadata on lakeFS. Possible use cases include: . | Creating a DataFrame for listing the objects in a specific commit or branch. | Computing changes between two commits. | Exporting your data for consumption outside lakeFS. | Bulk operations on the underlying storage. | . ",
    "url": "/v0.70/reference/spark-client.html#lakefs-spark-client",
    "relUrl": "/reference/spark-client.html#lakefs-spark-client"
  },"321": {
    "doc": "Spark Client",
    "title": "Getting Started",
    "content": "Start Spark Shell / PySpark with the --packages flag: . | Spark 2.x | Spark 3.x | Spark 3.x on Hadoop 3.x | . This client is compiled for Spark 2.4.7 and tested with it, but can work for higher versions. spark-shell --packages io.lakefs:lakefs-spark-client-247_2.11:0.2.0 . Alternatively an assembled jar is available on S3, at s3://treeverse-clients-us-east/lakefs-spark-client-247/0.2.0/lakefs-spark-client-247-assembly-0.2.0.jar . This client is compiled for Spark 3.0.1 with Hadoop 2 and tested with it, but can work for higher versions. spark-shell --packages io.lakefs:lakefs-spark-client-301_2.12:0.2.0 . Alternatively an assembled jar is available on S3, at s3://treeverse-clients-us-east/lakefs-spark-client-301/0.2.0/lakefs-spark-client-301-assembly-0.2.0.jar . This client is compiled for Spark 3.1.2 with Hadoop 3.2.1, but can work for other Spark versions and higher Hadoop versions. spark-shell --packages io.lakefs:lakefs-spark-client-312-hadoop3-assembly_2.12:0.2.0 . Alternatively an assembled jar is available on S3, at s3://treeverse-clients-us-east/lakefs-spark-client-312-hadoop3/0.2.0/lakefs-spark-client-312-hadoop3-assembly-0.2.0.jar . ",
    "url": "/v0.70/reference/spark-client.html#getting-started",
    "relUrl": "/reference/spark-client.html#getting-started"
  },"322": {
    "doc": "Spark Client",
    "title": "Configuration",
    "content": ". | To read metadata from lakeFS, the client should be configured with your lakeFS endpoint and credentials, using the following Hadoop configurations: . | Configuration | Description | . | spark.hadoop.lakefs.api.url | lakeFS API endpoint, e.g: http://lakefs.example.com/api/v1 | . | spark.hadoop.lakefs.api.access_key | The access key to use for fetching metadata from lakeFS | . | spark.hadoop.lakefs.api.secret_key | Corresponding lakeFS secret key | . | The client will also directly interact with your storage using Hadoop FileSystem. Therefore, your Spark session must be able to access the underlying storage of your lakeFS repository. There are various ways to do this, but for a non-production environment you can use the following Hadoop configurations: . | Configuration | Description | . | spark.hadoop.fs.s3a.access.key | Access key to use for accessing underlying storage on S3 | . | spark.hadoop.fs.s3a.secret.key | Corresponding secret key to use with S3 access key | . Assuming role on S3 (Hadoop 3 only) . The client includes support for assuming a separate role on S3 when running on Hadoop 3. It uses the same configuration used by S3AFileSystem to assume the role on S3A. Apache Hadoop AWS documentation has details under “Working with IAM Assumed Roles”. You will need to use the following Hadoop configurations: . | Configuration | Description | . | fs.s3a.aws.credentials.provider | Set to org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider | . | fs.s3a.assumed.role.arn | Set to the ARN of the role to assume | . | . ",
    "url": "/v0.70/reference/spark-client.html#configuration",
    "relUrl": "/reference/spark-client.html#configuration"
  },"323": {
    "doc": "Spark Client",
    "title": "Examples",
    "content": ". | Get a DataFrame for listing all objects in a commit: . import io.treeverse.clients.LakeFSContext val commitID = \"a1b2c3d4\" val df = LakeFSContext.newDF(spark, \"example-repo\", commitID) df.show /* output example: +------------+--------------------+--------------------+-------------------+----+ | key | address| etag| last_modified|size| +------------+--------------------+--------------------+-------------------+----+ | file_1 |791457df80a0465a8...|7b90878a7c9be5a27...|2021-03-05 11:23:30| 36| file_2 |e15be8f6e2a74c329...|95bee987e9504e2c3...|2021-03-05 11:45:25| 36| file_3 |f6089c25029240578...|32e2f296cb3867d57...|2021-03-07 13:43:19| 36| file_4 |bef38ef97883445c8...|e920efe2bc220ffbb...|2021-03-07 13:43:11| 13| +------------+--------------------+--------------------+-------------------+----+ */ . | Run SQL queries on your metadata: . df.createOrReplaceTempView(\"files\") spark.sql(\"SELECT DATE(last_modified), COUNT(*) FROM files GROUP BY 1 ORDER BY 1\") /* output example: +----------+--------+ | dt|count(1)| +----------+--------+ |2021-03-05| 2|2021-03-07| 2| +----------+--------+ */ . | . ",
    "url": "/v0.70/reference/spark-client.html#examples",
    "relUrl": "/reference/spark-client.html#examples"
  },"324": {
    "doc": "Spark Client",
    "title": "Spark Client",
    "content": " ",
    "url": "/v0.70/reference/spark-client.html",
    "relUrl": "/reference/spark-client.html"
  },"325": {
    "doc": "Spark",
    "title": "Using lakeFS with Spark",
    "content": "Apache Spark is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters . ",
    "url": "/v0.70/integrations/spark.html#using-lakefs-with-spark",
    "relUrl": "/integrations/spark.html#using-lakefs-with-spark"
  },"326": {
    "doc": "Spark",
    "title": "Table of contents",
    "content": ". | Two-tiered Spark support | Access lakeFS using the S3A gateway . | Configuration . | Per-bucket configuration | . | Reading Data | Writing Data | . | Access lakeFS using the lakeFS-specific Hadoop FileSystem . | Configuration . | Load the FileSystem JARs | Configure the lakeFS FileSystem and the underlying S3A FileSystem | Per-bucket and per-repo configuration | . | Reading Data | Writing Data | . | Case Study: SimilarWeb | . Note In all of the following examples, we set AWS and lakeFS credentials at runtime for clarity. In production, properties defining AWS credentials should be set using one of Hadoop’s standard ways of authenticating with S3. Similarly, properties defining lakeFS credentials should be configured in secure site files, not on the command line or inlined in code where they might be exposed. ",
    "url": "/v0.70/integrations/spark.html#table-of-contents",
    "relUrl": "/integrations/spark.html#table-of-contents"
  },"327": {
    "doc": "Spark",
    "title": "Two-tiered Spark support",
    "content": "lakeFS support in Spark has two tiers: . | Access lakeFS using the S3A gateway. | Access lakeFS using the lakeFS-specific Hadoop FileSystem. | . Using the S3A gateway is easier to configure and may be more suitable for legacy or small-scale applications. Using the lakeFS FileSystem requires a somewhat more complex configuration, but offers greatly increased performance. ",
    "url": "/v0.70/integrations/spark.html#two-tiered-spark-support",
    "relUrl": "/integrations/spark.html#two-tiered-spark-support"
  },"328": {
    "doc": "Spark",
    "title": "Access lakeFS using the S3A gateway",
    "content": "To use this mode you configure the Spark application to use S3A using the S3-compatible endpoint that the lakeFS server provides. Accordingly, all data flows through the lakeFS server. Accessing data in lakeFS from Spark is the same as accessing S3 data from Spark. The only changes you need to consider are: . | Setting the configurations to access lakeFS, | Accessing objects using the lakeFS S3 path convention. | . Configuration . To configure Spark to work with lakeFS, we set S3 Hadoop configuration to the lakeFS endpoint and credentials: . | Hadoop Configuration | Value | . | fs.s3a.access.key | Set to the lakeFS access key | . | fs.s3a.secret.key | Set to the lakeFS secret key | . | fs.s3a.endpoint | Set to the lakeFS S3-compatible API endpoint | . | fs.s3a.path.style.access | Set to true | . Here is how to do it: . | CLI | Scala | XML Configuration | . spark-shell --conf spark.hadoop.fs.s3a.access.key='AKIAlakefs12345EXAMPLE' \\ --conf spark.hadoop.fs.s3a.secret.key='abc/lakefs/1234567bPxRfiCYEXAMPLEKEY' \\ --conf spark.hadoop.fs.s3a.path.style.access=true \\ --conf spark.hadoop.fs.s3a.endpoint='https://lakefs.example.com' ... spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", \"AKIAlakefs12345EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.endpoint\", \"https://lakefs.example.com\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.path.style.access\", \"true\") . Add these into a configuration file, e.g. $SPARK_HOME/conf/hdfs-site.xml: . &lt;?xml version=\"1.0\"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.s3a.access.key&lt;/name&gt; &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.secret.key&lt;/name&gt; &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;https://lakefs.example.com&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . Per-bucket configuration . The above configuration will use lakeFS as the sole S3 endpoint. To use lakeFS in parallel with S3, you can configure Spark to use lakeFS only for specific bucket names. For example, to configure only example-repo to use lakeFS, set the following configurations: . | CLI | Scala | XML Configuration | . spark-shell --conf spark.hadoop.fs.s3a.bucket.example-repo.access.key='AKIAlakefs12345EXAMPLE' \\ --conf spark.hadoop.fs.s3a.bucket.example-repo.secret.key='abc/lakefs/1234567bPxRfiCYEXAMPLEKEY' \\ --conf spark.hadoop.fs.s3a.bucket.example-repo.endpoint='https://lakefs.example.com' \\ --conf spark.hadoop.fs.s3a.path.style.access=true . spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.access.key\", \"AKIAlakefs12345EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.endpoint\", \"https://lakefs.example.com\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.path.style.access\", \"true\") . Add these into a configuration file, e.g. $SPARK_HOME/conf/hdfs-site.xml: . &lt;?xml version=\"1.0\"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.s3a.bucket.example-repo.access.key&lt;/name&gt; &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.bucket.example-repo.secret.key&lt;/name&gt; &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.bucket.example-repo.endpoint&lt;/name&gt; &lt;value&gt;https://lakefs.example.com&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . With this configuration set, you read S3A paths with example-repo as the bucket will use lakeFS, while all other buckets will use AWS S3. Reading Data . To access objects in lakeFS, you need to use the lakeFS S3 gateway path conventions: . s3a://[REPOSITORY]/[BRANCH]/PATH/TO/OBJECT . Here is an example for reading a parquet file from lakeFS to a Spark DataFrame: . val repo = \"example-repo\" val branch = \"main\" val dataPath = s\"s3a://${repo}/${branch}/example-path/example-file.parquet\" val df = spark.read.parquet(dataPath) . You can now use this DataFrame like you’d normally do. Writing Data . Now simply write your results back to a lakeFS path: . df.write.partitionBy(\"example-column\").parquet(s\"s3a://${repo}/${branch}/output-path/\") . The data is now created in lakeFS as new changes in your branch. You can now commit these changes or revert them. ",
    "url": "/v0.70/integrations/spark.html#access-lakefs-using-the-s3a-gateway",
    "relUrl": "/integrations/spark.html#access-lakefs-using-the-s3a-gateway"
  },"329": {
    "doc": "Spark",
    "title": "Access lakeFS using the lakeFS-specific Hadoop FileSystem",
    "content": "To use this mode, you configure the Spark application to perform metadata operations on the lakeFS server and all data operations directly through the same underlying object store that lakeFS uses. The lakeFS FileSystem currently supports Spark with Hadoop Apache 2.7 using only the S3A Hadoop FileSystem for data access. In this mode, the Spark application will read and write directly from the underlying object store, significantly increasing application scalability and performance by reducing the load on the lakeFS server. Accessing data in lakeFS from Spark is the same as accessing S3 data from Spark. The only changes you need to make are: . | Configure Spark to access lakeFS for metadata and S3 or a compatible underlying object store to access data, | Use lakefs://repo/ref/path/to/data URIs to read and write data on lakeFS rather than s3a://... URIs. | . Configuration . To configure Spark to work using the lakeFS Hadoop FileSystem, you will need to load the filesystem JARs and then configure both that FileSystem and the underlying data access FileSystem. Load the FileSystem JARs . Add the package io.lakefs:hadoop-lakefs-assembly:&lt;VERSION&gt; to your Spark command: . --packages io.lakefs:hadoop-lakefs-assembly:0.1.6 . The jar is also available on a public S3 location: s3://treeverse-clients-us-east/hadoop/hadoop-lakefs-assembly-0.1.6.jar . Configure the lakeFS FileSystem and the underlying S3A FileSystem . Add Hadoop configuration to the underlying storage and additionally to lakeFS credentials. When using this mode, do not set the S3A endpoint URL to point at lakeFS - it should point at the underlying storage. | Hadoop Configuration | Value | . | fs.s3a.access.key | Set to the AWS S3 access key | . | fs.s3a.secret.key | Set to the AWS S3 secret key | . | fs.s3a.endpoint | Set to the AWS S3-compatible endpoint | . | fs.lakefs.impl | io.lakefs.LakeFSFileSystem | . | fs.lakefs.access.key | Set to the lakeFS access key | . | fs.lakefs.secret.key | Set to the lakeFS secret key | . | fs.lakefs.endpoint | Set to the lakeFS API URL | . When using AWS S3 itself, the default configuration works with us-east-1, so you may still need to configure fs.s3a.endpoint. Amazon provides S3 endpoints you can use. Note: The lakeFS Hadoop FileSystem uses the fs.s3a.* properties to directly access S3. If your cluster already has access to your buckets (for example, if you’re using an AWS instance profile), then you don’t need to configure these properties. permissions. Here is how to do it: . | CLI | Scala | XML Configuration | . spark-shell --conf spark.hadoop.fs.s3a.access.key='AKIAIOSFODNN7EXAMPLE' \\ --conf spark.hadoop.fs.s3a.secret.key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' \\ --conf spark.hadoop.fs.s3a.endpoint='https://s3.eu-central-1.amazonaws.com' \\ --conf spark.hadoop.fs.lakefs.impl=io.lakefs.LakeFSFileSystem \\ --conf spark.hadoop.fs.lakefs.access.key=AKIAlakefs12345EXAMPLE \\ --conf spark.hadoop.fs.lakefs.secret.key=abc/lakefs/1234567bPxRfiCYEXAMPLEKEY \\ --conf spark.hadoop.fs.lakefs.endpoint=https://lakefs.example.com/api/v1 \\ --packages io.lakefs:hadoop-lakefs-assembly:0.1.6 ... Ensure you load the lakeFS FileSystem into Spark by running it with --packages or --jars, and then run: . spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", \"AKIAIOSFODNN7EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.endpoint\", \"https://s3.eu-central-1.amazonaws.com\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.access.key\", \"AKIAlakefs12345EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.endpoint\", \"https://lakefs.example.com/api/v1\") . Make sure that you load the lakeFS FileSystem into Spark by running it with --packages or --jars, and then add these into a configuration file, e.g., $SPARK_HOME/conf/hdfs-site.xml: . &lt;?xml version=\"1.0\"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.s3a.access.key&lt;/name&gt; &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.secret.key&lt;/name&gt; &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;https://s3.eu-central-1.amazonaws.com&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.impl&lt;/name&gt; &lt;value&gt;io.lakefs.LakeFSFileSystem&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.access.key&lt;/name&gt; &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.secret.key&lt;/name&gt; &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.endpoint&lt;/name&gt; &lt;value&gt;https://lakefs.example.com/api/v1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . Per-bucket and per-repo configuration . As demonstrated above, S3 allows for per-bucket configuration. You can use this if: . | You need to use S3A directly to access data in an S3 outside of lakeFS, and | different credentials are required to access data inside that bucket. | . Refer to the Hadoop AWS guide Configuring different S3 buckets with Per-Bucket Configuration. There’s no need for per-repo configurations in lakeFS when all repositories are on the same lakeFS server. If you need to access repositories located on multiple lakeFS servers, configure multiple prefixes. For instance, you might configure both fs.lakefs.impl and fs.lakefs2.impl to be io.lakefs.LakeFSFileSystem, place separate endpoints and credentials under fs.lakefs.* and fs.lakefs2.*, and access the two servers using lakefs://... and lakefs2://... URLs. Reading Data . To access objects in lakeFS, you need to use the lakeFS path conventions: . lakefs://[REPOSITORY]/[BRANCH]/PATH/TO/OBJECT . Here’s an example for reading a parquet file from lakeFS to a Spark DataFrame: . val repo = \"example-repo\" val branch = \"main\" val dataPath = s\"lakefs://${repo}/${branch}/example-path/example-file.parquet\" val df = spark.read.parquet(dataPath) . You can now use this DataFrame like you would normally do. Writing Data . Now simply write your results back to a lakeFS path: . df.write.partitionBy(\"example-column\").parquet(s\"lakefs://${repo}/${branch}/output-path/\") . The data is now created in lakeFS as new changes in your branch. You can now commit these changes or revert them. ",
    "url": "/v0.70/integrations/spark.html#access-lakefs-using-the-lakefs-specific-hadoop-filesystem",
    "relUrl": "/integrations/spark.html#access-lakefs-using-the-lakefs-specific-hadoop-filesystem"
  },"330": {
    "doc": "Spark",
    "title": "Case Study: SimilarWeb",
    "content": "See how SimilarWeb is using lakeFS with Spark to manage algorithm changes in data pipelines. ",
    "url": "/v0.70/integrations/spark.html#case-study-similarweb",
    "relUrl": "/integrations/spark.html#case-study-similarweb"
  },"331": {
    "doc": "Spark",
    "title": "Spark",
    "content": " ",
    "url": "/v0.70/integrations/spark.html",
    "relUrl": "/integrations/spark.html"
  },"332": {
    "doc": "Try without installing",
    "title": "Try lakeFS without installing",
    "content": " ",
    "url": "/v0.70/quickstart/try.html#try-lakefs-without-installing",
    "relUrl": "/quickstart/try.html#try-lakefs-without-installing"
  },"333": {
    "doc": "Try without installing",
    "title": "lakeFS Playground",
    "content": "Experience lakeFS first hand with your own isolated environment. You can easily integrate it with your existing tools, and feel lakeFS in action in an environment similar to your own. Try lakeFS now without installing . ",
    "url": "/v0.70/quickstart/try.html#lakefs-playground",
    "relUrl": "/quickstart/try.html#lakefs-playground"
  },"334": {
    "doc": "Try without installing",
    "title": "lakeFS Docker “Everything Bagel”",
    "content": "Get a local lakeFS instance running in a Docker container. This environment includes lakeFS and other common data tools like Spark, dbt, Trino, Hive, and Jupyter. As a prerequisite, Docker is required to be installed on your machine. For download instructions, click here . The following commands can be run in your terminal to get the Bagel running: . | Clone the lakeFS repo: git clone https://github.com/treeverse/lakeFS.git | Start the Docker containers: cd lakeFS/deployments/compose &amp;&amp; docker compose up -d | . Once you have your Docker environment running, it is helpful to pull up the UI for lakeFS. To do this navigate to http://localhost:8000 in your browser. The access key and secret to login are found in the docker_compose.yml file in the lakefs-setup section. Once you are logged in, you should see a page that looks like below. The first thing to notice is in this environment, lakeFS comes with a repository called example already created, and the repo’s default branch is main. If your lakeFS installation doesn’t have the example repo created, you can use the green Create Repository button to do so: . ",
    "url": "/v0.70/quickstart/try.html#lakefs-docker-everything-bagel",
    "relUrl": "/quickstart/try.html#lakefs-docker-everything-bagel"
  },"335": {
    "doc": "Try without installing",
    "title": "Next Steps",
    "content": "You can now install lakeFS on your computer or deploy it on your cloud account. ",
    "url": "/v0.70/quickstart/try.html#next-steps",
    "relUrl": "/quickstart/try.html#next-steps"
  },"336": {
    "doc": "Try without installing",
    "title": "Try without installing",
    "content": " ",
    "url": "/v0.70/quickstart/try.html",
    "relUrl": "/quickstart/try.html"
  },"337": {
    "doc": "Upgrade lakeFS",
    "title": "Upgrading lakeFS",
    "content": "Upgrading lakeFS from a previous version usually just requires re-deploying with the latest image (or downloading the latest version if you’re using the binary). In some cases, the database may require a migration - check whether the release you’re upgrading to requires that. ",
    "url": "/v0.70/reference/upgrade.html#upgrading-lakefs",
    "relUrl": "/reference/upgrade.html#upgrading-lakefs"
  },"338": {
    "doc": "Upgrade lakeFS",
    "title": "When DB migrations are required",
    "content": "lakeFS 0.30.0 or greater . In case migration is required, you first need to stop the running lakeFS service. Using the lakefs binary for the new version, run the following: . lakefs migrate up . Deploy (or run) the new version of lakeFS. Note that an older version of lakeFS cannot run on a migrated database. Prior to lakeFS 0.30.0 . Note: with lakeFS &lt; 0.30.0, you should first upgrade to 0.30.0 following this guide. Then, proceed to upgrade to the newest version. Starting version 0.30.0, lakeFS handles your committed metadata in a new way, which is more robust and has better performance. To move your existing data, you will need to run the following upgrade commands. Verify lakeFS version == 0.30.0 (can skip if using Docker) . lakefs --version . Migrate data from the previous format: . lakefs migrate db . Or migrate using Docker image: . docker run --rm -it -e LAKEFS_DATABASE_CONNECTION_STRING=&lt;database connection string&gt; treeverse/lakefs:rocks-migrate migrate db . Once migrated, it is possible to now use more recent lakeFS versions. Please refer to their release notes for more information on ugrading and usage). If you want to start over, discarding your existing data, you need to explicitly state this in your lakeFS configuration file. To do so, add the following to your configuration (relevant only for 0.30.0): . cataloger: type: rocks . ",
    "url": "/v0.70/reference/upgrade.html#when-db-migrations-are-required",
    "relUrl": "/reference/upgrade.html#when-db-migrations-are-required"
  },"339": {
    "doc": "Upgrade lakeFS",
    "title": "Data Migration for Version v0.50.0",
    "content": "We discovered a bug in the way lakeFS is storing objects in the underlying object store. It affects only repositories on Azure and GCP, and not all of them. Issue #2397 describes the repository storage namespaces patterns that are affected by this bug. When first upgrading to a version greater or equal to v0.50.0, you must follow these steps: . | Stop lakeFS. | Perform a data migration (details below) | Start lakeFS with the new version. | After a successful run of the new version and validation that the objects are accessible, you can delete the old data prefix. | . Note: Migrating data is a delicate procedure. The lakeFS team is here to help, reach out to us on Slack. We’ll be happy to walk you through the process. Data migration . The following patterns have been impacted by the bug: . | Type | Storage Namespace pattern | Copy From | Copy To | . | gs | gs://bucket/prefix | gs://bucket//prefix/* | gs://bucket/prefix/* | . | gs | gs://bucket/prefix/ | gs://bucket//prefix/* | gs://bucket/prefix/* | . | azure | https://account.blob.core.windows.net/containerid | https://account.blob.core.windows.net/containerid//* | https://account.blob.core.windows.net/containerid/* | . | azure | https://account.blob.core.windows.net/containerid/ | https://account.blob.core.windows.net/containerid//* | https://account.blob.core.windows.net/containerid/* | . | azure | https://account.blob.core.windows.net/containerid/prefix/ | https://account.blob.core.windows.net/containerid/prefix// | https://account.blob.core.windows.net/containerid/prefix/* | . You can find the repositories storage namespaces with: . lakectl repo list . Or the settings tab in the UI. Migrating Google Storage data with gsutil . gsutil is a Python application that lets you access Cloud Storage from the command line. We can use it for copying the data between the prefixes in the Google bucket, and later on removing it. For every affected repository, copy its data with: . gsutil -m cp -r gs://&lt;BUCKET&gt;//&lt;PREFIX&gt;/ gs://&lt;BUCKET&gt;/ . Note the double slash after the bucket name. Migrating Azure Blob Storage data with AzCopy . AzCopy is a command-line utility that you can use to copy blobs or files to or from a storage account. We can use it for copying the data between the prefixes in the Azure storage account container, and later on removing it. First, you need to acquire an Account SAS. Using the Azure CLI: . az storage container generate-sas \\ --account-name &lt;ACCOUNT&gt; \\ --name &lt;CONTAINER&gt; \\ --permissions cdrw \\ --auth-mode key \\ --expiry 2021-12-31 . With the resulted SAS, use AzCopy to copy the files. If a prefix exists after the container: . azcopy copy \\ \"https://&lt;ACCOUNT&gt;.blob.core.windows.net/&lt;CONTAINER&gt;/&lt;PREFIX&gt;//?&lt;SAS_TOKEN&gt;\" \\ \"https://&lt;ACCOUNT&gt;.blob.core.windows.net/&lt;CONTAINER&gt;?&lt;SAS_TOKEN&gt;\" \\ --recursive=true . Or when using the container without a prefix: . azcopy copy \\ \"https://&lt;ACCOUNT&gt;.blob.core.windows.net/&lt;CONTAINER&gt;//?&lt;SAS_TOKEN&gt;\" \\ \"https://&lt;ACCOUNT&gt;.blob.core.windows.net/&lt;CONTAINER&gt;/./?&lt;SAS_TOKEN&gt;\" \\ --recursive=true . ",
    "url": "/v0.70/reference/upgrade.html#data-migration-for-version-v0500",
    "relUrl": "/reference/upgrade.html#data-migration-for-version-v0500"
  },"340": {
    "doc": "Upgrade lakeFS",
    "title": "Upgrade lakeFS",
    "content": " ",
    "url": "/v0.70/reference/upgrade.html",
    "relUrl": "/reference/upgrade.html"
  },"341": {
    "doc": "Versioning Internals",
    "title": "Versioning Internals",
    "content": " ",
    "url": "/v0.70/understand/versioning-internals.html",
    "relUrl": "/understand/versioning-internals.html"
  },"342": {
    "doc": "Versioning Internals",
    "title": "Table of contents",
    "content": ". | Overview | SSTable File Format (“Graveler File”) | Constructing a consistent view of the keyspace (i.e., a commit) | Representing references and uncommitted metadata | . ",
    "url": "/v0.70/understand/versioning-internals.html#table-of-contents",
    "relUrl": "/understand/versioning-internals.html#table-of-contents"
  },"343": {
    "doc": "Versioning Internals",
    "title": "Overview",
    "content": "Since commits in lakeFS are immutable, they are easy to store on an immutable object store. Older commits are rarely accessed, while newer commits are accessed very frequently, a tiered storage approach can work very well - the object store is the source of truth, while local disk and even RAM can be used to cache the more frequently accessed ones. Since they are immutable - once cached, you only need to evict them when space is running out. There’s no complex invalidation that needs to happen. In terms of storage format, commits are be stored as SSTables, compatible with RocksDB. SSTables were chosen as a storage format for 3 major reasons: . | Extremely high read throughput on modern hardware: using commits representing a 200m object repository (modeled after the S3 inventory of one of our design partners), we were able to achieve close to 500k random GetObject calls / second. This provides a very high throughput/cost ratio, probably as high as can be achieved on public clouds. | Being a known storage format means it’s relatively easy to generate and consume. Storing it in the object store makes it accessible to data engineering tools for analysis and distributed computation, effectively reducing the silo effect of storing it in an operational database. | The SSTable format supports delta encoding for keys which makes them very space efficient for data lakes where many keys share the same common prefixes. | . Each lakeFS commit is represented as a set of contiguous, non-overlapping SSTables that make up the entire keyspace of a repository at that commit. ",
    "url": "/v0.70/understand/versioning-internals.html#overview",
    "relUrl": "/understand/versioning-internals.html#overview"
  },"344": {
    "doc": "Versioning Internals",
    "title": "SSTable File Format (“Graveler File”)",
    "content": "lakeFS metadata is encoded into a format called “Graveler” - a standardized way to encode content-addressable key value pairs. This is what a Graveler file looks like: . Each Key/Value pair (“ValueRecord”) is constructed of a key, identity, and value. A simple identity could be, for example, a sha256 hash of the value’s bytes. It could be any sequence of bytes that uniquely identifies the value. As far as the Graveler is concerned, two ValueRecords are considered identical if their key and identity fields are equal. A Graveler file itself is content-addressable, i.e., similarly to Git, the name of the file is its identity. File identity is calculated based on the identity of the ValueRecords the file contains: . valueRecordID = h(h(valueRecord.key) || h(valueRecord.Identity)) fileID = h(valueRecordID1 + … + valueRecordIDN) . ",
    "url": "/v0.70/understand/versioning-internals.html#sstable-file-format-graveler-file",
    "relUrl": "/understand/versioning-internals.html#sstable-file-format-graveler-file"
  },"345": {
    "doc": "Versioning Internals",
    "title": "Constructing a consistent view of the keyspace (i.e., a commit)",
    "content": "We have two additional requirements for the storage format: . | Be space and time efficient when creating a commit - assuming a commit changes a single object out of a billion, we don’t want to write a full snapshot of the entire repository. Ideally, we’ll be able to reuse some data files that haven’t changed to make the commit operations (in both space and time) proportional to the size of the difference as opposed to the total size of the repository. | Allow an efficient diff between commits which runs in time proportional to the size of their difference and not their absolute sizes. | . To support these requirements, we decided to essentially build a 2-layer Merkle tree composed of a set of leaf nodes (“Range”) addressed by their content address, and a “Meta Range”, which is a special range containing all ranges, thus representing an entire consistent view of the keyspace: . Assuming commit B is derived from commit A, and only changed files in range e-f, it can reuse all ranges except for SSTable #N (the one containing the modified range of keys), which will be recreated with a new hash representing the state as exists after applying commit B’s changes. This will, in turn, also create a new Metarange since its hash is now changed as well (as it is derived from the hash of all contained ranges). Assuming most commits usually change related objects (i.e., that are likely to share some common prefix), the reuse ratio could be very high. We tested this assumption using S3 inventory from 2 design partners - we partitioned the keyspace to an arbitrary number of simulated blocks and measured their change over time. We saw a daily change rate of about 5-20%. Given the size of the repositories, it’s safe to assume that a single day would translate into multiple commits. At a modest 20 commits per day, a commit is expected to reuse &gt;= 99% of the previous commit blocks, so acceptable in terms of write amplification generated on commit. On the object store, ranges are stored in the following hierarchy: . &lt;lakefs root&gt; _lakefs/ &lt;range hash1&gt; &lt;range hash2&gt; &lt;range hashN&gt; ... &lt;metarange hash1&gt; &lt;metarange hash2&gt; &lt;metarange hashN&gt; ... &lt;data object hash1&gt; &lt;data object hash2&gt; &lt;data object hashN&gt; ... Note: This relatively flat structure could be modified in the future. Looking at the diagram above, it imposes no real limitations on the depth of the tree. A tree could easily be made recursive by having Meta Ranges point to other Meta Ranges - and still provide all the same characteristics. For simplicity, we decided to start with a fixed 2-level hierarchy. ",
    "url": "/v0.70/understand/versioning-internals.html#constructing-a-consistent-view-of-the-keyspace-ie-a-commit",
    "relUrl": "/understand/versioning-internals.html#constructing-a-consistent-view-of-the-keyspace-ie-a-commit"
  },"346": {
    "doc": "Versioning Internals",
    "title": "Representing references and uncommitted metadata",
    "content": "lakeFS always stores the object data in the storage namespace in the user’s object store, committed and uncommitted data alike. However, the lakeFS object metadata might be stored in either the object store or PostgresSQL. Unlike committed metadata which is immutable, uncommitted (or “staged”) metadata experiences frequent random writes and is very mutable in nature. This is also true for “refs” - in particular, branches, which are simply pointers to an underlying commit, are modified frequently: on every commit or merge operation. Both these types of metadata are not only mutable, but also require strong consistency guarantees while also being fault tolerant. If we can’t access the current pointer of the main branch, a big portion of the system is essentially down. Luckily, this is also much smaller set of metadata compared to the committed metadata. References and uncommitted metadata are currently stored on PostgreSQL for its strong consistency and transactional guarantees. In the future, we plan on eliminating the need for an RDBMS by using a pluggable Key-Value store interface that would allow the use of many databases that meet its naive requirements. Non-production single server installations can leverage an embedded key-value store like RocksDB that will allow running with only a single container. ",
    "url": "/v0.70/understand/versioning-internals.html#representing-references-and-uncommitted-metadata",
    "relUrl": "/understand/versioning-internals.html#representing-references-and-uncommitted-metadata"
  },"347": {
    "doc": "S3 Virtual-host addressing (advanced)",
    "title": "Configuring lakeFS to use S3 Virtual-Host addressing",
    "content": " ",
    "url": "/v0.70/setup/virtual-host-addressing.html#configuring-lakefs-to-use-s3-virtual-host-addressing",
    "relUrl": "/setup/virtual-host-addressing.html#configuring-lakefs-to-use-s3-virtual-host-addressing"
  },"348": {
    "doc": "S3 Virtual-host addressing (advanced)",
    "title": "Understanding virtual-host addressing",
    "content": "Some systems require S3 endpoints (such as lakeFS’ S3 Gateway) to support virtual-host style addressing. lakeFS supports this, but requires some configuration in order to extract the bucket name (used as the lakeFS repository ID) from the host address. For example: . GET http://foo.example.com/some/location . In this case, there’s no way for lakeFS to determine whether this is a virtual-host request where the endpoint url is example.com, the bucket name is foo and the path is /some/location, or a path-based request where the endpoint is foo.example.com, the bucket name is some and the path is location. This requires an extra step: defining an explicit set of DNS record for lakeFS S3 gateway. ",
    "url": "/v0.70/setup/virtual-host-addressing.html#understanding-virtual-host-addressing",
    "relUrl": "/setup/virtual-host-addressing.html#understanding-virtual-host-addressing"
  },"349": {
    "doc": "S3 Virtual-host addressing (advanced)",
    "title": "Adding an explicit S3 domain name to the S3 Gateway configuration",
    "content": "The first step would be to tell the lakeFS installation which hostnames are used for the S3 Gateway. This should be a different DNS record from the one used for e.g. the UI or API. Typically, if the lakeFS installation is served under lakefs.example.com, a good choice would be s3.lakefs.example.com. This could be done using either an environment variable: . LAKEFS_GATEWAYS_S3_DOMAIN_NAME=\"s3.lakefs.example.com\" . Or by adding the gateways.s3.domain_name setting to the lakeFS config.yaml file: . --- database: connection_string: \"...\" ... # This section defines an explict S3 gateway address that supports virtual-host addressing gateways: s3: domain_name: s3.lakefs.example.com . For more information on how to configure lakeFS, check out the configuration reference . ",
    "url": "/v0.70/setup/virtual-host-addressing.html#adding-an-explicit-s3-domain-name-to-the-s3-gateway-configuration",
    "relUrl": "/setup/virtual-host-addressing.html#adding-an-explicit-s3-domain-name-to-the-s3-gateway-configuration"
  },"350": {
    "doc": "S3 Virtual-host addressing (advanced)",
    "title": "Setting up the appropriate DNS records",
    "content": "Once your lakeFS installation is configured with an explicit S3 gateway endpoint address, you need to define two DNS records and have them point at your lakeFS installation. This requires two CNAME records: . | s3.lakefs.example.com - CNAME to lakefs.example.com. This will be used as the S3 endpoint when configuring clients and serve as our bare domain. | *.s3.lakefs.example.com - Also a CNAME to lakefs.example.com. This will resolve virtual-host requests such as example-repo.s3.lakefs.example.com that lakeFS would now know how to parse. | . For more information on how to configure these, see the official documentation of your DNS provider. On AWS, This could also be done using ALIAS records for a load balancer. ",
    "url": "/v0.70/setup/virtual-host-addressing.html#setting-up-the-appropriate-dns-records",
    "relUrl": "/setup/virtual-host-addressing.html#setting-up-the-appropriate-dns-records"
  },"351": {
    "doc": "S3 Virtual-host addressing (advanced)",
    "title": "S3 Virtual-host addressing (advanced)",
    "content": " ",
    "url": "/v0.70/setup/virtual-host-addressing.html",
    "relUrl": "/setup/virtual-host-addressing.html"
  }
}
