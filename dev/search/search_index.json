{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Lake!","text":"lakeFS brings software engineering best practices and applies them to data <p>lakeFS provides version control over the data lake, and uses Git-like semantics to create and access those versions. If you know git, you'll be right at home with lakeFS.</p> <p>With lakeFS, you can apply concepts to your data lake such as branching to create an isolated version of the data, committing to create a reproducible point in time, and merging in order to incorporate your changes in one atomic action.</p> \ud83d\udcfd\ufe0f lakeFS in under 2 minutes <p> </p>"},{"location":"#how-do-i-get-started","title":"How Do I Get Started?","text":"<p>The hands-on quickstart guides you through some core features of lakeFS.</p> <p>These include branching, merging, and rolling back changes to data.</p> <p>Tip</p> <p>You can use the 30-day free trial of lakeFS Cloud if you want to try out lakeFS without installing anything.</p>"},{"location":"#key-lakefs-features","title":"Key lakeFS Features","text":"<ul> <li>It is format-agnostic and works with both structured and unstructured data</li> <li>It works with numerous data tools and platforms.</li> <li>Your data stays in place, with no need to copy existing data</li> <li>It eliminates the need for data duplication using zero-copy branching.</li> <li>It maintains high performance over data lakes of any size</li> <li>It includes configurable garbage collection capabilities</li> <li>It is proven in production and has an active community</li> </ul>"},{"location":"#how-does-lakefs-work-with-other-tools","title":"How Does lakeFS Work With Other Tools?","text":"<p>lakeFS is an open source project that supports managing data in AWS S3, Azure Blob Storage, Google Cloud Storage (GCS), S3-Compatible storage solutions and even locally mounted directories. It integrates seamlessly with popular data frameworks such as Spark, AWS SageMaker, Pandas, Tensorflow, Polars, HuggingFace Datasets and many more.</p> <p>Info</p> <p>For more details and a full list see the integrations pages.</p> <p>With lakeFS, you can use any of the tools and libraries you are used to work with to read and write data directly from a repository.</p> <p>Example: lakeFS with Pandas</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt;\n&gt;&gt;&gt; df = pd.read_csv('lakefs://example-repository/main-branch/path/to.csv')\n</code></pre> <p>Using this method, lakeFS acts as a metadata layer: it figures out which objects need to be fetched from the underlying storage for that version of the data and then lets the client read or write these files directly from the storage using pre-signed URLs. This allows lakeFS to be both very efficient but also highly secure:</p> <p> </p> <p>Additionally, lakeFS maintains compatibility with the S3 API to minimize adoption friction. You can use it as a drop-in replacement for S3 from the perspective of any tool interacting with a data lake.</p> <p>Example</p> <p>For example, take the common operation of reading unstructured data from the object store using Boto3 (Python):</p> <pre><code>&gt;&gt;&gt; import boto3\n&gt;&gt;&gt;\n&gt;&gt;&gt; s3 = boto3.resource('s3')\n&gt;&gt;&gt; obj = s3.Object('example-repository', 'main-branch/path/image.png')\n&gt;&gt;&gt; image_data = obj.get()['Body'].read()\n</code></pre> <p>You can use the same methods and syntax you are already using to read and write data when using a lakeFS repository. This simplifies the adoption of lakeFS - minimal changes are needed to get started, making further changes an incremental process.</p>"},{"location":"#lakefs-is-git-for-data","title":"lakeFS is Git for Data","text":"<p>Git became ubiquitous when it comes to code because it had best supported engineering best practices required by developers, in particular:</p> <ul> <li>Collaborate during development.</li> <li>Reproduce and troubleshoot issues with a given version of the code</li> <li>Develop and Test in isolation</li> <li>Revert code to a stable version in case of an error</li> <li>Continuously integrate and deploy new code (CI/CD)</li> </ul> <p>lakeFS provides these exact benefits, that data practitioners are missing today, and enables them a clear intuitive Git-like interface to easily manage data like they manage code. Through its versioning engine, lakeFS enables the following built-in operations familiar from Git:</p> <ul> <li> Branch a consistent copy of a repository, isolated from other branches and their changes. Initial creation of a branch is a metadata operation that does not duplicate objects.</li> <li> Commit an immutable checkpoint containing a complete snapshot of a repository.</li> <li> Merge performed between two branches \u2014 merges atomically update one branch with the changes from another.</li> <li> Revert returns a repo to the exact state of a previous commit.</li> <li> Tag a pointer to a single immutable commit with a readable, meaningful name.</li> <li> Hooks run validations and actions when actions occur (<code>pre-merge</code>, <code>post-create-branch</code>, etc).</li> </ul> <p>Info</p> <p>See the object model for an in-depth definition of these, and the CLI reference for the full list of commands.</p> <p>Incorporating these operations into your data and model development provides the same collaboration and organizational benefits you get when managing application code with source control.</p>"},{"location":"#how-can-lakefs-help-me","title":"How Can lakeFS Help Me?","text":"<p>lakeFS helps you maintain a tidy data lake in several ways, including:</p>"},{"location":"#reproducibility-what-did-my-data-look-like-at-a-point-in-time","title":"Reproducibility: What Did My Data Look Like at a Point In Time?","text":"<p>Being able to look at data as it was at a given point is particularly useful in at least two scenarios:</p> <ol> <li> <p>Reproducibility of ML experiments</p> <p>ML experimentation is iterative, requiring the ability to reproduce specific results. With lakeFS, you can version all aspects of an ML experiment, including the data. This enables:</p> <p>Data Lineage: Track the transformation of data from raw datasets to the final version used in experiments, ensuring transparency and traceability.</p> <p>Zero-Copy Branching: Minimize storage use by creating lightweight branches of your data, allowing for easy experimentation across different versions.</p> <p>Easy Integration: Seamlessly integrate with ML tools like MLFlow, linking experiments directly to the exact data versions used, making reproducibility straightforward.</p> <p>lakeFS enhances your ML workflow by ensuring that all versions of data are easily accessible, traceable, and reproducible.</p> </li> <li> <p>Troubleshooting production problems</p> <p>In some cases, a user might report inconsistencies, question the accuracy, or simply report data or inference results as incorrect.</p> <p>Since data continuously changes, it is challenging to understand its state at the time of the error.</p> <p>With lakeFS you can create a branch from a commit to debug an issue in isolation.</p> </li> </ol> <p>\ud83d\udc49\ud83c\udffb Read More</p>"},{"location":"#collaboration-during-development-and-training","title":"Collaboration during development and training","text":"<p>With lakeFS, each member of the team can create their own branch, isolated from other people's changes.</p> <p>This allows to iterate on changes to an algorithm or transformation, without stepping on eachother's toes. These branches are centralized - they could be share among users for collaboration, and can even be merged.</p> <p>With lakeFS you can even open pull requests, allowing you to easily share changes with other members and collaborate on them.</p>"},{"location":"#isolated-devtest-environments-with-zero-copy-branching","title":"Isolated Dev/Test Environments with zero-copy branching","text":"<p>lakeFS makes creating isolated dev/test environments for transformations, model development, parallel experiments, and ETL processes- achieved through the use of zero-copy branches. This enables you to test and validate code changes on production data without impacting it, as well as run analysis and experiments on production data in an isolated clone.</p> <p>\ud83d\udc49\ud83c\udffb Read More</p>"},{"location":"#rollback-of-data-changes-and-recovery-from-data-errors","title":"Rollback of Data Changes and Recovery from Data Errors","text":"<p>Human error or misconfigurations can lead to erroneous data making its way into production or critical data being accidentally deleted. Traditional backups are often inadequate for recovery in these situations, as they may be outdated and require time-consuming object-level sifting.</p> <p>With lakeFS, you can avoid these inefficiencies by committing snapshots of data at well-defined times. This allows for instant recovery: simply identify a good historical commit and restore or copy from it with a single operation.</p> <p>\ud83d\udc49\ud83c\udffb Read More</p>"},{"location":"#establishing-data-quality-guarantees-write-audit-publish","title":"Establishing data quality guarantees - Write-Audit-Publish","text":"<p>The best way to deal with mistakes is to avoid them. A data source that is ingested into the lake introducing low-quality data should be blocked before exposure if possible.</p> <p>With lakeFS, you can achieve this by tying data quality tests to commit and merge operations via lakeFS hooks.</p> <p>\ud83d\udc49\ud83c\udffb Read more</p>"},{"location":"#next-step","title":"Next Step","text":"<p>Try lakeFS on the cloud or run it locally</p>"},{"location":"cloud/","title":"lakeFS Cloud","text":""},{"location":"cloud/#what-is-lakefs-cloud","title":"What is lakeFS cloud?","text":"<p>lakeFS Cloud is a single tenant, fully-managed lakeFS solution, providing high availability, auto-scaling, support and production-grade features.</p>"},{"location":"cloud/#why-did-we-build-lakefs-cloud","title":"Why did we build lakeFS cloud?","text":"<p>We built lakeFS cloud for three main reasons:</p> <ol> <li>We wanted to provide organizations with the benefits of lakeFS without the need to manage it, saving them the investment in infrastructure and work related to installation, upgrades, uptime and scale .</li> <li>We wanted to provide lakeFS cloud users with security that meets their needs, with SSO, SCIM, and RBAC.</li> <li>We wanted to provide additional functionality that reduces friction and allows fast implementation of version controlled data/ML/AI pipelines throughout their data lifecycle.</li> </ol>"},{"location":"cloud/#what-is-the-value-of-using-lakefs-cloud-as-a-managed-service","title":"What is the value of using lakeFS Cloud as a managed service?","text":"<p>The main advantages of using lakeFS Cloud, the lakeFS managed service are:</p> <ol> <li>No installation required, no cloud costs and devops efforts on installing and maintaining a lakeFS installation.</li> <li>All lakeFS services are managed and run by us, including Managed Garbage Collection.</li> <li>lakeFS cloud is highly available and includes a commitment to an uptime SLA.</li> <li>lakeFS cloud auto scales according to your needs. See lakeFS cloud scalability model for more details.</li> <li>Upgrades are done transparently on your lakeFS cloud environment</li> <li>The lakeFS team is committed to supporting you with an SLA for both issues and product enhancements.</li> </ol>"},{"location":"cloud/#which-security-features-does-lakefs-cloud-provide","title":"Which security features does lakeFS Cloud provide?","text":"<ol> <li>lakeFS Cloud is SOC2 Type II compliant. Contact us to get the certification.</li> <li>lakeFS Cloud version controls your data, without accessing it, using pre-signed URLs! Read more here.</li> <li>When using lakeFS Cloud, you are provided with a rich Role-Based Access Control functionality that allows for fine-grained control by associating permissions with users and groups, granting them specific actions on specific resources. This ensures data security and compliance within an organization.</li> <li>To easily manage users and groups, lakeFS Cloud provides SSO integration (including support for SAML, OIDC, AD FS, Okta, and Azure AD), supporting existing credentials from a trusted provider, eliminating separate logins.</li> <li>lakeFS Cloud supports SCIM for automatically provisioning and deprovisioning users and group memberships to allow organizations to maintain a single source of truth for their user database.</li> <li>STS Auth offers temporary, secure logins using an Identity Provider, simplifying user access and enhancing security.</li> <li>Authentication with AWS IAM Roles allows authentication using AWS IAM roles instead of lakeFS credentials, removing the need to maintain static credentials for lakeFS Enterprise users running on AWS.</li> <li>Auditing provides a detailed action log of events happening within lakeFS, including who performed which action, on which resource - and when.</li> <li>Private-Link support to ensure network security by only allowing access to your lakeFS Cloud installation from your cloud accounts</li> </ol>"},{"location":"cloud/#what-additional-functionality-does-lakefs-cloud-provide","title":"What additional functionality does lakeFS Cloud provide?","text":"<p>Using lakeFS cloud is not just a secure and managed way of using lakeFS OSS; it is much more than that. With lakeFS Cloud you enjoy:</p> <ol> <li>lakeFS Mount allows users to virtually mount a remote lakeFS repository onto a local directory. Once mounted, users can access the data as if it resides on their local filesystem, using any tool, library, or framework that reads from a local filesystem.</li> <li>lakeFS Metadata Search - Allows a granular search API to filter and query versioned objects based on attached metadata. This is especially useful for machine learning environments to filter by labels and file attributes</li> <li>lakeFS for Databricks - Provides a turnkey solution for Databricks customers for analytics, machine learning and business intelligence use cases including full support for Delta Lake tables, Unity Catalog, MLFlow and the rest of the Databricks product suite.</li> <li>lakeFS for Snowflake - Provides full integration into the Snowflake ecosystem, including full support for Iceberg managed tables.</li> <li>lakeFS Cross Cloud - Allows central management of repositories that span across multiple cloud providers including Azure, AWS, GCP and on-prem environments.</li> <li>Transactional Mirroring - allows replicating lakeFS repositories into consistent read-only copies in remote locations.</li> </ol> Feature OSS Cloud Format-agnostic data version control \u2705 \u2705 Cloud-agnostic \u2705 \u2705 Zero Clone copy for isolated environment \u2705 \u2705 Atomic Data Promotion (via merges) \u2705 \u2705 Data stays in one place \u2705 \u2705 Configurable Garbage Collection \u2705 \u2705 Data CI/CD using lakeFS hooks \u2705 \u2705 Integrates with your data stack \u2705 \u2705 Role Based Access Control (RBAC) \u2705 Single Sign On (SSO) \u2705 SCIM Support \u2705 IAM Roles \u2705 Mount Capability \u2705 Audit Logs \u2705 Transactional Mirroring (cross-region) \u2705 Managed Service (auto updates, scaling) \u2705 Managed Garbage Collection \u2705 SOC2 Compliant \u2705 Support SLA \u2705"},{"location":"cloud/#how-lakefs-cloud-interacts-with-your-infrastructure","title":"How lakeFS Cloud interacts with your infrastructure","text":"<p>Treeverse hosts and manages a dedicated lakeFS instance that interfaces with data held in your object store, such as S3.</p> <pre><code>flowchart TD\n    U[Users] --&gt; LFC\n\n    subgraph Your Infrastructure\n    IAMM[lakeFS Managed GC IAM Role] --&gt; ObjectStore[Client's Object Store]\n    IAMA[lakeFS Application IAM Role] --&gt; ObjectStore\n    end\n\n    subgraph Treeverse's Infrastructure\n    MGC[Managed Garbage Collection] --&gt; EMR[Elastic Map Reduce]\n    EMR --&gt; IAMM\n    MGC --&gt; CP\n    CP[Control Plane]\n    LFC --&gt; CP\n\n        subgraph Client's Tenant\n        LFC[lakeFS Cloud] --&gt; DB[Refstore Database]\n        end\n\n    LFC --&gt; IAMC[lakeFS Connector IAM Role]\n    IAMC --&gt;|ExternalID| IAMA\n    end</code></pre>"},{"location":"cloud/#setting-up-lakefs-cloud","title":"Setting up lakeFS Cloud","text":""},{"location":"cloud/#aws-azure","title":"AWS / Azure","text":"<p>Please follow the self-service setup wizard on lakeFS Cloud.</p>"},{"location":"cloud/#gcp","title":"GCP","text":"<p>Please contact us for onboarding instructions.</p>"},{"location":"cloud/#lakefs-cloud-scalability-model","title":"lakeFS Cloud Scalability Model","text":"<p>By default, a lakeFS Cloud installation supports:</p> <ul> <li>1,500 read operations/second across all branches on all repositories within a region</li> <li>1,500 write operations per second across all branches on all repositories within a region</li> </ul> <p>This limit can be increased by contacting support. </p> <p>Each lakeFS branch can sustain up to a maximum of 1,000 write operations/second and 3,000 read operations per second.  This scales horizontally, so for example, with 10 concurrent branches, a repository could sustain 10k writes/second and 30k reads/second, assuming load is distributed evenly between them.</p> <p>Reading committed data (e.g. from a commit ID or tag) could be scaled up horizontally to any desired capacity, and defaults to ~5,000 reads/second.</p>"},{"location":"enterprise/","title":"lakeFS Enterprise","text":""},{"location":"enterprise/#what-is-lakefs-enterprise","title":"What is lakeFS Enterprise?","text":"<p>lakeFS Enterprise is a commercially-supported version of lakeFS, offering additional features and functionalities that meet the needs of organizations from a production-grade system.</p>"},{"location":"enterprise/#why-did-we-build-lakefs-enterprise","title":"Why did we build lakeFS Enterprise?","text":"<p>lakeFS Enterprise was built for organizations that require the support, security standards and features required of a production-grade system and are not using public clouds, hence they cannot use lakeFS Cloud.</p>"},{"location":"enterprise/#what-is-the-value-of-using-lakefs-enterprise","title":"What is the value of using lakeFS Enterprise?","text":"<ol> <li>Support: the lakeFS team is committed to supporting you under an SLA for both issues and product enhancements.</li> <li>Security: Full support for a suite of security features and additional lakeFS functionality.</li> </ol>"},{"location":"enterprise/#what-security-features-does-lakefs-enterprise-provide","title":"What security features does lakeFS Enterprise provide?","text":"<p>With lakeFS Enterprise you\u2019ll receive access to the security package containing the following features:</p> <ol> <li>A rich Role-Based Access Control permission system that allows for fine-grained control by associating permissions with users and groups, granting them specific actions on specific resources. This ensures data security and compliance within an organization.</li> <li>To easily manage users and groups, lakeFS Enterprise provides SSO integration (including support for SAML, OIDC, ADFS, Okta, and Azure AD), supporting existing credentials from a trusted provider, eliminating separate logins.</li> <li>lakeFS Enterprise supports SCIM for automatically provisioning and deprovisioning users and group memberships to allow organizations to maintain a single source of truth for their user database.</li> <li>STS Auth offers temporary, secure logins using an Identity Provider, simplifying user access and enhancing security.</li> <li>Authentication with AWS IAM Roles allows authentication using AWS IAM roles instead of lakeFS credentials, removing the need to maintain static credentials for lakeFS Enterprise users running on AWS.</li> <li>Auditing provides a detailed action log of events happening within lakeFS, including who performed which action, on which resource - and when.</li> </ol>"},{"location":"enterprise/#what-additional-functionality-does-lakefs-enterprise-provide","title":"What additional functionality does lakeFS Enterprise provide?","text":"<ol> <li>lakeFS Mount - allows users to virtually mount a remote lakeFS repository onto a local directory. Once mounted, users can access the data as if it resides on their local filesystem, using any tool, library, or framework that reads from a local filesystem.</li> <li>Transactional Mirroring - allows replicating lakeFS repositories into consistent read-only copies in remote locations.</li> <li>Multiple Storage Backends - allows managing data stored across multiple storage locations: on-prem, hybrid, or multi-cloud.        </li> </ol> Feature OSS Enterprise Format-agnostic data version control \u2705 \u2705 Cloud-agnostic \u2705 \u2705 Zero Clone copy for isolated environment \u2705 \u2705 Atomic Data Promotion (via merges) \u2705 \u2705 Data stays in one place \u2705 \u2705 Configurable Garbage Collection \u2705 \u2705 Data CI/CD using lakeFS hooks \u2705 \u2705 Integrates with your data stack \u2705 \u2705 Role Based Access Control (RBAC) \u2705 Single Sign On (SSO) \u2705 SCIM Support \u2705 IAM Roles \u2705 Mount Capability \u2705 Audit Logs \u2705 Transactional Mirroring (cross-region) \u2705 Support SLA \u2705 <p>Tip</p> <p>You can learn more about the lakeFS Enterprise architecture, or follow the examples in the Quickstart guide.</p>"},{"location":"enterprise/architecture/","title":"Architecture","text":"<p>The lakeFS Enterprise software consists of two components: 1. lakeFS Enterprise: a proprietary version of lakeFS which is based on the OSS version and includes advanced functionality. 2. A proprietary component called Fluffy which includes lakeFS' Enterprise features.</p> <p></p> <p>[1] Any user request to lakeFS via Browser or Programmatic access (SDK, HTTP API, lakectl).</p> <p>[2] Reverse Proxy (e.g. NGINX, Traefik, K8S Ingress): will handle user requests and proxy between lakeFS server and fluffy server based on the path prefix while maintaining the same host.</p> <p>[3] lakeFS server - the main lakeFS service.</p> <p>[4] fluffy server - service that is responsible for the Enterprise features., it is separated by ports for security reasons.</p> <ol> <li>SSO auth (i.e Browser login via Azure AD, Okta, Auth0), default port 8000.</li> <li>RBAC authorization, default port 9000.</li> </ol> <p>[5] The KV Store - Where metadata is stored used both by lakeFS and fluffy.</p> <p>[6] SSO IdP - Identity provider (e.g. Azure AD, Okta, JumpCloud). fluffy implements SAML and Oauth2 protocols.</p> <p>For more details and pricing, please contact sales.</p> <p>Info</p> <p>Setting up lakeFS enterprise with an SSO IdP (OIDC, SAML or LDAP) requires configuring access from the IdP too.</p>"},{"location":"enterprise/configuration/","title":"lakeFS Enterprise Configuration Reference","text":"<p>Working with lakeFS Enterprise involve configuring both lakeFS and Fluffy. You can find the extended configuration references for both components below.</p>"},{"location":"enterprise/configuration/#lakefs-configuration","title":"lakeFS Configuration","text":"<p>See the full lakeFS Server Configuration</p>"},{"location":"enterprise/configuration/#fluffy-server-configuration","title":"Fluffy Server Configuration","text":"<p>Configuring Fluffy using a YAML configuration file and/or environment variables. The configuration file's location can be set with the '--config' flag. If not specified, the first file found in the following order will be used:</p> <ol> <li>./config.yaml</li> <li><code>$HOME</code>/fluffy/config.yaml</li> <li>/etc/fluffy/config.yaml</li> <li><code>$HOME</code>/.fluffy.yaml</li> </ol> <p>Configuration items can be controlled by environment variables, see below.</p>"},{"location":"enterprise/configuration/#reference","title":"Reference","text":"<p>This reference uses <code>.</code> to denote the nesting of values.</p> <ul> <li><code>logging.format</code> <code>(one of [\"json\", \"text\"] : \"text\")</code> - Format to output log message in</li> <li><code>logging.level</code> <code>(one of [\"TRACE\", \"DEBUG\", \"INFO\", \"WARN\", \"ERROR\", \"NONE\"] : \"INFO\")</code> - Logging level to output</li> <li> <p><code>logging.audit_log_level</code> <code>(one of [\"TRACE\", \"DEBUG\", \"INFO\", \"WARN\", \"ERROR\", \"NONE\"] : \"DEBUG\")</code> - Audit logs level to output.</p> <p>Note</p> <p>In case you configure this field to be lower than the main logger level, you won't be able to get the audit logs</p> </li> <li> <p><code>logging.output</code> <code>(string : \"-\")</code> - A path or paths to write logs to. A <code>-</code> means the standard output, <code>=</code> means the standard error.</p> </li> <li><code>logging.file_max_size_mb</code> <code>(int : 100)</code> - Output file maximum size in megabytes.</li> <li><code>logging.files_keep</code> <code>(int : 0)</code> - Number of log files to keep, default is all.</li> <li><code>logging.trace_request_headers</code> <code>(bool : false)</code> - If set to <code>true</code> and logging level is set to <code>TRACE</code>, logs request headers.</li> <li><code>listen_address</code> <code>(string : \"0.0.0.0:8000\")</code> - A <code>&lt;host&gt;:&lt;port&gt;</code> structured string representing the address to listen on</li> <li><code>database</code> - Configuration section for the Fluffy key-value store database. The database must be shared between lakeFS &amp; Fluffy</li> <li><code>database.type</code> <code>(string [\"postgres\"|\"dynamodb\"|\"cosmosdb\"|\"local\"] : )</code> - Fluffy database type</li> <li><code>database.postgres</code> - Configuration section when using <code>database.type=\"postgres\"</code><ul> <li><code>database.postgres.connection_string</code> <code>(string : \"postgres://localhost:5432/postgres?sslmode=disable\")</code> - PostgreSQL connection string to use</li> <li><code>database.postgres.max_open_connections</code> <code>(int : 25)</code> - Maximum number of open connections to the database</li> <li><code>database.postgres.max_idle_connections</code> <code>(int : 25)</code> - Maximum number of connections in the idle connection pool</li> <li><code>database.postgres.connection_max_lifetime</code> <code>(duration : 5m)</code> - Sets the maximum amount of time a connection may be reused <code>(valid units: ns|us|ms|s|m|h)</code></li> </ul> </li> <li> <p><code>database.dynamodb</code> - Configuration section when using <code>database.type=\"dynamodb\"</code></p> <ul> <li><code>database.dynamodb.table_name</code> <code>(string : \"kvstore\")</code> - Table used to store the data</li> <li> <p><code>database.dynamodb.scan_limit</code> <code>(int : 1025)</code> - Maximal number of items per page during scan operation</p> <p>Note</p> <p>Refer to the following AWS documentation for further information</p> </li> <li> <p><code>database.dynamodb.endpoint</code> <code>(string : )</code> - Endpoint URL for database instance</p> </li> <li><code>database.dynamodb.aws_region</code> <code>(string : )</code> - AWS Region of database instance</li> <li><code>database.dynamodb.aws_profile</code> <code>(string : )</code> - AWS named profile to use</li> <li><code>database.dynamodb.aws_access_key_id</code> <code>(string : )</code> - AWS access key ID</li> <li> <p><code>database.dynamodb.aws_secret_access_key</code> <code>(string : )</code> - AWS secret access key</p> <p>Note</p> <p><code>endpoint</code> <code>aws_region</code> <code>aws_access_key_id</code> <code>aws_secret_access_key</code> are not required and used mainly for experimental purposes when working with DynamoDB with different AWS credentials.</p> </li> <li> <p><code>database.dynamodb.health_check_interval</code> <code>(duration : 0s)</code> - Interval to run health check for the DynamoDB instance (won't run if equal to 0).</p> </li> <li><code>database.cosmosdb</code> - Configuration section when using <code>database.type=\"cosmosdb\"</code></li> <li><code>database.cosmosdb.key</code> <code>(string : \"\")</code> - If specified, will     be used to authenticate to the CosmosDB account. Otherwise, Azure SDK     default authentication (with env vars) will be used.</li> <li><code>database.cosmosdb.endpoint</code> <code>(string : \"\")</code> - CosmosDB account endpoint, e.g. <code>https://&lt;account&gt;.documents.azure.com/</code>.</li> <li><code>database.cosmosdb.database</code> <code>(string : \"\")</code> - CosmosDB database name.</li> <li><code>database.cosmosdb.container</code> <code>(string : \"\")</code> - CosmosDB container name.</li> <li><code>database.cosmosdb.throughput</code> <code>(int32 : )</code> - CosmosDB container's RU/s. If not set - the default CosmosDB container throughput is used.</li> <li> <p><code>database.cosmosdb.autoscale</code> <code>(bool : false)</code> - If set, CosmosDB container throughput is autoscaled (See CosmosDB docs for minimum throughput requirement). Otherwise, uses \"Manual\" mode (Docs).</p> </li> <li> <p><code>database.local</code> - Configuration section when using <code>database.type=\"local\"</code></p> <ul> <li><code>database.local.path</code> <code>(string : \"~/fluffy/metadata\")</code> - Local path on the filesystem to store embedded KV metadata</li> <li><code>database.local.sync_writes</code> <code>(bool: true)</code> - Ensure each write is written to the disk. Disable to increase performance</li> <li><code>database.local.prefetch_size</code> <code>(int: 256)</code> - How many items to prefetch when iterating over embedded KV records</li> <li><code>database.local.enable_logging</code> <code>(bool: false)</code> - Enable trace logging for local driver</li> </ul> </li> <li><code>auth</code> - Configuration section for the Fluffy authentication services, like SAML or OIDC.</li> <li><code>auth.encrypt.secret_key</code> <code>(string : required)</code> - Same value given to lakeFS. A random (cryptographically safe) generated string that is used for encryption and HMAC signing</li> <li><code>auth.logout_redirect_url</code> <code>(string : \"/auth/login\")</code> - The address to redirect to after a successful logout, e.g. login.</li> <li><code>auth.post_login_redirect_url</code> <code>(string : '')</code> - Required when SAML is enabled. The address to redirect after a successful login. For most common configurations, setting to <code>/</code> will redirect to lakeFS homepage.</li> <li><code>auth.serve_listen_address</code> <code>(string : '')</code> - If set, an endpoint serving RBAC requests binds to this address.</li> <li><code>auth.serve_disable_authentication</code> <code>(bool : false)</code> - Unsafe. Disables authentication to the RBAC server.</li> <li><code>auth.ldap</code></li> <li><code>auth.ldap.server_endpoint</code> <code>(string : required)</code> - The LDAP server address, e.g. 'ldaps://ldap.company.com:636'</li> <li><code>auth.ldap.bind_dn</code> <code>(string : required)</code> - The bind string, e.g. 'uid=,ou=Users,o=,dc=,dc=com' <li><code>auth.ldap.bind_password</code> <code>(string : required)</code> - The password for the user to bind.</li> <li><code>auth.ldap.username_attribute</code> <code>(string : required)</code> - The user name attribute, e.g. 'uid'</li> <li><code>auth.ldap.user_base_dn</code> <code>(string : required)</code> - The search request base dn, e.g. 'ou=Users,o=,dc=,dc=com' <li><code>auth.ldap.user_filter</code> <code>(string : required)</code> - The search request user filter, e.g. '(objectClass=inetOrgPerson)'</li> <li><code>auth.ldap.connection_timeout_seconds</code> <code>(int : required)</code> - The timeout for a single connection</li> <li><code>auth.ldap.request_timeout_seconds</code> <code>(int : required)</code> - The timeout for a single request</li> <li><code>auth.saml</code> Configuration section for SAML</li> <li><code>auth.saml.enabled</code> <code>(bool : false)</code> - Enables SAML Authentication.</li> <li><code>auth.saml.sp_root_url</code> <code>(string : '')</code> - The base lakeFS-URL, e.g. 'https://' <li><code>auth.saml.sp_x509_key_path</code> <code>(string : '')</code> - The path to the private key, e.g '/etc/saml_certs/rsa_saml_private.cert'</li> <li><code>auth.saml.sp_x509_cert_path</code> <code>(string : '')</code> - The path to the public key, '/etc/saml_certs/rsa_saml_public.pem'</li> <li><code>auth.saml.sp_sign_request</code> <code>(bool : 'false')</code> SPSignRequest some IdP require the SLO request to be signed</li> <li><code>auth.saml.sp_signature_method</code> <code>(string : '')</code> SPSignatureMethod optional valid signature values depending on the IdP configuration, e.g. 'http://www.w3.org/2001/04/xmldsig-more#rsa-sha256'</li> <li><code>auth.saml.idp_metadata_url</code> <code>(string : '')</code> - The URL for the metadata server, e.g. 'https:///federationmetadata/2007-06/federationmetadata.xml' <li><code>auth.saml.idp_skip_verify_tls_cert</code> <code>(bool : false)</code> - Insecure skip verification of the IdP TLS certificate, like when signed by a private CA</li> <li><code>auth.saml.idp_authn_name_id_format</code> <code>(string : 'urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified')</code> - The format used in the NameIDPolicy for authentication requests</li> <li><code>auth.saml.idp_request_timeout</code> <code>(duration : '10s')</code> The timeout for remote authentication requests.</li> <li><code>auth.saml.external_user_id_claim_name</code> <code>(string : '')</code> - The claim name to use as the user identifier with an IdP mostly for logout</li> <li><code>auth.oidc</code> Configuration section for OIDC</li> <li><code>auth.oidc.enabled</code> <code>(bool : false)</code> - Enables OIDC Authentication.</li> <li><code>auth.oidc.url</code> <code>(string : '')</code> - The OIDC provider url, e.g. 'https://oidc-provider-url.com/'</li> <li><code>auth.oidc.client_id</code> <code>(string : '')</code> - The application's ID.</li> <li><code>auth.oidc.client_secret</code> <code>(string : '')</code> - The application's secret.</li> <li><code>auth.oidc.callback_base_url</code> <code>(string : '')</code> - A default callback address of the Fluffy server.</li> <li> <p><code>auth.oidc.callback_base_urls</code> <code>(string[] : '[]')</code></p> <p>Note</p> <p>You may configure a list of URLs that the OIDC provider may redirect to. This allows lakeFS to be accessed from multiple hostnames while retaining federated auth capabilities. If the provider redirects to a URL not in this list, the login will fail. This property and callback_base_url are mutually exclusive.</p> </li> <li> <p><code>auth.oidc.authorize_endpoint_query_parameters</code> <code>(bool : map[string]string)</code> - key/value parameters that are passed to a provider's authorization endpoint.</p> </li> <li><code>auth.oidc.logout_endpoint_query_parameters</code> <code>(string[] : '[]')</code> - The query parameters that will be used to redirect the user to the OIDC provider after logout, e.g. '[returnTo, https:///oidc/login]' <li><code>auth.oidc.logout_client_id_query_parameter</code> <code>(string : '')</code> - The claim name that represents the client identifier in the OIDC provider</li> <li><code>auth.oidc.additional_scope_claims</code> <code>(string[] : '[]')</code> - Specifies optional requested permissions, other than <code>openid</code> and <code>profile</code> that are being used.</li> <li><code>auth.cache</code> Configuration section for RBAC service cache</li> <li><code>auth.cache.enabled</code> <code>(bool : true)</code> - Enables RBAC service cache</li> <li><code>auth.cache.size</code> <code>(int : 1024)</code> - Number of users, policies and credentials to cache.</li> <li><code>auth.cache.ttl</code> <code>(duration : 20s)</code> - Cache items time to live expiry.</li> <li><code>auth.cache.jitter</code> <code>(duration : 3s)</code> - Cache items time to live jitter.</li> <li><code>auth.external</code> - Configuration section for the external authentication methods</li> <li><code>auth.external.aws_auth</code> - Configuration section for authenticating to lakeFS using AWS presign get-caller-identity request: External Principals AWS Auth</li> <li><code>auth.external.aws_auth.enabled</code> <code>(bool : false)</code> - If true, external principals API will be enabled, e.g auth service and login api's.</li> <li><code>auth.external.aws_auth.get_caller_identity_max_age</code> <code>(duration : 15m)</code> - The maximum age in seconds for the GetCallerIdentity request to be valid, the max is 15 minutes enforced by AWS, smaller TTL can be set.</li> <li><code>auth.authentication_api.external_principals_enabled</code> <code>(bool : false)</code> - If true, external principals API will be enabled, e.g auth service and login api's.</li> <li><code>auth.external.aws_auth.valid_sts_hosts</code> <code>([]string)</code> - The default are all the valid AWS STS hosts (<code>sts.amazonaws.com</code>, <code>sts.us-east-2.amazonaws.com</code> etc).</li> <li><code>auth.external.aws_auth.required_headers</code> <code>(map[string]string : )</code> - Headers that must be present by the client when doing login request (e.g <code>X-LakeFS-Server-ID: &lt;lakefs.ingress.domain&gt;</code>).</li> <li><code>auth.external.aws_auth.optional_headers</code> <code>(map[string]string : )</code> - Optional headers that can be present by the client when doing login request.</li> <li><code>auth.external.aws_auth.http_client.timeout</code> <code>(duration : 10s)</code> - The timeout for the HTTP client used to communicate with AWS STS.</li> <li><code>auth.external.aws_auth.http_client.skip_verify</code> <code>(bool : false)</code> - Skip SSL verification with AWS STS.</li>"},{"location":"enterprise/configuration/#using-environment-variables","title":"Using Environment Variables","text":"<p>All the configuration variables can be set or overridden using environment variables. To set an environment variable, prepend <code>FLUFFY_</code> to its name, convert it to upper case, and replace <code>.</code> with <code>_</code>:</p> <p>For example, <code>logging.format</code> becomes <code>FLUFFY_LOGGING_FORMAT</code>, <code>auth.saml.enabled</code> becomes <code>FLUFFY_AUTH_SAML_ENABLED</code>, etc.</p> <p>To set a value for a <code>map[string]string</code> type field, use the syntax <code>key1=value1,key2=value2,...</code>.</p>"},{"location":"enterprise/troubleshooting/","title":"Troubleshooting lakeFS Enterprise","text":"<p>A lakeFS Enterprise deployment has multiple moving parts that must all be deployed and configured correctly. This is especially true during initial setup. To help troubleshoot issues, both lakeFS and fluffy include the <code>flare</code> command.</p>"},{"location":"enterprise/troubleshooting/#the-flare-command","title":"The <code>flare</code> command","text":""},{"location":"enterprise/troubleshooting/#synopsis","title":"Synopsis","text":"<p>Both <code>lakefs</code> and <code>fluffy</code> include the flare command</p> <pre><code>lakefs flare [flags]\nfluffy flare [flags]\n</code></pre>"},{"location":"enterprise/troubleshooting/#flags","title":"Flags","text":"<pre><code>--env-var-filename      the name of the file environment variables will be written to (default: lakefs-env.txt)\n--include-env-vars      should environment variables be collected by flare (default: true)\n-o, --output                Output path relative to the current path\n-p, --package               Package generated artifacts into a .zip file (default: false)\n--stdout                Output to stdout instead of files (default: false)\n--zip-filename          The name of the zip file created when the -p option is used (default: lakefs-flare.zip)\n</code></pre>"},{"location":"enterprise/troubleshooting/#example-usage","title":"Example Usage","text":"<pre><code># This will run flare with output set to stdout, which is redirected into a file\n$ ./lakefs flare --stdout &gt; lakefs.flare\n# The same works for fluffy\n$ ./fluffy flare --stdout &gt; fluffy.flare\n</code></pre>"},{"location":"enterprise/troubleshooting/#what-information-does-the-flare-command-collect","title":"What Information Does the <code>flare</code> Command Collect?","text":""},{"location":"enterprise/troubleshooting/#configuration","title":"Configuration","text":"<p>Both lakeFS and fluffy allow configuration to be supplied in multiple ways: configuration file, environment variables, <code>.env</code> files, and command flags. The <code>flare</code> command collects the fully resolved final configuration used by the lakeFS/fluffy process.</p>"},{"location":"enterprise/troubleshooting/#environment-variables","title":"Environment Variables","text":"<p>When troubleshooting, it's important to get a view of the environment in which lakeFS/fluffy are running. This is especially true for container-based deployment environments, like Kubernetes, where env vars are used extensively. The <code>flare</code> command collects environment variables with the following prefixes:</p> <ul> <li><code>LAKEFS_</code></li> <li><code>FLUFFY_</code></li> <li><code>HTTP_</code></li> <li><code>HOSTNAME</code></li> </ul>"},{"location":"enterprise/troubleshooting/#sanitization-and-secret-redaction","title":"Sanitization and Secret Redaction","text":"<p>Both configuration and env var include sensitive secrets. The <code>flare</code> command has a multi-step process for redacting secrets from what it collects. The <code>flare</code> command is able to detect the following types of secrets:</p> <ul> <li>AWS static credentials</li> <li>Azure storage keys</li> <li>Basic HTTP auth username/password</li> <li>JWTs</li> <li>Bearer auth headers</li> <li>GitHub tokens</li> <li>Certificate private keys</li> </ul> <p>Aside from the specific secret type listed above, <code>flare</code> also has the ability to detect and redact generic high-entropy strings, which are likely to be secrets.</p> <p>Redacted secrets are replaced by a <code>SHA512</code> hash of the value. This allows comparing them (e.g., between lakeFS and fluffy) without exposing the actual values.</p>"},{"location":"enterprise/troubleshooting/#usage-collect-and-send-flare","title":"Usage - Collect and Send Flare","text":"<p>The following script is intended to be run locally and assumes that lakeFS and fluffy are deployed to a Kubernetes cluster, since this is the recommended setup. Running this script requires that <code>kubectl</code> be installed on the machine it is being run from and that <code>kubectl</code> is configured with the correct context and credentials to access the cluster. Aside from running the <code>flare</code> command on both lakeFS and fluffy, this script also fetches the logs from all running pods of lakeFS and fluffy.</p>"},{"location":"enterprise/troubleshooting/#step-1-set-script-variables","title":"Step 1 - Set Script Variables","text":"<p>At the top of the script you'll find the <code>Variables</code> block. It is important to change these values according to how lakeFS is deployed in your cluster.  </p> <p><code>NAMESPACE</code> - The K8s namespace where lakeFS and fluffy are deployed <code>LAKEFS_DEPLOYMENT</code> - The name of the lakeFS K8s deployment <code>FLUFFY_DEPLOYMENT</code> - The name of the fluffy K8s deployment <code>LAKEFS_LOGS_OUTPUT_FILE</code> - The name of the local file where lakeFS logs will be saved <code>FLUFFY_LOGS_OUTPUT_FILE</code> - The name of the local file where fluffy logs will be saved <code>LAKEFS_FLARE_FILE</code> - The name of the local file where the lakeFS <code>flare</code> result will be saved <code>FLUFFY_FLARE_FILE</code> - The name of the local file where the fluffy <code>flare</code> result will be saved  </p>"},{"location":"enterprise/troubleshooting/#step-2-execute-the-script","title":"Step 2 - Execute the Script","text":"<pre><code>#!/bin/bash\n\nRED='\\033[0;31m'\nNC='\\033[0m'\n\n# Variables\nNAMESPACE=lakefs-prod\nLAKEFS_DEPLOYMENT=lakefs-server\nFLUFFY_DEPLOYMENT=lakefs-fluffy\nLAKEFS_LOGS_OUTPUT_FILE=lakefs.log\nFLUFFY_LOGS_OUTPUT_FILE=fluffy.log\nLAKEFS_FLARE_FILE=lakefs.flare\nFLUFFY_FLARE_FILE=fluffy.flare\n\n# Find kubectl\nKUBECTLCMD=$(which kubectl)\nif [ -z \"$KUBECTLCMD\" ]\nthen\n    echo -e \"${RED}Couldn't find kubectl in path${NC}\"\n    exit 1\nfi\n\n$KUBECTLCMD get pods -o name -n $NAMESPACE | grep pod/$LAKEFS_DEPLOYMENT | xargs -I {} $KUBECTLCMD logs -n $NAMESPACE --all-containers=true --prefix --ignore-errors --timestamps {} &gt; $LAKEFS_LOGS_OUTPUT_FILE\n$KUBECTLCMD get pods -o name -n $NAMESPACE | grep pod/$FLUFFY_DEPLOYMENT | xargs -I {} $KUBECTLCMD logs -n $NAMESPACE --all-containers=true --prefix --ignore-errors --timestamps {} &gt; $FLUFFY_LOGS_OUTPUT_FILE\n\n$KUBECTLCMD exec deployment/$LAKEFS_DEPLOYMENT -- ./lakefs flare --stdout &gt; $LAKEFS_FLARE_FILE\n$KUBECTLCMD exec deployment/$FLUFFY_DEPLOYMENT -- ./lakefs flare --stdout &gt; $FLUFFY_FLARE_FILE\n</code></pre>"},{"location":"enterprise/troubleshooting/#step-3-inspect-the-output-files","title":"Step 3 - Inspect the Output Files","text":"<p>After executing the script you should have four files: lakeFS/fluffy logs and lakeFS/fluffy flare output. Before sharing these files, please review them to make sure all secrets were correctly redacted and that all the collected information is shareable.</p>"},{"location":"enterprise/troubleshooting/#step-4-zip-output-files-and-attach-to-support-ticket","title":"Step 4 - Zip Output Files and Attach to Support Ticket","text":"<p>Once you're done inspecting the files, you can zip them into a single file and attach it to a Support Ticket in the support portal.</p>"},{"location":"enterprise/troubleshooting/#new-ticket","title":"New Ticket","text":"<p>When filing a new ticket, you can attach the zip file using the file upload input at the bottom of the new ticket form.</p> <p></p>"},{"location":"enterprise/troubleshooting/#existing-ticket","title":"Existing Ticket","text":"<p>To add a file to an existing ticket, click the ticket subject in the support portal. On the ticket details attach a file as a new response. You can also add text to the response, if you'd like to add further details.</p> <p></p>"},{"location":"enterprise/upgrade/","title":"Upgrade","text":"<p>For upgrading from lakeFS enterprise to a newer version see lakefs migration.</p>"},{"location":"enterprise/getstarted/","title":"Get Started with lakeFS Enterprise","text":""},{"location":"enterprise/getstarted/#request-a-trial-license","title":"Request a Trial License","text":"<p>To start a 30-day free trial, please contact us. You will be granted with a token that allows downloading a Docker image that includes lakeFS Enterprise features.</p>"},{"location":"enterprise/getstarted/#install-lakefs-enterprise","title":"Install lakeFS Enterprise","text":"<p>lakeFS Enterprise offers two installation methods: * Quickstart: The fastest way to get started with lakeFS Enterprise. Intended for testing and evaluation. * Production deployment: Run lakeFS Enterprise on a Kubernetes cluster on AWS, Azure, GCP, or on-prem.</p>"},{"location":"enterprise/getstarted/install/","title":"Install","text":"<p>Info</p> <p>For production deployments of lakeFS Enterprise, follow this guide.</p>"},{"location":"enterprise/getstarted/install/#lakefs-enterprise-architecture","title":"lakeFS Enterprise Architecture","text":"<p>We recommend to review the lakeFS Enterprise architecture to understand the components you will be deploying.</p>"},{"location":"enterprise/getstarted/install/#deploy-lakefs-enterprise-on-kubernetes","title":"Deploy lakeFS Enterprise on Kubernetes","text":"<p>The guide is using the lakeFS Helm Chart to deploy a fully functional lakeFS Enterprise.</p> <p>The guide includes example configurations, follow the steps below and adjust the example configurations according to:</p> <ul> <li>The platform you run on: among the platform supported by lakeFS</li> <li>Type of KV store you use</li> <li>Your SSO IdP and protocol</li> </ul>"},{"location":"enterprise/getstarted/install/#prerequisites","title":"Prerequisites","text":"<ol> <li>You have a Kubernetes cluster running in one of the platforms supported by lakeFS.</li> <li>Helm is installed</li> <li>Access to download dockerhub/fluffy from Docker Hub. Contact us to gain access to lakeFS Enterprise features.</li> <li>A KV Database that will be shared by lakeFS and Fluffy. The available options are dependent in your deployment platform.</li> <li>A proxy server configured to route traffic between the lakeFS and Fluffy servers, see Reverse Proxy in lakeFS Enterprise architecture.</li> </ol>"},{"location":"enterprise/getstarted/install/#optional","title":"Optional","text":"<ol> <li>Access to configure your SSO IdP supported by lakeFS Enterprise.</li> </ol> <p>Info</p> <p>You can install lakeFS Enterprise without configuring SSO and still benefit from all other lakeFS Enterprise features.</p>"},{"location":"enterprise/getstarted/install/#add-the-lakefs-helm-chart","title":"Add the lakeFS Helm Chart","text":"<ul> <li>Add the lakeFS Helm repository with <code>helm repo add lakefs https://charts.lakefs.io</code></li> <li>The chart contains a values.yaml file you can customize to suit your needs as you follow this guide. Use <code>helm show values lakefs/lakefs</code> to see the default values.</li> <li>While customizing your values.yaml file, note to configure <code>fluffy.image.privateRegistry.secretToken</code> with the token Docker Hub token you received.</li> </ul>"},{"location":"enterprise/getstarted/install/#authentication-configuration","title":"Authentication Configuration","text":"<p>Authentication in lakeFS Enterprise is handled by the Fluffy SSO service which runs side-by-side to lakeFS. This section explains what Fluffy configurations are required for configuring the SSO service. See this configuration reference for additional Fluffy configurations.</p> <p>See SSO for lakeFS Enterprise for the supported identity providers and protocols.</p> <p>The examples below include example configuration for each of the supported SSO protocols. Note the IdP-specific details you'll need to replace with your IdP details.</p> OpenID ConnectSAML (With Azure AD)LDAP <p>The following <code>values</code> file will run lakeFS Enterprise with OIDC integration.</p> <p>Tip</p> <p>The full OIDC configurations explained [here][lakefs-sso-enterprise-spec-oidc].</p> <pre><code>lakefsConfig: |\n  logging:\n      level: \"INFO\"\n  blockstore:\n    type: s3\n  auth:\n    oidc:\n      # the claim that's provided by the OIDC provider (e.g Okta) that will be used as the username according to OIDC provider claims provided after successful authentication\n      friendly_name_claim_name: \"&lt;some-oidc-provider-claim-name&gt;\"\n      default_initial_groups: [\"Developers\", \"Admins\"]\n      # if true then the value of friendly_name_claim_name will be refreshed during each login to maintain the latest value\n      # and the the claim value (i.e user name) will be stored in the lakeFS database\n      persist_friendly_name: true\n    ui_config:\n      login_cookie_names:\n        - internal_auth_session\n        - oidc_auth_session\ningress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  hosts:\n    # the ingress that will be created for lakeFS\n    - host: &lt;lakefs.acme.com&gt;\n      paths:\n      - /\n\n##################################################\n########### lakeFS enterprise - FLUFFY ###########\n##################################################\n\nfluffy:\n  enabled: true\n  image:\n    repository: treeverse/fluffy\n    pullPolicy: IfNotPresent\n    privateRegistry:\n      enabled: true\n      secretToken: &lt;dockerhub-token-fluffy-image&gt;\n  fluffyConfig: |\n    logging:\n      format: \"json\"\n      level: \"INFO\"\n    auth:\n      logout_redirect_url: https://oidc-provider-url.com/logout/example\n      oidc:\n        enabled: true\n        url: https://oidc-provider-url.com/\n        client_id: &lt;oidc-client-id&gt;\n        callback_base_url: https://&lt;lakefs.acme.com&gt;\n        # the claim name that represents the client identifier in the OIDC provider (e.g Okta)\n        logout_client_id_query_parameter: client_id\n        # the query parameters that will be used to redirect the user to the OIDC provider (e.g Okta) after logout\n        logout_endpoint_query_parameters:\n          - returnTo\n          - https://&lt;lakefs.acme.com&gt;/oidc/login\n  secrets:\n    create: true\n  sso:\n    enabled: true\n    oidc:\n      enabled: true\n      # secret given by the OIDC provider (e.g auth0, Okta, etc) store in kind: Secret\n      client_secret: &lt;oidc-client-secret&gt;\n  rbac:\n    enabled: true\n\nuseDevPostgres: true\n</code></pre> <p>The following <code>values</code> file will run lakeFS Enterprise with SAML using Azure AD as the IdP.</p> <p>You can use this example configuration to configure Active Directory Federation Services (AD FS) with SAML.</p> <p>Tip</p> <p>The full SAML configurations explained [here][lakefs-sso-enterprise-spec-saml].</p> <p>The following <code>values</code> file will run lakeFS Enterprise with LDAP.</p> <p>Tip</p> <p>The full LDAP configurations explained [here][lakefs-sso-enterprise-spec-ldap].</p> <pre><code>lakefsConfig: |\n  logging:\n      level: \"INFO\"\n  blockstore:\n    type: local\n  auth:\n    remote_authenticator:\n      enabled: true\n      # RBAC group for first time users\n      default_user_group: \"Developers\"\n    ui_config:\n      login_cookie_names:\n        - internal_auth_session\n\ningress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  hosts:\n    - host: &lt;lakefs.acme.com&gt;\n      paths:\n      - /\n\nfluffy:\n  enabled: true\n  image:\n    privateRegistry:\n      enabled: true\n      secretToken: &lt;dockerhub-token-fluffy-image&gt;\n  fluffyConfig: |\n    logging:\n      level: \"INFO\"\n    auth:\n      post_login_redirect_url: /\n      ldap:\n        server_endpoint: 'ldaps://ldap.company.com:636'\n        bind_dn: uid=&lt;bind-user-name&gt;,ou=Users,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com\n        username_attribute: uid\n        user_base_dn: ou=Users,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com\n        user_filter: (objectClass=inetOrgPerson)\n        connection_timeout_seconds: 15\n        request_timeout_seconds: 17\n  secrets:\n    create: true\n  sso:\n    enabled: true\n    ldap:\n      enabled: true\n      bind_password: &lt;ldap bind password&gt;\n  rbac:\n    enabled: true\n\nuseDevPostgres: true\n</code></pre> <p>See additional examples on GitHub we provide for each authentication method (oidc, adfs, ldap, rbac, IAM etc).</p>"},{"location":"enterprise/getstarted/install/#azure-app-configuration","title":"Azure App Configuration","text":"<ol> <li>Create an Enterprise Application with SAML toolkit - see Azure quickstart</li> <li>Add users: App &gt; Users and groups: Attach users and roles from their existing AD users   list - only attached users will be able to login to lakeFS.</li> <li>Configure SAML: App &gt;  Single sign-on &gt; SAML:</li> <li>Entity ID: Add 2 ID\u2019s, lakefs-url + lakefs-url/saml/metadata (e.g. https://lakefs.acme.com and https://lakefs.acme.com/saml/metadata)</li> <li>Reply URL: lakefs-url/saml (e.g. https://lakefs.acme.com/saml)</li> <li>Sign on URL: lakefs-url/sso/login-saml (e.g. https://lakefs.acme.com/sso/login-saml)</li> <li>Relay State (Optional, controls where to redirect after login): /</li> </ol>"},{"location":"enterprise/getstarted/install/#saml-configuration","title":"SAML Configuration","text":"<ol> <li>Configure SAML application in your IdP (i.e Azure AD) and replace the required parameters into the <code>values.yaml</code> below.</li> <li>To generate certificates keypair use: `openssl req -x509 -newkey rsa:2048 -keyout myservice.key -out myservice.cert -days 365 -nodes -subj \"/CN=lakefs.acme.com\" -</li> </ol> <pre><code>secrets:\n  authEncryptSecretKey: \"some random secret string\"\n\nlakefsConfig: |\n  logging:\n      level: \"DEBUG\"\n  blockstore:\n    type: local\n  auth:\n    cookie_auth_verification:\n      # claim name to use for friendly name in lakeFS UI\n      friendly_name_claim_name: displayName\n      external_user_id_claim_name: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name\n      default_initial_groups:\n        - \"Developers\"\n    encrypt:\n      secret_key: shared-secrey-key\n    ui_config:\n      login_cookie_names:\n        - internal_auth_session\n        - saml_auth_session\ningress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  annotations: {}\n  hosts:\n    - host: &lt;lakefs.acme.com&gt;\n      paths:\n      - /\n\nfluffy:\n  enabled: true\n  image:\n    repository: treeverse/fluffy\n    pullPolicy: IfNotPresent\n    privateRegistry:\n      enabled: true\n      secretToken: &lt;dockerhub-token-fluffy-image&gt;\n  fluffyConfig: |\n    logging:\n      format: \"json\"\n      level: \"DEBUG\"\n    auth:\n      # redirect after logout\n      logout_redirect_url: https://&lt;lakefs.acme.com&gt;\n      saml:\n        sp_sign_request: false\n        sp_signature_method: \"http://www.w3.org/2001/04/xmldsig-more#rsa-sha256\"\n        idp_metadata_url: https://login.microsoftonline.com/&lt;...&gt;/federationmetadata/2007-06/federationmetadata.xml?appid=&lt;app-id&gt;\n        # the default id format urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified\n        # idp_authn_name_id_format: \"urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified\"\n        external_user_id_claim_name: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name\n        idp_skip_verify_tls_cert: true\n  secrets:\n    create: true\n  sso:\n    enabled: true\n    saml:\n      enabled: true\n      createSecret: true\n      lakeFSServiceProviderIngress: https://&lt;lakefs.acme.com&gt;\n      certificate:\n        # certificate and private key for the SAML service provider to sign outgoing SAML requests\n        saml_rsa_public_cert: |\n          -----BEGIN CERTIFICATE-----\n          ...\n          -----END CERTIFICATE-----\n        saml_rsa_private_key: |\n          -----BEGIN PRIVATE KEY-----\n          ...\n          -----END PRIVATE KEY-----\n  rbac:\n    enabled: true\n</code></pre>"},{"location":"enterprise/getstarted/install/#database-configuration","title":"Database Configuration","text":"<p>In this section, you will learn how to configure lakeFS Enterprise to work with the KV Database you created (see prerequisites.</p> <p>Notes:</p> <ul> <li>By default, the lakeFS Helm chart comes with <code>useDevPostgres: true</code>, you should change it to <code>useDevPostgres: false</code> for Fluffy to work with your KV Database and be suitable for production needs.</li> <li>The KV database is shared between lakeFS and Fluffy, and therefore both services must use the same configuration.</li> <li>See fluffy and lakeFS <code>database</code> configuration.</li> </ul> <p>The database configuration structure between lakeFS and fluffy can be set directly via <code>fluffyConfig</code>, via K8S Secret Kind, and <code>lakefsConfig</code> or via environment variables.</p> Postgres via environment variablesVia fluffyConfigPostgres via shared Secret kind <p>This example uses Postgres as KV Database. lakeFS is configured via <code>lakefsConfig</code> and Fluffy via environment with the same database configuration.</p> <pre><code>useDevPostgres: false\nlakefsConfig: |\n  database:\n    type: postgres\n    postgres:\n      connection_string: &lt;postgres connection string&gt;\n\nfluffy:\n  extraEnvVars:\n    - name: FLUFFY_DATABASE_TYPE\n      value: postgres\n    - name: FLUFFY_DATABASE_POSTGRES_CONNECTION_STRING\n      value: '&lt;postgres connection string&gt;'\n</code></pre> <p>This example uses DynamoDB as KV Database.</p> <pre><code># disable dev postgres\nuseDevPostgres: false\n\nlakefsConfig: |\ndatabase:\n    type: dynamodb\n    dynamodb:\n    table_name: &lt;table&gt;\n    aws_profile: &lt;profile&gt;\nfluffyConfig: |\n    database:\n    type: dynamodb\n    dynamodb:\n        table_name: &lt;table&gt;\n        aws_profile: &lt;profile&gt;\n        aws_region: &lt;region&gt;\n</code></pre> <p>This example uses Postgres as KV Database. The chart will create a <code>kind: Secret</code> holding the database connection string, and the lakeFS and Fluffy will use it.</p> <pre><code>useDevPostgres: false\nsecrets:\nauthEncryptSecretKey: shared-key-hello\ndatabaseConnectionString: &lt;postgres connection string&gt;\n\nlakefsConfig: |\ndatabase:\n    type: postgres\nfluffyConfig: |\ndatabase:\n    type: postgres\n</code></pre>"},{"location":"enterprise/getstarted/install/#install-the-lakefs-helm-chart","title":"Install the lakeFS Helm Chart","text":"<p>After populating your values.yaml file with the relevant configuration, in the desired K8S namespace run <code>helm install lakefs lakefs/lakefs -f values.yaml</code></p>"},{"location":"enterprise/getstarted/install/#access-the-lakefs-ui","title":"Access the lakeFS UI","text":"<p>In your browser go to the to the Ingress host to access lakeFS UI.</p>"},{"location":"enterprise/getstarted/install/#log-collection","title":"Log Collection","text":"<p>The recommended practice for collecting logs would be sending them to the container std (default configuration) and letting an external service to collect them to a sink. An example for logs collector would be fluentbit that can collect container logs, format them and ship them to a target like S3.</p> <p>There are 2 kinds of logs, regular logs like an API error or some event description used for debugging and audit_logs that are describing a user action (i.e create branch). The distinction between regular logs and audit_logs is in the boolean field log_audit. lakeFS and fluffy share the same configuration structure under logging.* section in the config.</p>"},{"location":"enterprise/getstarted/install/#advanced-deployment-configurations","title":"Advanced Deployment Configurations","text":"<p>The following example demonstrates a scenario where you need to configure an HTTP proxy for lakeFS and Fluffy, TLS certificates for the Ingress and extending the K8S manifests without forking the Helm chart.</p> <pre><code>ingress:\n  enabled: true\n  ingressClassName: &lt;class-name&gt;\n  # configure TLS certificate for the Ingress\n  tls:\n    - hosts:\n      - lakefs.acme.com\n      secretName: somesecret\n  hosts:\n    - host: lakefs.acme.com\n      paths:\n       - /\n\n# configure proxy for lakeFS\nextraEnvVars:\n  - name: HTTP_PROXY\n    value: 'http://my.company.proxy:8081'\n  - name: HTTPS_PROXY\n    value: 'http://my.company.proxy:8081'\n\nfluffy:\n  # configure proxy for fluffy\n  extraEnvVars:\n    - name: HTTP_PROXY\n      value: 'http://my.company.proxy:8081'\n    - name: HTTPS_PROXY\n      value: 'http://my.company.proxy:8081'\n\n# advanced: extra manifests to extend the K8S resources\nextraManifests:\n  - apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: '{% raw %}{{ .Values.fluffy.name }}{% endraw %}-extra-config'\n    data:\n      config.yaml: my-data\n</code></pre>"},{"location":"enterprise/getstarted/migrate-from-oss/","title":"Migrate From lakeFS Open-Source to lakeFS Enterprise","text":"<p>To migrate from lakeFS Open Source to lakeFS Enterprise, follow the steps below:</p> <ol> <li>Make sure you have the Fluffy Docker token, if not contact us to gain access to Fluffy. You will be granted with a token that enables downloading dockerhub/fluffy from Docker Hub.</li> <li>Update lakeFS docker image to enterprise. Use <code>treeverse/lakefs-enterprise</code> instead of <code>treeverse/lakefs</code>. The image can be pulled using the same token as Fluffy.</li> <li>Sanity Test (Optional): Install a new test lakeFS Enterprise before moving your current production setup. Test the setup &gt; login &gt; Create repository etc. Once everything seems to work, delete and cleanup the test setup and we will move to the migration process.</li> <li>Follow lakeFS Enterprise installation guide<ol> <li>Make sure that you meet the prerequisites</li> <li>Update your existing <code>values.yaml</code> file for your deployment</li> </ol> </li> <li>DB Migration: we are going to use the same DB for both lakeFS and Fluffy, so we need to migrate the DB schema.</li> <li>Make sure to SSH / exec into the lakeFS server (old pre-upgrade version), the point is to use the same lakefs configuration file when running a migration.<ol> <li>If upgrading <code>lakefs</code> version do this or skip to the next step: Install the new lakeFS binary, if not use the existing one (the one you are running).</li> <li>Run the command: <code>LAKEFS_AUTH_UI_CONFIG_RBAC=internal lakefs migrate up</code> (use the new binary if upgrading lakeFS version).</li> <li>You should expect to see a log message saying Migration completed successfully.</li> <li>During this short db migration process please make sure not to make any policy / RBAC related changes.</li> </ol> </li> <li>Once the migration completed - Upgrade your helm release with the modified <code>values.yaml</code> and the new version and run <code>helm ugprade</code>.</li> <li> <p>Login to the new lakeFS pod: Execute the following command, make sure you have proper credentials, or discard to get new ones:</p> <pre><code>lakefs setup --user-name &lt;admin&gt; --access-key-id &lt;key&gt; --secret-access-key &lt;secret&gt; --no-check\n</code></pre> </li> </ol> <p>Warning</p> <p>Please note that the newly set up lakeFS instance remains inaccessible to users until full setup completion, due to the absence of established credentials within the system.</p>"},{"location":"enterprise/getstarted/quickstart/","title":"Quickstart","text":"<p>Follow these quickstarts to try out lakeFS Enterprise.</p> <p>Warning</p> <p>lakeFS Enterprise Quickstarts are not suitable for production use-cases.  See the installation guide to set up a production-grade lakeFS Enterprise installation</p>"},{"location":"enterprise/getstarted/quickstart/#lakefs-enterprise-sample","title":"lakeFS Enterprise Sample","text":"<p>The lakeFS Enterprise Sample is the quickest way to experience the value of lakeFS Enterprise features in a containerized environment. This Docker-based setup is ideal if you want to easily interact with lakeFS without the hassle of integration and experiment with lakeFS without writing code.</p> <p>By running the lakeFS Enterprise Sample, you will be getting a ready-to-use environment including the following containers:</p> <ul> <li>lakeFS</li> <li>Fluffy (includes lakeFS Enterprise features)</li> <li>Postgres: used by lakeFS and Fluffy as a shared KV store</li> <li>MinIO container: used as the storage connected to lakeFS</li> <li>Jupyter notebooks setup: Pre-populated with notebooks that demonstrate lakeFS Enterprise' capabilities</li> <li>Apache Spark: this is useful for interacting with data you'll manage with lakeFS</li> </ul> <p>Checkout the RBAC demo notebook to see lakeFS Enterprise Role-Based Access Control capabilities in action.</p>"},{"location":"enterprise/getstarted/quickstart/#docker-quickstart","title":"Docker Quickstart","text":""},{"location":"enterprise/getstarted/quickstart/#prerequisites","title":"Prerequisites","text":"<ol> <li>You have installed Docker Compose version <code>2.23.1</code> or higher on your machine.</li> <li>Access to download dockerhub/fluffy from Docker Hub. Contact us to gain access to Fluffy.</li> <li>With the token you've been granted, login locally to Docker Hub with <code>docker login -u externallakefs -p &lt;TOKEN&gt;</code>.</li> </ol> <p> The quickstart docker-compose files below create a lakeFS server that's connected to a local blockstore and spin up the following containers:</p> <ul> <li>lakeFS</li> <li>Fluffy (includes lakeFS Enterprise features)</li> <li>Postgres: used by lakeFS and Fluffy as a shared KV store</li> </ul> <p>You can choose from the following options:</p> <ol> <li>Recommended: A fully functional lakeFS Enterprise setup without SSO support</li> <li> <p>Advanced: A fully functional lakeFS Enterprise setup including SSO support with OIDC integration configured</p> <p>Info</p> <p>If you can postpone the evaluation of the SSO integration, we suggest starting without it to speed up overall testing. The SSO integration requires additional configurations and is best addressed later.</p> </li> </ol> Recommended (SSO Disabled)Advanced (SSO Enabled) <ol> <li>Create a <code>docker-compose.yaml</code> file with the following content</li> <li>Run <code>docker compose up</code> in the same directory as the <code>docker-compose.yaml</code> file.</li> <li>In your browser, go to http://localhost:8080 to access lakeFS UI.</li> </ol> <pre><code>version: \"3\"\nservices:\n  lakefs:\n    image: \"treeverse/lakefs-enterprise:latest\"\n    command: \"RUN\"\n    ports:\n      - \"8080:8080\"\n    depends_on:\n      - \"postgres\"\n    environment:\n      - LAKEFS_LISTEN_ADDRESS=0.0.0.0:8080\n      - LAKEFS_LOGGING_LEVEL=DEBUG\n      - LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"random_secret\"\n      - LAKEFS_AUTH_API_ENDPOINT=http://fluffy:9000/api/v1\n      - LAKEFS_AUTH_API_SUPPORTS_INVITES=true\n      - LAKEFS_AUTH_UI_CONFIG_RBAC=internal\n      - LAKEFS_AUTH_AUTHENTICATION_API_ENDPOINT=http://localhost:8000/api/v1\n      - LAKEFS_AUTH_AUTHENTICATION_API_EXTERNAL_PRINCIPALS_ENABLED=true\n      - LAKEFS_DATABASE_TYPE=postgres\n      - LAKEFS_DATABASE_POSTGRES_CONNECTION_STRING=postgres://lakefs:lakefs@postgres/postgres?sslmode=disable\n      - LAKEFS_BLOCKSTORE_TYPE=local\n      - LAKEFS_BLOCKSTORE_LOCAL_PATH=/home/lakefs\n      - LAKEFS_BLOCKSTORE_LOCAL_IMPORT_ENABLED=true\n    entrypoint: [\"/app/wait-for\", \"postgres:5432\", \"--\", \"/app/lakefs\", \"run\"]\n    configs:\n      - source: lakefs.yaml\n        target: /etc/lakefs/config.yaml\n  postgres:\n    image: \"postgres:11\"\n    ports:\n      - \"5433:5432\"\n    environment:\n      POSTGRES_USER: lakefs\n      POSTGRES_PASSWORD: lakefs\n\n  fluffy:\n    image: \"${FLUFFY_REPO:-treeverse}/fluffy:${TAG:-latest}\"\n    command: \"${COMMAND:-run}\"\n    ports:\n      - \"8000:8000\"\n      - \"9000:9000\"\n    depends_on:\n      - \"postgres\"\n    environment:\n      - FLUFFY_LOGGING_LEVEL=DEBUG\n      - FLUFFY_DATABASE_TYPE=postgres\n      - FLUFFY_DATABASE_POSTGRES_CONNECTION_STRING=postgres://lakefs:lakefs@postgres/postgres?sslmode=disable\n      - FLUFFY_AUTH_ENCRYPT_SECRET_KEY=\"random_secret\"\n      - FLUFFY_AUTH_SERVE_LISTEN_ADDRESS=0.0.0.0:9000\n      - FLUFFY_LISTEN_ADDRESS=0.0.0.0:8000\n      - FLUFFY_AUTH_SERVE_DISABLE_AUTHENTICATION=true\n      - FLUFFY_AUTH_POST_LOGIN_REDIRECT_URL=http://localhost:8080/\n    entrypoint: [ \"/app/wait-for\", \"postgres:5432\", \"--\", \"/app/fluffy\" ]\n\nconfigs:\n  lakefs.yaml:\n    content: |\n      auth:\n        ui_config:\n          login_cookie_names:\n            - internal_auth_session\n</code></pre> <p>This setup uses OIDC as the SSO authentication method thus requiring a valid OIDC configuration.</p> <ol> <li>Create a <code>docker-compose.yaml</code> with the content below.</li> <li>Create a <code>.env</code> file with the configurations below in the same directory as the <code>docker-compose.yaml</code>, docker compose will automatically use that.</li> <li>Run <code>docker compose up</code> in the same directory as the <code>docker-compose.yaml</code> file.</li> <li>Validate the OIDC configuration:</li> <li>In your browser, go to http://localhost:8080 to access lakeFS UI</li> <li>Complete the Setup process, and login with your Admin credentials</li> <li>Logout and try to login again, you will be redirected to the OIDC login page.</li> </ol> <p><code>.env</code></p> <pre><code>FLUFFY_AUTH_OIDC_CLIENT_ID=\nFLUFFY_AUTH_OIDC_CLIENT_SECRET=\n# The name of the query parameter that is used to pass the client ID to the logout endpoint of the SSO provider, i.e client_id\nFLUFFY_AUTH_OIDC_LOGOUT_CLIENT_ID_QUERY_PARAMETER=\nFLUFFY_AUTH_OIDC_URL=https://my-sso.com/\nFLUFFY_AUTH_LOGOUT_REDIRECT_URL=https://my-sso.com/logout\n# Optional: display a friendly name in the lakeFS UI by specifying which claim from the provider to show (i.e name, nickname, email etc)\nLAKEFS_AUTH_OIDC_FRIENDLY_NAME_CLAIM_NAME=\n</code></pre> <p><code>docker-compose.yaml</code></p> <pre><code>version: \"3\"\nservices:\n  lakefs:\n    image: \"treeverse/lakefs-enterprise:latest\"\n    command: \"RUN\"\n    ports:\n      - \"8080:8080\"\n    depends_on:\n      - \"postgres\"\n    environment:\n      - LAKEFS_LISTEN_ADDRESS=0.0.0.0:8080\n      - LAKEFS_LOGGING_LEVEL=DEBUG\n      - LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"random_secret\"\n      - LAKEFS_AUTH_API_ENDPOINT=http://fluffy:9000/api/v1\n      - LAKEFS_AUTH_API_SUPPORTS_INVITES=true\n      - LAKEFS_AUTH_UI_CONFIG_LOGIN_URL=http://localhost:8000/oidc/login\n      - LAKEFS_AUTH_UI_CONFIG_LOGOUT_URL=http://localhost:8000/oidc/logout\n      - LAKEFS_AUTH_UI_CONFIG_RBAC=internal\n      - LAKEFS_AUTH_AUTHENTICATION_API_ENDPOINT=http://localhost:8000/api/v1\n      - LAKEFS_AUTH_AUTHENTICATION_API_EXTERNAL_PRINCIPALS_ENABLED=true\n      - LAKEFS_DATABASE_TYPE=postgres\n      - LAKEFS_DATABASE_POSTGRES_CONNECTION_STRING=postgres://lakefs:lakefs@postgres/postgres?sslmode=disable\n      - LAKEFS_BLOCKSTORE_TYPE=local\n      - LAKEFS_BLOCKSTORE_LOCAL_PATH=/home/lakefs\n      - LAKEFS_BLOCKSTORE_LOCAL_IMPORT_ENABLED=true\n      - LAKEFS_AUTH_OIDC_FRIENDLY_NAME_CLAIM_NAME=${LAKEFS_AUTH_OIDC_FRIENDLY_NAME_CLAIM_NAME}\n    entrypoint: [\"/app/wait-for\", \"postgres:5432\", \"--\", \"/app/lakefs\", \"run\"]\n    configs:\n      - source: lakefs.yaml\n        target: /etc/lakefs/config.yaml\n  postgres:\n    image: \"postgres:11\"\n    ports:\n      - \"5433:5432\"\n    environment:\n      POSTGRES_USER: lakefs\n      POSTGRES_PASSWORD: lakefs\n\n  fluffy:\n    image: \"${FLUFFY_REPO:-treeverse}/fluffy:${TAG:-latest}\"\n    command: \"${COMMAND:-run}\"\n    ports:\n      - \"8000:8000\"\n      - \"9000:9000\"\n    depends_on:\n      - \"postgres\"\n    environment:\n      - FLUFFY_LOGGING_LEVEL=DEBUG\n      - FLUFFY_DATABASE_TYPE=postgres\n      - FLUFFY_DATABASE_POSTGRES_CONNECTION_STRING=postgres://lakefs:lakefs@postgres/postgres?sslmode=disable\n      - FLUFFY_AUTH_ENCRYPT_SECRET_KEY=\"random_secret\"\n      - FLUFFY_AUTH_SERVE_LISTEN_ADDRESS=0.0.0.0:9000\n      - FLUFFY_LISTEN_ADDRESS=0.0.0.0:8000\n      - FLUFFY_AUTH_SERVE_DISABLE_AUTHENTICATION=true\n      - FLUFFY_AUTH_LOGOUT_REDIRECT_URL=${FLUFFY_AUTH_LOGOUT_REDIRECT_URL}\n      - FLUFFY_AUTH_POST_LOGIN_REDIRECT_URL=http://localhost:8080/\n      - FLUFFY_AUTH_OIDC_ENABLED=true\n      - FLUFFY_AUTH_OIDC_URL=${FLUFFY_AUTH_OIDC_URL}\n      - FLUFFY_AUTH_OIDC_CLIENT_ID=${FLUFFY_AUTH_OIDC_CLIENT_ID}\n      - FLUFFY_AUTH_OIDC_CLIENT_SECRET=${FLUFFY_AUTH_OIDC_CLIENT_SECRET}\n      - FLUFFY_AUTH_OIDC_CALLBACK_BASE_URL=http://localhost:8000\n      - FLUFFY_AUTH_OIDC_LOGOUT_CLIENT_ID_QUERY_PARAMETER=${FLUFFY_AUTH_OIDC_LOGOUT_CLIENT_ID_QUERY_PARAMETER}\n    entrypoint: [ \"/app/wait-for\", \"postgres:5432\", \"--\", \"/app/fluffy\" ]\n    configs:\n      - source: fluffy.yaml\n        target: /etc/fluffy/config.yaml\n\n#This tweak is unfortunate but also necessary. logout_endpoint_query_parameters is a list\n#of strings which isn't parsed nicely as env vars.\nconfigs:\n  lakefs.yaml:\n    content: |\n      auth:\n        ui_config:\n          login_cookie_names:\n            - internal_auth_session\n            - oidc_auth_session\n        oidc:\n          # friendly_name_claim_name: \"name\"\n          default_initial_groups:\n            - Admins\n\n  fluffy.yaml:\n    content: |\n      auth:\n        oidc:\n          logout_endpoint_query_parameters:\n            - returnTo\n            - http://localhost:8080/oidc/login\n</code></pre>"},{"location":"enterprise/getstarted/quickstart/#kubernetes-helm-chart-quickstart","title":"Kubernetes Helm Chart Quickstart","text":"<p>In order to use lakeFS Enterprise and Fluffy, we provided out of the box setup, see lakeFS Helm chart configuration.</p> <p>The values below create a fully functional lakeFS Enterprise setup without SSO support. The created setup is connected to a local blockstore, and spins up the following pods:</p> <ul> <li>lakeFS</li> <li>Fluffy (includes lakeFS Enterprise features)</li> <li>Postgres: used by lakeFS and Fluffy as a shared KV store</li> </ul> <p>Info</p> <p>If you can postpone the evaluation of the SSO integration, we suggest starting without it to speed up overall testing. The SSO integration requires additional configurations and is best addressed later. To try lakeFS Enterprise SSO capability on a Kubernetes cluster, check out the production deployment guide.</p>"},{"location":"enterprise/getstarted/quickstart/#prerequisites_1","title":"Prerequisites","text":"<ol> <li>You have a Kubernetes cluster running in one of the platforms supported by lakeFS.</li> <li>Helm is installed</li> <li>Access to download dockerhub/fluffy from Docker Hub. Contact us to gain access to Fluffy.</li> </ol>"},{"location":"enterprise/getstarted/quickstart/#instructions","title":"Instructions","text":"<ol> <li>Add the lakeFS Helm repository with <code>helm repo add lakefs https://charts.lakefs.io</code></li> <li>Create a <code>values.yaml</code> file with the following content and make sure to replace <code>&lt;fluffy-docker-registry-token&gt;</code> with the token Docker Hub token you recieved, <code>&lt;lakefs.acme.com&gt;</code> and <code>&lt;ingress-class-name&gt;</code>.</li> <li>In the desired K8S namespace run <code>helm install lakefs lakefs/lakefs -f values.yaml</code></li> <li>In your browser go to the Ingress host to access lakeFS UI.</li> </ol> <pre><code>lakefsConfig: |\n  logging:\n      level: \"DEBUG\"\n  blockstore:\n    type: local\ningress:\n  enabled: true\n  ingressClassName: &lt;ingress-class-name&gt;\n  annotations: {}\n  hosts:\n    - host: &lt;lakefs.acme.com&gt;\n      paths:\n       - /\nfluffy:\n  enabled: true\n  image:\n    privateRegistry:\n      enabled: true\n      secretToken: &lt;fluffy-docker-registry-token&gt;\n  fluffyConfig: |\n    logging:\n      level: \"DEBUG\"\n  secrets:\n    create: true\n  sso:\n    enabled: false\n  rbac:\n    enabled: true\n\n# useDevPostgres is true by default and will override any other db configuration, set false for configuring your own db\nuseDevPostgres: true\n</code></pre>"},{"location":"howto/","title":"lakeFS - How To","text":""},{"location":"howto/#installation-and-upgrades","title":"Installation and upgrades","text":"<ul> <li> <p>Step-by-step instructions for deploying and configuring lakeFS on AWS, GCP, Azure, and on-premises.</p> </li> <li> <p>Details on how to upgrade lakeFS</p> </li> </ul>"},{"location":"howto/#getting-data-in-and-out-of-lakefs","title":"Getting data in and out of lakeFS","text":"<ul> <li>Import and Export Data from lakeFS</li> <li>Copy data to/from lakeFS</li> <li>Using external Data Catalogs with data stored on lakeFS</li> <li>Migrating away from lakeFS</li> <li>Working with lakeFS data locally</li> </ul>"},{"location":"howto/#actions-and-hooks-in-lakefs","title":"Actions and Hooks in lakeFS","text":"<ul> <li>Use Actions and Hooks as part of your workflow to validate data, enforce constraints, and do more when events occur.</li> </ul>"},{"location":"howto/#branch-protection","title":"Branch Protection","text":"<ul> <li>Branch Protection prevents commits directly to a branch. This is a good way to enforce good practice and make sure that changes to important branches are only done by a merge.</li> </ul>"},{"location":"howto/#pull-requests","title":"Pull Requests","text":"<ul> <li>Improve collaboration over data with Pull Requests.</li> </ul>"},{"location":"howto/#lakefs-sizing-guide","title":"lakeFS Sizing Guide","text":"<ul> <li> <p>This comprehensive guide details all you need to know to correctly size and test your lakeFS deployment for production use at scale, including:</p> </li> <li> <p>System Requirements</p> </li> <li>Scaling factors</li> <li>Benchmarks</li> <li>Important metrics</li> <li>Reference architectures</li> </ul>"},{"location":"howto/#garbage-collection","title":"Garbage Collection","text":"<ul> <li>lakeFS will keep all of your objects forever, unless you tell it otherwise. Use Garbage Collection (GC) to remove objects from the underlying storage.     If you want GC to happen automatically then you can use Managed Garbage Collection which is available as part of lakeFS Cloud.</li> </ul>"},{"location":"howto/#private-link","title":"Private Link","text":"<ul> <li>Private Link enables lakeFS Cloud to interact with your infrastructure using private networking.</li> </ul>"},{"location":"howto/#unity-delta-sharing","title":"Unity Delta Sharing","text":"<ul> <li>lakeFS Unity Delta Sharing provides a read-only experience from Unity Catalog for lakeFS customers.</li> </ul>"},{"location":"howto/backup-and-restore/","title":"Backup and Restore Repository","text":"<p>This section explains how to backup and restore lakeFS repository for different use-cases:</p> <ol> <li>Disaster Recovery: you want to backup the repository regularly so you can restore it in case of any disaster.  You'd also need to make sure to backup the repository's storage namespace to another, preferably geographically separate location.</li> <li>Migrate Repository: you want to migrate a repository from one environment to another lakeFS environment.</li> <li>Clone Repository: you want to clone a repository.</li> </ol> <p>Tip</p> <p>Refer to Python Sample Notebooks to backup, migrate or clone a lakeFS repository</p>"},{"location":"howto/backup-and-restore/#commit-changes","title":"Commit Changes","text":"<p>Backup process doesn't backup uncommitted data, so make sure to commit any staged writes before running the backup. But this is an optional process.</p> <p>You can manually commit the changes by using lakeFS UI or you can programmatically commit any uncommitted changes. This Python example shows how to programmatically loop through all branches in lakeFS and commit any uncommitted data but this might take a lot of time if you have many branches in the repo:</p> <pre><code>import lakefs\n\nrepo = lakefs.Repository(\"example-repo\")\n\nfor branch in repo.branches():\n    for diff in repo.branch(branch.id).uncommitted():\n        repo.branch(branch.id).commit(message='Committed changes to backup the repository')\n</code></pre>"},{"location":"howto/backup-and-restore/#backup-repository","title":"Backup Repository","text":""},{"location":"howto/backup-and-restore/#dump-metadata","title":"Dump Metadata","text":"<p>Dump metadata/refs of the repository by using lakeFS API or CLI.</p> <ul> <li>Example code to dump metadata by using lakeFS Python SDK (this process will create <code>_lakefs/refs_manifest.json</code> file in your storage namespace for the repository):</li> </ul> <pre><code>lakefs_sdk_client.internal_api.dump_refs(\"example-repo\")\n</code></pre> <ul> <li>Example commands to dump metadata by using lakeFS CLI and upload to S3 storage for the repository:</li> </ul> <pre><code>lakectl refs-dump lakefs://example-repo &gt; refs_manifest.json\n\naws s3 cp refs_manifest.json s3://source-bucket-name/example-repo/_lakefs/refs_manifest.json\n</code></pre> <ul> <li>Example commands to dump metadata by using lakeFS CLI and upload to Azure Blob storage for the repository:</li> </ul> <pre><code>lakectl refs-dump lakefs://example-repo &gt; refs_manifest.json\n\naz storage blob upload --file refs_manifest.json --container-name sourceContainer --name example-repo/_lakefs/refs_manifest.json --account-name source-storage-account-name --account-key &lt;source-storage-account-key&gt;\n</code></pre> <p>Warning</p> <p>Shutdown lakeFS services immediately after dumping the metadata so nobody can make any changes in the source repository.</p>"},{"location":"howto/backup-and-restore/#copy-data-to-backup-storage-location","title":"Copy Data to Backup Storage Location","text":"<p>Copy the repository's storage namespace to another, preferably geographically separate location. Copy command depends on the type of object storage and the tool that you use.</p> <ul> <li>Example S3 command:</li> </ul> <pre><code>aws s3 sync s3://source-bucket-name/example-repo s3://target-bucket-name/example-repo\n</code></pre> <ul> <li>Example Azure azcopy command:</li> </ul> <pre><code>azcopy copy 'https://source-storage-account-name.blob.core.windows.net/sourceContainer/example-repo/*?source_container_SAS_token' 'https://target-storage-account-name.blob.core.windows.net/targetContainer/example-repo?target_container_SAS_token' --recursive\n</code></pre> <p>Info</p> <p>You can restart lakeFS services after copying the data to backup storage location.</p>"},{"location":"howto/backup-and-restore/#restore-repository","title":"Restore Repository","text":""},{"location":"howto/backup-and-restore/#create-a-new-bare-repository","title":"Create a new Bare Repository","text":"<p>Create a bare lakeFS repository with a new name if you want to clone the repository or use the same repository name if you want to migrate or restore the repository.</p> <ul> <li>Python example to create a bare lakeFS repository using S3 storage:</li> </ul> <pre><code>lakefs.Repository(\"target-example-repo\").create(bare=True, storage_namespace=\"s3://target-bucket-name/example-repo\", default_branch=\"same-default-branch-as-in-source-repo\")\n</code></pre> <ul> <li>Python example to create a bare lakeFS repository using Azure storage:</li> </ul> <pre><code>lakefs.Repository(\"target-example-repo\").create(bare=True, storage_namespace=\"https://target-storage-account-name.blob.core.windows.net/targetContainer/example-repo\", default_branch=\"same-default-branch-as-in-source-repo\")\n</code></pre> <ul> <li>lakeFS CLI command to create a bare lakeFS repository using S3 storage:</li> </ul> <pre><code>lakectl repo create-bare lakefs://target-example-repo s3://target-bucket-name/example-repo --default-branch \"same-default-branch-as-in-source-repo\"\n</code></pre> <ul> <li>lakeFS CLI command to create a bare lakeFS repository using Azure storage:</li> </ul> <pre><code>lakectl repo create-bare lakefs://target-example-repo https://target-storage-account-name.blob.core.windows.net/targetContainer/example-repo --default-branch \"same-default-branch-as-in-source-repo\"\n</code></pre>"},{"location":"howto/backup-and-restore/#restore-metadata-to-new-repository","title":"Restore Metadata to new Repository","text":"<p>Run restore_refs to load back all commits, tags and branches.</p> <ul> <li>Python example to restore metadata to new repository. First download metadata(refs_manifest.json) file created by metadata dump process:</li> </ul> <pre><code>aws s3 cp s3://target-bucket-name/example-repo/_lakefs/refs_manifest.json .\n</code></pre> <pre><code>azcopy copy 'https://target-storage-account-name.blob.core.windows.net/targetContainer/example-repo/_lakefs/refs_manifest.json?&lt;target_container_SAS_token&gt;' .\n</code></pre> <p>Then read refs_manifest.json file and restore metadata to new repository:</p> <pre><code>with open('./refs_manifest.json') as file:\n    refs_manifest_json = json.load(file)\n    print(refs_manifest_json)\n\ntarget_lakefs_sdk_client.internal_api.restore_refs(target_repo_name, refs_manifest_json)\n</code></pre> <ul> <li>lakeFS CLI command to restore metadata to new repository using S3 storage:</li> </ul> <pre><code>aws s3 cp s3://target-bucket-name/example-repo/_lakefs/refs_manifest.json - | lakectl refs-restore lakefs://target-example-repo --manifest -\n</code></pre> <ul> <li>lakeFS CLI command to restore metadata to new repository using Azure storage:</li> </ul> <pre><code>az storage blob download --container-name targetContainer --name example-repo/_lakefs/refs_manifest.json --account-name target-storage-account-name --account-key &lt;target-storage-account-key&gt; | lakectl refs-restore lakefs://target-example-repo --manifest -\n</code></pre> <p>Tip</p> <p>If you are running backups regularly, it is highly advised to test the restore process periodically to make sure that you are able to restore the repository in case of disaster.</p>"},{"location":"howto/backup-and-restore/#python-helper-script-for-backup-and-restore","title":"Python Helper Script for Backup and Restore","text":"<p>For more streamlined repository backup and restore operations, you can use the <code>lakefs-refs.py</code> script available in the lakeFS repository.</p>"},{"location":"howto/backup-and-restore/#overview","title":"Overview","text":"<p>The <code>lakefs-refs.py</code> script automates the backup and restore procedures described in this document. It handles all the necessary steps to dump and restore lakeFS repository references, making the process simpler and less error-prone for repository migration and backup purposes.</p>"},{"location":"howto/backup-and-restore/#prerequisites","title":"Prerequisites","text":"<p>Install the required dependencies:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"howto/backup-and-restore/#usage-examples","title":"Usage Examples","text":""},{"location":"howto/backup-and-restore/#dump-repository-references","title":"Dump Repository References","text":"<p>The script provides an easy way to dump repository metadata:</p> <pre><code>python lakefs-refs.py dump &lt;repository-name&gt; [--all] [--commit] [--rm] [--endpoint-url &lt;url&gt;] [--access-key-id &lt;key&gt;] [--secret-access-key &lt;secret&gt;]\n</code></pre> <p>Options:</p> <ul> <li><code>&lt;repository-name&gt;</code>: Name of the specific repository to dump</li> <li><code>--all</code>: Dump all repositories instead of a specific one</li> <li><code>--commit</code>: Commit any uncommitted changes before dumping</li> <li><code>--rm</code>: Delete repository definition after successful dump</li> <li>Authentication options (see below)</li> </ul>"},{"location":"howto/backup-and-restore/#restore-repository-references","title":"Restore Repository References","text":"<p>For restoring repository metadata from manifest files:</p> <pre><code>python lakefs-refs.py restore &lt;manifest-file&gt; [&lt;manifest-file2&gt; ...] [--ignore-storage-id] [--endpoint-url &lt;url&gt;] [--access-key-id &lt;key&gt;] [--secret-access-key &lt;secret&gt;]\n</code></pre> <p>Options:</p> <ul> <li><code>&lt;manifest-file&gt;</code>: One or more manifest files to restore</li> <li><code>--ignore-storage-id</code>: Create repository without storage_id (useful when migrating to a different storage backend)</li> <li>Authentication options (see below)</li> </ul>"},{"location":"howto/backup-and-restore/#authentication","title":"Authentication","text":"<p>The script uses the same authentication method as <code>lakectl</code>, supporting authentication via command line parameters, environment variables (<code>LAKECTL_*</code>), or the <code>~/.lakectl.yaml</code> configuration file.</p>"},{"location":"howto/catalog_exports/","title":"Data Catalogs Export","text":""},{"location":"howto/catalog_exports/#about-data-catalogs-export","title":"About Data Catalogs Export","text":"<p>Data Catalog Export is all about integrating query engines (like Spark, AWS Athena, Presto, etc.) with lakeFS.</p> <p>Data Catalogs (such as Hive Metastore or AWS Glue) store metadata for services (such as Spark, Trino and Athena). They contain metadata such as the location of the table, information about columns, partitions and much more.</p> <p>With Data Catalog Exports, one can leverage the versioning capabilities of lakeFS in external data warehouses and query engines to access tables with branches and commits. </p> <p>At the end of this guide, you will be able to query lakeFS data from Athena, Trino and other catalog-dependent tools:</p> <pre><code>USE main;\nUSE my_branch; -- any branch\nUSE v101; -- or tag\n\nSELECT * FROM users \nINNER JOIN events \nON users.id = events.user_id; -- SQL stays the same, branch or tag exist as schema\n</code></pre>"},{"location":"howto/catalog_exports/#how-it-works","title":"How it works","text":"<p>Several well known formats exist today let you export existing tables in lakeFS into a \"native\" object store representation which does not require copying the data outside of lakeFS.</p> <p>These are metadata representations and can be applied automatically through hooks.</p>"},{"location":"howto/catalog_exports/#table-decleration","title":"Table Decleration","text":"<p>After creating a lakeFS repository, configure tables as table descriptor objects on the repository on the path <code>_lakefs_tables/TABLE.yaml</code>. Note: the Glue exporter can currently only export tables of <code>type: hive</code>.  We expect to add more.</p>"},{"location":"howto/catalog_exports/#hive-tables","title":"Hive tables","text":"<p>Hive metadata server tables are essentially just a set of objects that share a prefix, with no table metadata stored on the object store.  You need to configure prefix, partitions, and schema.</p> <pre><code>name: animals\ntype: hive\npath: path/to/animals/\npartition_columns: ['year']\nschema:\n  type: struct\n  fields:\n    - name: year\n      type: integer\n      nullable: false\n      metadata: {}\n    - name: page\n      type: string\n      nullable: false\n      metadata: {}\n    - name: site\n      type: string\n      nullable: true\n      metadata:\n        comment: a comment about this column\n</code></pre> <p>Tip</p> <p>Useful types recognized by Hive include <code>integer</code>, <code>long</code>, <code>short</code>, <code>string</code>, <code>double</code>, <code>float</code>, <code>date</code>, and <code>timestamp</code>.</p>"},{"location":"howto/catalog_exports/#catalog-exporters","title":"Catalog Exporters","text":"<p>Exporters are code packages accessible through Lua integration. Each exporter is exposed as a Lua function under the package namespace <code>lakefs/catalogexport</code>.  Call them from hooks to connect lakeFS tables to various catalogs.</p>"},{"location":"howto/catalog_exports/#currently-supported-exporters","title":"Currently supported exporters","text":"Exporter Description Notes Symlink exporter Writes metadata for the table using Hive's SymlinkTextInputFormat AWS Glue Catalog (+ Athena) exporter Creates a table in Glue using Hive's format and updates the location to symlink files (reuses Symlink Exporter). See a step-by-step guide on how to integrate with Glue Exporter Delta Lake table exporter Export Delta Lake tables from lakeFS to an external storage Unity Catalog exporter The Unity Catalog exporter serves the purpose of registering a Delta Lake table in Unity Catalog. It operates in conjunction with the Delta Lake exporter. In this workflow, the Delta Lake exporter is utilized to export a Delta Lake table from lakeFS. Subsequently, the obtained result is passed to the Unity Catalog exporter to facilitate its registration within Unity Catalog. See a step-by-step guide on how to integrate with Unity Catalog ExporterCurrently, only AWS S3 storage is supported"},{"location":"howto/catalog_exports/#running-an-exporter","title":"Running an Exporter","text":"<p>Exporters are meant to run as Lua hooks.</p> <p>Configure the actions trigger by using events and branches.  Of course, you can add additional custom filtering logic to the Lua script if needed. The default table name when exported is <code>${repository_id}_${_lakefs_tables/TABLE.md(name field)}_${ref_name}_${short_commit}</code>.</p> <p>Example of an action that will be triggered when a <code>post-commit</code> event happens in the <code>export_table</code> branch.</p> <pre><code>name: Glue Table Exporter\ndescription: export my table to glue  \non:\n  post-commit:\n    branches: [\"export_table\"]\nhooks:\n  - id: my_exporter\n    type: lua\n    properties:\n      # exporter script location\n      script_path: \"scripts/my_export_script.lua\"\n      args:\n        # table descriptor\n        table_source: '_lakefs_tables/my_table.yaml'\n</code></pre> <p>Tip: Actions can be extended to customize any desired behavior, for example validating branch names since they are part of the table name: </p> <pre><code># _lakefs_actions/validate_branch_name.yaml\nname: validate-lower-case-branches \non:\n  pre-create-branch:\nhooks:\n  - id: check_branch_id\n    type: lua\n    properties:\n      script: |\n        regexp = require(\"regexp\")\n        if not regexp.match(\"^[a-z0-9\\\\_\\\\-]+$\", action.branch_id) then\n          error(\"branches must be lower case, invalid branch ID: \" .. action.branch_id)\n        end\n</code></pre>"},{"location":"howto/catalog_exports/#flow","title":"Flow","text":"<p>The following diagram demonstrates what happens when a lakeFS Action triggers runs a lua hook that calls an exporter.</p> <pre><code>sequenceDiagram\n    note over Lua Hook: lakeFS Action trigger. &lt;br&gt; Pass Context for the export.\n    Lua Hook-&gt;&gt;Exporter: export request\n    note over Table Registry: _lakefs_tables/TABLE.yaml\n    Exporter-&gt;&gt;Table Registry: Get table descriptor\n    Table Registry-&gt;&gt;Exporter: Parse table structure\n    Exporter-&gt;&gt;Object Store: materialize an exported table\n    Exporter-&gt;&gt;Catalog: register object store location\n    Query Engine--&gt;Catalog: Query\n    Query Engine--&gt;Object Store: Query</code></pre>"},{"location":"howto/copying/","title":"Copying data to/from lakeFS","text":""},{"location":"howto/copying/#using-distcp","title":"Using DistCp","text":"<p>Apache Hadoop DistCp (distributed copy) is a tool used for large inter/intra-cluster copying. You can easily use it with your lakeFS repositories.</p> <p>Info</p> <p>In the following examples, we set AWS credentials on the command line for clarity.  In production, you should set these properties using one of Hadoop's standard ways of Authenticating with S3. </p>"},{"location":"howto/copying/#between-lakefs-repositories","title":"Between lakeFS repositories","text":"<p>You can use DistCP to copy between two different lakeFS repositories. Replace the access key pair with your lakeFS access key pair:</p> <pre><code>hadoop distcp \\\n  -Dfs.s3a.path.style.access=true \\\n  -Dfs.s3a.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\\n  -Dfs.s3a.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\\n  -Dfs.s3a.endpoint=\"https://lakefs.example.com\" \\\n  \"s3a://example-repo-1/main/example-file.parquet\" \\\n  \"s3a://example-repo-2/main/example-file.parquet\"\n</code></pre>"},{"location":"howto/copying/#between-s3-buckets-and-lakefs","title":"Between S3 buckets and lakeFS","text":"<p>To copy data from an S3 bucket to a lakeFS repository, use Hadoop's per-bucket configuration. In the following examples, replace the first access key pair with your lakeFS key pair, and the second one with your AWS IAM key pair:</p>"},{"location":"howto/copying/#from-s3-to-lakefs","title":"From S3 to lakeFS","text":"<pre><code>hadoop distcp \\\n  -Dfs.s3a.path.style.access=true \\\n  -Dfs.s3a.bucket.example-repo.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\\n  -Dfs.s3a.bucket.example-repo.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\\n  -Dfs.s3a.bucket.example-repo.endpoint=\"https://lakefs.example.com\" \\\n  -Dfs.s3a.bucket.example-bucket.access.key=\"AKIAIOSFODNN3EXAMPLE\" \\\n  -Dfs.s3a.bucket.example-bucket.secret.key=\"wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY\" \\\n  \"s3a://example-bucket/example-file.parquet\" \\\n  \"s3a://example-repo/main/example-file.parquet\"\n</code></pre>"},{"location":"howto/copying/#from-lakefs-to-s3","title":"From lakeFS to S3","text":"<pre><code>hadoop distcp \\\n  -Dfs.s3a.path.style.access=true \\\n  -Dfs.s3a.bucket.example-repo.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\\n  -Dfs.s3a.bucket.example-repo.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\\n  -Dfs.s3a.bucket.example-repo.endpoint=\"https://lakefs.example.com\" \\\n  -Dfs.s3a.bucket.example-bucket.access.key=\"AKIAIOSFODNN3EXAMPLE\" \\\n  -Dfs.s3a.bucket.example-bucket.secret.key=\"wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY\" \\\n  \"s3a://example-repo/main/myfile\" \\\n  \"s3a://example-bucket/myfile\"\n</code></pre>"},{"location":"howto/copying/#using-rclone","title":"Using Rclone","text":"<p>Rclone is a command line program to sync files and directories between cloud providers. To use it with lakeFS, create an Rclone remote as describe below and then use it as you would any other Rclone remote.</p>"},{"location":"howto/copying/#creating-a-remote-for-lakefs-in-rclone","title":"Creating a remote for lakeFS in Rclone","text":"<p>To add the remote to Rclone, choose one of the following options:</p>"},{"location":"howto/copying/#option-1-add-an-entry-in-your-rclone-configuration-file","title":"Option 1: Add an entry in your Rclone configuration file","text":"<ul> <li> <p>Find the path to your Rclone configuration file and copy it for the next step.</p> <pre><code>rclone config file\n# output:\n# Configuration file is stored at:\n# /home/myuser/.config/rclone/rclone.conf\n</code></pre> </li> <li> <p>If your lakeFS access key is already set in an AWS profile or environment variables, run the following command, replacing the endpoint property with your lakeFS endpoint:</p> <pre><code>cat &lt;&lt;EOT &gt;&gt; /home/myuser/.config/rclone/rclone.conf\n[lakefs]\ntype = s3\nprovider = Other\nendpoint = https://lakefs.example.com\nno_check_bucket = true\nEOT\n</code></pre> </li> <li> <p>Otherwise, also include your lakeFS access key pair in the Rclone configuration file:</p> <pre><code>cat &lt;&lt;EOT &gt;&gt; /home/myuser/.config/rclone/rclone.conf\n[lakefs]\ntype = s3\nprovider = Other\nenv_auth = false\naccess_key_id = AKIAIOSFODNN7EXAMPLE\nsecret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nendpoint = https://lakefs.example.com\nno_check_bucket = true\nEOT\n</code></pre> </li> </ul>"},{"location":"howto/copying/#option-2-use-the-rclone-interactive-config-command","title":"Option 2: Use the Rclone interactive config command","text":"<p>Run this command and follow the instructions:</p> <pre><code>rclone config\n\n\nChoose AWS S3 as your type of storage, and enter your lakeFS endpoint as your S3 endpoint.\nYou will have to choose whether you use your environment for authentication (recommended),\nor enter the lakeFS access key pair into the Rclone configuration. Select \"Edit advanced\nconfig\" and accept defaults for all values except `no_check_bucket`:\n\nIf set, don't attempt to check the bucket exists or create it.\n\nThis can be useful when trying to minimize the number of transactions\nRclone carries out, if you know the bucket exists already.\n\nThis might also be needed if the user you're using doesn't have bucket\ncreation permissions. Before v1.52.0, this would have passed silently\ndue to a bug.\n\nEnter a boolean value (true or false). Press Enter for the default (\"false\").\nno_check_bucket&gt; yes\n</code></pre>"},{"location":"howto/copying/#syncing-s3-and-lakefs","title":"Syncing S3 and lakeFS","text":"<pre><code>rclone sync mys3remote://mybucket/path/ lakefs:example-repo/main/path\n</code></pre>"},{"location":"howto/copying/#syncing-a-local-directory-and-lakefs","title":"Syncing a local directory and lakeFS","text":"<pre><code>rclone sync /home/myuser/path/ lakefs:example-repo/main/path\n</code></pre>"},{"location":"howto/export/","title":"Exporting Data","text":"<p>The export operation copies all data from a given lakeFS commit to a designated object store location.</p> <p>For instance, the contents <code>lakefs://example/main</code> might be exported on <code>s3://company-bucket/example/latest</code>. Clients entirely unaware of lakeFS could use that base URL to access latest files on <code>main</code>. Clients aware of lakeFS can continue to use the lakeFS S3 endpoint to access repository files on <code>s3://example/main</code>, as well as other versions and uncommitted versions.</p> <p>Possible use-cases:</p> <ol> <li>External consumers of data don't have access to your lakeFS installation.</li> <li>Some data pipelines in the organization are not fully migrated to lakeFS.</li> <li>You want to experiment with lakeFS as a side-by-side installation first.</li> <li>Create copies of your data lake in other regions (taking into account read pricing).</li> </ol>"},{"location":"howto/export/#exporting-data-with-spark","title":"Exporting Data With Spark","text":""},{"location":"howto/export/#using-spark-submit","title":"Using spark-submit","text":"<p>You can use the export main in three different modes:</p> <ol> <li> <p>Export all the objects from branch <code>example-branch</code> on <code>example-repo</code> repository to S3 location <code>s3://example-bucket/prefix/</code>:</p> <pre><code>.... example-repo s3://example-bucket/prefix/ --branch=example-branch\n</code></pre> </li> <li> <p>Export all the objects from a commit <code>c805e49bafb841a0875f49cd555b397340bbd9b8</code> on <code>example-repo</code> repository to S3 location <code>s3://example-bucket/prefix/</code>:</p> <pre><code>.... example-repo s3://example-bucket/prefix/ --commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8\n</code></pre> </li> <li> <p>Export only the diff between branch <code>example-branch</code> and commit <code>c805e49bafb841a0875f49cd555b397340bbd9b8</code>    on <code>example-repo</code> repository to S3 location <code>s3://example-bucket/prefix/</code>:</p> <pre><code>.... example-repo s3://example-bucket/prefix/ --branch=example-branch --prev_commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8\n</code></pre> </li> </ol> <p>The complete <code>spark-submit</code> command would look as follows:</p> <pre><code>spark-submit --conf spark.hadoop.lakefs.api.url=https://&lt;LAKEFS_ENDPOINT&gt;/api/v1 \\\n    --conf spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY_ID&gt; \\\n    --conf spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_ACCESS_KEY&gt; \\\n    --packages io.lakefs:lakefs-spark-client_2.12:0.14.3 \\\n    --class io.treeverse.clients.Main export-app example-repo s3://example-bucket/prefix \\\n    --branch=example-branch\n</code></pre> <p>Info</p> <p>The command assumes that the Spark cluster has permissions to write to <code>s3://example-bucket/prefix</code>. Otherwise, add <code>spark.hadoop.fs.s3a.access.key</code> and <code>spark.hadoop.fs.s3a.secret.key</code> with the proper credentials.</p>"},{"location":"howto/export/#networking","title":"Networking","text":"<p>Spark export communicates with the lakeFS server.  Very large repositories may require increasing a read timeout.  If you run into timeout errors during communication from the Spark job to lakeFS consider increasing these timeouts:</p> <ul> <li>Add <code>-c spark.hadoop.lakefs.api.read.timeout_seconds=TIMEOUT_IN_SECONDS</code>   (default 10) to allow lakeFS more time to respond to requests.</li> <li>Add <code>-c   spark.hadoop.lakefs.api.connection.timeout_seconds=TIMEOUT_IN_SECONDS</code>   (default 10) to wait longer for lakeFS to accept connections.</li> </ul>"},{"location":"howto/export/#using-custom-code-notebookspark","title":"Using custom code (Notebook/Spark)","text":"<p>Set up lakeFS Spark metadata client with the endpoint and credentials as instructed in the previous page.</p> <p>The client exposes the <code>Exporter</code> object with three export options:</p> <p>Export all the objects at the HEAD of a given branch. Does not include files that were added to that branch but were not committed.</p> Scala <pre><code>exportAllFromBranch(branch: String)\n</code></pre> <p>Export ALL objects from a commit:</p> Scala <pre><code>exportAllFromCommit(commitID: String)\n</code></pre> <p>Export just the diff between a commit and the HEAD of a branch.</p> <p>This is an ideal option for continuous exports of a branch since it will change only the files that have been changed since the previous commit.</p> Scala <pre><code>exportFrom(branch: String, prevCommitID: String)\n</code></pre>"},{"location":"howto/export/#successfailure-indications","title":"Success/Failure Indications","text":"<p>When the Spark export operation ends, an additional status file will be added to the root object storage destination. If all files were exported successfully, the file path will be of the form: <code>EXPORT_&lt;commitID&gt;_&lt;ISO-8601-time-UTC&gt;_SUCCESS</code>. For failures: the form will be<code>EXPORT_&lt;commitID&gt;_&lt;ISO-8601-time-UTC&gt;_FAILURE</code>, and the file will include a log of the failed files operations.</p>"},{"location":"howto/export/#export-rounds-spark-success-files","title":"Export Rounds (Spark success files)","text":"<p>Some files should be exported before others, e.g., a Spark <code>_SUCCESS</code> file exported before other files under the same prefix might send the wrong indication.</p> <p>The export operation may contain several rounds within the same export. A failing round will stop the export of all the files of the next rounds.</p> <p>By default, lakeFS will use the <code>SparkFilter</code> and have 2 rounds for each export. The first round will export any non-Spark <code>_SUCCESS</code> files. Second round will export all Spark's <code>_SUCCESS</code> files. You may override the default behavior by passing a custom <code>filter</code> to the Exporter.</p>"},{"location":"howto/export/#example","title":"Example","text":"<p>First configure the <code>Exporter</code> instance:</p> Scala <pre><code>import io.treeverse.clients.{ApiClient, Exporter}\nimport org.apache.spark.sql.SparkSession\n\nval endpoint = \"http://&lt;LAKEFS_ENDPOINT&gt;/api/v1\"\nval accessKey = \"&lt;LAKEFS_ACCESS_KEY_ID&gt;\"\nval secretKey = \"&lt;LAKEFS_SECRET_ACCESS_KEY&gt;\"\n\nval repo = \"example-repo\"\n\nval spark = SparkSession.builder().appName(\"I can export\").master(\"local\").getOrCreate()\nval sc = spark.sparkContext\nsc.hadoopConfiguration.set(\"lakefs.api.url\", endpoint)\nsc.hadoopConfiguration.set(\"lakefs.api.access_key\", accessKey)\nsc.hadoopConfiguration.set(\"lakefs.api.secret_key\", secretKey)\n\n// Add any required spark context configuration for s3\nval rootLocation = \"s3://company-bucket/example/latest\"\n\nval apiClient = new ApiClient(endpoint, accessKey, secretKey)\nval exporter = new Exporter(spark, apiClient, repo, rootLocation)\n</code></pre> <p>Now you can export all objects from <code>main</code> branch to <code>s3://company-bucket/example/latest</code>:</p> Scala <pre><code>val branch = \"main\"\nexporter.exportAllFromBranch(branch)\n</code></pre> <p>Assuming a previous successful export on commit <code>f3c450d8cd0e84ac67e7bc1c5dcde9bef82d8ba7</code>, you can alternatively export just the difference between <code>main</code> branch and the commit:</p> Scala <pre><code>val branch = \"main\"\nval commit = \"f3c450d8cd0e84ac67e7bc1c5dcde9bef82d8ba7\"\nexporter.exportFrom(branch, commit)\n</code></pre>"},{"location":"howto/export/#exporting-data-with-docker","title":"Exporting Data with Docker","text":"<p>This option is recommended if you don't have Spark at your tool-set. It doesn't support distribution across machines, therefore may have a lower performance. Using this method, you can export data from lakeFS to S3 using the export options (in a similar way to the Spark export):</p> <ol> <li>Export all objects from a branch <code>example-branch</code> on <code>example-repo</code> repository to S3 location <code>s3://destination-bucket/prefix/</code>:     <pre><code>.... example-repo s3://destination-bucket/prefix/ --branch=\"example-branch\"\n</code></pre></li> <li>Export all objects from a commit <code>c805e49bafb841a0875f49cd555b397340bbd9b8</code> on <code>example-repo</code> repository to S3 location <code>s3://destination-bucket/prefix/</code>:     <pre><code>.... example-repo s3://destination-bucket/prefix/ --commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8\n</code></pre></li> <li>Export only the diff between branch <code>example-branch</code> and commit <code>c805e49bafb841a0875f49cd555b397340bbd9b8</code>    on <code>example-repo</code> repository to S3 location <code>s3://destination-bucket/prefix/</code>:     <pre><code>.... example-repo s3://destination-bucket/prefix/ --branch=\"example-branch\" --prev_commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8\n</code></pre></li> </ol> <p>You will need to add the relevant environment variables. The complete <code>docker run</code> command would look like:</p> <pre><code>docker run \\\n    -e LAKEFS_ACCESS_KEY_ID=XXX -e LAKEFS_SECRET_ACCESS_KEY=YYY \\\n    -e LAKEFS_ENDPOINT=https://&lt;LAKEFS_ENDPOINT&gt;/ \\\n    -e AWS_ACCESS_KEY_ID=XXX -e AWS_SECRET_ACCESS_KEY=YYY \\\n    treeverse/lakefs-rclone-export:latest \\\n        example-repo \\\n        s3://destination-bucket/prefix/ \\\n        --branch=\"example-branch\"\n</code></pre> <p>Note</p> <p>This feature uses rclone, and specifically rclone sync. This can change the destination path, therefore the s3 destination location must be designated to lakeFS export.</p>"},{"location":"howto/import/","title":"Import Data","text":"<p>Tip</p> <p>This section describes how to import existing data into a lakeFS repository, without copying it. If you are interested in copying data into lakeFS, see Copying data to/from lakeFS._</p>"},{"location":"howto/import/#importing-data-into-lakefs","title":"Importing data into lakeFS","text":""},{"location":"howto/import/#prerequisites","title":"Prerequisites","text":"<ul> <li>Importing is permitted for users in the Supers (open-source) group or the SuperUsers (Cloud/Enterprise) group.    To learn how lakeFS Cloud and lakeFS Enterprise users can fine-tune import permissions, see Fine-grained permissions below.</li> <li>The lakeFS server must have permissions to list the objects in the source bucket.</li> <li>The source bucket must be on the same cloud provider and in the same region as your repository.</li> </ul>"},{"location":"howto/import/#using-the-lakefs-ui","title":"Using the lakeFS UI","text":"<ol> <li>In your repository's main page, click the Import button to open the import dialog.</li> <li>Under Import from, fill in the location on your object store you would like to import from.</li> <li>Fill in the import destination in lakeFS. This should be a path under the current branch.</li> <li>Add a commit message, and optionally commit metadata.</li> <li>Press Import.</li> </ol> <p>Once the import is complete, a new commit containing the imported objects will be created in the destination branch.</p> <p></p>"},{"location":"howto/import/#using-the-cli-lakectl-import","title":"Using the CLI: lakectl import","text":"<p>The lakectl import command acts the same as the UI import wizard. It commits the changes to the selected branch.</p> AWS S3 or S3 API Compatible storageAzure BlobGoogle Cloud Storage <pre><code>lakectl import \\\n--from s3://bucket/optional/prefix/ \\\n--to lakefs://my-repo/my-branch/optional/path/\n</code></pre> <pre><code>lakectl import \\\n--from https://storageAccountName.blob.core.windows.net/container/optional/prefix/ \\\n--to lakefs://my-repo/my-branch/optional/path/\n</code></pre> <pre><code>lakectl import \\\n--from gs://bucket/optional/prefix/ \\\n--to lakefs://my-repo/my-branch/optional/path/\n</code></pre> <p>Note</p> <ol> <li>Any previously existing objects under the destination prefix will be deleted.</li> <li>The import duration depends on the amount of imported objects, but will roughly be a few thousand objects per second.</li> <li>For security reasons, if you are using lakeFS on top of your local disk (<code>blockstore.type=local</code>), you need to enable the import feature explicitly. To do so, set the <code>blockstore.local.import_enabled</code> to <code>true</code> and specify the allowed import paths in <code>blockstore.local.allowed_external_prefixes</code> (see configuration reference). When using lakectl or the lakeFS UI, you can currently import only directories locally. If you need to import a single file, use the HTTP API or API Clients with <code>type=object</code> in the request body and <code>destination=&lt;full-path-to-file&gt;</code>.</li> <li>Making changes to data in the original bucket will not be reflected in lakeFS, and may cause inconsistencies.</li> </ol>"},{"location":"howto/import/#examples","title":"Examples","text":"<p>To explore practical examples and real-world use cases of importing data into lakeFS, we recommend checking out our comprehensive blog post on the subject.</p>"},{"location":"howto/import/#fine-grained-permissions","title":"Fine-grained permissions","text":"<p>Info</p> <p>Available on lakeFS Cloud and lakeFS Enterprise</p> <p>With RBAC support, The lakeFS user running the import command should have the following permissions in lakeFS: <code>fs:WriteObject</code>, <code>fs:CreateCommit</code>, <code>fs:ImportFromStorage</code> and <code>fs:ImportCancel</code>.</p> <p>As mentioned above, all of these permissions are available by default to the Supers (open-source) group or the SuperUsers (Cloud/Enterprise).</p>"},{"location":"howto/import/#provider-specific-permissions","title":"Provider-specific permissions","text":"<p>In addition, the following for provider-specific permissions may be required:</p> AWS S3 or S3 API Compatible storageAzureGoogle Cloud Storage <p>lakeFS needs access to the imported location to first list the files to import and later read the files upon users request.</p> <p>There are some use cases where the user would like to import from a destination which isn't owned by the account running lakeFS. For example, importing public datasets to experiment with lakeFS and Spark.</p> <p>lakeFS will require additional permissions to read from public buckets. For example, for S3 public buckets, the following policy needs to be attached to the lakeFS S3 service-account to allow access to public buckets, while blocking access to other owned buckets:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"PubliclyAccessibleBuckets\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetBucketVersioning\",\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucketMultipartUploads\",\n                \"s3:ListBucketVersions\",\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\",\n                \"s3:AbortMultipartUpload\",\n                \"s3:ListMultipartUploadParts\"\n            ],\n            \"Resource\": [\"*\"],\n            \"Condition\": {\n                \"StringNotEquals\": {\n                    \"s3:ResourceAccount\": \"&lt;YourAccountID&gt;\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> <p>Note</p> <p>The use of the <code>adls</code> hint for ADLS Gen2 storage accounts is deprecated, please use the original source url for import.</p> <p>See [Azure deployment][deploy-azure-storage-account-creds] on limitations when using account credentials.</p> <p>No specific prerequisites</p>"},{"location":"howto/local-checkouts/","title":"Working with lakeFS Data Locally","text":"<p>lakeFS is a scalable data version control system designed to scale to billions of objects. The larger the data, the less feasible it becomes to consume it from a single machine. lakeFS addresses this challenge by enabling efficient management of large-scale data stored remotely. </p> <p>In addition to its capability to manage large datasets, lakeFS offers the flexibility to work with versioned data by exposing it as a local filesystem directory.  </p> <p>This page explains lakeFS Mount and <code>lakectl local</code>: two common ways of exposing lakeFS data locally, with different performance characteristics.  </p>"},{"location":"howto/local-checkouts/#use-cases","title":"Use cases","text":""},{"location":"howto/local-checkouts/#local-development-of-ml-models","title":"Local development of ML models","text":"<p>The development of machine learning models is a dynamic and iterative process, including experimentation with various data versions,  transformations, algorithms, and hyperparameters. To optimize this iterative workflow, experiments must be conducted with speed,  ease of tracking, and reproducibility in mind. Localizing the model data during development enhances the development process.  It accelerates the development process by enabling interactive and offline development and reducing data access latency. </p> <p>The local availability of data is required to seamlessly integrate data version control systems and source control systems like Git. This integration is vital for achieving model reproducibility, allowing for a more efficient and collaborative  model development environment.</p>"},{"location":"howto/local-checkouts/#data-locality-for-optimized-gpu-utilization","title":"Data Locality for Optimized GPU Utilization","text":"<p>Training Deep Learning models requires expensive GPUs. In the context of running such programs, the goal is to optimize GPU usage and prevent them from sitting idle. Many deep learning tasks involve accessing images, and in some cases, the same images are accessed multiple times. Localizing the data can eliminate redundant round trip times to access remote  storage, resulting in cost savings.</p>"},{"location":"howto/local-checkouts/#lakefs-mount-efficiently-expose-lakefs-data-as-a-local-directory","title":"lakeFS Mount: Efficiently expose lakeFS Data as a local directory","text":"<p>Info</p> <p>Mount requires no installation, please contact us to get access.</p>"},{"location":"howto/local-checkouts/#prerequisites","title":"Prerequisites:","text":"<ul> <li>A working lakeFS Server running either lakeFS Enterprise or lakeFS Cloud</li> <li>You\u2019ve installed the <code>lakectl</code> command line utility: this is the official lakeFS command line interface, on top of which lakeFS Mount is built.</li> <li>lakectl is configured properly to access your lakeFS server as detailed in the configuration instructions</li> </ul>"},{"location":"howto/local-checkouts/#mounting-a-lakefs-reference-as-a-local-directory","title":"Mounting a lakeFS reference as a local directory","text":"<p>lakeFS Mount works by exposing a virtual mountpoint on the host computer. </p> <p>This \"acts\" as a local directory, allowing applications to read write and interact with data as it is all local to the machine, while lakeFS Mount optimizes this behind the scenes by lazily fetching data as requested, caching accessed objects and efficiently managing metadata to ensure best in class performance. Read more about how lakeFS Mount optimizes performance</p> <p>Mounting a reference is a single command:</p> <pre><code>everest mount lakefs://example-repo/example-branch/path/to/data/ ./my_local_dir\n</code></pre> <p>Once executed, the <code>my_local_dir</code> directory should appear to have the contents of the remote path we provided. We can verify this:</p> <pre><code>ls -l ./my_local_dir/\n</code></pre> <p>Which should return the listing of the mounted path.</p> <p>Tip</p> <p>lakeFS Mount allows quite a bit of tuning to ensure optimal performance. Read more about how lakeFS Mount works and how to configure it.</p>"},{"location":"howto/local-checkouts/#reading-from-a-mount","title":"Reading from a mount","text":"<p>Reading from a lakeFS Mount requires no special tools, integrations or SDKs! Simply point your code to the directory and read from it as if it was in fact local:</p> <pre><code>#!/usr/bin/env python\nimport glob\n\nfor image_path in glob.glob('./my_local_dir/*.png'):\n    with open(image_path, 'rb') as f:\n        process(f)\n</code></pre>"},{"location":"howto/local-checkouts/#unmounting","title":"Unmounting","text":"<p>When done, simply run:</p> <pre><code>everest umount ./my_local_dir\n</code></pre> <p>This will unmount the lakeFS Mount, cleaning up background tasks</p>"},{"location":"howto/local-checkouts/#lakectl-local-sync-lakefs-data-with-a-local-directory","title":"lakectl local: Sync lakeFS data with a local directory","text":"<p>The local command of lakeFS' CLI lakectl enables working with lakeFS data locally by copying the data onto the host machine. It allows syncing local directories with remote lakeFS locations,  and to seamlessly integrate lakeFS with Git.</p> <p>Here are the available lakectl local commands: </p> Command What it does Notes init Connects between a local directory and a lakeFS remote URI to enable data sync To undo a directory init, delete the .lakefs_ref.yaml file created in the initialized directory clone Clones lakeFS data from a path into an empty local directory and initializes the directory A directory can only track a single lakeFS remote location. i.e., you cannot clone data into an already initialized directory list Lists directories that are synced with lakeFS It is recommended to follow any init or clone command with a list command to verify its success status Shows remote and local changes to the directory and the remote location it tracks commit Commits changes from local directory to the lakeFS branch it tracks Uncommitted changes to directories connected to lakeFS remote locations will not reflect in lakeFS until after doing lakectl local commit. pull Fetches latest changes from a lakeFS remote location into a connected local directory checkout Syncs a local directory with the state of a lakeFS ref <p>Warning</p> <p>The data size you work with locally should be reasonable for smooth operation on a local machine which is typically no larger than 15 GB.  </p>"},{"location":"howto/local-checkouts/#example-using-lakectl-local-in-tandem-with-git","title":"Example: Using lakectl local in tandem with Git","text":"<p>We are going to develop an ML model that predicts whether an image is an Alpaca or not. Our goal is to improve the input  for the model. The code for the model is versioned by Git while the model dataset is versioned by lakeFS. We will be using lakectl local to tie code versions to data versions to achieve model reproducibility.  </p>"},{"location":"howto/local-checkouts/#setup","title":"Setup","text":"<p>To get start with, we have initialized a Git repo called <code>is_alpaca</code> that includes the model code: </p> <p>We also created a lakeFS repository and uploaded the is_alpaca train dataset  by Kaggel into it: </p>"},{"location":"howto/local-checkouts/#create-an-isolated-environment-for-experiments","title":"Create an Isolated Environment for Experiments","text":"<p>Our goal is to improve the model predictions. To meet our goal, we will experiment with editing the training dataset.  We will run our experiments in isolation to not change anything until after we are certain the data is improved and ready.    </p> <p>Let's create a new lakeFS branch called <code>experiment-1</code>. Our is_alpaca dataset is accessible on that branch,  and we will interact with the data from that branch only. </p> <p>On the code side, we will create a Git branch also called <code>experiment-1</code> to not pollute our main branch with a dataset which is under tuning.</p>"},{"location":"howto/local-checkouts/#clone-lakefs-data-into-a-local-git-repository","title":"Clone lakeFS Data into a Local Git Repository","text":"<p>Inspecting the <code>train.py</code> script, we can see that it expects an input on the <code>input</code> directory.</p> <pre><code>#!/usr/bin/env python\nimport tensorflow as tf\n\ninput_location = './input'\nmodel_location = './models/is_alpaca.h5'\n\ndef get_ds(subset):\n    return tf.keras.utils.image_dataset_from_directory(\n        input_location, validation_split=0.2, subset=subset,\n        seed=123, image_size=(244, 244), batch_size=32)\n\ntrain_ds = get_ds(\"training\")\nval_ds = get_ds(\"validation\")\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Rescaling(1./255),\n    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(2)])\n\n# Fit and save\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\nmodel.fit(train_ds, validation_data=val_ds, epochs=3)\nmodel.save(model_location)\n</code></pre> <p>This means that to be able to locally develop our model and experiment with it we need to have the is_alpaca dataset managed by lakeFS available locally on that path. To do that, we will use the <code>lakectl local clone</code>  command from our local Git repository root:</p> <p><pre><code>lakectl local clone lakefs://is-alpaca/experiment-1/dataset/train/ input\n</code></pre> This command will do a diff between out local input directory (that did not exist until now) and the provided lakeFS path  and identify that there are files to be downloaded from lakeFS.</p> <pre><code>Successfully cloned lakefs://is-alpaca/experiment-1/dataset/train/ to ~/ml_models/is_alpaca/input\n\nClone Summary:\n\nDownloaded: 250\nUploaded: 0\nRemoved: 0\n</code></pre> <p>Running <code>lakectl local list</code> from our Git repository root will show that the  <code>input</code> directory is now in sync with a lakeFS prefix (Remote URI), and what lakeFS version of the data (Synced Commit)  the is it tracking:</p> <pre><code> is_alpaca % lakectl local list                 \n+-----------+------------------------------------------------+------------------------------------------------------------------+\n| DIRECTORY | REMOTE URI                                     | SYNCED COMMIT                                                    |\n+-----------+------------------------------------------------+------------------------------------------------------------------+\n| input     | lakefs://is-alpaca/experiment-1/dataset/train/ | 589f87704418c6bac80c5a6fc1b52c245af347b9ad1ea8d06597e4437fae4ca3 |\n+-----------+------------------------------------------------+------------------------------------------------------------------+\n</code></pre>"},{"location":"howto/local-checkouts/#tie-code-version-and-data-version","title":"Tie Code Version and Data Version","text":"<p>Now let's tell Git to stage the dataset we've added and inspect our Git branch status:</p> <pre><code>is_alpaca % git add input/\nis_alpaca % git status \nOn branch experiment-1\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n    new file:   input/.lakefs_ref.yaml\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n    modified:   .gitignore\n</code></pre> <p>We can see that the <code>.gitignore</code> file changed, and that the files we cloned from lakeFS into the <code>input</code> directory are not  tracked by git. This is intentional - remember that lakeFS is the one managing the data. But wait, what is this special <code>input/.lakefs_ref.yaml</code> file that Git does track?  </p> <p><pre><code>is_alpaca % cat input/.lakefs_ref.yaml\n\nsrc: lakefs://is-alpaca/experiment-1/dataset/train/s\nat_head: 589f87704418c6bac80c5a6fc1b52c245af347b9ad1ea8d06597e4437fae4ca3\n</code></pre> This file includes the lakeFS version of the data that the Git repository is currently pointing to.</p> <p>Let's commit the changes to Git with:</p> <p><pre><code>git commit -m \"added is_alpaca dataset\" \n</code></pre> By committing to Git, we tie the current code version of the model to the dataset version in lakeFS as it appears in <code>input/.lakefs_ref.yaml</code>.</p>"},{"location":"howto/local-checkouts/#experiment-and-version-results","title":"Experiment and Version Results","text":"<p>We ran the train script on the cloned input, and it generated a model. Now, let's use the model to predict whether an axolotl is an alpaca.</p> <p>A reminder - this is how an axolotl looks like - not like an alpaca!</p> <p></p> <p>Here are the (surprising) results:</p> <p><pre><code>is_alpaca % ./predict.py ~/axolotl1.jpeg\n{'alpaca': 0.32112, 'not alpaca': 0.07260383}\n</code></pre> We expected the model to provide a more concise prediction, so let's try to improve it. To do that, we will add additional images of axolotls to the model input directory: </p> <pre><code>is_alpaca % cp ~/axolotls_images/* input/not_alpaca\n</code></pre> <p>To inspect what changes we made to out dataset we will use lakectl local status.</p> <pre><code>is_alpaca % lakectl local status input \ndiff 'local:///ml_models/is_alpaca/input' &lt;--&gt; 'lakefs://is-alpaca/589f87704418c6bac80c5a6fc1b52c245af347b9ad1ea8d06597e4437fae4ca3/dataset/train/'...\ndiff 'lakefs://is-alpaca/589f87704418c6bac80c5a6fc1b52c245af347b9ad1ea8d06597e4437fae4ca3/dataset/train/' &lt;--&gt; 'lakefs://is-alpaca/experiment-1/dataset/train/'...\n\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 SOURCE \u2551 CHANGE \u2551 PATH                       \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 local  \u2551 added  \u2551 not_alpaca/axolotl2.jpeg \u2551\n\u2551 local  \u2551 added  \u2551 not_alpaca/axolotl3.png  \u2551\n\u2551 local  \u2551 added  \u2551 not_alpaca/axolotl4.jpeg \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n</code></pre> <p>At this point, the dataset changes are not yet tracked by lakeFS. We will validate that by looking at the uncommitted changes area of our experiment branch and verifying it is empty. </p> <p>To commit these changes to lakeFS we will use lakectl local commit:</p> <pre><code>is_alpaca % lakectl local commit input -m \"add images of axolotls to the training dataset\"\n\nGetting branch: experiment-1\n\ndiff 'local:///ml_models/is_alpaca/input' &lt;--&gt; 'lakefs://is-alpaca/589f87704418c6bac80c5a6fc1b52c245af347b9ad1ea8d06597e4437fae4ca3/dataset/train/'...\nupload not_alpaca/axolotl3.png              ... done! [5.04KB in 679ms]\nupload not_alpaca/axolotl2.jpeg             ... done! [38.31KB in 685ms]\nupload not_alpaca/axolotl4.jpeg             ... done! [7.70KB in 718ms]\n\nSync Summary:\n\nDownloaded: 0\nUploaded: 3\nRemoved: 0\n\nFinished syncing changes. Perform commit on branch...\nCommit for branch \"experiment-1\" completed.\n\nID: 0b376f01b925a075851bbaffacf104a80de04a43ed7e56054bf54c42d2c8cce6\nMessage: add images of axolotls to the training dataset\nTimestamp: 2024-02-08 17:41:20 +0200 IST\nParents: 589f87704418c6bac80c5a6fc1b52c245af347b9ad1ea8d06597e4437fae4ca3\n</code></pre> <p>Looking at the lakeFS UI we can see that the lakeFS commit includes metadata that tells us what was the code version of  the linked Git repository at the time of the commit. </p> <p>Inspecting the Git repository, we can see that the input/.lakefs_ref.yaml is pointing to the latest lakeFS commit <code>0b376f01b925a075851bbaffacf104a80de04a43ed7e56054bf54c42d2c8cce6</code>.  </p> <p>We will now re-train our model with the modified dataset and give a try to predict whether an axolotl is an alpaca: </p> <p><pre><code>is_alpaca % ./predict.py ~/axolotl1.jpeg\n{'alpaca': 0.12443, 'not alpaca': 0.47260383}\n</code></pre> Results are indeed more accurate.</p>"},{"location":"howto/local-checkouts/#sync-a-local-directory-with-lakefs","title":"Sync a Local Directory with lakeFS","text":"<p>Now that we think that the latest version of our model generates reliable predictions, let's validate it against a test dataset rather than against a single picture. We will use the test dataset provided by Kaggel.  Let's create a local <code>testDataset</code> directory in our git repository and populate it with the test dataset. </p> <p>Now, we will use  lakectl local init to sync the <code>testDataset</code> directory with our lakeFS repository: </p> <pre><code>is_alpaca % lakectl local init lakefs://is-alpaca/main/dataset/test/ testDataset \nLocation added to /is_alpaca/.gitignore\nSuccessfully linked local directory '/is_alpaca/testDataset' with remote 'lakefs://is-alpaca/main/dataset/test/'\n</code></pre> <p>And validate that the directory was linked successfully: </p> <pre><code>is_alpaca % lakectl local list                                                           \n+-------------+-------------------------------------------------+------------------------------------------------------------------+\n| DIRECTORY   | REMOTE URI                                      | SYNCED COMMIT                                                    |\n+-------------+-------------------------------------------------+------------------------------------------------------------------+\n| input       | lakefs://is-alpaca/main/dataset/train/          | 0b376f01b925a075851bbaffacf104a80de04a43ed7e56054bf54c42d2c8cce6 |\n| testDataset | lakefs://is-alpaca/main/dataset/test/           | 0b376f01b925a075851bbaffacf104a80de04a43ed7e56054bf54c42d2c8cce6 |\n+-------------+-------------------------------------------------+------------------------------------------------------------------+\n</code></pre> <p>Now we will tell Git to track the <code>testDataset</code> directory with <code>git add testDataset</code>, and as we saw earlier Git will only track the <code>testDataset/.lakefs_ref.yaml</code>  for that directory rather than its content.  </p> <p>To see the difference between our local <code>testDataset</code> directory and its lakeFS location <code>lakefs://is-alpaca/main/dataset/test/</code>  we will use lakectl local status:</p> <pre><code>is_alpaca % lakectl local status testDataset \n\ndiff 'local:///ml_models/is_alpaca/testDataset' &lt;--&gt; 'lakefs://is-alpaca/0b376f01b925a075851bbaffacf104a80de04a43ed7e56054bf54c42d2c8cce6/dataset/test/'...\ndiff 'lakefs://is-alpaca/0b376f01b925a075851bbaffacf104a80de04a43ed7e56054bf54c42d2c8cce6/dataset/test/' &lt;--&gt; 'lakefs://is-alpaca/main/dataset/test/'...\n\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 SOURCE \u2551 CHANGE \u2551 PATH                           \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 local  \u2551 added  \u2551 alpaca/alpaca (1).jpg          \u2551\n\u2551 local  \u2551 added  \u2551 alpaca/alpaca (10).jpg         \u2551\n\u2551    .         .                  .                \u2551 \n\u2551    .         .                  .                \u2551\n\u2551    .         .                  .                \u2551\n\u2551 local  \u2551 added  \u2551 not_alpaca/not_alpaca (9).jpg  \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n</code></pre> <p>We can see that multiple files were locally added to the synced directory.  </p> <p>To apply these changes to lakeFS we will commit them: </p> <pre><code>is_alpaca % lakectl local commit testDataset -m \"add is_alpaca test dataset to lakeFS\" \n\nGetting branch: experiment-1\n\ndiff 'local:///ml_models/is_alpaca/testDataset' &lt;--&gt; 'lakefs://is-alpaca/0b376f01b925a075851bbaffacf104a80de04a43ed7e56054bf54c42d2c8cce6/dataset/test/'...\nupload alpaca/alpaca (23).jpg            ... done! [113.81KB in 1.241s]\nupload alpaca/alpaca (26).jpg            ... done! [102.74KB in 1.4s]\n          .                                             .\n          .                                             .\nupload not_alpaca/not_alpaca (42).jpg    ... done! [886.93KB in 14.336s]\n\nSync Summary:\n\nDownloaded: 0\nUploaded: 77\nRemoved: 0\n\nFinished syncing changes. Perform commit on branch...\nCommit for branch \"experiment-1\" completed.\n\nID: c8be7f4f5c13dd2e489ae85e6f747230bfde8e50f9cd9b6af20b2baebfb576cf\nMessage: add is_alpaca test dataset to lakeFS\nTimestamp: 2024-02-10 12:31:53 +0200 IST\nParents: 0b376f01b925a075851bbaffacf104a80de04a43ed7e56054bf54c42d2c8cce6\n</code></pre> <p>Looking at the lakFS UI we see that our test data is now available at lakeFS: </p> <p>Finally, we will Git commit the local changes to link between the Git and lakeFS repositories state.</p> <p>Tip</p> <p>While syncing a local directory with a lakeF prefix, it is recommended to first commit the data to lakeFS and then  do a Git commit that will include the changes done to the <code>.lakefs_ref.yaml</code> for the synced directory. Reasoning is that only after committing the data to lakeFS, the <code>.lakefs_ref.yaml</code> file points to a lakeFS commit that includes the added  content from the directory.</p>"},{"location":"howto/local-checkouts/#reproduce-model-results","title":"Reproduce Model Results","text":"<p>What if we wanted to re-run the model that predicted that an axolotl is more likely to be an alpaca?  This question translates into the question: \"How do I roll back my code and data to the time before we optimized the train dataset?\" Which translates to: \"What was the Git commit ID at this point?\"</p> <p>Searching our Git log we find this commit:</p> <pre><code>commit 5403ec29903942b692aabef404598b8dd3577f8a\n\n    added is_alpaca dataset\n</code></pre> <p>So, all we have to do now is <code>git checkout 5403ec29903942b692aabef404598b8dd3577f8a</code> and we are good to reproduce the model results! </p> <p>Checkout our article about ML Data Version Control and Reproducibility at Scale to get another example for how lakeFS and Git work seamlessly together.     </p>"},{"location":"howto/migrate-away/","title":"Migrating away from lakeFS","text":""},{"location":"howto/migrate-away/#copying-data-from-a-lakefs-repository-to-an-s3-bucket","title":"Copying data from a lakeFS repository to an S3 bucket","text":"<p>The simplest way to migrate away from lakeFS is by copying data from a lakeFS repository to an S3 bucket (or any other object store).</p> <p>For smaller repositories, you can do this by using the AWS CLI or Rclone. For larger repositories, running distcp with lakeFS as the source is also an option.</p>"},{"location":"howto/mirroring/","title":"Mirroring","text":"<p>Info</p> <p>Mirroring is only available for lakeFS Cloud.</p>"},{"location":"howto/mirroring/#what-is-lakefs-mirroring","title":"What is lakeFS mirroring?","text":"<p>Mirroring in lakeFS allows replicating a lakeFS repository (\"source\") into read-only copies (\"mirror\") in different locations.</p> <p>Unlike conventional mirroring, data isn't simply copied between regions - lakeFS Cloud tracks the state of each commit, advancing the commit log on the mirror only once a commit has been fully replicated and all data is available.</p> <p></p>"},{"location":"howto/mirroring/#uses-cases","title":"Uses cases","text":""},{"location":"howto/mirroring/#disaster-recovery","title":"Disaster recovery","text":"<p>Typically, object stores provide a replication/batch copy API to allow for disaster recovery: as new objects are written, they are asynchronously copied to other geo locations. </p> <p>In the case of regional failure, users can rely on the other geo-locations which should contain relatively-up-to-date state.</p> <p>The problem is reasoning about what managed to arrive by the time of disaster and what hasn't:</p> <ul> <li>have all the necessary files for a given dataset arrived?</li> <li>in cases there are dependencies between datasets, are all dependencies also up to date?</li> <li>what is currently in-flight or haven't even started replicating yet?</li> </ul> <p>Reasoning about these is non-trivial, especially in the face of a regional disaster, however ensuring business continuity might require that we have these answers.</p> <p>Using lakeFS mirroring makes it much easier to answer: we are guaranteed that the latest commit that exists in the replica is in a consistent state and is fully usable, even if it isn't the absolute latest commit - it still reflects a known, consistent, point in time.</p>"},{"location":"howto/mirroring/#data-locality","title":"Data Locality","text":"<p>For certain workloads, it might be cheaper to have data available in multiple regions: Expensive hardware such as GPUs might fluctuate in price, so we'd want to pick the region that currently offers the best pricing. The difference could easily offset to cost of the replicated data.</p> <p>The challenge is reproducibility - Say we have an ML training job that reads image files from a path in the object store. Which files existed at the time of training?</p> <p>If data is constantly flowing between regions, this might be harder to answer than we think. And even if we know - how can we recreate that exact state if we want to run the process again (for example, to rebuild that model for troubleshooting).</p> <p>Using consistent commits solves this problem - with lakeFS mirroring, it is guaranteed that a commit ID, regardless of location, will always contain the exact same data.</p> <p>We can train our model in region A, and a month later feed the same commit ID into another region - and get back the same results.</p>"},{"location":"howto/mirroring/#setting-up-mirroring","title":"Setting up mirroring","text":""},{"location":"howto/mirroring/#configuring-bucket-replication-on-s3","title":"Configuring bucket replication on S3","text":"<p>The objects within the repository are copied using your cloud provider's object store replication mechanism. For AWS S3, please refer to the AWS S3 replication documentation to make sure your lakeFS repository's storage namespace (source) is replicated to the region you'd like your mirror to be located on (target).</p> <p>After setting the replication rule, new objects will be replicated to the destination bucket. </p> <p>In order to replicate the existing objects, we'd need to manually copy them - however, we can use S3 batch jobs to do this.</p>"},{"location":"howto/mirroring/#creating-a-lakefs-user-with-a-replicator-policy","title":"Creating a lakeFS user with a \"replicator\" policy","text":"<p>On our source lakeFS installation, under Administration create a new user that will be used by the replication subsystem. The user should have the following RBAC policy attached:</p> <pre><code>{\n   \"id\": \"ReplicationPolicy\",\n   \"statement\": [\n      {\n         \"action\": [\n            \"fs:ReadRepository\",\n            \"fs:CreateRepository\",\n            \"fs:UpdateRepository\",\n            \"fs:DeleteRepository\",\n            \"fs:ListRepositories\",\n            \"fs:AttachStorageNamespace\",\n            \"fs:ReadObject\",\n            \"fs:WriteObject\",\n            \"fs:DeleteObject\",\n            \"fs:ListObjects\",\n            \"fs:CreateCommit\",\n            \"fs:ReadCommit\",\n            \"fs:ListCommits\",\n            \"fs:CreateBranch\",\n            \"fs:DeleteBranch\",\n            \"fs:RevertBranch\",\n            \"fs:ReadBranch\",\n            \"fs:ListBranches\"\n         ],\n         \"effect\": \"allow\",\n         \"resource\": \"*\"\n      }\n   ]\n}\n</code></pre> <p>Alternatively, we can create a policy with a narrower scope, only for a specific repository and/or mirror:</p> <pre><code>{\n   \"id\": \"ReplicationPolicy\",\n   \"statement\": [\n      {\n         \"action\": [\n            \"fs:ListRepositories\"\n         ],\n         \"effect\": \"allow\",\n         \"resource\": \"*\"\n      },\n      {\n         \"action\": [\n            \"fs:ReadRepository\",\n            \"fs:ReadObject\",\n            \"fs:ListObjects\",\n            \"fs:ReadCommit\",\n            \"fs:ListCommits\",\n            \"fs:ReadBranch\",\n            \"fs:ListBranches\"\n         ],\n         \"effect\": \"allow\",\n         \"resource\": \"arn:lakefs:fs:::repository/{sourceRepositoryId}\"\n      },\n      {\n         \"action\": [\n            \"fs:ReadRepository\",\n            \"fs:CreateRepository\",\n            \"fs:UpdateRepository\",\n            \"fs:DeleteRepository\",\n            \"fs:AttachStorageNamespace\",\n            \"fs:ReadObject\",\n            \"fs:WriteObject\",\n            \"fs:DeleteObject\",\n            \"fs:ListObjects\",\n            \"fs:CreateCommit\",\n            \"fs:ReadCommit\",\n            \"fs:ListCommits\",\n            \"fs:CreateBranch\",\n            \"fs:DeleteBranch\",\n            \"fs:RevertBranch\",\n            \"fs:ReadBranch\",\n            \"fs:ListBranches\"\n         ],\n         \"effect\": \"allow\",\n         \"resource\": \"arn:lakefs:fs:::repository/{mirrorId}\"\n      },\n      {\n         \"action\": [\n            \"fs:AttachStorageNamespace\"\n         ],\n         \"effect\": \"allow\",\n         \"resource\": \"arn:lakefs:fs:::namespace/{DestinationStorageNamespace}\"\n      }\n   ]\n}\n</code></pre> <p>Once a user has been created and the replication policy attached to it, create an access key and secret to be used by the mirroring process.</p>"},{"location":"howto/mirroring/#authorizing-the-lakefs-mirror-process-to-use-the-replication-user","title":"Authorizing the lakeFS Mirror process to use the replication user","text":"<p>Please contact Treeverse customer success to connect the newly created user with the mirroring process  </p>"},{"location":"howto/mirroring/#configuring-repository-replication","title":"Configuring repository replication","text":"<p>Replication has a stand-alone HTTP API. In this example, we'll use cURL, but feel free to use any HTTP client or library:</p> <p><pre><code>curl --location 'https://&lt;ORGANIZATION_ID&gt;.&lt;SOURCE_REGION&gt;.lakefscloud.io/service/replication/v1/repositories/&lt;SOURCE_REPO&gt;/mirrors' \\\n--header 'Content-Type: application/json' \\\n-u &lt;ACCESS_KEY_ID&gt;:&lt;SECRET_ACCESS_KEY&gt; \\\n-X POST \\\n--data '{\n    \"name\": \"&lt;MIRROR_NAME&gt;\",\n    \"region\": \"&lt;MIRROR_REGION&gt;\",\n    \"storage_namespace\": \"&lt;MIRROR_STORAGE_NAMESPACE&gt;\"\n}'\n</code></pre> Using the following parameters:</p> <ul> <li><code>ORGANIZATION_ID</code> - The ID as it appears in the URL of your lakeFS installation (e.g. <code>https://my-org.us-east-1.lakefscloud.io/</code>)</li> <li><code>SOURCE_REGION</code> - The region where our source repository is hosted</li> <li><code>SOURCE_REPO</code> - Name of the repository acting as our replication source. It should exist</li> <li><code>ACCESS_KEY_ID</code> &amp; <code>SECRET_ACCESS_KEY</code> - Credentials for your lakeFS user (make sure you have the necessary RBAC permissions as listed below)</li> <li><code>MIRROR_NAME</code> - Name used for the read-only mirror to be created on the destination region</li> <li><code>MIRROR_STORAGE_NAMESPACE</code> - Location acting as the replication target for the storage namespace of our source repository</li> </ul>"},{"location":"howto/mirroring/#mirroring-and-garbage-collection","title":"Mirroring and Garbage Collection","text":"<p>Garbage collection won't run on mirrored repositories.  Deletions from garbage collection should be replicated from the source:</p> <ol> <li>Enable DELETED marker replication on the source bucket.</li> <li>Create a lifecycle policy on the destination bucket to delete the objects with the DELETED marker.</li> </ol>"},{"location":"howto/mirroring/#rbac","title":"RBAC","text":"<p>These are the required RBAC permissions for working with the new cross-region replication feature:</p> <p>Creating a Mirror:</p> Action ARN <code>fs:CreateRepository</code> <code>arn:lakefs:fs:::repository/{mirrorId}</code> <code>fs:MirrorRepository</code> <code>arn:lakefs:fs:::repository/{sourceRepositoryId}</code> <code>fs:AttachStorageNamespace</code> <code>arn:lakefs:fs:::namespace/{storageNamespace}</code> <p>Editing Mirrored Branches:</p> Action ARN <code>fs:MirrorRepository</code> <code>arn:lakefs:fs:::repository/{sourceRepositoryId}</code> <p>Deleting a Mirror:</p> Action ARN <code>fs:DeleteRepository</code> <code>arn:lakefs:fs:::repository/{mirrorId}</code> <p>Listing/Getting Mirrors for a Repository:</p> Action ARN <code>fs:ListRepositories</code> <code>*</code>"},{"location":"howto/mirroring/#other-replication-operations","title":"Other replication operations","text":""},{"location":"howto/mirroring/#listing-all-mirrors-for-a-repository","title":"Listing all mirrors for a repository","text":"<pre><code>curl --location 'https://&lt;ORGANIZATION_ID&gt;.&lt;SOURCE_REGION&gt;.lakefscloud.io/service/replication/v1/repositories/&lt;SOURCE_REPO&gt;/mirrors' \\\n-u &lt;ACCESS_KEY_ID&gt;:&lt;SECRET_ACCESS_KEY&gt; -s\n</code></pre>"},{"location":"howto/mirroring/#getting-a-specific-mirror","title":"Getting a specific mirror","text":"<pre><code>url --location 'https://&lt;ORGANIZATION_ID&gt;.&lt;SOURCE_REGION&gt;.lakefscloud.io/service/replication/v1/repositories/&lt;SOURCE_REPO&gt;/mirrors/&lt;MIRROR_ID&gt;' \\\n-u &lt;ACCESS_KEY_ID&gt;:&lt;SECRET_ACCESS_KEY&gt; -s\n</code></pre>"},{"location":"howto/mirroring/#deleting-a-specific-mirror","title":"Deleting a specific mirror","text":"<pre><code>curl --location --request DELETE 'https://&lt;ORGANIZATION_ID&gt;.&lt;SOURCE_REGION&gt;.lakefscloud.io/service/replication/v1/repositories/&lt;SOURCE_REPO&gt;/mirrors/&lt;MIRROR_ID&gt;' \\\n-u &lt;ACCESS_KEY_ID&gt;:&lt;SECRET_ACCESS_KEY&gt;\n</code></pre>"},{"location":"howto/mirroring/#limitations","title":"Limitations","text":"<ol> <li>Mirroring is currently only supported on AWS S3 and lakeFS Cloud for AWS</li> <li>Read-only mirrors cannot be written to. Mirroring is one-way, from source to destination(s)</li> <li>Currently, only branches are mirrored. Tags and arbitrary commits that do not belong to any branch are not replicated</li> <li>lakeFS Hooks will only run on the source repository, not its replicas</li> <li>Replication is still asynchronous: reading from a branch will always return a valid commit that the source has pointed to, but it is not guaranteed to be the latest commit the source branch is pointing to.</li> </ol>"},{"location":"howto/multiple-storage-backends/","title":"Multi-Storage Backend","text":"<p>Info</p> <p>Multi-storage backend support is only available to licensed lakeFS Enterprise customers. Contact us to get started!</p>"},{"location":"howto/multiple-storage-backends/#what-is-multi-storage-backend-support","title":"What is Multi-storage Backend Support?","text":"<p>lakeFS multi-storage backend support enables seamless data management across multiple storage systems \u2014 on-premises, across public clouds, or hybrid environments. This capability makes lakeFS a unified data management platform for all organizational data assets, which is especially critical in AI/ML environments that rely on diverse datasets stored in multiple locations.</p> <p>With a multi-store setup, lakeFS can connect to and manage any combination of supported storage systems, including:</p> <ul> <li>AWS S3</li> <li>Azure Blob</li> <li>Google Cloud Storage</li> <li>other S3-compatible storage</li> <li>local storage</li> </ul> <p>Note</p> <p>Multi-storage backends support is available from version 1.51.0 of lakeFS Enterprise.</p>"},{"location":"howto/multiple-storage-backends/#use-cases","title":"Use Cases","text":"<ol> <li>Distributed Data Management:<ul> <li>Eliminate data silos and enable seamless cross-cloud collaboration.</li> <li>Maintain version control across different storage providers for consistency and reproducibility.</li> <li>Ideal for AI/ML environments where datasets are distributed across multiple storage locations.</li> </ul> </li> <li>Unified Data Access:<ul> <li>Access data across multiple storage backends using a single, consistent URI format.</li> </ul> </li> <li>Centralized Access Control &amp; Governance:<ul> <li>Access permissions and policies can be centrally managed across all connected storage systems using lakeFS RBAC.</li> <li>Compliance and security controls remain consistent, regardless of where the data is stored.</li> </ul> </li> </ol>"},{"location":"howto/multiple-storage-backends/#configuration","title":"Configuration","text":"<p>To configure your lakeFS server to connect to multiple storage backends, define them under the <code>blockstores</code> section in your server configurations. The <code>blockstores.stores</code> field is an array of storage backends, each with its own configuration.</p> <p>For a complete list of available options, refer to the server configuration reference.</p> <p>Note</p> <p>If you're upgrading from a single-store lakeFS setup, refer to the upgrade guidelines to ensure a smooth transition.</p>"},{"location":"howto/multiple-storage-backends/#example-configurations","title":"Example Configurations","text":"On-PremMulti-CloudHybrid <p>This example setup configures lakeFS to manage data across two separate MinIO instances:</p> <pre><code>blockstores:\nsigning:\n    secret_key: \"some-secret\"\nstores:\n    - id: \"minio-prod\"\n    description: \"Primary on-prem MinIO storage for production data\"\n    type: \"s3\"\n    s3:\n        force_path_style: true\n        endpoint: 'http://minio-prod.local'\n        discover_bucket_region: false\n        credentials:\n        access_key_id: \"prod_access_key\"\n        secret_access_key: \"prod_secret_key\"\n    - id: \"minio-backup\"\n    description: \"Backup MinIO storage for disaster recovery\"\n    type: \"s3\"\n    s3:\n        force_path_style: true\n        endpoint: 'http://minio-backup.local'\n        discover_bucket_region: false\n        credentials:\n        access_key_id: \"backup_access_key\"\n        secret_access_key: \"backup_secret_key\"\n</code></pre> <p>This example setup configures lakeFS to manage data across two public cloud providers: AWS and Azure:</p> <pre><code>blockstores:\nsigning:\n    secret_key: \"some-secret\"\nstores:\n    - id: \"s3-prod\"\n    description: \"AWS S3 storage for production data\"\n    type: \"s3\"\n    s3:\n        region: \"us-east-1\"\n    - id: \"azure-analytics\"\n    description: \"Azure Blob storage for analytics data\"\n    type: \"azure\"\n    azure:\n        storage_account: \"analytics-account\"\n        storage_access_key: \"EXAMPLE45551FSAsVVCXCF\"\n</code></pre> <p>This hybrid setup allows lakeFS to manage data across both cloud and on-prem storages.</p> <pre><code>blockstores:\nsigning:\n    secret_key: \"some-secret\"\nstores:\n    - id: \"s3-archive\"\n    description: \"AWS S3 storage for long-term archival\"\n    type: \"s3\"\n    s3:\n        region: \"us-west-2\"\n    - id: \"minio-fast-access\"\n    description: \"On-prem MinIO for high-performance workloads\"\n    type: \"s3\"\n    s3:\n        force_path_style: true\n        endpoint: 'http://minio.local'\n        discover_bucket_region: false\n        credentials:\n        access_key_id: \"minio_access_key\"\n        secret_access_key: \"minio_secret_key\"\n</code></pre>"},{"location":"howto/multiple-storage-backends/#key-considerations","title":"Key Considerations","text":"<ul> <li>Unique Blockstore IDs: Each storage must have a unique id.</li> <li>Persistence of Blockstore IDs: Once defined, an id must not change.</li> <li>S3 Authentication Handling:</li> <li>All standard S3 authentication methods are supported.</li> <li>Every blockstore needs to be authenticated.  So make sure to configure a profile or static credentials for all storages of type <code>s3</code>.       S3 storage will use the credentials chain by default, so you might be able to use that for one storage.</li> </ul> <p>Warning</p> <p>Changing a storage ID is not supported and may result in unexpected behavior. Ensure IDs remain consistent once configured.</p>"},{"location":"howto/multiple-storage-backends/#upgrading-from-a-single-storage-backend-to-multiple-storage-backends","title":"Upgrading from a single storage backend to Multiple Storage backends","text":"<p>When upgrading from a single storage backend to a multi-storage setup, follow these guidelines:</p> <ul> <li>Use the new <code>blockstores</code> structure, replacing the existing <code>blockstore</code> configuration. Note that <code>blockstore</code> and <code>blockstores</code>   configurations are mutually exclusive - lakeFS does not support both simultaneously.</li> <li>Define all previously available single-blockstore settings under their respective storage backends.</li> <li>The <code>signing.secret_key</code> is a required setting global to all connected stores.</li> <li>Set <code>backward_compatible: true</code> for the existing storage backend to ensure:</li> <li>Existing repositories continue to use the original storage backend.</li> <li>Newly created repositories default to this backend unless explicitly assigned a different one, to ensure a non-breaking upgrade process.</li> <li>This setting is mandatory \u2014 lakeFS will not function if it is unset.</li> <li>Do not remove this setting as long as you need to support repositories created before the upgrade.     If removed, lakeFS will fail to start because it will treat existing repositories as disconnected from any configured storage.</li> </ul>"},{"location":"howto/multiple-storage-backends/#adding-or-removing-a-storage-backend","title":"Adding or Removing a Storage Backend","text":"<p>To add a storage backend, update the server configuration with the new storage entry and restart the server.</p> <p>To remove a storage backend:</p> <ul> <li>Delete all repositories associated with the storage backend. (definition only)</li> <li>Remove the storage entry from the configuration.</li> <li>Restart the server.</li> </ul> <p>Warning</p> <p>lakeFS will fail to start if there are repositories defined on a removed storage. Ensure all necessary cleanup is completed before removing a storage backend.</p>"},{"location":"howto/multiple-storage-backends/#listing-connected-storage-backends","title":"Listing Connected Storage Backends","text":"<p>The Get Config API endpoint returns a list of storage configurations. In multi-storage setups, this is the recommended method to list connected storage backends and view their details.</p>"},{"location":"howto/multiple-storage-backends/#troubleshooting","title":"Troubleshooting","text":"Issue Cause Solution Blockstore ID conflicts Duplicate <code>id</code> values in <code>stores</code> Ensure each storage backend has a unique ID Missing <code>backward_compatible</code> Upgrade from single to multi-storage without setting the flag Add <code>backward_compatible: true</code> for the existing storage Unsupported configurations in OSS or unlicensed Enterprise accounts Using multi-storage features in an unsupported setup Contact us to start using the feature"},{"location":"howto/multiple-storage-backends/#migrating-from-multiple-storage-backend-to-single-storage-backend","title":"Migrating from Multiple Storage Backend to Single Storage Backend","text":"<p>Once you upgrade to a multi-storage setup, you cannot simply revert back by changing the configuration from <code>blockstores</code> to <code>blockstore</code>. The internal repository metadata format changes to support multiple storage backends, and is not backward compatible with the single storage format. If you need to consolidate your data and revert from a multi-storage setup to a single storage backend, you'll need to perform a full migration by following these steps:</p>"},{"location":"howto/multiple-storage-backends/#overview","title":"Overview","text":"<p>The migration process involves:</p> <ol> <li>Dumping repository references from the multi-storage setup</li> <li>Deleting repositories in the multi-storage environment</li> <li>Configuring lakeFS with a single storage backend</li> <li>[Optional]: Copying repository data to the new single storage location</li> <li>Restoring repositories to the single storage environment</li> </ol>"},{"location":"howto/multiple-storage-backends/#step-by-step-guide","title":"Step-by-Step Guide","text":"<p>Use the <code>lakefs-refs.py</code> script, instruction on how to aquire found in Backup and Restore.</p> <ol> <li> <p>Dump Repository References     To dump the repository metadata:</p> <pre><code># Dump a single repository\npython lakefs-refs.py dump my-repository\n\n# Or dump all repositories\npython lakefs-refs.py dump --all\n</code></pre> <p>This will create manifest files for each repository.</p> <p>Optionally, you can use the <code>--rm</code> flag to automatically delete repositories after successful dump:</p> <pre><code># Dump and delete a single repository\npython lakefs-refs.py dump my-repository --rm\n\n# Or dump and delete all repositories\npython lakefs-refs.py dump --all --rm\n</code></pre> </li> <li> <p>Delete Source Repositories (if not using --rm flag)</p> <p>If you didn't use the <code>--rm</code> flag in step 1, you'll need to delete the repositories manually. Note that deleting a repository only removes the repository record from lakeFS - it does not delete the actual data files or metadata from your storage.</p> <p>You can delete repositories through:</p> <p>a. The lakeFS UI b. Using lakectl (ex: <code>lakectl repo delete lakefs://my-repository</code>)</p> </li> <li> <p>Configure lakeFS Single Storage</p> <ul> <li>Update your lakeFS configuration to use a single storage backend (using the <code>blockstore</code> section instead of <code>blockstores</code>).</li> <li>Start or restart lakeFS after applying the new configuration</li> <li>Example of single storage backend configuration: (multi storage backend example can be found in the Configuration section)</li> </ul> <pre><code>...\nblockstore:\n    type: s3\n    s3:\n    region: us-east-1\n...\n</code></pre> </li> <li> <p>Copy Repository Data (if needed)</p> <p>If the repositories you want to restore were created on a storage system that lakeFS is no longer connected to:</p> <p>a. Copy data from the old storage locations to the new one:</p> <pre><code># Example for S3\naws s3 sync s3://old-bucket/path/to/storge-namespace s3://new-bucket/path/to/storage-namespace\n\n# Alternative: Using rclone for cross-provider transfers\n# rclone supports various storage providers (S3, Azure, GCS, etc.)\nrclone sync azure:old-container/path/to/storage-namespace aws:new-bucket/path/to/storage-namespace\n</code></pre> <p>b. Update the manifest file, created on step 1 (dump repository references) with the new storage namespace:</p> <pre><code>{\n    \"repository\": {\n    \"name\": \"my-repository\",\n    \"storage_namespace\": \"s3://new-bucket/path/to/repo\", // ... update here ...\n    \"default_branch\": \"main\",\n    \"storage_id\": \"storage-1\"\n    },\n    \"refs\": {\n    // ... existing refs data ...\n    }\n}\n</code></pre> </li> <li> <p>Restore Repositories     !!! note         If you copied the data to a new location in step 4, make sure to update the storage namespace in the manifest files before restoring.</p> <p>Use the <code>--ignore-storage-id</code> flag to ensure repositories are created without storage IDs in the single-storage environment:</p> <pre><code># Restore a single repository\npython lakefs-refs.py restore my-repository_manifest.json --ignore-storage-id\n\n# Or restore multiple repositories\npython lakefs-refs.py restore repo1_manifest.json repo2_manifest.json --ignore-storage-id\n</code></pre> </li> </ol>"},{"location":"howto/multiple-storage-backends/#important-notes","title":"Important Notes","text":"<ul> <li>Keep the manifest files safe as they contain repository metadata</li> <li>If using different storage backends, ensure proper access permissions to copy the data</li> <li>The <code>--commit</code> flag can be used if you want to ensure all changes are committed before dumping</li> <li>Make sure the new storage backend has sufficient space for all repository data</li> <li>A lakeFS instance configured with a single storage type will not start if repositories created on multiple storage setup still exist</li> </ul>"},{"location":"howto/multiple-storage-backends/#working-with-repositories","title":"Working with Repositories","text":"<p>After setting up lakeFS Enterprise to connect with multiple storage backends, this section explains how to use these connected storages when working with lakeFS.</p> <p>With multiple storage backends configured, lakeFS repositories are now linked to a specific storage. Together with the repository's storage namespace, this defines the exact location in the underlying storage where the repository's data is stored.</p> <p>The choice of storage backend impacts the following lakeFS operations:</p>"},{"location":"howto/multiple-storage-backends/#creating-a-repository","title":"Creating a Repository","text":"<p>In a multi-storage setup, users must specify a storage ID when creating a repository. This can be done using the following methods:</p> UICLIAPIHigh-Level Python SDK <p>Select a storage backend from the dropdown menu. </p> <p>Use the <code>--storage-id</code> flag with the repo create command:</p> <pre><code>lakectl repo create lakefs://my-repo s3://my-bucket --storage-id my-storage\n</code></pre> <p>Note</p> <p>The <code>--storage-id</code> flag is currently hidden in the CLI.</p> <p>Use the <code>storage_id</code> parameter in the Create Repository endpoint.</p> <p>Starting from version 0.9.0 of the High-level Python SDK, you can use <code>kwargs</code> to pass <code>storage_id</code> dynamically when calling the create repository method:</p> <pre><code>import lakefs\n\nrepo = lakefs.Repository(\"example-repo\").create(storage_namespace=\"s3://storage-bucket/repos/example-repo\", storage_id=\"my-storage-id\")\n</code></pre> <p>Important Notes</p> <p>*In multi-storage setups where a storage backend is marked as <code>backward_compatible: true</code>, repository creation requests without a storage ID will default to this storage. * If no storage backend is marked as <code>backward_compatible</code>, repository creation requests without a storage ID will fail. * Each repository is linked to a single backend and stores data within a single storage namespace on that backend.</p>"},{"location":"howto/multiple-storage-backends/#viewing-repository-details","title":"Viewing Repository Details","text":"<p>To check which storage backend is associated with a repository:</p> UIAPI <p>The storage ID is displayed under \"Storage\" in the repository settings page. </p> <p>Use the List Repositories endpoint. Its response includes the storage ID.</p>"},{"location":"howto/multiple-storage-backends/#importing-data-into-a-repository","title":"Importing Data into a Repository","text":"<p>Importing data into a repository is supported when the credentials used for the repository's backing blockstore allow read and list access to the storage location.</p>"},{"location":"howto/multiple-storage-backends/#limitations","title":"Limitations","text":""},{"location":"howto/multiple-storage-backends/#supported-storages","title":"Supported storages","text":"<p>Multi-storage backend support has been validated on:</p> <ul> <li>Self-managed S3-compatible object storage (MinIO)</li> <li>Amazon S3</li> <li>Local storage</li> </ul> <p>Warning</p> <p>Other storage backends may work but have not been officially tested. If you're interested in exploring additional configurations, please reach contact us.</p>"},{"location":"howto/multiple-storage-backends/#unsupported-clients","title":"Unsupported clients","text":"<p>The following clients do not currently support working with multiple storage backends. However, we are actively working to bridge this gap:</p> <ul> <li>Spark-based GC</li> <li>Spark client</li> <li>lakeFS Hadoop FileSystem</li> <li>Everest</li> </ul>"},{"location":"howto/private-link/","title":"Private Link","text":"<p>Info</p> <p>PrivateLink is only applicable to lakeFS Cloud</p> <p>Private Link enables lakeFS Cloud to interact with your infrastructure using private networking.</p>"},{"location":"howto/private-link/#supported-vendors","title":"Supported Vendors","text":"<p>At the moment, we support Private-Link with AWS and Azure. If you are looking for Private Link for GCP please contact us.</p> AWSAzure <p>Azure Private Link enables secure access to Azure services from a private endpoint within your virtual network. By using Azure Private Link with lakeFS, you can securely access lakeFS services without exposing traffic to the public internet. In this manual, we will guide you through the steps to enable Azure Private Link to your lakeFS instance.</p>"},{"location":"howto/private-link/#access-methods","title":"Access Methods","text":"<p>There are two types of Private Link implementation:</p> <ul> <li> <p>Front-End Access refers to API and UI access. Use this option if you'd like your lakeFS application to be exposed only to your infrastructure and not to the whole internet.</p> </li> <li> <p>Back-End Access refers to the network communication between the lakeFS clusters we host, and your infrastructure. Use this option if you'd like lakeFS to communicate with your servers privately and not over the internet.</p> </li> </ul> <p>The two types of access are not mutually exclusive nor are they dependent on each other.</p>"},{"location":"howto/private-link/#setting-up-private-link","title":"Setting up Private Link","text":""},{"location":"howto/private-link/#front-end-access","title":"Front-End Access","text":"<p>Prerequisites:</p> <ul> <li>Administrator access to your AWS account</li> <li>In order for us to communicate with your account privately, we'll need to create a service endpoint on our end first.</li> </ul> <p>Steps:</p> <ol> <li>Login to your AWS account</li> <li>Go to AWS VPC Service</li> <li>Filter the relevant VPC &amp; Navigate to Endpoints</li> <li>Click Create endpoint</li> <li>Fill in the following:<ul> <li>Name: lakefs-cloud</li> <li>Service category: Other endpoint services</li> <li>Service name: input from Treeverse team (see prerequisites)</li> <li>Click Verify service</li> <li>Pick the VPC you'd like to expose this service to.</li> <li>Click Create endpoint</li> </ul> </li> </ol> <p>Now you can access your infrastructure privately using the endpoint DNS name. If you would like to change the DNS name to a friendly one please contact support@treeverse.io.</p>"},{"location":"howto/private-link/#back-end-access","title":"Back-End Access","text":"<p>Prerequisites:</p> <ul> <li>Administrator access to your AWS account</li> </ul> <p>Steps:</p> <ol> <li>Login to your AWS account</li> <li>Go to AWS VPC Service</li> <li>Filter the relevant VPC &amp; Navigate to Endpoints</li> <li>Click endpoint service</li> <li>Fill in the following:<ul> <li>Name: lakefs-cloud</li> <li>Load Balancer Type: Network</li> <li>Available load balancers: pick the load balancer you'd like lakefs-cloud to send events to.</li> <li>Click Create</li> </ul> </li> <li>Pick the newly created Endpoint Service from within the Endpoint Services page.</li> <li>Navigate to the Allow principals tab.</li> <li>Click Allow principals</li> <li>Fill in the following ARN: <code>arn:aws:iam::924819537486:root</code></li> <li>Click Allow principals</li> </ol> <p>That's it on your end! Now, we'll need the service name you've just created in order to associate it with our infrastructure, once we do, we'll be ready to use the back-end access privately.</p>"},{"location":"howto/private-link/#register-your-azure-subscription","title":"Register your Azure subscription","text":"<p>To automatically approve private endpoint connections to the lakeFS network, please provide us with your subscription. If required, you can register multiple subscriptions.</p>"},{"location":"howto/private-link/#create-an-azure-private-link-connection-to-lakefs-cloud","title":"Create an Azure Private Link connection to lakeFS Cloud","text":"<p>Once your subscription is in our trusted subscriptions navigate to the Azure portal and do the following steps:</p> <ol> <li>Navigate to the private endpoint</li> <li>Click Create</li> <li>On the first step (basics):<ul> <li>Select your subscription</li> <li>Specify the desired resource group used to access lakeFS</li> <li>Provide a name for your private endpoint instance</li> <li>Specify the region of your lakeFS instance</li> </ul> </li> <li>On the second step (Resource)<ul> <li>In connection method select <code>connect to an Azure resource by resource ID or alias</code></li> <li>Insert the alias provided by us into the Resource ID or alias</li> <li>No need to add a request message</li> </ul> </li> <li>Continue with the steps and run Review + Create</li> </ol>"},{"location":"howto/private-link/#create-a-dns-entry-for-your-private-endpoint","title":"Create a DNS entry for your private endpoint","text":"<p>Update your DNS server to resolve your account URL (which will be provided by us) to the Private Link IP address. You can add the DNS entry to your on-premises DNS server or private DNS on your VNet, to access lakeFS services.</p>"},{"location":"howto/protect-branches/","title":"Branch Protection Rules","text":"<p>Define branch protection rules to prevent direct changes and commits to specific branches. Only merges are allowed into protected branches. Together with the power of pre-merge hooks, you can run validations on your data before it reaches your important branches and is exposed to consumers.</p> <p>You can create rules for a specific branch or any branch that matches a name pattern you specify with glob syntax (supporting <code>?</code> and <code>*</code> wildcards).</p>"},{"location":"howto/protect-branches/#how-it-works","title":"How it works","text":"<p>When at least one protection rule applies to a branch, the branch is protected. The following operations will fail on protected branches:</p> <ol> <li>Object write operations: upload and delete objects.</li> <li>Branch operations: commit and reset uncommitted changes.</li> </ol> <p>To operate on a protected branch, merge commits from other branches into it. Use pre-merge hooks to validate the changes before they are merged.</p> <p>Note</p> <p>Reverting a previous commit using <code>lakectl branch revert</code> is allowed on a protected branch.</p>"},{"location":"howto/protect-branches/#managing-branch-protection-rules","title":"Managing branch protection rules","text":"<p>This section explains how to use the lakeFS UI to manage rules. You can also use the command line and API.</p>"},{"location":"howto/protect-branches/#reaching-the-branch-protection-rules-page","title":"Reaching the branch protection rules page","text":"<ol> <li>On lakeFS, navigate to the main page of the repository.</li> <li>Click on the Settings tab.</li> <li>In the left menu, click Branches.</li> </ol>"},{"location":"howto/protect-branches/#adding-a-rule","title":"Adding a rule","text":"<p>To add a new rule, click the Add button. In the dialog, enter the branch name pattern and then click Create.</p> <p></p>"},{"location":"howto/protect-branches/#deleting-a-rule","title":"Deleting a rule","text":"<p>To delete a rule, click the Delete button next to it.</p> <p></p>"},{"location":"howto/pull-requests/","title":"Pull Requests","text":"<p>A pull request is a proposal to merge a set of changes from one branch into another. In a pull request, collaborators can review the proposed set of changes before they integrate the changes. Pull requests display the differences, or diffs, between the content in the source branch and the content in the target branch.</p>"},{"location":"howto/pull-requests/#open-a-pull-request","title":"Open a Pull Request","text":"<p>Create a branch, and make all the necessary changes in that branch. When your changes are ready for review, head over to the Pull Requests tab in your repository. Choose your source branch and target branch, add a title and description (optional, and markdown is supported).</p> <p></p> <p>When ready, click Create Pull Request. You will be redirected to the newly created pull request page.</p>"},{"location":"howto/pull-requests/#review-changes","title":"Review Changes","text":"<p>Run validation checks or automated data quality tests to ensure that the changes meet your standards.</p> <p></p> <p>Every Pull Request is assigned a unique ID. You can share the Pull Request's URL with others to review the change.</p> <p>As with any lakeFS reference, reviewers can take the source branch, query, test and modify it as necessary prior to merging.</p>"},{"location":"howto/pull-requests/#merge-or-close","title":"Merge or Close","text":"<p>Once the review is complete and all checks have passed, click the Merge pull request button to merge the changes into the target branch.</p> <p></p> <p>The data is now updated in a controlled and transparent manner.</p> <p>If the changes are not ready to be merged, you can close the pull request without merging the changes, by clicking the Close pull request button.</p>"},{"location":"howto/pull-requests/#view-pull-requests","title":"View Pull Requests","text":"<p>You can view all open and closed pull requests in the Pull Requests tab in your repository. The tabs (Open, Closed) allow you to filter the list of pull requests according to their status.</p>"},{"location":"howto/scim/","title":"System for Cross-domain Identity Management (SCIM)","text":"<p>Info</p> <p>SCIM support is available on lakeFS Cloud and lakeFS Enterprise</p> <p>lakeFS Cloud includes an SCIM v2.0 compliant server, which can integrate with SCIM clients (IdPs) to automate provisioning/de-provisioning of users and groups.  </p>"},{"location":"howto/scim/#officially-supported-clients-idps-capabilities-and-limitations","title":"Officially Supported Clients (IdPs), Capabilities, and Limitations","text":""},{"location":"howto/scim/#supported-clients-idps","title":"Supported Clients (IdPs)","text":"<p>Currently, the lakeFS Cloud SCIM server has been tested and validated with Entra ID (a.k.a Azure AD). However, with SCIM v2.0 being an accepted standard, any SCIM-compliant IdP should be able to integrate with lakeFS Cloud.</p>"},{"location":"howto/scim/#capabilities","title":"Capabilities","text":""},{"location":"howto/scim/#user-provisioning","title":"User Provisioning","text":"<ul> <li>Create users: Users and members of groups assigned to the application will be provisioned in lakeFS Cloud</li> <li>Update user attributes: Changes to supported user attributes are synced to lakeFS Cloud</li> <li>Deactivate users: Deactivating a user or removing their assignment to the application will disable them in lakeFS Cloud</li> <li>User adoption: Users that are already found in lakeFS Cloud will be \"adopted\" by the IdP and not re-created</li> </ul>"},{"location":"howto/scim/#group-provisioning","title":"Group Provisioning","text":"<ul> <li>Create groups: Groups assigned to the application are created in lakeFS Cloud and any group user members are created and added to the group in lakeFS Cloud</li> <li>Update group name: When a synced group is renamed in the IdP, it will be renamed in lakeFS Cloud</li> <li>Add/remove members: When members are added/removed from an assigned group, they will be added/removed from the group in lakeFS Cloud</li> <li>Group adoption: Groups that already exist in lakeFS Cloud will be \"adopted\" by the IdP and not re-created</li> </ul>"},{"location":"howto/scim/#user-attributes-and-consent","title":"User Attributes and Consent","text":"<p>The lakeFS Cloud SCIM server requires the minimum set of user attributes required for provisioning. The required attributes are a sub-set of the basic user profile, which is exchanged during federated authentication/SSO login. User consent is requested by the IdP upon first login to lakeFS Cloud.</p>"},{"location":"howto/scim/#known-limitations","title":"Known Limitations","text":"<ul> <li>User and group policies can only be managed in lakeFS   This means groups and users newly created via SCIM only have basic read permissions. The lakeFS UI or API must be used to attach policies to those users and groups. However, if a user is created and added to an existing group with an attached policy, that user will receive the permissions allowed by the policy attached to the group.</li> <li>Only direct group memberships are provisioned via SCIM   Both Okta and Entra ID only support syncing direct group membership via SCIM. This means that if you assign a group to the application, only its user members will be provisioned via SCIM. SCIM provisioning will not cascade to member groups and their members, and so forth.</li> </ul>"},{"location":"howto/scim/#enabling-scim-in-lakefs-cloud","title":"Enabling SCIM in lakeFS Cloud","text":"<p>To enable SCIM support in lakeFS Cloud, you need to log into the cloud admin. In the cloud admin, SCIM settings are under Access &gt; Settings. SCIM is not enabled by default, so to enable SCIM for the organization, click the Setup Provisioning Button.</p> <p></p> <p>Clicking the button will enable SCIM for the organization and provide the details you'll need to set up your IdP to work with lakeFS Cloud SCIM.</p> <p></p> <p>To set up your IdP, you'll need the lakeFS Cloud SCIM provisioning endpoint and you'll also need to generate an integration token. When creating a new integration token, you can optionally provide a description for future reference.</p> <p>Info</p> <p>The token value is only presented once, right after creation. Make sure to copy the token, as its value isn't stored and cannot be retrieved after the initial creation.</p>"},{"location":"howto/scim/#setting-up-scim-provisioning-in-entra-id-aka-azure-ad","title":"Setting Up SCIM Provisioning in Entra ID (a.k.a Azure AD)","text":"<p>Note</p> <p>This guide assumes you've already set up an Entra ID enterprise application for federated authentication to lakeFS Cloud.</p> <p>In the Entra ID admin dashboard, go to Enterprise Applications and choose the lakeFS Cloud enterprise application from the list. Then click Provisioning in the sidebar and then Get Started.</p> <ol> <li>In the provisioning settings set mode to Automatic</li> <li>In Tenant URL enter the URL from the lakeFS Cloud provisioning settings. You will need to append <code>?aadOptscim062020</code> to the end of the URL to ensure proper integration with Entra ID.</li> <li>In Secret Token paste the token you copied in the previous step. If you haven't created a token yet, you may do so now</li> <li>Click Test Connection</li> <li>If the test fails, please ensure you've entered the correct SCIM endpoint URL from lakeFS Cloud and copied the token correctly. Otherwise, click \"Save\" at the top of the settings panel</li> <li>Configure provisioning attribute mappings(this determines which attributes are sent to the lakeFS SCIM endpoint)</li> </ol>"},{"location":"howto/scim/#required-attributes","title":"Required Attributes","text":"<p>The LakeFS SCIM implementation has a number of attributes that it expects to see in requests. Missing, incorrect, or extraneous attributes will generally result in a 400 error code.</p>"},{"location":"howto/scim/#user-resource-attributes","title":"User Resource Attributes","text":"<ul> <li>username: Unique identifier for the User, typically used by the user to directly authenticate to the service provider. Each User MUST include a non-empty userName value. This identifier MUST be unique across the service provider's entire set of Users. REQUIRED.</li> <li>externalId: A String that is an identifier for the resource as defined by the provisioning client. REQUIRED.</li> <li>emails: Email addresses for the user. The value SHOULD be canonicalized by the service provider, e.g., 'bjensen@example.com' instead of 'bjensen@EXAMPLE.COM'. Canonical type values of 'work', 'home', and 'other'. One should be marked as primary. REQUIRED.</li> <li>active: A Boolean value indicating the User's administrative status.</li> </ul>"},{"location":"howto/scim/#group-resource-attributes","title":"Group Resource Attributes","text":"<ul> <li>displayName: A human-readable name for the Group. REQUIRED.</li> <li>externalId: A String that is an identifier for the resource as defined by the provisioning client. REQUIRED.</li> <li>members: A list of members of the Group.</li> </ul> <p>Info</p> <p>lakeFS Cloud is designed to work with the default attribute mapping for users and groups provided by Entra ID. If your organization has customized the user and/or group entities in Entra ID, you might want to set mappings in accordance with those. You can find details of how this is done in the Entra ID documentation.  </p> <p>Incorrectly modifying these mappings can break provisioning functionality, so it's advised to do so cautiously and only when necessary.</p>"},{"location":"howto/sizing-guide/","title":"Sizing guide","text":"<p>Info</p> <p>For a scalable managed lakeFS service with guaranteed SLAs, try lakeFS Cloud</p>"},{"location":"howto/sizing-guide/#system-requirements","title":"System Requirements","text":""},{"location":"howto/sizing-guide/#operating-systems-and-isa","title":"Operating Systems and ISA","text":"<p>lakeFS can run on MacOS and Linux. Windows binaries are available but not rigorously tested -  we don't recommend deploying lakeFS to production on Windows. x86_64 and arm64 architectures are supported for both MacOS and Linux.</p>"},{"location":"howto/sizing-guide/#memory-and-cpu-requirements","title":"Memory and CPU requirements","text":"<p>lakeFS servers require a minimum of 512mb of RAM and 1 CPU core.  For high throughput, additional CPUs help scale requests across different cores.  \"Expensive\" operations such as large diff or commit operations can take advantage of multiple cores. </p>"},{"location":"howto/sizing-guide/#network","title":"Network","text":"<p>If using the data APIs such as the S3 Gateway,  lakeFS will require enough network bandwidth to support the planned concurrent network upload/download operations. For most cloud providers, more powerful machines (i.e., more expensive and usually containing more CPU cores) also provide increased network bandwidth.</p> <p>If using only the metadata APIs (for example, only using the Hadoop/Spark clients), network bandwidth is minimal,  at roughly 1Kb per request.</p>"},{"location":"howto/sizing-guide/#disk","title":"Disk","text":"<p>lakeFS greatly benefits from fast local disks.  A lakeFS instance doesn't require any strong durability guarantees from the underlying storage,  as the disk is only ever used as a local caching layer for lakeFS metadata and not for long-term storage. lakeFS is designed to work with ephemeral disks -  these are usually based on NVMe and are tied to the machine's lifecycle.  Using ephemeral disks lakeFS can provide a very high throughput/cost ratio,  probably the best that could be achieved on a public cloud, so we recommend those.</p> <p>A local cache of at least 512 MiB should be provided.  For large installations (managing &gt;100 concurrently active branches, with &gt;100M objects per commit), we recommend allocating at least 10 GiB - since it's a caching layer over a relatively slow storage (the object store),  see Important metrics below to understand how to size this: it should be big enough to hold all commit metadata for actively referenced commits.</p>"},{"location":"howto/sizing-guide/#lakefs-kv-store","title":"lakeFS KV Store","text":"<p>lakeFS uses a key-value database to manage branch references, authentication and authorization information  and to keep track of currently uncommitted data across branches. Please refer to the relevant driver tab for best practices, requirements and benchmarks.</p>"},{"location":"howto/sizing-guide/#storage","title":"Storage","text":"<p>The dataset stored in the metadata store is relatively modest as most metadata is pushed down into the object store.  Required storage is mostly a factor of the amount of uncommitted writes across all branches at any given point in time:  in the range of 150 MiB per every 100,000 uncommitted writes. </p> <p>We recommend starting at 10 GiB for a production deployment, as it will likely be more than enough.</p> PostgreSQLDynamoDB <p>RAM Since the data size is small, it's recommended to provide enough memory to hold the vast majority of that data in RAM. Cloud providers will save you the need to tune this parameter - it will be set to a fixed percentage the chosen instance's available RAM (25% on AWS RDS, 30% on Google Cloud SQL). It is recommended that you check with your selected cloud provider for configuration and provisioning information for you database. For self-managed database instances follow these best practices</p> <p>Ideally, configure the shared_buffers of your PostgreSQL instances to be large enough to contain the currently active dataset. Pick a database instance with enough RAM to accommodate this buffer size at roughly x4 the size given for <code>shared_buffers</code>. For example, if an installation has ~500,000 uncommitted writes at any given time, it would require about 750 MiB of <code>shared_buffers</code> that would require about 3 GiB of RAM.</p> <p>CPU PostgreSQL CPU cores help scale concurrent requests. 1 CPU core for every 5,000 requests/second is ideal.</p> <p>lakeFS will create a DynamoDB table for you, defaults to on-demand capacity setting. No need to specify how much read and write throughput you expect your application to perform, as DynamoDB instantly accommodates your workloads as they ramp up or down.</p> <p>You can customize the table settings to provisioned capacity which allows you to manage and optimize your costs by allocating read/write capacity in advance (see Benchmarks)</p> <p>Notes</p> <ul> <li>Using DynamoDB on-demand capacity might generate unwanted costs if the table is abused, if you'd like to cap your costs, make sure to change the table to use provisioned capacity instead.  </li> <li>lakeFS doesn't manage the DynamoDB's table lifecycle, we've included the table creation in order to help evaluating the system with minimal effort, any change to the table beyond the table creation - will need to be handled manually or by 3rd party tools.</li> </ul> <p>RAM Managed by AWS.</p> <p>CPU Managed by AWS.</p>"},{"location":"howto/sizing-guide/#scaling-factors","title":"Scaling factors","text":"<p>Scaling lakeFS, like most data systems, moves across two axes: throughput of requests (amount per given timeframe) and latency (time to complete a single request).</p>"},{"location":"howto/sizing-guide/#understanding-latency-and-throughput-considerations","title":"Understanding latency and throughput considerations","text":"<p>Most lakeFS operations are designed to be very low in latency. Assuming a well-tuned local disk cache (see Storage above), most critical path operations (writing objects, requesting objects, deleting objects) are designed to complete in &lt;25ms at p90. Listing objects obviously requires accessing more data, but should always be on-par with what the underlying object store can provide, and in most cases, it's actually faster. At the worst case, for directory listing with 1,000 common prefixes returned, expect a latency of 75ms at p90.</p> <p>Managing branches (creating them, listing them and deleting them) are all constant-time operations, generally taking &lt;30ms at p90.</p> <p>Committing and merging can take longer, as they are proportional to the amount of changes introduced. This is what makes lakeFS optimal for large Data Lakes - the amount of changes introduced per commit usually stays relatively stable while the entire data set usually grows over time. This means lakeFS will provide predictable performance: committing 100 changes will take roughly the same amount of time whether the resulting commit contains 500 or 500 million objects.</p> <p>See Data Model for more information.</p> <p>Scaling throughput depends very much on the amount of CPU cores available to lakeFS. In many cases, it's easier to scale lakeFS across a fleet of smaller cloud instances (or containers) than scale up with machines that have many cores. In fact, lakeFS works well in both cases. Most critical path operations scale very well across machines.</p>"},{"location":"howto/sizing-guide/#benchmarks","title":"Benchmarks","text":"PostgresSQLDynamoDB <p>All benchmarks below were measured using 2 x c5ad.4xlarge instances on AWS us-east-1. Similar results can be achieved on Google Cloud using a <code>c2-standard-16</code> machine type, with an attached local SSD. On Azure, you can use a <code>Standard_F16s_v2</code> virtual machine.</p> <p>The PostgreSQL instance that was used is a db.m6g.2xlarge (8 vCPUs, 32 GB RAM). Equivalent machines on Google Cloud or Azure should yield similar results.</p> <p>The example repository we tested against contains the metadata of a large lakeFS installation, where each commit contains ~180,000,000 objects (representing ~7.5 Petabytes of data).</p> <p>All tests are reproducible using the lakectl abuse command, so use it to properly size and tune your setup. All tests are accompanied by the relevant <code>lakectl abuse</code> command that generated them.</p> <p>All benchmarks below were measured using m5.xlarge instance on AWS us-east-1.</p> <p>The DynamoDB table that was used was provisioned with 500/1000 read/write capacity.</p> <p>The example repository we tested against contains the metadata of a large lakeFS installation, where each commit contains ~100,000,000 objects (representing ~3.5 Petabytes of data).</p> <p>All tests are reproducible using the lakectl abuse command, so use it to properly size and tune your setup. All tests are accompanied by the relevant lakectl abuse command that generated them.</p>"},{"location":"howto/sizing-guide/#random-reads","title":"Random reads","text":"<p>This test generates random read requests to lakeFS, in a given commit. Paths are requested randomly from a file containing a set of preconfigured (and existing) paths.</p> <p>command executed:</p> <pre><code>lakectl abuse random-read \\\n    --from-file randomly_selected_paths.txt \\\n    --amount 500000 \\\n    --parallelism 128 \\\n    lakefs://example-repo/&lt;commit hash&gt;\n</code></pre> <p>Note lakeFS version &lt;= v0.33.1 uses '@' (instead of '/') as separator between repository and commit hash.</p> <p>Result Histogram (raw):</p> <pre><code>Histogram (ms):\n1   0\n2   0\n5   37945\n7   179727\n10  296964\n15  399682\n25  477502\n50  499625\n75  499998\n100 499998\n250 500000\n350 500000\n500 500000\n750 500000\n1000    500000\n5000    500000\nmin 3\nmax 222\ntotal   500000\n</code></pre> <p>So 50% of all requests took &lt;10ms, while 99.9% of them took &lt;50ms</p> <p>throughput:</p> <p>Average throughput during the experiment was 10851.69 requests/second</p>"},{"location":"howto/sizing-guide/#random-writes","title":"Random Writes","text":"<p>This test generates random write requests to a given lakeFS branch. All the paths are pre-generated and don't overwrite each other (as overwrites are relatively rare in a Data Lake setup).</p> <p>command executed:</p> <pre><code>lakectl abuse random-write \\\n    --amount 500000 \\\n    --parallelism 64 \\\n    lakefs://example-repo/main\n</code></pre> <p>Note lakeFS version &lt;= v0.33.1 uses '@' (instead of '/') as separator between repository and branch.</p> <p>Result Histogram (raw):</p> <pre><code>Histogram (ms):\n1   0\n2   0\n5   30715\n7   219647\n10  455807\n15  498144\n25  499535\n50  499742\n75  499784\n100 499802\n250 500000\n350 500000\n500 500000\n750 500000\n1000    500000\n5000    500000\nmin 3\nmax 233\ntotal   500000\n</code></pre> <p>So, 50% of all requests took &lt;10ms, while 99.9% of them took &lt;25ms.</p> <p>throughput:</p> <p>The average throughput during the experiment was 7595.46 requests/second.</p>"},{"location":"howto/sizing-guide/#branch-creation","title":"Branch creation","text":"<p>This test creates branches from a given reference.</p> <p>command executed:</p> <pre><code>lakectl abuse create-branches \\\n    --amount 500000 \\\n    --branch-prefix \"benchmark-\" \\\n    --parallelism 256 \\\n    lakefs://example-repo/&lt;commit hash&gt;\n</code></pre> <p>Note lakeFS version &lt;= v0.33.1 uses '@' (instead of '/') as separator between repository and commit hash.</p> <p>Result Histogram (raw):</p> <pre><code>Histogram (ms):\n1   0\n2   1\n5   5901\n7   39835\n10  135863\n15  270201\n25  399895\n50  484932\n75  497180\n100 499303\n250 499996\n350 500000\n500 500000\n750 500000\n1000    500000\n5000    500000\nmin 2\nmax 304\ntotal   500000\n</code></pre> <p>So, 50% of all requests took &lt;15ms, while 99.9% of them took &lt;100ms.</p> <p>throughput:</p> <p>The average throughput during the experiment was 7069.03 requests/second.</p>"},{"location":"howto/sizing-guide/#random-reads_1","title":"Random reads","text":"<p>This test generates random read requests to lakeFS, in a given commit. Paths are requested randomly from a file containing a set of preconfigured (and existing) paths.</p> <p>command executed:</p> <pre><code>lakectl abuse random-read \\\n    --from-file randomly_selected_paths.txt \\\n    --amount 500000 \\\n    --parallelism 128 \\\n    lakefs://example-repo/&lt;commit hash&gt;\n</code></pre> <p>Result Histogram (raw): Provisioned read capacity units = 1000 Provisioned write capacity units = 1000</p> <pre><code>Histogram (ms):\n1   0\n2   0\n5   0\n7   0\n10  0\n15  0\n25  122\n50  47364\n75  344489\n100 460404\n250 497912\n350 498016\n500 498045\n750 498111\n1000 498176\n5000 499478\nmin 18\nmax 52272\ntotal 500000\n</code></pre> <p>Result Histogram (raw): Provisioned read capacity units = 500 Provisioned write capacity units = 500</p> <pre><code>Histogram (ms):\n1   0\n2   0\n5   0\n7   0\n10  0\n15  1\n25  2672\n50  239661\n75  420171\n100 470146\n250 486603\n350 486715\n500 486789\n750 487443\n1000    488113\n5000    493201\nmin 14\nmax 648085\ntotal   499998\n</code></pre>"},{"location":"howto/sizing-guide/#random-writes_1","title":"Random Writes","text":"<p>This test generates random write requests to a given lakeFS branch. All the paths are pre-generated and don't overwrite each other (as overwrites are relatively rare in a Data Lake setup).</p> <p>command executed:</p> <pre><code>lakectl abuse random-write \\\n    --amount 500000 \\\n    --parallelism 64 \\\n    lakefs://example-repo/main\n</code></pre> <p>Result Histogram (raw): Provisioned read capacity units = 1000 Provisioned write capacity units = 1000</p> <p><pre><code>Histogram (ms):\n1   0\n2   0\n5   0\n7   0\n10  0\n15  0\n25  24\n50  239852\n75  458504\n100 485225\n250 493687\n350 493872\n500 493960\n750 496239\n1000    499194\n5000    500000\nmin 23\nmax 4437\ntotal   500000\n</code></pre> Result Histogram (raw): Provisioned read capacity units = 500 Provisioned write capacity units = 500</p> <pre><code>Histogram (ms):\n1   0\n2   0\n5   0\n7   0\n10  0\n15  0\n25  174\n50  266460\n75  462641\n100 484486\n250 490633\n350 490856\n500 490984\n750 492973\n1000 495605\n5000 498920\nmin 21\nmax 50157\ntotal 500000\n</code></pre>"},{"location":"howto/sizing-guide/#branch-creation_1","title":"Branch creation","text":"<p>This test creates branches from a given reference.</p> <p>command executed:</p> <pre><code>lakectl abuse create-branches \\\n    --amount 500000 \\\n    --branch-prefix \"benchmark-\" \\\n    --parallelism 256 \\\n    lakefs://example-repo/&lt;commit hash&gt;\n</code></pre> <p>Result Histogram (raw): Provisioned read capacity units = 1000 Provisioned write capacity units = 1000</p> <pre><code>Histogram (ms):\n1   0\n2   0\n5   0\n7   0\n10  0\n15  0\n25  0\n50  628\n75  26153\n100 58099\n250 216160\n350 307078\n500 406165\n750 422898\n1000    431332\n5000    475848\nmin 41\nmax 430725\ntotal   490054\n</code></pre> <p>Result Histogram (raw): Provisioned read capacity units = 500 Provisioned write capacity units = 500</p> <pre><code>Histogram (ms):\n1   0\n2   0\n5   0\n7   0\n10  0\n15  0\n25  0\n50  3132\n75  155570\n100 292745\n250 384224\n350 397258\n500 431141\n750 441360\n1000 445597\n5000 469538\nmin 39\nmax 760626\ntotal 497520\n</code></pre>"},{"location":"howto/sizing-guide/#important-metrics","title":"Important metrics","text":"<p>lakeFS exposes metrics using the Prometheus protocol.  Every lakeFS instance exposes a <code>/metrics</code> endpoint that could be used to extract them. </p> <p>Here are a few notable metrics to keep track of when sizing lakeFS:</p> <p><code>api_requests_total</code> - Tracks throughput of API requests over time.</p> <p><code>api_request_duration_seconds</code> - Histogram of latency per operation type.</p> <p><code>gateway_request_duration_seconds</code> - Histogram of latency per S3 Gateway operation.</p> PostgreSQLDynamoDB <p><code>dynamo_request_duration_seconds</code> - Time spent doing DynamoDB requests.</p> <p><code>dynamo_consumed_capacity_total</code> - The capacity units consumed by operation.</p> <p><code>dynamo_failures_total</code> - The total number of errors while working for kv store.</p>"},{"location":"howto/sizing-guide/#reference-architectures","title":"Reference architectures","text":"<p>Below are a few example architectures for lakeFS deployment. </p>"},{"location":"howto/sizing-guide/#reference-architecture-data-scienceresearch-environment","title":"Reference Architecture: Data Science/Research environment","text":"<p>Use case: Manage Machine learning or algorithms development.  Use lakeFS branches to achieve both isolation and reproducibility of experiments.  Data being managed by lakeFS is both structured tabular data,  as well as unstructured sensor and image data used for training.  Assuming a team of 20-50 researchers, with a dataset size of 500 TiB across 20M objects.</p> <p>Environment: lakeFS will be deployed on Kubernetes.  managed by AWS EKS  with PostgreSQL on AWS RDS Aurora</p> <p>Sizing: Since most of the work is done by humans (vs. automated pipelines), most experiments tend to be small in scale,  reading and writing 10s to 1000s of objects.  The expected amount of branches active in parallel is relatively low, around 1-2 per user,  each representing a small amount of uncommitted changes at any given point in time.  Let's assume 5,000 uncommitted writes per branch = ~500k. </p> <p>To support the expected throughput, a single moderate lakeFS instance should be more than enough,  since requests per second would be on the order of 10s to 100s.  For high availability, we'll deploy 2 pods with 1 CPU core and 1 GiB of RAM each.</p> <p>Since the PostgreSQL instance is expected to hold a very small dataset  (at 500k, expected dataset size is <code>150MiB (for 100k records) * 5 = 750MiB</code>).  To ensure we have enough RAM to hold this, we'll need 3 GiB of RAM, so, a very moderate Aurora instance <code>db.t3.large</code> (2 vCPUs, 8 GB RAM) will be more than enough. An equivalent database instance on GCP or Azure should give similar results.</p> <p></p>"},{"location":"howto/sizing-guide/#reference-architecture-automated-production-pipelines","title":"Reference Architecture: Automated Production Pipelines","text":"<p>Use case: Manage multiple concurrent data pipelines using Apache Spark and Airflow.  Airflow DAGs start by creating a branch for isolation and for CI/CD.  Data being managed by lakeFS is structured, tabular data. The total dataset size is 10 PiB, spanning across 500M objects.  The expected throughput is 10k reads/second + 2k writes per second across 100 concurrent branches.</p> <p>Environment: lakeFS will be deployed on Kubernetes.  managed by AWS EKS  with PostgreSQL on AWS RDS</p> <p>Sizing: Data pipelines tend to be bursty in nature:  reading in a lot of objects concurrently, doing some calculation or aggregation, and then writing many objects concurrently.  The expected amount of branches active in parallel is high,  with many Airflow DAGs running per day, each representing a moderate amount of uncommitted changes at any given point in time.  Let's assume 1,000 uncommitted writes/branch * 2500 branches = ~2.5M records. </p> <p>To support the expected throughput, looking the benchmarking numbers above,  we're doing roughly 625 requests/core, so 24 cores should cover our peak traffic. We can deploy <code>6 * 4 CPU core pods</code>.</p> <p>On to the PostgreSQL instance - at 500k, the expected dataset size is <code>150MiB (for 100k records) * 25 = 3750 MiB</code>.  To ensure we have enough RAM to hold this, we'll need at least 15 GiB of RAM, so we'll go with a <code>db.r5.xlarge</code> (4 vCPUs, 32GB RAM) Aurora instance. An equivalent database instance on GCP or Azure should give similar results.</p> <p></p>"},{"location":"howto/unity-delta-sharing/","title":"Unity Delta Sharing","text":"<p>Info</p> <p>Available on lakeFS Cloud</p> <p>Warning</p> <p>Please note, as of June 15th 2024, the Unity Delta Sharing feature will be removed. To integrate lakeFS with Unity Catalog, refer to the Unity integration docs.</p>"},{"location":"howto/unity-delta-sharing/#introduction","title":"Introduction","text":"<p>lakeFS Unity Delta Sharing provides a read-only experience from Unity Catalog for lakeFS customers.  Currently, this is available as a private preview.  It provides full read-only functionality for Unity Catalog.  It does not provide a \"self-service\" experience to set up the service.</p>"},{"location":"howto/unity-delta-sharing/#setup","title":"Setup","text":"<p>This guide explains how to set up and use lakeFS Delta Sharing.  Currently, you will have to configure lakeFS Delta Sharing in collaboration with Treeverse Customer Success.  Once set up is complete, you will of course be able to use lakeFS Delta Sharing on existing and on new tables without further assistance.</p>"},{"location":"howto/unity-delta-sharing/#1-collect-data-and-initial-setup","title":"1. Collect data and initial setup","text":"<ul> <li> <p>Select a <code>Delta Sharing configuration URL</code>.  This is a single location on lakeFS to hold the top-level configuration of lakeFS Delta Sharing across all   repositories of your organization.  Typically, it will have the form <code>lakefs://REPO/main/lakefs_delta_sharing.yaml</code> for one of your   repositories.  A longer path may be supplied - however, we do recommend keeping it on the <code>main</code> branch, as this object represents state   for the entire installation.</p> </li> <li> <p>Create a user <code>lakefs-delta-sharing-service</code> for lakeFS Delta Sharing, and an access key for that user.  It should have at least read permissions for   the configuration URL and for all repositories and all data accesses by Unity.  lakeFS Delta Sharing will these credentials to communicate with lakeFS.</p> </li> </ul> <p>Communicate these items to Customer Success:</p> <ul> <li>Configuration URL</li> <li>Access key ID and secret access key for user <code>lakefs-delta-sharing-service</code>.</li> </ul> <p>Note</p> <p>All YAML files extensions used in this guide must be <code>yaml</code>. Do not use a <code>yml</code> extension instead.</p>"},{"location":"howto/unity-delta-sharing/#2-initial-configuration","title":"2. Initial configuration","text":"<p>Select a secret authorization token to share Unity catalog.  Unity catalog will use to authenticate to the lakeFS Delta Sharing server. You might use this command on Linux:</p> <pre><code>head -c 50 /dev/random | base64\n</code></pre> <p>Create a file <code>lakefs_delta_sharing.yaml</code> and place it at the config URL selected above.  It should look like this:</p> <pre><code>authorization_token: \"GENERATED TOKEN\"\n# Map lakeFS repositories to Unity shares\nrepositories:\n    - id: sample-repo\n      share_name: undev\n      # List branches and prefixes to export.  Each of these branches (and only\n      # these branches) will be available as a schema on Unity.\n      branches:\n          - main\n          - staging\n          - dev_*\n    - id: repo2\n      share_name: share_two\n      branches:\n      - \"*\"\n</code></pre> <p>Note that a plain \"*\" line must be quoted in YAML.</p> <p>Upload it to your config URL.  For instance if the config URL is <code>lakefs://repo/main/lakefs_delta_sharing.yaml</code>, you might use:</p> <pre><code>lakectl fs upload -s ./lakefs_delta_sharing.yaml lakefs://repo/main/lakefs_delta_sharing.yaml\n</code></pre>"},{"location":"howto/unity-delta-sharing/#3-connect-unity-to-lakefs-delta-sharing","title":"3. Connect Unity to lakeFS Delta Sharing","text":"<p>You now need to configure Unity to use the lakeFS Delta Sharing server. Create a share provider file <code>config.share.json</code>; see the Delta Sharing manual:</p> <pre><code>{\n  \"shareCredentialsVersion\": 1,\n  \"endpoint\": \"https://ORG_ID.REGION.lakefscloud.io/service/delta-sharing/v1\",\n  \"bearerToken\": \"GENERATED TOKEN\",\n  \"expirationTime\": \"2030-01-01T00:00:00.0Z\"\n}\n</code></pre> <p>\"GENERATED TOKEN\" is the secret authorization token use above.</p> <p>Install the databricks cli.  We will use it to create Delta Share on Unity.  Follow the instructions to configure it.</p> <p>Run the provider creation command:</p> <pre><code>databricks unity-catalog providers create \\\n    --name lakefs-cloud \\\n    --recipient-profile-json-file config.share.json\n</code></pre> <p>Go to \"Data &gt;&gt; Delta Sharing\" on the DataBricks environment.  Once Treeverse have configured lakeFS Delta Sharing on your account with your config URL, the \"lakefs-cloud\" provider should appear under \"Shared with me\".</p> <p></p> <p>Click the provider to see its shares.</p> <p></p> <p>You can now create a catalog from these shares.</p> <p></p> <p>And you can see schemas for each of the branches that you configured in the share.  Here branch name <code>dev_experiment1</code> matches the pattern <code>dev_*</code> that we defined in the configuration object <code>lakefs-delta-sharing.yaml</code>, so it appears as a schema.</p> <p></p> <p>At this point you have configured Delta Sharing on lakeFS, and DataBricks to communicate with lakeFS delta sharing.  No further Treeverse involvement is required. Updates to <code>lakefs_delta_sharing.yaml</code> will update within a minute of uploading a new version.</p>"},{"location":"howto/unity-delta-sharing/#4-configure-tables","title":"4. Configure tables","text":"<p>Everything is ready: lakeFS repositories are configured as shares, and branches are configured as schemas.  Now you can define tables!  Once a repository is shared, its tables are configured as a table descriptor object on the repository on the path <code>_lakefs_tables/TABLE.yaml</code>.</p>"},{"location":"howto/unity-delta-sharing/#delta-lake-tables","title":"Delta Lake tables","text":"<p>Delta Lake format includes full metadata, so you only need to configure the prefix:</p> <pre><code>name: users\ntype: delta\npath: path/to/users/\n</code></pre> <p>Note</p> <p>The filename of the 'yaml' file containing the table definition must match the 'name' of the table itself. In the example above, '_lakefs_tables/users.yaml'.</p> <p>When placed inside <code>_lakefs_tables/users.yaml</code> this defines a table <code>users</code> on the prefix <code>path/to/users/</code> (so <code>path/to/users/</code> holds the prefix <code>_delta_log</code>).</p>"},{"location":"howto/unity-delta-sharing/#hive-tables","title":"Hive tables","text":"<p>Hive metadata server tables are essentially just a set of objects that share a prefix, with no table metadata stored on the object store.  You need to configure prefix, partitions, and schema.</p> <pre><code>name: clicks\ntype: hive\npath: path/to/clicks/\npartition_columns: ['year']\nschema:\n  type: struct\n  fields:\n    - name: year\n      type: integer\n      nullable: false\n      metadata: {}\n    - name: page\n      type: string\n      nullable: false\n      metadata: {}\n    - name: site\n      type: string\n      nullable: true\n      metadata:\n        comment: a comment about this column\n</code></pre> <p>Useful types recognized by DataBricks Photon include <code>integer</code>, <code>long</code>, <code>short</code>, <code>string</code>, <code>double</code>, <code>float</code>, <code>date</code>, and <code>timestamp</code>. For exact type mappings, and whether to specify a field as <code>nullable: false</code>, refer to DataBricks Photon documentation.</p>"},{"location":"howto/virtual-host-addressing/","title":"Configuring lakeFS to use S3 Virtual-Host addressing","text":""},{"location":"howto/virtual-host-addressing/#understanding-virtual-host-addressing","title":"Understanding virtual-host addressing","text":"<p>Some systems require S3 endpoints (such as lakeFS's S3 Gateway) to support virtual-host style addressing.</p> <p>lakeFS supports this, but requires some configuration in order to extract the bucket name (used as the lakeFS repository ID) from the host address.</p> <p>For example:</p> <pre><code>GET http://foo.example.com/some/location\n</code></pre> <p>There are two ways to interpret the URL above:</p> <ul> <li>as a virtual-host URL where the endpoint URL is <code>example.com</code>, the bucket name is <code>foo</code>, and the path is <code>/some/location</code></li> <li>as a path-based URL where the endpoint is <code>foo.example.com</code>, the bucket name is <code>some</code> and the path is <code>location</code></li> </ul> <p>By default, lakeFS reads URLs as path-based. To read the URL as a virtual-host request, lakeFS requires additional configuration which includes  defining an explicit set of DNS records for the lakeFS S3 gateway.</p>"},{"location":"howto/virtual-host-addressing/#adding-an-explicit-s3-domain-name-to-the-s3-gateway-configuration","title":"Adding an explicit S3 domain name to the S3 Gateway configuration","text":"<p>The first step would be to tell the lakeFS installation which hostnames are used for the S3 Gateway. This should be a different DNS record from the one used for e.g. the UI or API.</p> <p>Typically, if the lakeFS installation is served under <code>lakefs.example.com</code>, a good choice would be <code>s3.lakefs.example.com</code>.</p> <p>This could be done using either an environment variable:</p> <pre><code>LAKEFS_GATEWAYS_S3_DOMAIN_NAME=\"s3.lakefs.example.com\"\n</code></pre> <p>Or by adding the <code>gateways.s3.domain_name</code> setting to the lakeFS <code>config.yaml</code> file:</p> <pre><code>---\ndatabase:\n  connection_string: \"...\"\n\n...\n\n# This section defines an explict S3 gateway address that supports virtual-host addressing\ngateways:\n  s3:\n    domain_name: s3.lakefs.example.com\n</code></pre> <p>Note</p> <p>For more information on how to configure lakeFS, check out the configuration reference</p>"},{"location":"howto/virtual-host-addressing/#setting-up-the-appropriate-dns-records","title":"Setting up the appropriate DNS records","text":"<p>Once our lakeFS installation is configured with an explicit S3 gateway endpoint address, we need to define 2 DNS records and have them point at our lakeFS installation. This requires 2 CNAME records:</p> <ol> <li><code>s3.lakefs.example.com</code> - CNAME to <code>lakefs.example.com</code>. This would be used as the S3 endpoint when configuring clients and will serve as our bare domain.</li> <li><code>*.s3.lakefs.example.com</code> - Also a CNAME to <code>lakefs.example.com</code>. This will resolve virtual-host requests such as <code>example-repo.s3.lakefs.example.com</code> that lakeFS would now know how to parse.</li> </ol> <p>Learn More</p> <p>For more information on how to configure these, see the official documentation of your DNS provider. On AWS, This could also be done using ALIAS records for a load balancer.</p>"},{"location":"howto/deploy/","title":"Deploy and Setup lakeFS","text":"<p>Tip</p> <p>The instructions given here are for a self-managed deployment of lakeFS.  For a hosted lakeFS service with guaranteed SLAs, try lakeFS Cloud</p> <p>This section will guide you through deploying lakeFS on top of an object store. You will require a database, and can optionally configure authentication using providers specific to your deployment platform. </p> <p>Which options are available depends on your deployment platform. For example, the object store available on Azure differs from that on AWS. </p> <p></p>"},{"location":"howto/deploy/#deployment-and-setup-details","title":"Deployment and Setup Details","text":"<p>lakeFS releases include binaries for common operating systems, a containerized option or a Helm chart.</p> <p>Check out our guides below for full deployment details: </p> <ul> <li>AWS</li> <li>Azure</li> <li>GCP</li> <li>On-premises and other cloud providers</li> </ul>"},{"location":"howto/deploy/aws/","title":"Deploy lakeFS on AWS","text":"<p>Tip</p> <p>The instructions given here are for a self-managed deployment of lakeFS on AWS.  For a hosted lakeFS service with guaranteed SLAs, try lakeFS Cloud</p> <p>When you deploy lakeFS on AWS these are the options available to use:</p> <p></p> <p>This guide walks you through the options available and how to configure them, finishing with configuring and running lakeFS itself and creating your first repository.</p> <p>\u23f0 Expected deployment time: 25 min</p>"},{"location":"howto/deploy/aws/#grant-lakefs-permissions-to-dynamodb","title":"Grant lakeFS permissions to DynamoDB","text":"<p>By default, lakeFS will create the required DynamoDB table if it does not already exist. You'll have to give the IAM role used by lakeFS the following permissions:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListAndDescribe\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:List*\",\n                \"dynamodb:DescribeReservedCapacity*\",\n                \"dynamodb:DescribeLimits\",\n                \"dynamodb:DescribeTimeToLive\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"kvstore\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:BatchGet*\",\n                \"dynamodb:DescribeTable\",\n                \"dynamodb:Get*\",\n                \"dynamodb:Query\",\n                \"dynamodb:Scan\",\n                \"dynamodb:BatchWrite*\",\n                \"dynamodb:CreateTable\",\n                \"dynamodb:Delete*\",\n                \"dynamodb:Update*\",\n                \"dynamodb:PutItem\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:*:*:table/kvstore\"\n        }\n    ]\n}\n</code></pre> <p>Tip</p> <p>You can also use lakeFS with PostgreSQL instead of DynamoDB! See the configuration reference for more information.</p>"},{"location":"howto/deploy/aws/#run-the-lakefs-server","title":"Run the lakeFS server","text":"EC2EKS <p>Connect to your EC2 instance using SSH:</p> <ol> <li>Create a <code>config.yaml</code> on your EC2 instance, with the following parameters:     <pre><code>---\ndatabase:\n    type: \"dynamodb\"\n\nauth:\n    encrypt:\n    # replace this with a randomly-generated string. Make sure to keep it safe!\n    secret_key: \"[ENCRYPTION_SECRET_KEY]\"\n\nblockstore:\n    type: s3\n</code></pre></li> <li>[Download the binary][downloads] to run on the EC2 instance.</li> <li> <p>Run the <code>lakefs</code> binary on the EC2 instance:     <pre><code>lakefs --config config.yaml run\n</code></pre></p> <p>Note</p> <p>It's preferable to run the binary as a service using systemd or your operating system's facilities.</p> </li> </ol> <p>Advanced: Deploying lakeFS behind an AWS Application Load Balancer</p> <ol> <li>Your security groups should allow the load balancer to access the lakeFS server.</li> <li>Create a target group with a listener for port 8000.</li> <li>Setup TLS termination using the domain names you wish to use (e.g., <code>lakefs.example.com</code> and potentially <code>s3.lakefs.example.com</code>, <code>*.s3.lakefs.example.com</code> if using virtual-host addressing).</li> <li>Configure the health-check to use the exposed <code>/_health</code> URL</li> </ol> <p>You can install lakeFS on Kubernetes using a Helm chart.</p> <p>To install lakeFS with Helm:</p> <ol> <li>Copy the Helm values file relevant for S3:     <pre><code>secrets:\n    # replace this with a randomly-generated string\n    authEncryptSecretKey: [ENCRYPTION_SECRET_KEY]\nlakefsConfig: |\n    database:\n        type: dynamodb\n    blockstore:\n        type: s3\n</code></pre></li> <li>Fill in the missing values and save the file as <code>conf-values.yaml</code>. For more configuration options, see our Helm chart README.     !!! note         The <code>lakefsConfig</code> parameter is the lakeFS configuration documented here but without sensitive information.         Sensitive information like <code>databaseConnectionString</code> is given through separate parameters, and the chart will inject it into Kubernetes secrets.</li> <li> <p>In the directory where you created <code>conf-values.yaml</code>, run the following commands:     <pre><code># Add the lakeFS repository\nhelm repo add lakefs https://charts.lakefs.io\n# Deploy lakeFS\nhelm install my-lakefs lakefs/lakefs -f conf-values.yaml\n</code></pre></p> <p>my-lakefs is the Helm Release name.</p> <p>Warning</p> <p>Make sure the Kubernetes nodes have access to all buckets/containers with which you intend to use with lakeFS. If you can't provide such access, configure lakeFS with an AWS key-pair.</p> </li> </ol>"},{"location":"howto/deploy/aws/#load-balancing","title":"Load balancing","text":"<p>To configure a load balancer to direct requests to the lakeFS servers you can use the <code>LoadBalancer</code> Service type or a Kubernetes Ingress. By default, lakeFS operates on port 8000 and exposes a <code>/_health</code> endpoint that you can use for health checks.</p> <p>Tip</p> <p>The NGINX Ingress Controller by default limits the client body size to 1 MiB.</p> <p>Some clients use bigger chunks to upload objects - for example, multipart upload to lakeFS using the [S3 Gateway][s3-gateway] or a simple PUT request using the [OpenAPI Server][openapi].</p> <p>Checkout the Nginx documentation for increasing the limit, or an example of Nginx configuration with MinIO.</p>"},{"location":"howto/deploy/aws/#prepare-your-s3-bucket","title":"Prepare your S3 bucket","text":"<ol> <li>Take note of the bucket name you want to use with lakeFS</li> <li>Use the following as your bucket policy, filling in the placeholders:</li> </ol> Standard PermissionsStandard Permissions (with s3express)Minimal Permissions (Advanced) <pre><code>{\n    \"Id\": \"lakeFSPolicy\",\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"lakeFSObjects\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:AbortMultipartUpload\",\n                \"s3:ListMultipartUploadParts\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\"arn:aws:s3:::[BUCKET_NAME_AND_PREFIX]/*\"],\n            \"Principal\": {\n                \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"]\n            }\n        },\n        {\n            \"Sid\": \"lakeFSBucket\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucketMultipartUploads\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\"arn:aws:s3:::[BUCKET]\"],\n            \"Principal\": {\n                \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"]\n            }\n        }\n    ]\n}\n</code></pre> <ul> <li>Replace <code>[BUCKET_NAME]</code>, <code>[ACCOUNT_ID]</code> and <code>[IAM_ROLE]</code> with values relevant to your environment.</li> <li><code>[BUCKET_NAME_AND_PREFIX]</code> can be the bucket name. If you want to minimize the bucket policy permissions, use the bucket name together with a prefix (e.g. <code>example-bucket/a/b/c</code>). This way, lakeFS will be able to create repositories only under this specific path (see: [Storage Namespace][understand-repository]).</li> <li>lakeFS will try to assume the role <code>[IAM_ROLE]</code>.</li> </ul> <p>To use an S3 Express One Zone directory bucket, use the following policy. Note the <code>lakeFSDirectoryBucket</code> statement which is specifically required for using a directory bucket.</p> <pre><code>{\n    \"Id\": \"lakeFSPolicy\",\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"lakeFSObjects\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:AbortMultipartUpload\",\n                \"s3:ListMultipartUploadParts\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\"arn:aws:s3:::[BUCKET_NAME_AND_PREFIX]/*\"],\n            \"Principal\": {\n                \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"]\n            }\n        },\n        {\n            \"Sid\": \"lakeFSBucket\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucketMultipartUploads\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\"arn:aws:s3:::[BUCKET]\"],\n            \"Principal\": {\n                \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"]\n            }\n        },\n        {\n            \"Sid\": \"lakeFSDirectoryBucket\",\n            \"Action\": [\n                \"s3express:CreateSession\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3express:[REGION]:[ACCOUNT_ID]:bucket/[BUCKET_NAME]\"\n        }\n    ]\n}\n</code></pre> <ul> <li>Replace <code>[BUCKET_NAME]</code>, <code>[ACCOUNT_ID]</code> and <code>[IAM_ROLE]</code> with values relevant to your environment.</li> <li><code>[BUCKET_NAME_AND_PREFIX]</code> can be the bucket name. If you want to minimize the bucket policy permissions, use the bucket name together with a prefix (e.g. <code>example-bucket/a/b/c</code>). This way, lakeFS will be able to create repositories only under this specific path (see: [Storage Namespace][understand-repository]).</li> <li>lakeFS will try to assume the role <code>[IAM_ROLE]</code>.</li> </ul> <p>If required lakeFS can operate without accessing the data itself, this permission section is useful if you are using presigned URLs mode or the lakeFS Hadoop FileSystem Spark integration. Since this FileSystem performs many operations directly on the storage, lakeFS requires less permissive permissions, resulting in increased security.</p> <p>lakeFS always requires permissions to access the <code>_lakefs</code> prefix under your storage namespace, in which metadata is stored ([learn more][understand-commits]).</p> <p>By setting this policy without presign mode you'll be able to perform only metadata operations through lakeFS, meaning that you'll not be able to use lakeFS to upload or download objects. Specifically you won't be able to:</p> <ul> <li>Upload objects using the lakeFS GUI (Works with presign mode)</li> <li>Upload objects through Spark using the S3 gateway</li> <li>Run <code>lakectl fs</code> commands (unless using presign mode with <code>--pre-sign</code> flag)</li> <li>Use Actions and Hooks</li> </ul> <pre><code>{\n    \"Id\": \"[POLICY_ID]\",\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"lakeFSObjects\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::[STORAGE_NAMESPACE]/_lakefs/*\"\n            ],\n            \"Principal\": {\n                \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"]\n            }\n        },\n        {\n            \"Sid\": \"lakeFSBucket\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\"arn:aws:s3:::[BUCKET]\"],\n            \"Principal\": {\n                \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"]\n            }\n        }\n    ]\n}\n</code></pre> <p>We can use presigned URLs mode without allowing access to the data from the lakeFS server directly. We can achieve this by using condition keys such as aws:referer, aws:SourceVpc and aws:SourceIp.</p> <p>For example, assume the following scenario</p> <ul> <li>lakeFS is deployed outside the company (i.e lakeFS cloud or other VPC not <code>vpc-123</code>)</li> <li>We don't want lakeFS to be able to access the data, so we use presign URL, we still need lakeFS role to be able to sign the URL.</li> <li>We want to allow access from the internal company VPC: <code>vpc-123</code>.</li> </ul> <pre><code>{\n    \"Sid\": \"allowLakeFSRoleFromCompanyOnly\",\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n        \"AWS\": \"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"\n    },\n    \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::[BUCKET]/*\",\n    ],\n    \"Condition\": {\n        \"StringEquals\": {\n            \"aws:SourceVpc\": \"vpc-123\"\n        }\n    }\n}\n</code></pre>"},{"location":"howto/deploy/aws/#s3-storage-tier-classes","title":"S3 Storage Tier Classes","text":"<p>lakeFS currently supports the following S3 Storage Classes:</p> <ol> <li>S3 Standard - The default AWS S3 storage tier. Fully supported.</li> <li>S3 Express One-Zone - Fully supported.</li> <li>S3 Glacier Instant Retrival - Supported with limitations: currently, pre-signed URLs are not supported when using Instant Retrival. The outstanding feature request could be tracked here.</li> </ol> <p>Other storage classes are currently unsupported - either because they have not been tested with lakeFS or because they cannot be supported.</p> <p>If you need lakeFS to support a storage tier that isn't currently on the supported list, please open an issue on GitHub.</p>"},{"location":"howto/deploy/aws/#alternative-use-an-aws-user","title":"Alternative: use an AWS user","text":"<p>lakeFS can authenticate with your AWS account using an AWS user, using an access key and secret. To allow this, change the policy's Principal accordingly:</p> <pre><code> \"Principal\": {\n   \"AWS\": [\"arn:aws:iam::&lt;ACCOUNT_ID&gt;:user/&lt;IAM_USER&gt;\"]\n }\n</code></pre>"},{"location":"howto/deploy/aws/#create-the-admin-user","title":"Create the admin user","text":"<p>When you first open the lakeFS UI, you will be asked to create an initial admin user.</p> <ol> <li>Open <code>http://&lt;lakefs-host&gt;/</code> in your browser. If you haven't set up a load balancer, this will likely be <code>http://&lt;instance ip address&gt;:8000/</code></li> <li>On first use, you'll be redirected to the setup page:    </li> <li>Follow the steps to create an initial administrator user. Save the credentials you\u2019ve received somewhere safe, you won\u2019t be able to see them again!    </li> <li>Follow the link and go to the login screen. Use the credentials from the previous step to log in.</li> </ol>"},{"location":"howto/deploy/aws/#create-your-first-repository","title":"Create your first repository","text":"<ol> <li>Use the credentials from the previous step to log in</li> <li>Click Create Repository and choose Blank Repository.    </li> <li>Under Storage Namespace, enter a path to your desired location on the object store. This is where data written to this repository will be stored.</li> <li>Click Create Repository</li> <li>You should now have a configured repository, ready to use!    </li> </ol> <p>Congratulations</p> <p>Your environment is now ready \ud83e\udd29</p>"},{"location":"howto/deploy/azure/","title":"Deploy lakeFS on Azure","text":"<p>Tip</p> <p>The instructions given here are for a self-managed deployment of lakeFS on Azure.  For a hosted lakeFS service with guaranteed SLAs, try lakeFS Cloud</p> <p>When you deploy lakeFS on Azure these are the options available to use:</p> <p></p> <p>This guide walks you through the options available and how to configure them, finishing with configuring and running lakeFS itself and creating your first repository.</p> <p>\u23f0 Expected deployment time: 25 min</p>"},{"location":"howto/deploy/azure/#object-storage","title":"Object Storage","text":"<p>lakeFS supports the following Azure Storage types:</p> <ol> <li>Azure Blob Storage</li> <li>Azure Data Lake Storage Gen2 (HNS)</li> </ol> <p>Data Lake Storage Gen1 is not supported.</p>"},{"location":"howto/deploy/azure/#authentication-method","title":"Authentication Method","text":"<p>lakeFS supports two ways to authenticate with Azure.</p> Identity Based Authentication (recommended)Storage Account Credentials <p>lakeFS uses environment variables to determine credentials to use for authentication. The following authentication methods are supported:</p> <ol> <li>Managed Service Identity (MSI)</li> <li>Service Principal RBAC</li> <li>Azure CLI</li> </ol> <p>For deployments inside the Azure ecosystem it is recommended to use a managed identity.</p> <p>More information on authentication methods and environment variables can be found here</p> <p>Storage account credentials can be set directly in the lakeFS configuration using the following parameters:</p> <ul> <li><code>blockstore.azure.storage_account</code></li> <li><code>blockstore.azure.storage_access_key</code></li> </ul> <p>Limitations</p> <p>Please note that using this authentication method limits lakeFS to the scope of the given storage account.</p> <p>Specifically, the following operations will not work:</p> <ol> <li>Import of data from different storage accounts</li> <li>Copy/Read/Write of data that was imported from a different storage account</li> <li>Create pre-signed URL for data that was imported from a different storage account</li> </ol>"},{"location":"howto/deploy/azure/#how-to-create-service-principal-for-resource-group","title":"How to Create Service Principal for Resource Group","text":"<p>It is recommended to create a resource group that consists of all the resources lakeFS should have access to.</p> <p>Using a resource group will allow dynamic removal/addition of services from the group, effectively providing/preventing access for lakeFS to these resources without requiring any changes in configuration in lakeFS or providing lakeFS with any additional credentials.</p> <p>The minimal role required for the service principal is \"Storage Blob Data Contributor\"</p> <p>The following Azure CLI command creates a service principal for a resource group called \"lakeFS\" with permission to access (read/write/delete) Blob Storage resources in the resource group and with an expiry of 5 years</p> <pre><code>az ad sp create-for-rbac \\\n--role \"Storage Blob Data Contributor\" \\\n--scopes /subscriptions/XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX/resourceGroups/lakeFS --years 5\n\nCreating 'Storage Blob Data Contributor' role assignment under scope '/subscriptions/947382ea-681a-4541-99ab-b718960c6289/resourceGroups/lakeFS'\nThe output includes credentials that you must protect. Be sure that you do not include these credentials in your code or check the credentials into your source control. For more information, see https://aka.ms/azadsp-cli\n{\n\"appId\": \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\",\n\"displayName\": \"azure-cli-2023-01-30-06-18-30\",\n\"password\": \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\",\n\"tenant\": \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"\n}\n</code></pre> <p>The command output should be used to populate the following environment variables:</p> <pre><code>AZURE_CLIENT_ID      =  $appId\nAZURE_TENANT_ID      =  $tenant\nAZURE_CLIENT_SECRET  =  $password\n</code></pre> <p>Danger</p> <p>Service Principal credentials have an expiry date and lakeFS will lose access to resources unless credentials are renewed on time.</p> <p>Info</p> <p>It is possible to provide both account based credentials and environment variables to lakeFS. In that case - lakeFS will use the account credentials for any access to data located in the given account, and will try to use the identity credentials for any data located outside the given account.</p>"},{"location":"howto/deploy/azure/#kv-store","title":"K/V Store","text":"<p>lakeFS stores metadata in a database for its versioning engine. This is done via a Key-Value interface that can be implemented on any DB engine and lakeFS comes with several built-in driver implementations (You can read more about it here).</p> <p>The database used doesn't have to be a dedicated K/V database.</p> CosmosDBPostgreSQL <p>CosmosDB is a managed database service provided by Azure.</p> <p>lakeFS supports CosmosDB For NoSQL as a database backend.</p> <ol> <li>Follow the official Azure documentation on how to create a CosmosDB account for NoSQL and connect to it.</li> <li>Once your CosmosDB account is set up, you can create a Database for lakeFS. For lakeFS ACID guarantees, make sure to select the Bounded staleness consistency, for single region deployments.</li> <li>Create a new container in the database and select type <code>partitionKey</code> as the Partition key (case sensitive).</li> <li>Pass the endpoint, database name and container name to lakeFS as described in the [configuration guide][config-reference-azure-block]. You can either pass the CosmosDB's account read-write key to lakeFS, or use a managed identity to authenticate to CosmosDB, as described earlier.</li> </ol> <p>Below we show you how to create a database on Azure Database, but you can use any PostgreSQL database as long as it's accessible by your lakeFS installation.</p> <p>If you already have a database, take note of the connection string and skip to the next step</p> <ol> <li>Follow the official Azure documentation on how to create a PostgreSQL instance and connect to it. Make sure that you're using PostgreSQL version &gt;= 11.</li> <li>Once your Azure Database for PostgreSQL server is set up and the server is in the Available state, take note of the endpoint and username. </li> <li>Make sure your Access control roles allow you to connect to the database instance.</li> </ol>"},{"location":"howto/deploy/azure/#4-run-the-lakefs-server","title":"4. Run the lakeFS server","text":"<p>Now that you've chosen and configured object storage, a K/V store, and authentication\u2014you're ready to configure and run lakeFS. There are three different ways you can run lakeFS:</p> Azure VMDockerAzure Kubernetes Service (AKS) <p>Connect to your VM instance using SSH:</p> <ol> <li>Create a <code>config.yaml</code> on your VM, with the following parameters: <pre><code>---\ndatabase:\n    type: \"postgres\"\n    postgres:\n    connection_string: \"[DATABASE_CONNECTION_STRING]\"\n\nauth:\n    encrypt:\n    # replace this with a randomly-generated string. Make sure to keep it safe!\n    secret_key: \"[ENCRYPTION_SECRET_KEY]\"\n\nblockstore:\n    type: azure\n    azure:\n</code></pre></li> <li>Download the binary to run on the VM.</li> <li>Run the <code>lakefs</code> binary: <pre><code>lakefs --config config.yaml run\n</code></pre></li> </ol> <p>Note</p> <p>It's preferable to run the binary as a service using systemd or your operating system's facilities.</p> <p>To support container-based environments, you can configure lakeFS using environment variables. Here is a <code>docker run</code> command to demonstrate starting lakeFS using Docker:</p> <pre><code>docker run \\\n    --name lakefs \\\n    -p 8000:8000 \\\n    -e LAKEFS_DATABASE_TYPE=\"postgres\" \\\n    -e LAKEFS_DATABASE_POSTGRES_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\\n    -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\\n    -e LAKEFS_BLOCKSTORE_TYPE=\"azure\" \\\n    -e LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCOUNT=\"[YOUR_STORAGE_ACCOUNT]\" \\\n    -e LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCESS_KEY=\"[YOUR_ACCESS_KEY]\" \\\n    treeverse/lakefs:latest run\n</code></pre> <p>See the [reference][config-envariables] for a complete list of environment variables.</p> <p>You can install lakeFS on Kubernetes using a Helm chart.</p> <p>To install lakeFS with Helm:</p> <ol> <li>Copy the Helm values file relevant for Azure Blob:</li> </ol> <p><pre><code>secrets:\n    # replace this with the connection string of the database you created in a previous step:\n    databaseConnectionString: [DATABASE_CONNECTION_STRING]\n    # replace this with a randomly-generated string\n    authEncryptSecretKey: [ENCRYPTION_SECRET_KEY]\nlakefsConfig: |\n    blockstore:\n        type: azure\n        azure:\n    #  If you chose to authenticate via access key, unmark the following rows and insert the values from the previous step\n    #  storage_account: [your storage account]\n    #  storage_access_key: [your access key]\n</code></pre> 1. Fill in the missing values and save the file as <code>conf-values.yaml</code>. For more configuration options, see our Helm chart README.</p> <p>Note</p> <p>The <code>lakefsConfig</code> parameter is the lakeFS configuration documented here but without sensitive information. Sensitive information like <code>databaseConnectionString</code> is given through separate parameters, and the chart will inject it into Kubernetes secrets.</p> <ol> <li>In the directory where you created <code>conf-values.yaml</code>, run the following commands: <pre><code># Add the lakeFS repository\nhelm repo add lakefs https://charts.lakefs.io\n# Deploy lakeFS\nhelm install my-lakefs lakefs/lakefs -f conf-values.yaml\n</code></pre> my-lakefs is the Helm Release name.</li> </ol>"},{"location":"howto/deploy/azure/#load-balancing","title":"Load balancing","text":"<p>To configure a load balancer to direct requests to the lakeFS servers you can use the <code>LoadBalancer</code> Service type or a Kubernetes Ingress. By default, lakeFS operates on port <code>8000</code> and exposes a <code>/_health</code> endpoint that you can use for health checks.</p> <p>Info</p> <p>The NGINX Ingress Controller by default limits the client body size to 1 MiB.</p> <p>Some clients use bigger chunks to upload objects - for example, multipart upload to lakeFS using the [S3-compatible Gateway][s3-gateway] or a simple PUT request using the [OpenAPI Server][openapi].</p> <p>Check out the Nginx documentation for increasing the limit, or an example of Nginx configuration with MinIO.</p>"},{"location":"howto/deploy/azure/#create-the-admin-user","title":"Create the admin user","text":"<p>When you first open the lakeFS UI, you will be asked to create an initial admin user.</p> <ol> <li>Open <code>http://&lt;lakefs-host&gt;/</code> in your browser. If you haven't set up a load balancer, this will likely be <code>http://&lt;instance ip address&gt;:8000/</code></li> <li>On first use, you'll be redirected to the setup page:    </li> <li>Follow the steps to create an initial administrator user. Save the credentials you\u2019ve received somewhere safe, you won\u2019t be able to see them again!    </li> <li>Follow the link and go to the login screen. Use the credentials from the previous step to log in.</li> </ol>"},{"location":"howto/deploy/azure/#create-your-first-repository","title":"Create your first repository","text":"<ol> <li>Use the credentials from the previous step to log in</li> <li>Click Create Repository and choose Blank Repository.    </li> <li>Under Storage Namespace, enter a path to your desired location on the object store. This is where data written to this repository will be stored.</li> <li>Click Create Repository</li> <li>You should now have a configured repository, ready to use!    </li> </ol> <p>Congratulations</p> <p>Your environment is now ready \ud83e\udd29</p>"},{"location":"howto/deploy/gcp/","title":"Deploy lakeFS on GCP","text":"<p>Tip</p> <p>The instructions given here are for a self-managed deployment of lakeFS on GCP.  For a hosted lakeFS service with guaranteed SLAs, please contact us for details of lakeFS Cloud on GCP.</p> <p>When you deploy lakeFS on GCP these are the options available to use:</p> <p></p> <p>\u23f0 Expected deployment time: 25 min</p>"},{"location":"howto/deploy/gcp/#create-a-database","title":"Create a Database","text":"<p>lakeFS requires a PostgreSQL database to synchronize actions on your repositories. We will show you how to create a database on Google Cloud SQL, but you can use any PostgreSQL database as long as it's accessible by your lakeFS installation.</p> <p>If you already have a database, take note of the connection string and skip to the next step</p> <ol> <li>Follow the official Google documentation on how to create a PostgreSQL instance.    Make sure you're using PostgreSQL version &gt;= 11.</li> <li>On the Users tab in the console, create a user. The lakeFS installation will use it to connect to your database.</li> <li>Choose the method by which lakeFS will connect to your database. Google recommends using    the SQL Auth Proxy.</li> </ol>"},{"location":"howto/deploy/gcp/#run-the-lakefs-server","title":"Run the lakeFS Server","text":"GCE InstanceDockerGKE <ol> <li>Save the following configuration file as <code>config.yaml</code>:</li> </ol> <pre><code>---\ndatabase:\n    type: \"postgres\"\n    postgres:\n    connection_string: \"[DATABASE_CONNECTION_STRING]\"\nauth:\n    encrypt:\n    # replace this with a randomly-generated string:\n    secret_key: \"[ENCRYPTION_SECRET_KEY]\"\nblockstore:\n    type: gs\n    # Uncomment the following lines to give lakeFS access to your buckets using a service account:\n    # gs:\n    #   credentials_json: [YOUR SERVICE ACCOUNT JSON STRING]\n</code></pre> <ol> <li>Download the binary to run on the GCE instance.</li> <li>Run the <code>lakefs</code> binary on the GCE machine: <pre><code>lakefs --config config.yaml run\n</code></pre> Note: it is preferable to run the binary as a service using systemd or your operating system's facilities.</li> </ol> <p>To support container-based environments like Google Cloud Run, lakeFS can be configured using environment variables. Here is a <code>docker run</code> command to demonstrate starting lakeFS using Docker:</p> <pre><code>docker run \\\n    --name lakefs \\\n    -p 8000:8000 \\\n    -e LAKEFS_DATABASE_TYPE=\"postgres\" \\\n    -e LAKEFS_DATABASE_POSTGRES_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\\n    -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\\n    -e LAKEFS_BLOCKSTORE_TYPE=\"gs\" \\\n    treeverse/lakefs:latest run\n</code></pre> <p>See the [reference][config-envariables] for a complete list of environment variables.</p> <p>You can install lakeFS on Kubernetes using a Helm chart.</p> <p>To install lakeFS with Helm:</p> <ol> <li>Copy the Helm values file relevant for Google Storage:</li> </ol> <p><pre><code>secrets:\n    # replace DATABASE_CONNECTION_STRING with the connection string of the database you created in a previous step.\n    # e.g.: postgres://postgres:myPassword@localhost/postgres:5432\n    databaseConnectionString: [DATABASE_CONNECTION_STRING]\n    # replace this with a randomly-generated string\n    authEncryptSecretKey: [ENCRYPTION_SECRET_KEY]\nlakefsConfig: |\n    blockstore:\n        type: gs\n        # Uncomment the following lines to give lakeFS access to your buckets using a service account:\n        # gs:\n        #   credentials_json: [YOUR SERVICE ACCOUNT JSON STRING]\n</code></pre> 1. Fill in the missing values and save the file as <code>conf-values.yaml</code>. For more configuration options, see our Helm chart README.</p> <p>Note</p> <p>The <code>lakefsConfig</code> parameter is the lakeFS configuration documented here but without sensitive information. Sensitive information like <code>databaseConnectionString</code> is given through separate parameters, and the chart will inject it into Kubernetes secrets.</p> <ol> <li>In the directory where you created <code>conf-values.yaml</code>, run the following commands:</li> </ol> <pre><code># Add the lakeFS repository\nhelm repo add lakefs https://charts.lakefs.io\n# Deploy lakeFS\nhelm install my-lakefs lakefs/lakefs -f conf-values.yaml\n</code></pre> <p>my-lakefs is the Helm Release name.</p>"},{"location":"howto/deploy/gcp/#load-balancing","title":"Load balancing","text":"<p>To configure a load balancer to direct requests to the lakeFS servers you can use the <code>LoadBalancer</code> Service type or a Kubernetes Ingress. By default, lakeFS operates on port 8000 and exposes a <code>/_health</code> endpoint that you can use for health checks.</p> <p>Tip</p> <p>The NGINX Ingress Controller by default limits the client body size to 1 MiB.</p> <p>Some clients use bigger chunks to upload objects - for example, multipart upload to lakeFS using the [S3-compatible Gateway][s3-gateway] or a simple PUT request using the [OpenAPI Server][openapi].</p> <p>Checkout the Nginx documentation for increasing the limit, or an example of Nginx configuration with MinIO.</p>"},{"location":"howto/deploy/gcp/#create-the-admin-user","title":"Create the admin user","text":"<p>When you first open the lakeFS UI, you will be asked to create an initial admin user.</p> <ol> <li>Open <code>http://&lt;lakefs-host&gt;/</code> in your browser. If you haven't set up a load balancer, this will likely be <code>http://&lt;instance ip address&gt;:8000/</code></li> <li>On first use, you'll be redirected to the setup page:    </li> <li>Follow the steps to create an initial administrator user. Save the credentials you\u2019ve received somewhere safe, you won\u2019t be able to see them again!    </li> <li>Follow the link and go to the login screen. Use the credentials from the previous step to log in.</li> </ol>"},{"location":"howto/deploy/gcp/#create-your-first-repository","title":"Create your first repository","text":"<ol> <li>Use the credentials from the previous step to log in</li> <li>Click Create Repository and choose Blank Repository.    </li> <li>Under Storage Namespace, enter a path to your desired location on the object store. This is where data written to this repository will be stored.</li> <li>Click Create Repository</li> <li>You should now have a configured repository, ready to use!    </li> </ol> <p>Congratulations</p> <p>Your environment is now ready \ud83e\udd29</p>"},{"location":"howto/deploy/onprem/","title":"On-Premises Deployment","text":"<p>Tip</p> <p>The instructions given here are for a self-managed deployment of lakeFS. For a hosted lakeFS service with guaranteed SLAs, try lakeFS Cloud</p> <p>\u23f0 Expected deployment time: 25 min</p>"},{"location":"howto/deploy/onprem/#prerequisites","title":"Prerequisites","text":"<p>To use lakeFS on-premises, you can either use the local blockstore adapter or have access to an S3-compatible object store such as MinIO.</p> <p>For more information on how to set up MinIO, see the official deployment guide</p>"},{"location":"howto/deploy/onprem/#setting-up-a-database","title":"Setting up a database","text":"<p>lakeFS requires a PostgreSQL database to synchronize actions on your repositories. This section assumes that you already have a PostgreSQL &gt;= 11.0 database accessible.</p>"},{"location":"howto/deploy/onprem/#setting-up-a-lakefs-server","title":"Setting up a lakeFS Server","text":"LinuxDockerKubernetes <p>Connect to your host using SSH: 1. Create a <code>config.yaml</code> on your VM, with the following parameters: <pre><code>---\ndatabase:\n    type: \"postgres\"\n    postgres:\n    connection_string: \"[DATABASE_CONNECTION_STRING]\"\n\nauth:\n    encrypt:\n    # replace this with a randomly-generated string. Make sure to keep it safe!\n    secret_key: \"[ENCRYPTION_SECRET_KEY]\"\n\nblockstore:\n    type: s3\n    s3:\n        force_path_style: true\n        endpoint: http://&lt;minio_endpoint&gt;\n        discover_bucket_region: false\n        credentials:\n        access_key_id: &lt;minio_access_key&gt;\n        secret_access_key: &lt;minio_secret_key&gt;\n</code></pre></p> <p>Info</p> <p>Notice that the lakeFS Blockstore type is set to <code>s3</code> - This configuration works with S3-compatible storage engines such as MinIO.</p> <ol> <li>Download the binary to the server.</li> <li>Run the <code>lakefs</code> binary: <pre><code>lakefs --config config.yaml run\n</code></pre></li> </ol> <p>Note</p> <p>It's preferable to run the binary as a service using systemd or your operating system's facilities.</p> <p>To support container-based environments, you can configure lakeFS using environment variables. Here is a <code>docker run</code> command to demonstrate starting lakeFS using Docker:</p> <pre><code>docker run \\\n    --name lakefs \\\n    -p 8000:8000 \\\n    -e LAKEFS_DATABASE_TYPE=\"postgres\" \\\n    -e LAKEFS_DATABASE_POSTGRES_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\\n    -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\\n    -e LAKEFS_BLOCKSTORE_TYPE=\"s3\" \\\n    -e LAKEFS_BLOCKSTORE_S3_FORCE_PATH_STYLE=\"true\" \\\n    -e LAKEFS_BLOCKSTORE_S3_ENDPOINT=\"http://&lt;minio_endpoint&gt;\" \\\n    -e LAKEFS_BLOCKSTORE_S3_DISCOVER_BUCKET_REGION=\"false\" \\\n    -e LAKEFS_BLOCKSTORE_S3_CREDENTIALS_ACCESS_KEY_ID=\"&lt;minio_access_key&gt;\" \\\n    -e LAKEFS_BLOCKSTORE_S3_CREDENTIALS_SECRET_ACCESS_KEY=\"&lt;minio_secret_key&gt;\" \\\n    treeverse/lakefs:latest run\n</code></pre> <p>Info</p> <p>Notice that the lakeFS Blockstore type is set to <code>s3</code> - This configuration works with S3-compatible storage engines such as MinIO.</p> <p>See the [reference][config-envariables] for a complete list of environment variables.</p> <p>You can install lakeFS on Kubernetes using a Helm chart.</p> <p>To install lakeFS with Helm:</p> <ol> <li> <p>Copy the Helm values file relevant for S3-Compatible storage (MinIO in this example):</p> <pre><code>secrets:\n    # replace this with the connection string of the database you created in a previous step:\n    databaseConnectionString: [DATABASE_CONNECTION_STRING]\n    # replace this with a randomly-generated string\n    authEncryptSecretKey: [ENCRYPTION_SECRET_KEY]\nlakefsConfig: |\n    blockstore:\n        type: s3\n        s3:\n        force_path_style: true\n        endpoint: http://&lt;minio_endpoint&gt;\n        discover_bucket_region: false\n        credentials:\n            access_key_id: &lt;minio_access_key&gt;\n            secret_access_key: &lt;minio_secret_key&gt;\n</code></pre> <p>Tip</p> <p>Notice that the lakeFS Blockstore type is set to <code>s3</code> - This configuration works with S3-compatible storage engines such as MinIO</p> </li> <li> <p>Fill in the missing values and save the file as <code>conf-values.yaml</code>. For more configuration options, see our Helm chart README.     !!! note     The <code>lakefsConfig</code> parameter is the lakeFS configuration documented here but without sensitive information.     Sensitive information like <code>databaseConnectionString</code> is given through separate parameters, and the chart will inject it into Kubernetes secrets.</p> </li> <li>In the directory where you created <code>conf-values.yaml</code>, run the following commands:     <pre><code># Add the lakeFS repository\nhelm repo add lakefs https://charts.lakefs.io\n# Deploy lakeFS\nhelm install my-lakefs lakefs/lakefs -f conf-values.yaml\n</code></pre></li> </ol> <p>my-lakefs is the Helm Release name.</p>"},{"location":"howto/deploy/onprem/#load-balancing","title":"Load balancing","text":"<p>To configure a load balancer to direct requests to the lakeFS servers you can use the <code>LoadBalancer</code> Service type or a Kubernetes Ingress. By default, lakeFS operates on port 8000 and exposes a <code>/_health</code> endpoint that you can use for health checks.</p> <p>Info</p> <p>The NGINX Ingress Controller by default limits the client body size to 1 MiB.</p> <p>Some clients use bigger chunks to upload objects - for example, multipart upload to lakeFS using the [S3-compatible Gateway][s3-gateway] or a simple PUT request using the [OpenAPI Server][openapi].</p> <p>Checkout the Nginx documentation for increasing the limit, or an example of Nginx configuration with MinIO.</p>"},{"location":"howto/deploy/onprem/#secure-connection","title":"Secure connection","text":"<p>Using a load balancer or cluster manager for TLS/SSL termination is recommended. It helps speed the decryption process and reduces the processing burden from lakeFS.</p> <p>In case lakeFS needs to listen and serve with HTTPS, for example for development purposes, update its config yaml with the following section:</p> <pre><code>tls:\n  enabled: true\n  cert_file: server.crt   # provide path to your certificate file\n  key_file: server.key    # provide path to your server private key\n</code></pre>"},{"location":"howto/deploy/onprem/#local-blockstore","title":"Local Blockstore","text":"<p>You can configure a block adapter to a POSIX compatible storage location shared by all lakeFS instances. Using the shared storage location, both data and metadata will be stored there.</p> <p>Using the local blockstore import and allowing lakeFS access to a specific prefix, it is possible to import files from a shared location. Import is not enabled by default, as it doesn't assume the local path is shared and there is a security concern about accessing a path outside the specified in the blockstore configuration. Enabling is done by <code>blockstore.local.import_enabled</code> and <code>blockstore.local.allowed_external_prefixes</code> as described in the configuration reference.</p>"},{"location":"howto/deploy/onprem/#sample-configuration-using-local-blockstore","title":"Sample configuration using local blockstore","text":"<pre><code>database:\n  type: \"postgres\"\n  postgres:\n    connection_string: \"[DATABASE_CONNECTION_STRING]\"\n\nauth:\n  encrypt:\n    # replace this with a randomly-generated string. Make sure to keep it safe!\n    secret_key: \"[ENCRYPTION_SECRET_KEY]\"\n\nblockstore:\n  type: local\n  local:\n    path: /shared/location/lakefs_data    # location where data and metadata kept by lakeFS\n    import_enabled: true                  # required to be true to enable import files\n                                          # from `allowed_external_prefixes` locations\n    allowed_external_prefixes:\n      - /shared/location/files_to_import  # location with files we can import into lakeFS, require access from lakeFS\n</code></pre>"},{"location":"howto/deploy/onprem/#limitations","title":"Limitations","text":"<ul> <li>Using a local adapter on a shared location is relativly new and not battle-tested yet</li> <li>lakeFS doesn't control the way a shared location is managed across machines</li> <li>When using lakectl or the lakeFS UI, you can currently import only directories. If you need to import a single file, use the HTTP API or API Clients with <code>type=object</code> in the request body and <code>destination=&lt;full-path-to-file&gt;</code>.</li> <li>Garbage collector (for committed and uncommitted) and lakeFS Hadoop FileSystem currently unsupported</li> </ul>"},{"location":"howto/deploy/onprem/#create-the-admin-user","title":"Create the admin user","text":"<p>When you first open the lakeFS UI, you will be asked to create an initial admin user.</p> <ol> <li>Open <code>http://&lt;lakefs-host&gt;/</code> in your browser. If you haven't set up a load balancer, this will likely be <code>http://&lt;instance ip address&gt;:8000/</code></li> <li>On first use, you'll be redirected to the setup page:    </li> <li>Follow the steps to create an initial administrator user. Save the credentials you\u2019ve received somewhere safe, you won\u2019t be able to see them again!    </li> <li>Follow the link and go to the login screen. Use the credentials from the previous step to log in.</li> </ol>"},{"location":"howto/deploy/onprem/#create-your-first-repository","title":"Create your first repository","text":"<ol> <li>Use the credentials from the previous step to log in</li> <li>Click Create Repository and choose Blank Repository.    </li> <li>Under Storage Namespace, enter a path to your desired location on the object store. This is where data written to this repository will be stored.</li> <li>Click Create Repository</li> <li>You should now have a configured repository, ready to use!    </li> </ol> <p>Congratulations</p> <p>Your environment is now ready \ud83e\udd29</p>"},{"location":"howto/deploy/upgrade/","title":"Upgrading lakeFS","text":"<p>Info</p> <p>For a fully managed lakeFS service with guaranteed SLAs, try lakeFS Cloud</p> <p>Upgrading lakeFS from a previous version usually just requires re-deploying with the latest image (or downloading the latest version if you're using the binary). If you're upgrading, check whether the release requires a migration.</p>"},{"location":"howto/deploy/upgrade/#when-db-migrations-are-required","title":"When DB migrations are required","text":""},{"location":"howto/deploy/upgrade/#lakefs-01030-or-greater","title":"lakeFS 0.103.0 or greater","text":"<p>Version 0.103.0 added support for rolling KV upgrade. This means that users who already migrated to the KV ref-store (versions 0.80.0 and above) no longer have to pass through specific versions for migration. This includes ACL migration which was introduced in lakeFS version 0.98.0. Running <code>lakefs migrate up</code> on the latest lakeFS version will perform all the necessary migrations up to that point.</p>"},{"location":"howto/deploy/upgrade/#lakefs-0800-or-greater-kv-migration","title":"lakeFS 0.80.0 or greater (KV Migration)","text":"<p>Starting with version 0.80.2, lakeFS has transitioned from using a PostgreSQL based database implementation to a Key-Value datastore interface supporting multiple database implementations. More information can be found here. Users upgrading from a previous version of lakeFS must pass through the KV migration version (0.80.2) before upgrading to newer versions of lakeFS.</p> <p>Important</p> <p>Pre Migrate Requirements:</p> <ul> <li>Users using OS environment variables for database configuration must define the <code>connection_string</code> explicitly or as environment variable before proceeding with the migration.</li> <li>Database storage free capacity of at least twice the amount of the currently used capacity</li> <li>It is strongly recommended to perform these additional steps:</li> <li>Commit all uncommitted data on branches</li> <li>Create a snapshot of your database</li> <li>By default, old database tables are not being deleted by the migration process, and should be removed manually after a successful migration. To enable table drop as part of the migration, set the <code>database.drop_tables</code> configuration param to <code>true</code></li> </ul>"},{"location":"howto/deploy/upgrade/#migration-steps","title":"Migration Steps","text":"<p>For each lakeFS instance currently running with the database</p> <ol> <li> <p>Modify the <code>database</code> section under lakeFS configuration yaml:</p> <ol> <li>Add <code>type</code> field with <code>\"postgres\"</code> as value</li> <li>Copy the current configuration parameters to a new section called <code>postgres</code> <pre><code>---\ndatabase:\ntype: \"postgres\"\nconnection_string: \"postgres://localhost:5432/postgres?sslmode=disable\"\nmax_open_connections: 20\n\npostgres:\n  connection_string: \"postgres://localhost:5432/postgres?sslmode=disable\"\n  max_open_connections: 20\n</code></pre></li> </ol> </li> <li> <p>Stop all lakeFS instances</p> </li> <li>Using the <code>lakefs</code> binary for the new version (0.80.2), run the following:    <pre><code>lakefs migrate up\n</code></pre></li> <li>lakeFS will run the migration process, which in the end should display the following message with no errors:    <pre><code>time=\"2022-08-10T14:46:25Z\" level=info msg=\"KV Migration took 717.629563ms\" func=\"pkg/logging.(*logrusEntryWrapper).Infof\" file=\"build/pkg/logging/logger.go:246\" TempDir=/tmp/kv_migrate_2913402680\n</code></pre></li> <li>It is now possible to remove the old database configuration. The updated configuration should look as such:    <pre><code>---\ndatabase:\n type: \"postgres\"\n\n postgres:\n   connection_string: \"postgres://localhost:5432/postgres?sslmode=disable\"\n   max_open_connections: 20\n</code></pre></li> <li>Deploy (or run) the new version of lakeFS.</li> </ol>"},{"location":"howto/deploy/upgrade/#lakefs-0300-or-greater","title":"lakeFS 0.30.0 or greater","text":"<p>In case migration is required, you first need to stop the running lakeFS service. Using the <code>lakefs</code> binary for the new version, run the following:</p> <pre><code>lakefs migrate up\n</code></pre> <p>Deploy (or run) the new version of lakeFS.</p> <p>Note that an older version of lakeFS cannot run on a migrated database.</p>"},{"location":"howto/deploy/upgrade/#prior-to-lakefs-0300","title":"Prior to lakeFS 0.30.0","text":"<p>Note</p> <p>with lakeFS &lt; 0.30.0, you should first upgrade to 0.30.0 following this guide. Then, proceed to upgrade to the newest version.</p> <p>Starting version 0.30.0, lakeFS handles your committed metadata in a new way, which is more robust and has better performance. To move your existing data, you will need to run the following upgrade commands.</p> <p>Verify lakeFS version == 0.30.0 (can skip if using Docker)</p> <pre><code>lakefs --version\n</code></pre> <p>Migrate data from the previous format:</p> <pre><code>lakefs migrate db\n</code></pre> <p>Or migrate using Docker image:</p> <pre><code>docker run --rm -it -e LAKEFS_DATABASE_CONNECTION_STRING=&lt;database connection string&gt; treeverse/lakefs:rocks-migrate migrate db\n</code></pre> <p>Once migrated, it is possible to now use more recent lakeFS versions. Please refer to their release notes for more information on ugrading and usage).</p> <p>If you want to start over, discarding your existing data, you need to explicitly state this in your lakeFS configuration file. To do so, add the following to your configuration (relevant only for 0.30.0):</p> <pre><code>cataloger:\n  type: rocks\n</code></pre>"},{"location":"howto/deploy/upgrade/#data-migration-for-version-v0500","title":"Data Migration for Version v0.50.0","text":"<p>Warning</p> <p>If you are using a version before 0.50.0, you must first perform the previous upgrade to that version.</p>"},{"location":"howto/garbage-collection/gc/","title":"Garbage Collection","text":"<p>Tip</p> <p>lakeFS Cloud users enjoy a managed garbage collection service, and do not need to run this Spark program.</p> <p>Tip</p> <p>lakeFS Enterprise users can run a stand alone GC program, instead of this Spark program.</p> <p>By default, lakeFS keeps all your objects forever. This allows you to travel back in time to previous versions of your data. However, sometimes you may want to remove the objects from the underlying storage completely. Reasons for this include cost-reduction and privacy policies.</p> <p>The garbage collection (GC) job is a Spark program that removes the following from the underlying storage:</p> <ol> <li>Committed objects that have been deleted (or replaced) in lakeFS, and are considered expired according to rules you define.</li> <li>Uncommitted objects that are no longer accessible<ul> <li>For example, objects deleted before ever being committed.</li> </ul> </li> </ol>"},{"location":"howto/garbage-collection/gc/#garbage-collection-rules","title":"Garbage collection rules","text":"<p>Info</p> <p>These rules only apply to objects that have been committed at some point. Without retention rules, only inaccessible uncommitted objects will be removed by the job.</p> <p>Garbage collection rules determine for how long an object is kept in the storage after it is deleted (or replaced) in lakeFS. For every branch, the GC job retains deleted objects for the number of days defined for the branch. In the absence of a branch-specific rule, the default rule for the repository is used. If an object is present in more than one branch ancestry, it is removed only after the retention period has ended for all relevant branches.</p> <p>Example GC rules for a repository:</p> <pre><code>{\n  \"default_retention_days\": 14,\n  \"branches\": [\n    {\"branch_id\": \"main\", \"retention_days\": 21},\n    {\"branch_id\": \"dev\", \"retention_days\": 7}\n  ]\n}\n</code></pre> <p>In the above example, objects will be retained for 14 days after deletion by default. However, if present in the branch <code>main</code>, objects will be retained for 21 days. Objects present only in the <code>dev</code> branch will be retained for 7 days after they are deleted.</p>"},{"location":"howto/garbage-collection/gc/#how-to-configure-garbage-collection-rules","title":"How to configure garbage collection rules","text":"<p>To define retention rules, either use the <code>lakectl</code> command, the lakeFS web UI, or API:</p> CLIWeb UI <p>Create a JSON file with your GC rules:</p> <pre><code>cat &lt;&lt;EOT &gt;&gt; example_repo_gc_rules.json\n{\n\"default_retention_days\": 14,\n\"branches\": [\n    {\"branch_id\": \"main\", \"retention_days\": 21},\n    {\"branch_id\": \"dev\", \"retention_days\": 7}\n]\n}\nEOT\n</code></pre> <p>Set the GC rules using <code>lakectl</code>: <pre><code>lakectl gc set-config lakefs://example-repo -f example_repo_gc_rules.json \n</code></pre></p> <p>From the lakeFS web UI:</p> <ol> <li>Navigate to the main page of your repository.</li> <li>Go to Settings -&gt; Garbage Collection.</li> <li>Click Edit policy and paste your GC rule into the text box as a JSON.</li> <li>Save your changes.</li> </ol> <p></p>"},{"location":"howto/garbage-collection/gc/#how-to-run-the-garbage-collection-job","title":"How to run the garbage collection job","text":"<p>To run the job, use the following <code>spark-submit</code> command (or using your preferred method of running Spark programs).</p> AWSAzureGCP <pre><code>spark-submit --class io.treeverse.gc.GarbageCollection \\\n    --packages org.apache.hadoop:hadoop-aws:2.7.7 \\\n    -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1  \\\n    -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\\n    -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\\n    -c spark.hadoop.fs.s3a.access.key=&lt;S3_ACCESS_KEY&gt; \\\n    -c spark.hadoop.fs.s3a.secret.key=&lt;S3_SECRET_KEY&gt; \\\n    http://treeverse-clients-us-east.s3-website-us-east-1.amazonaws.com/lakefs-spark-client/0.14.3/lakefs-spark-client-assembly-0.14.3.jar \\\n    example-repo us-east-1\n</code></pre> <p>If you want to access your storage using the account key:</p> <pre><code>spark-submit --class io.treeverse.gc.GarbageCollection \\\n    --packages org.apache.hadoop:hadoop-aws:3.2.1 \\\n    -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1  \\\n    -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\\n    -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\\n    -c spark.hadoop.fs.azure.account.key.&lt;AZURE_STORAGE_ACCOUNT&gt;.dfs.core.windows.net=&lt;AZURE_STORAGE_ACCESS_KEY&gt; \\\n    http://treeverse-clients-us-east.s3-website-us-east-1.amazonaws.com/lakefs-spark-client/0.14.3/lakefs-spark-client-assembly-0.14.3.jar \\\n    example-repo\n</code></pre> <p>Or, if you want to access your storage using an Azure service principal:</p> <pre><code>spark-submit --class io.treeverse.gc.GarbageCollection \\\n    --packages org.apache.hadoop:hadoop-aws:3.2.1 \\\n    -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1  \\\n    -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\\n    -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\\n    -c spark.hadoop.fs.azure.account.auth.type.&lt;AZURE_STORAGE_ACCOUNT&gt;.dfs.core.windows.net=OAuth \\\n    -c spark.hadoop.fs.azure.account.oauth.provider.type.&lt;AZURE_STORAGE_ACCOUNT&gt;.dfs.core.windows.net=org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider \\\n    -c spark.hadoop.fs.azure.account.oauth2.client.id.&lt;AZURE_STORAGE_ACCOUNT&gt;.dfs.core.windows.net=&lt;application-id&gt; \\\n    -c spark.hadoop.fs.azure.account.oauth2.client.secret.&lt;AZURE_STORAGE_ACCOUNT&gt;.dfs.core.windows.net=&lt;service-credential-key&gt; \\\n    -c spark.hadoop.fs.azure.account.oauth2.client.endpoint.&lt;AZURE_STORAGE_ACCOUNT&gt;.dfs.core.windows.net=https://login.microsoftonline.com/&lt;directory-id&gt;/oauth2/token \\\n    http://treeverse-clients-us-east.s3-website-us-east-1.amazonaws.com/lakefs-spark-client/0.14.3/lakefs-spark-client-assembly-0.14.3.jar \\\n    example-repo\n</code></pre> <p>Note</p> <ul> <li>On Azure, GC was tested only on Spark 3.3.0, but may work with other Spark and Hadoop versions.</li> <li>In case you don't have <code>hadoop-azure</code> package as part of your environment, you should add the package to your spark-submit with <code>--packages org.apache.hadoop:hadoop-azure:3.2.1</code></li> <li>For GC to work on Azure blob, soft delete should be disabled.</li> </ul> <p>Warning</p> <p>At the moment, only the \"mark\" phase of the Garbage Collection is supported for GCP. That is, this program will output a list of expired objects, and you will have to delete them manually. We have concrete plans to extend this support to actually delete the objects.</p> <pre><code>spark-submit --class  io.treeverse.gc.GarbageCollection \\\n    --jars https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar \\\n    -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1  \\\n    -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\\n    -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\\n    -c spark.hadoop.google.cloud.auth.service.account.enable=true \\\n    -c spark.hadoop.google.cloud.auth.service.account.json.keyfile=&lt;PATH_TO_JSON_KEYFILE&gt; \\\n    -c spark.hadoop.fs.gs.project.id=&lt;GCP_PROJECT_ID&gt; \\\n    -c spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem \\\n    -c spark.hadoop.fs.AbstractFileSystem.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS \\\n    -c spark.hadoop.lakefs.gc.do_sweep=false  \\\n    http://treeverse-clients-us-east.s3-website-us-east-1.amazonaws.com/lakefs-spark-client/0.14.3/lakefs-spark-client-assembly-0.14.3.jar \\\n    example-repo\n</code></pre> <p>This program will not delete anything. Instead, it will find all the objects that are safe to delete and save a list containing all their keys, in Parquet format. The list will then be found under the path:</p> <pre><code>gs://&lt;STORAGE_NAMESPACE&gt;/_lakefs/retention/gc/unified/&lt;RUN_ID&gt;/deleted/\n</code></pre> <p>Note that this is a path in your Google Storage bucket, and not in your lakeFS repository. It is now safe to remove the objects that appear in this list directly from the storage.</p> <p>You will find the list of objects removed by the job in the storage namespace of the repository. It is saved in Parquet format under <code>_lakefs/retention/gc/unified/&lt;RUN_ID&gt;/deleted/</code>.</p>"},{"location":"howto/garbage-collection/gc/#mark-and-sweep-stages","title":"Mark and Sweep stages","text":"<p>You can break the job into two stages:</p> <ul> <li>Mark: find objects to remove, without actually removing them.</li> <li>Sweep: remove the objects.</li> </ul>"},{"location":"howto/garbage-collection/gc/#mark-only-mode","title":"Mark-only mode","text":"<p>To make GC run the mark stage only, add the following to your spark-submit command:</p> <pre><code>spark.hadoop.lakefs.gc.do_sweep=false\n</code></pre> <p>In mark-only mode, GC will write the keys of the expired objects under: <code>&lt;REPOSITORY_STORAGE_NAMESPACE&gt;/_lakefs/retention/gc/unified/&lt;MARK_ID&gt;/</code>. MARK_ID is generated by the job. You can find it in the driver's output:</p> <pre><code>Report for mark_id=gmc6523jatlleurvdm30 path=s3a://example-bucket/_lakefs/retention/gc/unified/gmc6523jatlleurvdm30\n</code></pre>"},{"location":"howto/garbage-collection/gc/#sweep-only-mode","title":"Sweep-only mode","text":"<p>To make GC run the sweep stage only, add the following properties to your spark-submit command:</p> <pre><code>spark.hadoop.lakefs.gc.do_mark=false\nspark.hadoop.lakefs.gc.mark_id=&lt;MARK_ID&gt; # Replace &lt;MARK_ID&gt; with the identifier you obtained from a previous mark-only run\n</code></pre>"},{"location":"howto/garbage-collection/gc/#garbage-collection-notes","title":"Garbage collection notes","text":"<ol> <li>In order for an object to be removed, it must not exist on the HEAD of any branch.    You should remove stale branches to prevent them from retaining old objects.    For example, consider a branch that has been merged to <code>main</code> and has become stale.    An object which is later deleted from <code>main</code> will always be present in the stale branch, preventing it from being removed.</li> <li>lakeFS will never delete objects outside your repository's storage namespace.    In particular, objects that were imported using <code>lakectl import</code> or the UI import wizard will not be affected by GC jobs.</li> <li>In cases where deleted objects are brought back to life while a GC job is running (for example, by reverting a commit),    the objects may or may not be deleted.</li> <li>Garbage collection does not remove any commits: you will still be able to use commits containing removed objects,    but trying to read these objects from lakeFS will result in a <code>410 Gone</code> HTTP status.</li> </ol>"},{"location":"howto/garbage-collection/managed-gc/","title":"Managed Garbage Collection","text":"<p>lakeFS Cloud</p> <p>Note</p> <p>Managed GC is only available for lakeFS Cloud. If you are using the self-managed lakeFS, garbage collection is available to run manually.</p>"},{"location":"howto/garbage-collection/managed-gc/#benefits-of-using-managed-gc","title":"Benefits of using managed GC","text":"<ul> <li>The quick and safe way to delete your unnecessary objects</li> <li>No operational overhead</li> <li>SLA for when your objects are deleted</li> <li>Support from the Treeverse team</li> </ul>"},{"location":"howto/garbage-collection/managed-gc/#how-it-works","title":"How it works","text":"<p>Similarly to the self-managed lakeFS, managed GC uses garbage collection rules to determine which objects to delete. However, it uses our super-fast and efficient engine to detect stale objects and branches (depends on your configuration) and prioritize them for deletion.</p>"},{"location":"howto/garbage-collection/managed-gc/#setting-up","title":"Setting up","text":"<p>Enable managed GC through the lakeFS Cloud onboarding setup wizard. This will create additional cloud resources for us to use and have access to delete those objects.</p>"},{"location":"howto/garbage-collection/standalone-gc/","title":"Standalone Garbage Collection","text":"<p>Info</p> <p>Standalone GC is only available for lakeFS Enterprise.</p> <p>Warning</p> <p>Standalone GC is experimental and offers limited capabilities compared to the Spark-backed GC. For large scale environments, we recommend using the Spark-backed solution.</p>"},{"location":"howto/garbage-collection/standalone-gc/#what-is-standalone-gc","title":"What is Standalone GC?","text":"<p>Standalone GC is a simplified version of the Spark-backed GC that runs without any external dependencies, delivered as a standalone docker image. It supports S3 and self-managed S3 compatible storages such as MinIO.    </p>"},{"location":"howto/garbage-collection/standalone-gc/#limitations","title":"Limitations","text":"<ol> <li>No horizontal scalability: Only a single instance of <code>lakefs-sgc</code> can operate on a given repository at a time.</li> <li>Mark phase only: Standalone GC supports only the mark phase, identifying objects for deletion but not executing  the sweep stage to delete them. It functions similarly to the GC's mark-only mode.</li> <li>Only supports AWS S3 and S3-compatible object storages. However, supporting Azure blob and GCS are in our roadmap.</li> </ol>"},{"location":"howto/garbage-collection/standalone-gc/#installation","title":"Installation","text":""},{"location":"howto/garbage-collection/standalone-gc/#step-1-obtain-dockerhub-token","title":"Step 1: Obtain DockerHub token","text":""},{"location":"howto/garbage-collection/standalone-gc/#lakefs-enterprise-customers","title":"lakeFS Enterprise customers","text":"<p>Contact your account manager to verify that Standalone GC is included in your license. Then use your DockerHub token for  the <code>externallakefs</code> user.</p>"},{"location":"howto/garbage-collection/standalone-gc/#new-to-lakefs-enterprise","title":"New to lakeFS Enterprise","text":"<p>Please contact us to get trial access to Standalone GC.</p>"},{"location":"howto/garbage-collection/standalone-gc/#step-2-login-to-dockerhub-with-this-token","title":"Step 2: Login to DockerHub with this token","text":"<pre><code>docker login -u &lt;token&gt;\n</code></pre>"},{"location":"howto/garbage-collection/standalone-gc/#step-3-download-the-docker-image","title":"Step 3: Download the docker image","text":"<p>Download the <code>treeverse/lakefs-sgc</code> image from Docker Hub:</p> <pre><code>docker pull treeverse/lakefs-sgc:&lt;tag&gt;\n</code></pre>"},{"location":"howto/garbage-collection/standalone-gc/#setup","title":"Setup","text":""},{"location":"howto/garbage-collection/standalone-gc/#permissions","title":"Permissions","text":"<p>To run <code>lakefs-sgc</code>, you need both AWS (or S3-compatible) storage and lakeFS user permissions as outlined below:</p>"},{"location":"howto/garbage-collection/standalone-gc/#storage-permissions","title":"Storage permissions","text":"<p>The minimum required permissions for AWS or S3-compatible storage are:</p> <p><pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:GetObject\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::some-bucket/some/prefix/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::some-bucket\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:ListAllMyBuckets\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::*\"\n      ]\n    }\n  ]\n}\n</code></pre> In this example, the repository storage namespace is <code>s3://some-bucket/some/prefix</code>.</p>"},{"location":"howto/garbage-collection/standalone-gc/#lakefs-permissions","title":"lakeFS permissions","text":"<p>The minimum required permissions for lakeFS are:</p> <pre><code>{\n  \"statement\": [\n    {\n      \"action\": [\n        \"retention:PrepareGarbageCollectionCommits\",\n        \"retention:PrepareGarbageCollectionUncommitted\",\n        \"fs:ReadConfig\",\n        \"fs:ReadRepository\",\n        \"fs:ListObjects\",\n        \"fs:ReadConfig\"\n      ],\n      \"effect\": \"allow\",\n      \"resource\": \"arn:lakefs:fs:::repository/&lt;repository&gt;\"\n    }\n  ]\n}\n</code></pre>"},{"location":"howto/garbage-collection/standalone-gc/#credentials","title":"Credentials","text":"<p>Standalone GC supports S3 and S3-compatible storage backends and relies on AWS credentials for authentication. To set up credentials on the <code>lakefs-sgc</code> docker container, follow AWS guidelines, such as those outlined in this guide. For details on how to pass credentials to <code>lakefs-sgc</code>, refer to the instructions in How to Run Standalone GC.</p>"},{"location":"howto/garbage-collection/standalone-gc/#using-s3-compatible-clients","title":"Using S3-compatible clients","text":"<p><code>lakefs-sgc</code> leverages AWS credentials to work seamlessly with S3-compatible storage solutions, such as MinIO.  Follow the steps below to set up and use <code>lakefs-sgc</code> with an S3-compatible client:</p> <ol> <li>Add a profile to your <code>~/.aws/config</code> file:     <code>[profile minio]    region = us-east-1    endpoint_url = &lt;MinIO URL&gt;    s3 =        signature_version = s3v4</code></li> <li>Add an access and secret keys to your <code>~/.aws/credentials</code> file:     <code>[minio]    aws_access_key_id     = &lt;MinIO access key&gt;    aws_secret_access_key = &lt;MinIO secret key&gt;</code></li> <li>Run the <code>lakefs-sgc</code> docker image and pass it the <code>minio</code> profile - see example below.</li> </ol>"},{"location":"howto/garbage-collection/standalone-gc/#configuration","title":"Configuration","text":"<p>The following configuration keys are available:</p> Key Description Default value Possible values <code>logging.format</code> Logs output format \"text\" \"text\",\"json\" <code>logging.level</code> Logs level \"info\" \"error\",\"warn\",info\",\"debug\",\"trace\" <code>logging.output</code> Where to output the logs to \"-\" \"-\" (stdout), \"=\" (stderr), or any string for file path <code>cache_dir</code> Directory to use for caching data during run ~/.lakefs-sgc/data string <code>aws.max_page_size</code> Max number of items per page when listing objects in AWS 1000 number <code>aws.s3.addressing_path_style</code> Whether or not to use path-style when reading objects from AWS true boolean <code>lakefs.endpoint_url</code> The URL to the lakeFS installation - should end with <code>/api/v1</code> NOT SET URL <code>lakefs.access_key_id</code> Access key to the lakeFS installation NOT SET string <code>lakefs.secret_access_key</code> Secret access key to the lakeFS installation NOT SET string <p>These keys can be provided in the following ways: 1. Config file: Create a YAML file with the keys, each <code>.</code> is a new nesting level. \\    For example, <code>logging.level</code> will be:    <pre><code>logging:\n  level: &lt;value&gt; # info,debug...\n</code></pre>    Then, pass it to the program using the <code>--config path/to/config.yaml</code> argument. 2. Environment variables: by setting <code>LAKEFS_SGC_&lt;KEY&gt;</code>, with uppercase letters and <code>.</code>s converted to <code>_</code>s. \\    For example <code>logging.level</code> will be:    <pre><code>export LAKEFS_SGC_LOGGING_LEVEL=info\n</code></pre></p> <p>Example (minimalistic) config file: <pre><code>logging:\n  level: debug\nlakefs:\n  endpoint_url: https://your.url/api/v1\n  access_key_id: &lt;lakeFS access key&gt;\n  secret_access_key: &lt;lakeFS secret key&gt;\n</code></pre></p>"},{"location":"howto/garbage-collection/standalone-gc/#how-to-run-standalone-gc","title":"How to Run Standalone GC?","text":""},{"location":"howto/garbage-collection/standalone-gc/#command-line-reference","title":"Command line reference","text":""},{"location":"howto/garbage-collection/standalone-gc/#flags","title":"Flags","text":"<ul> <li><code>-c, --config</code>: config file to use (default is $HOME/.lakefs-sgc.yaml)</li> </ul>"},{"location":"howto/garbage-collection/standalone-gc/#commands","title":"Commands","text":"<p>run</p> <p>Usage:</p> <p><code>lakefs-sgc run &lt;repository&gt;</code></p> <p>Flags:</p> <ul> <li><code>--cache-dir</code>: directory to cache read files (default is <code>$HOME/.lakefs-sgc/data/</code>)</li> <li><code>--parallelism</code>: number of parallel downloads for metadata files (default 10)</li> <li><code>--presign</code>: use pre-signed URLs when downloading/uploading data (recommended) (default true)</li> </ul> <p>To run standalone GC, choose the method you prefer to pass AWS credentials and invoke the commands below.  </p>"},{"location":"howto/garbage-collection/standalone-gc/#directly-passing-in-credentials-parsed-from-awscredentials","title":"Directly passing in credentials parsed from <code>~/.aws/credentials</code>","text":"<pre><code>docker run \\\n    -e AWS_REGION=&lt;region&gt; \\\n    -e AWS_SESSION_TOKEN=\"$(grep 'aws_session_token' ~/.aws/credentials | awk -F' = ' '{print $2}')\" \\\n    -e AWS_ACCESS_KEY_ID=\"$(grep 'aws_access_key_id' ~/.aws/credentials | awk -F' = ' '{print $2}')\" \\\n    -e AWS_SECRET_ACCESS_KEY=\"$(grep 'aws_secret_access_key' ~/.aws/credentials | awk -F' = ' '{print $2}')\" \\\n    -e LAKEFS_SGC_LAKEFS_ENDPOINT_URL=&lt;lakefs endpoint URL&gt; \\\n    -e LAKEFS_SGC_LAKEFS_ACCESS_KEY_ID=&lt;lakefs accesss key&gt; \\\n    -e LAKEFS_SGC_LAKEFS_SECRET_ACCESS_KEY=&lt;lakefs secret key&gt; \\\n    -e LAKEFS_SGC_LOGGING_LEVEL=debug \\\n    treeverse/lakefs-sgc:&lt;tag&gt; run &lt;repository&gt;\n</code></pre>"},{"location":"howto/garbage-collection/standalone-gc/#mounting-the-aws-directory","title":"Mounting the <code>~/.aws</code> directory","text":"<p>When working with S3-compatible clients, it's often more convenient to mount the <code>~/.aws</code> directory and pass in the desired profile.</p> <p>First, change the permissions for <code>~/.aws/*</code> to allow the docker container to read this directory:</p> <pre><code>chmod 644 ~/.aws/*\n</code></pre> <p>Then, run the docker image and mount <code>~/.aws</code> to the <code>lakefs-sgc</code> home directory on the docker container:</p> <pre><code>docker run \\\n--network=host \\\n-v ~/.aws:/home/lakefs-sgc/.aws \\\n-e AWS_REGION=us-east-1 \\\n-e AWS_PROFILE=&lt;profile&gt; \\\n-e LAKEFS_SGC_LAKEFS_ENDPOINT_URL=&lt;lakefs endpoint URL&gt; \\\n-e LAKEFS_SGC_LAKEFS_ACCESS_KEY_ID=&lt;lakefs accesss key&gt; \\\n-e LAKEFS_SGC_LAKEFS_SECRET_ACCESS_KEY=&lt;lakefs secret key&gt; \\\n-e LAKEFS_SGC_LOGGING_LEVEL=debug \\\ntreeverse/lakefs-sgc:&lt;tag&gt; run &lt;repository&gt;\n</code></pre>"},{"location":"howto/garbage-collection/standalone-gc/#get-the-list-of-objects-marked-for-deletion","title":"Get the List of Objects Marked for Deletion","text":"<p><code>lakefs-sgc</code> will write its reports to <code>&lt;REPOSITORY_STORAGE_NAMESPACE&gt;/_lakefs/retention/gc/reports/&lt;RUN_ID&gt;/</code>. \\ RUN_ID is generated during runtime by the Standalone GC. You can find it in the logs:</p> <pre><code>\"Marking objects for deletion\" ... run_id=gcoca17haabs73f2gtq0\n</code></pre> <p>In this prefix, you'll find 2 objects:</p> <ul> <li> <p><code>deleted.csv</code> - Containing all marked objects in a CSV containing one <code>address</code> column.</p> <p>Example</p> <pre><code>address\n\"data/gcnobu7n2efc74lfa5ug/csfnri7n2efc74lfa69g,_e7P9j-1ahTXtofw7tWwJUIhTfL0rEs_dvBrClzc_QE\"\n\"data/gcnobu7n2efc74lfa5ug/csfnri7n2efc74lfa78g,mKZnS-5YbLzmK0pKsGGimdxxBlt8QZzCyw1QeQrFvFE\"\n...\n</code></pre> <ul> <li><code>summary.json</code> - A small json summarizing the GC run. </li> </ul> <p>Example</p> <pre><code>{\n    \"run_id\": \"gcoca17haabs73f2gtq0\",\n    \"success\": true,\n    \"first_slice\": \"gcss5tpsrurs73cqi6e0\",\n    \"start_time\": \"2024-10-27T13:19:26.890099059Z\",\n    \"cutoff_time\": \"2024-10-27T07:19:26.890099059Z\",\n    \"num_deleted_objects\": 33000\n}\n</code></pre> </li> </ul>"},{"location":"howto/garbage-collection/standalone-gc/#delete-marked-objects","title":"Delete marked objects","text":"<p>We recommend starting by backing up the marked objects to a different bucket before deleting them. After ensuring the  backup is complete, you can proceed to delete the objects directly from the backup location.</p> <p>Use the following script to backup marked objects to another bucket:</p> <pre><code># Update these variables with your actual values\nstorage_ns=&lt;storage namespace (s3://...)&gt;\noutput_bucket=&lt;output bucket (s3://...)&gt;\nrun_id=&lt;GC run id&gt;\n\n# Download the CSV file\naws s3 cp \"$storage_ns/_lakefs/retention/gc/reports/$run_id/deleted.csv\" \"./run_id-$run_id.csv\"\n\n# Move all addresses to the output bucket under the \"run_id=$run_id\" prefix\ncat run_id-$run_id.csv | tail -n +2 | xargs -I {} aws s3 mv \"$storage_ns/{}\" \"$output_bucket/run_id=$run_id/\"\n</code></pre> <p>To delete the marked objects, use the following script:</p> <pre><code># Update these variables with your actual values\noutput_bucket=&lt;output bucket (s3://...)&gt;\nrun_id=&lt;GC run id&gt;\n\naws s3 rm $output_bucket/run_id=$run_id --recursive\n</code></pre> <p>Tip</p> <p>Remember to periodically delete the backups to actually reduce storage costs.</p>"},{"location":"howto/garbage-collection/standalone-gc/#lab-tests","title":"Lab tests","text":"<p>Standalone GC was tested on the lakeFS setup below.   </p>"},{"location":"howto/garbage-collection/standalone-gc/#repository-spec","title":"Repository spec","text":"<ul> <li>100k objects</li> <li>250 commits</li> <li>100 branches</li> </ul>"},{"location":"howto/garbage-collection/standalone-gc/#machine-spec","title":"Machine spec","text":"<ul> <li>4GiB RAM</li> <li>8 CPUs</li> </ul>"},{"location":"howto/garbage-collection/standalone-gc/#testing-results","title":"Testing results","text":"<ul> <li>Time: &lt; 5m</li> <li>Disk space: 123MB</li> </ul>"},{"location":"howto/hooks/","title":"Actions and Hooks in lakeFS","text":"<p>Like other version control systems, lakeFS allows you to configure Actions to trigger when predefined events occur. There are numerous uses for Actions, including:</p> <ol> <li>Format Validator:    A webhook that checks new files to ensure they are of a set of allowed data formats.</li> <li>Schema Validator:    A webhook that reads new Parquet and ORC files to ensure they don't contain a block list of column names (or name prefixes).    This is useful for avoiding accidental PII exposure.</li> <li>Integration with external systems:    Post-merge and post-commit hooks could be used to export metadata about the change to another system. A common example is exporting <code>symlink.txt</code> files that allow e.g. AWS Athena to read data from lakeFS.</li> <li>Notifying downstream consumers:    Running a post-merge hook to trigger an Airflow DAG or to send a Webhook to an API, notifying it of the change that happened</li> </ol> <p>For step-by-step examples of hooks in action check out the lakeFS Quickstart and the lakeFS samples repository.</p>"},{"location":"howto/hooks/#overview","title":"Overview","text":"<p>An action defines one or more hooks to execute. lakeFS supports three types of hook:</p> <ol> <li>Lua - uses an embedded Lua VM</li> <li>Webhook - makes a REST call to an external URL</li> <li>Airflow - triggers a DAG in Airflow</li> </ol> <p>\"Before\" hooks must run successfully before their action. If the hook fails, it aborts the action. Lua hooks and Webhooks are synchronous, and lakeFS waits for them to run to completion. Airflow hooks are asynchronous: lakeFS stops waiting as soon as Airflow accepts triggering the DAG.</p>"},{"location":"howto/hooks/#configuration","title":"Configuration","text":"<p>There are two parts to configuration an Action:</p> <ol> <li>Create an Action file and upload it to the lakeFS repository</li> <li>Configure the hook(s) that you specified in the Action file. How these are configured will depend on the type of hook.</li> </ol>"},{"location":"howto/hooks/#action-files","title":"Action files","text":"<p>An Action is a list of Hooks with the same trigger configuration, i.e. an event will trigger all Hooks under an Action or none at all.</p> <p>The Hooks under an Action are ordered and so is their execution.</p> <p>Before each hook execution the <code>if</code> boolean expression is evaluated. The expression can use the functions <code>success()</code> and <code>failure()</code>, which return true if the hook's actions succeeded or failed, respectively.</p> <p>By default, when <code>if</code> is empty or omitted, the step will run only if no error occurred (the same as <code>success()</code>).</p>"},{"location":"howto/hooks/#action-file-schema","title":"Action File schema","text":"Property Description Data Type Required Default Value <code>name</code> Identifes the Action file String no Action filename <code>on</code> List of events that will trigger the hooks List yes <code>on&lt;event&gt;.branches</code> Glob pattern list of branches that triggers the hooks List no Not applicable to Tag events. If empty, Action runs on all branches <code>hooks</code> List of hooks to be executed List yes <code>hook.id</code> ID of the hook, must be unique within the action. String yes <code>hook.type</code> Type of the hook (types) String yes <code>hook.description</code> Description for the hook String no <code>hook.if</code> Expression that will be evaluated before execute the hook String no No value is the same as evaluate <code>success()</code> <code>hook.properties</code> Hook's specific configuration, see Lua, WebHook, and Airflow for details Dictionary true"},{"location":"howto/hooks/#example-action-file","title":"Example Action File","text":"<p>_lakefs_actions/file_checker.yaml</p> <pre><code>name: Good files check\ndescription: set of checks to verify that branch is good\non:\npre-commit:\npre-merge:\n    branches:\n    - main\nhooks:\n- id: no_temp\n    type: webhook\n    description: checking no temporary files found\n    properties:\n    url: \"https://example.com/webhook?notmp=true?t=1za2PbkZK1bd4prMuTDr6BeEQwWYcX2R\"\n- id: no_freeze\n    type: webhook\n    description: check production is not in dev freeze\n    properties:\n    url: \"https://example.com/webhook?nofreeze=true?t=1za2PbkZK1bd4prMuTDr6BeEQwWYcX2R\"\n- id: alert\n    type: webhook\n    if: failure()\n    description: notify alert system when check failed\n    properties:\n    url: \"https://example.com/alert\"\n    query_params:\n        title: good files webhook failed\n- id: notification\n    type: webhook\n    if: true\n    description: notify that will always run - no matter if one of the previous steps failed\n    properties:\n    url: \"https://example.com/notification\"\n    query_params:\n        title: good files completed\n</code></pre> <p>Note</p> <p>lakeFS will validate action files only when an Event has occurred.  Use <code>lakectl actions validate &lt;path&gt;</code> to validate your action files locally.</p>"},{"location":"howto/hooks/#uploading-action-files","title":"Uploading Action files","text":"<p>Action files should be uploaded with the prefix <code>_lakefs_actions/</code> to the lakeFS repository. When an actionable event (see Supported Events above) takes place, lakeFS will read all files with prefix <code>_lakefs_actions/</code> in the repository branch where the action occurred. A failure to parse an Action file will result with a failing Run.</p> <p>For example, lakeFS will search and execute all the matching Action files with the prefix <code>lakefs://example-repo/feature-1/_lakefs_actions/</code> on:</p> <ol> <li>Commit to <code>feature-1</code> branch on <code>example-repo</code> repository.</li> <li>Merge to <code>main</code> branch from <code>feature-1</code> branch on <code>repo1</code> repository.</li> </ol>"},{"location":"howto/hooks/#supported-events","title":"Supported Events","text":"Event Description <code>prepare-commit</code> (EXPERIMENTAL) Runs before the commit occurs; branch modification will be included in the commit <code>pre-commit</code> Runs when the commit occurs, before the commit is finalized <code>post-commit</code> Runs after the commit is finalized <code>pre-merge</code> Runs on the source branch when the merge occurs, before the merge is finalized <code>post-merge</code> Runs on the merge result, after the merge is finalized <code>pre-create-branch</code> Runs on the source branch prior to creating a new branch <code>post-create-branch</code> Runs on the new branch after the branch was created <code>pre-delete-branch</code> Runs prior to deleting a branch <code>post-delete-branch</code> Runs after the branch was deleted <code>pre-revert</code> Runs prior to performing a revert operation on a branch <code>post-revert</code> Runs after performing a revert operation on a branch <code>pre-create-tag</code> Runs prior to creating a new tag <code>post-create-tag</code> Runs after the tag was created <code>pre-delete-tag</code> Runs prior to deleting a tag <code>post-delete-tag</code> Runs after the tag was deleted <p>Warning</p> <p>The <code>prepare-commit</code> hook is experimental. During its execution (between <code>prepare-commit</code> and <code>pre-commit</code>), other changes may be applied to the branch as there is no branch-level locking mechanism at this point. If you need to verify or validate the content that will be committed, use the <code>pre-commit</code> hook instead, as it provides a consistent view of the changes that will be included in the commit.</p> <p>lakeFS Actions are handled per repository and cannot be shared between repositories. A failure of any Hook under any Action of a <code>pre-*</code> event will result in aborting the lakeFS operation that is taking place. Hook failures under any Action of a <code>post-*</code> event will not revert the operation.</p> <p>Hooks are managed by Action files that are written to a prefix in the lakeFS repository. This allows configuration-as-code inside lakeFS, where Action files are declarative and written in YAML.</p>"},{"location":"howto/hooks/#runs-api-cli","title":"Runs API &amp; CLI","text":"<p>A Run is an instantiation of the repository's Action files when the triggering event occurs. For example, if your repository contains a pre-commit hook, every commit would generate a Run for that specific commit.</p> <p>lakeFS will fetch, parse and filter the repository Action files and start to execute the Hooks under each Action. All executed Hooks (each with <code>hook_run_id</code>) exist in the context of that Run (<code>run_id</code>).</p> <p>The lakeFS API and lakectl expose the results of executions per repository, branch, commit, and specific Action. The endpoint also allows to download the execution log of any executed Hook under each Run for observability.</p>"},{"location":"howto/hooks/#result-files","title":"Result Files","text":"<p>The metadata section of lakeFS repository with each Run contains two types of files:</p> <ol> <li><code>_lakefs/actions/log/&lt;runID&gt;/&lt;hookRunID&gt;.log</code> - Execution log of the specific Hook run.</li> <li><code>_lakefs/actions/log/&lt;runID&gt;/run.manifest</code> - Manifest with all Hooks execution for the run with their results and additional metadata.</li> </ol> <p>Note</p> <p>Metadata section of a lakeFS repository is where lakeFS keeps its metadata, like commits and metaranges. Metadata files stored in the metadata section aren't accessible like user stored files.</p>"},{"location":"howto/hooks/airflow/","title":"Airflow Hooks","text":"<p>Airflow Hook triggers a DAG run in an Airflow installation using Airflow's REST API. The hook run succeeds if the DAG was triggered, and fails otherwise.</p>"},{"location":"howto/hooks/airflow/#action-file-airflow-hook-properties","title":"Action file Airflow hook properties","text":"<p>Info</p> <p>See the Action configuration for overall configuration schema and details</p> Property Description Data Type Example Required Environment Variables Supported url The URL of the Airflow instance String <code>http://localhost:8080</code> yes no dag_id The DAG to trigger String <code>example_dag</code> yes no username The name of the Airflow user performing the request String <code>admin</code> yes no password The password of the Airflow user performing the request String <code>admin</code> yes yes dag_conf DAG run configuration that will be passed as is JSON no no wait_for_dag Wait for DAG run to complete and reflect state (default: false) Boolean no no timeout Time to wait for the DAG run to complete (default: 1m) String (golang's Duration representation) no no <p>Example</p> <pre><code>...\nhooks:\n  - id: trigger_my_dag\n    type: airflow\n    description: Trigger an example_dag\n    properties:\n      url: \"http://localhost:8000\"\n      dag_id: \"example_dag\"\n      username: \"admin\"\n      password: \"{% raw %}{{{% endraw %} ENV.AIRFLOW_SECRET {% raw %}}}{% endraw %}\"\n      dag_conf:\n          some: \"additional_conf\"\n...\n</code></pre>"},{"location":"howto/hooks/airflow/#hook-record-in-configuration-field","title":"Hook Record in configuration field","text":"<p>lakeFS will add an entry to the Airflow request configuration property (<code>conf</code>) with the event that triggered the action.</p> <p>The key of the record will be <code>lakeFS_event</code> and the value will match the one described here</p>"},{"location":"howto/hooks/lua/","title":"Lua Hooks","text":"<p>lakeFS supports running hooks without relying on external components using an embedded Lua VM</p> <p>Using Lua hooks, it is possible to pass a Lua script to be executed directly by the lakeFS server when an action occurs.</p> <p>The Lua runtime embedded in lakeFS is limited for security reasons. It provides a narrow set of APIs and functions that by default do not allow:</p> <ol> <li>Accessing any of the running lakeFS server's environment</li> <li>Accessing the local filesystem available the lakeFS process</li> </ol>"},{"location":"howto/hooks/lua/#action-file-lua-hook-properties","title":"Action File Lua Hook Properties","text":"<p>Info</p> <p>See the Action configuration for overall configuration schema and details.</p> Property Description Data Type Required Default Value <code>args</code> One or more arguments to pass to the hook Dictionary false <code>script</code> An inline Lua script String either this or <code>script_path</code> must be specified <code>script_path</code> The path in lakeFS to a Lua script String either this or <code>script</code> must be specified"},{"location":"howto/hooks/lua/#example-lua-hooks","title":"Example Lua Hooks","text":"<p>For more examples and configuration samples, check out the examples/hooks/ directory in the lakeFS repository. You'll also find step-by-step examples of hooks in action in the lakeFS samples repository.</p> <p>Display information about an event</p> <p>This example will print out a JSON representation of the event that occurred:</p> <pre><code>name: dump_all\non:\n  post-commit:\n  post-merge:\n  post-create-tag:\n  post-create-branch:\nhooks:\n  - id: dump_event\n    type: lua\n    properties:\n      script: |\n        json = require(\"encoding/json\")\n        print(json.marshal(action))\n</code></pre> <p>Ensure that a commit includes a mandatory metadata field</p> <p>A more useful example: ensure every commit contains a required metadata field:</p> <pre><code>name: pre commit metadata field check\non:\npre-commit:\n    branches:\n    - main\n    - dev\nhooks:\n  - id: ensure_commit_metadata\n    type: lua\n    properties:\n      args:\n        notebook_url: {\"pattern\": \"my-jupyter.example.com/.*\"}\n        spark_version:  {}\n      script_path: lua_hooks/ensure_metadata_field.lua\n</code></pre> <p>Lua code at <code>lakefs://repo/main/lua_hooks/ensure_metadata_field.lua</code>:</p> <pre><code>regexp = require(\"regexp\")\nfor k, props in pairs(args) do\n  current_value = action.commit.metadata[k]\n  if current_value == nil then\n    error(\"missing mandatory metadata field: \" .. k)\n  end\n  if props.pattern and not regexp.match(props.pattern, current_value) then\n    error(\"current value for commit metadata field \" .. k .. \" does not match pattern: \" .. props.pattern .. \" - got: \" .. current_value)\n  end\nend\n</code></pre> <p>For more examples and configuration samples, check out the examples/hooks/ directory in the lakeFS repository.</p>"},{"location":"howto/hooks/lua/#lua-library-reference","title":"Lua Library reference","text":"<p>The Lua runtime embedded in lakeFS is limited for security reasons. The provided APIs are shown below.</p>"},{"location":"howto/hooks/lua/#arraytable","title":"<code>array(table)</code>","text":"<p>Helper function to mark a table object as an array for the runtime by setting <code>_is_array: true</code> metatable field.</p>"},{"location":"howto/hooks/lua/#aws","title":"<code>aws</code>","text":""},{"location":"howto/hooks/lua/#awss3_client","title":"<code>aws/s3_client</code>","text":"<p>S3 client library.</p> <p>Example</p> <pre><code>local aws = require(\"aws\")\n-- pass valid AWS credentials\nlocal client = aws.s3_client(\"ACCESS_KEY_ID\", \"SECRET_ACCESS_KEY\", \"REGION\")\n</code></pre>"},{"location":"howto/hooks/lua/#awss3_clientget_objectbucket-key","title":"<code>aws/s3_client.get_object(bucket, key)</code>","text":"<p>Returns the body (as a Lua string) of the requested object and a boolean value that is true if the requested object exists</p>"},{"location":"howto/hooks/lua/#awss3_clientput_objectbucket-key-value","title":"<code>aws/s3_client.put_object(bucket, key, value)</code>","text":"<p>Sets the object at the given bucket and key to the value of the supplied value string</p>"},{"location":"howto/hooks/lua/#awss3_clientdelete_objectbucket-key","title":"<code>aws/s3_client.delete_object(bucket [, key])</code>","text":"<p>Deletes the object at the given key</p>"},{"location":"howto/hooks/lua/#awss3_clientlist_objectsbucket-prefix-continuation_token-delimiter","title":"<code>aws/s3_client.list_objects(bucket [, prefix, continuation_token, delimiter])</code>","text":"<p>Returns a table of results containing the following structure:</p> <ul> <li><code>is_truncated</code>: (boolean) whether there are more results to paginate through using the continuation token</li> <li><code>next_continuation_token</code>: (string) to pass in the next request to get the next page of results</li> <li><code>results</code> (table of tables) information about the objects (and prefixes if a delimiter is used)</li> </ul> <p>a result could in one of the following structures</p> <pre><code>{\n   [\"key\"] = \"a/common/prefix/\",\n   [\"type\"] = \"prefix\"\n}\n</code></pre> <p>or:</p> <pre><code>{\n   [\"key\"] = \"path/to/object\",\n   [\"type\"] = \"object\",\n   [\"etag\"] = \"etagString\",\n   [\"size\"] = 1024,\n   [\"last_modified\"] = \"2023-12-31T23:10:00Z\"\n}\n</code></pre>"},{"location":"howto/hooks/lua/#awss3_clientdelete_recursivebucket-prefix","title":"<code>aws/s3_client.delete_recursive(bucket, prefix)</code>","text":"<p>Deletes all objects under the given prefix</p>"},{"location":"howto/hooks/lua/#awsglue","title":"<code>aws/glue</code>","text":"<p>Glue client library.</p> <p>Example</p> <pre><code>local aws = require(\"aws\")\n-- pass valid AWS credentials\nlocal glue = aws.glue_client(\"ACCESS_KEY_ID\", \"SECRET_ACCESS_KEY\", \"REGION\")\n</code></pre>"},{"location":"howto/hooks/lua/#awsgluecreate_databasedatabase-options","title":"<code>aws/glue.create_database(database, options)</code>","text":"<p>Create a new Database in Glue Catalog.</p> <p>Parameters:</p> <ul> <li><code>database(string)</code>: Glue Database name.</li> <li><code>options(table)</code> (optional):</li> <li><code>error_on_already_exists(boolean)</code>: Whether the call fail with an error if a DB with this name already exists</li> <li><code>create_db_input(Table)</code>: a Table that is passed \"as is\" to AWS and is parallel to the AWS SDK CreateDatabaseInput</li> </ul> <p>Example</p> <pre><code>local opts = {\n    error_on_already_exists = false,\n    create_db_input = {DatabaseInput = {Description = \"Created via LakeFS Action\"}, Tags = {Owner = \"Joe\"}}\n}\nglue.create_database(db, opts)\n</code></pre>"},{"location":"howto/hooks/lua/#awsgluedelete_databasedatabase-catalog_id","title":"<code>aws/glue.delete_database(database, catalog_id)</code>","text":"<p>Delete an existing Database in Glue Catalog.</p> <p>Parameters:</p> <ul> <li><code>database(string)</code>: Glue Database name.</li> <li><code>catalog_id(string)</code> (optional): Glue Catalog ID</li> </ul> <p>Example</p> <pre><code>glue.delete_database(db, \"461129977393\")\n</code></pre>"},{"location":"howto/hooks/lua/#awsglueget_tabledatabase-table-catalog_id","title":"<code>aws/glue.get_table(database, table [, catalog_id)</code>","text":"<p>Describe a table from the Glue Catalog.</p> <p>Example</p> <pre><code>local table, exists = glue.get_table(db, table_name)\nif exists then\n    print(json.marshal(table))\n</code></pre>"},{"location":"howto/hooks/lua/#awsgluecreate_tabledatabase-table_input-catalog_id","title":"<code>aws/glue.create_table(database, table_input, [, catalog_id])</code>","text":"<p>Create a new table in Glue Catalog. The <code>table_input</code> argument is a JSON that is passed \"as is\" to AWS and is parallel to the AWS SDK TableInput</p> <p>Example</p> <pre><code>local json = require(\"encoding/json\")\nlocal input = {\n    Name = \"my-table\",\n    PartitionKeys = array(partitions),\n    -- etc...\n}\nlocal json_input = json.marshal(input)\nglue.create_table(\"my-db\", table_input)\n</code></pre>"},{"location":"howto/hooks/lua/#awsglueupdate_tabledatabase-table_input-catalog_id-version_id-skip_archive","title":"<code>aws/glue.update_table(database, table_input, [, catalog_id, version_id, skip_archive])</code>","text":"<p>Update an existing Table in Glue Catalog. The <code>table_input</code> is the same as the argument in <code>glue.create_table</code> function.</p>"},{"location":"howto/hooks/lua/#awsgluedelete_tabledatabase-table_input-catalog_id","title":"<code>aws/glue.delete_table(database, table_input, [, catalog_id])</code>","text":"<p>Delete an existing Table in Glue Catalog.</p>"},{"location":"howto/hooks/lua/#azure","title":"<code>azure</code>","text":""},{"location":"howto/hooks/lua/#azureblob_client","title":"<code>azure/blob_client</code>","text":"<p>Azure blob client library.</p> <p>Example</p> <pre><code>local azure = require(\"azure\")\n-- pass valid Azure credentials\nlocal client = azure.blob_client(\"AZURE_STORAGE_ACCOUNT\", \"AZURE_ACCESS_KEY\")\n</code></pre>"},{"location":"howto/hooks/lua/#azureblob_clientget_objectpath_uri","title":"<code>azure/blob_client.get_object(path_uri)</code>","text":"<p>Returns the body (as a Lua string) of the requested object and a boolean value that is true if the requested object exists <code>path_uri</code> - A valid Azure blob storage uri in the form of <code>https://myaccount.blob.core.windows.net/mycontainer/myblob</code></p>"},{"location":"howto/hooks/lua/#azureblob_clientput_objectpath_uri-value","title":"<code>azure/blob_client.put_object(path_uri, value)</code>","text":"<p>Sets the object at the given bucket and key to the value of the supplied value string <code>path_uri</code> - A valid Azure blob storage uri in the form of <code>https://myaccount.blob.core.windows.net/mycontainer/myblob</code></p>"},{"location":"howto/hooks/lua/#azureblob_clientdelete_objectpath_uri","title":"<code>azure/blob_client.delete_object(path_uri)</code>","text":"<p>Deletes the object at the given key <code>path_uri</code> - A valid Azure blob storage uri in the form of <code>https://myaccount.blob.core.windows.net/mycontainer/myblob</code></p>"},{"location":"howto/hooks/lua/#azureabfss_transform_pathpath","title":"<code>azure/abfss_transform_path(path)</code>","text":"<p>Transform an HTTPS Azure URL to a ABFSS scheme. Used by the delta_exporter function to support Azure Unity catalog use cases <code>path</code> - A valid Azure blob storage URL in the form of <code>https://myaccount.blob.core.windows.net/mycontainer/myblob</code></p>"},{"location":"howto/hooks/lua/#crypto","title":"<code>crypto</code>","text":""},{"location":"howto/hooks/lua/#cryptoaesencryptcbckey-plaintext","title":"<code>crypto/aes/encryptCBC(key, plaintext)</code>","text":"<p>Returns a ciphertext for the aes encrypted text</p>"},{"location":"howto/hooks/lua/#cryptoaesdecryptcbckey-ciphertext","title":"<code>crypto/aes/decryptCBC(key, ciphertext)</code>","text":"<p>Returns the decrypted (plaintext) string for the encrypted ciphertext</p>"},{"location":"howto/hooks/lua/#cryptohmacsign_sha256message-key","title":"<code>crypto/hmac/sign_sha256(message, key)</code>","text":"<p>Returns a SHA256 hmac signature for the given message with the supplied key (using the SHA256 hashing algorithm)</p>"},{"location":"howto/hooks/lua/#cryptohmacsign_sha1message-key","title":"<code>crypto/hmac/sign_sha1(message, key)</code>","text":"<p>Returns a SHA1 hmac signature for the given message with the supplied key (using the SHA1 hashing algorithm)</p>"},{"location":"howto/hooks/lua/#cryptomd5digestdata","title":"<code>crypto/md5/digest(data)</code>","text":"<p>Returns the MD5 digest (string) of the given data</p>"},{"location":"howto/hooks/lua/#cryptosha256digestdata","title":"<code>crypto/sha256/digest(data)</code>","text":"<p>Returns the SHA256 digest (string) of the given data</p>"},{"location":"howto/hooks/lua/#databricksclientdatabricks_host-databricks_service_principal_token","title":"<code>databricks/client(databricks_host, databricks_service_principal_token)</code>","text":"<p>Returns a table representing a Databricks client with the <code>register_external_table</code> and <code>create_or_get_schema</code> methods.</p>"},{"location":"howto/hooks/lua/#databricksclientcreate_schemaschema_name-catalog_name-get_if_exists","title":"<code>databricks/client.create_schema(schema_name, catalog_name, get_if_exists)</code>","text":"<p>Creates a schema, or retrieves it if exists, in the configured Databricks host's Unity catalog. If a schema doesn't exist, a new schema with the given <code>schema_name</code> will be created under the given <code>catalog_name</code>. Returns the created/fetched schema name.</p> <p>Parameters:</p> <ul> <li><code>schema_name(string)</code>: The required schema name</li> <li><code>catalog_name(string)</code>: The catalog name under which the schema will be created (or from which it will be fetched)</li> <li><code>get_if_exists(boolean)</code>: In case of failure due to an existing schema with the given <code>schema_name</code> in the given <code>catalog_name</code>, return the schema.</li> </ul> <p>Example</p> <pre><code>local databricks = require(\"databricks\")\nlocal client = databricks.client(\"https://my-host.cloud.databricks.com\", \"my-service-principal-token\")\nlocal schema_name = client.create_schema(\"main\", \"mycatalog\", true)\n</code></pre>"},{"location":"howto/hooks/lua/#databricksclientexecute_statementstatement-warehouse_id-catalog_name-schema_name","title":"<code>databricks/client.execute_statement(statement, warehouse_id, catalog_name, schema_name)</code>","text":"<p>Parameters:</p> <ul> <li><code>statement(boolean)</code>: The SQL statement to execute on the databricks table</li> <li><code>warehouse_id(string)</code>: The SQL warehouse ID used in Databricks to run the <code>CREATE TABLE</code> query (fetched from the SQL warehouse</li> <li><code>catalog_name(string)</code>: The catalog name under which the schema will be created (or from which it will be fetched)</li> <li><code>schema_name(string)</code>: The required schema name   <code>status</code>, return the SQL status i.e. SUCCEEDED or an error code/message</li> </ul> <p>Example</p> <pre><code>local databricks = require(\"databricks\")\nlocal client = databricks.client(\"https://my-host.cloud.databricks.com\", \"my-service-principal-token\")\nlocal statement = \"ALTER TABLE \" .. table_descriptor.name .. \" ALTER COLUMN ID SET MASK mask_num\"\ndatabricks_client.execute_statement(statement, args.warehouse_id, table_descriptor.catalog, table_descriptor.schema)\n</code></pre>"},{"location":"howto/hooks/lua/#databricksclientregister_external_tabletable_name-physical_path-warehouse_id-catalog_name-schema_name-metadata","title":"<code>databricks/client.register_external_table(table_name, physical_path, warehouse_id, catalog_name, schema_name, metadata)</code>","text":"<p>Registers an external table under the provided warehouse ID, catalog name, and schema name. In order for this method call to succeed, an external location should be configured in the catalog, with the <code>physical_path</code>'s root storage URI (for example: <code>s3://mybucket</code>). Returns the table's creation status.</p> <p>Parameters:</p> <ul> <li><code>table_name(string)</code>: Table name.</li> <li><code>physical_path(string)</code>: A location to which the external table will refer, e.g. <code>s3://mybucket/the/path/to/mytable</code>.</li> <li><code>warehouse_id(string)</code>: The SQL warehouse ID used in Databricks to run the <code>CREATE TABLE</code> query (fetched from the SQL warehouse <code>Connection Details</code>, or by running <code>databricks warehouses get</code>, choosing your SQL warehouse and fetching its ID).</li> <li><code>catalog_name(string)</code>: The name of the catalog under which a schema will be created (or fetched from).</li> <li><code>schema_name(string)</code>: The name of the schema under which the table will be created.</li> <li><code>metadata(table)</code>: A table of metadata to be added to the table's registration. The metadata table should be of the form:   <code>{key1 = \"value1\", key2 = \"value2\", ...}</code>.</li> </ul> <p>Example</p> <pre><code>local databricks = require(\"databricks\")\nlocal client = databricks.client(\"https://my-host.cloud.databricks.com\", \"my-service-principal-token\")\nlocal status = client.register_external_table(\"mytable\", \"s3://mybucket/the/path/to/mytable\", \"examwarehouseple\", \"my-catalog-name\", \"myschema\")\n</code></pre> <ul> <li>For the Databricks permissions needed to run this method, check out the Unity Catalog Exporter docs.</li> </ul>"},{"location":"howto/hooks/lua/#encodingbase64encodedata","title":"<code>encoding/base64/encode(data)</code>","text":"<p>Encodes the given data to a base64 string</p>"},{"location":"howto/hooks/lua/#encodingbase64decodedata","title":"<code>encoding/base64/decode(data)</code>","text":"<p>Decodes the given base64 encoded data and return it as a string</p>"},{"location":"howto/hooks/lua/#encodingbase64url_encodedata","title":"<code>encoding/base64/url_encode(data)</code>","text":"<p>Encodes the given data to an unpadded alternate base64 encoding defined in RFC 4648.</p>"},{"location":"howto/hooks/lua/#encodingbase64url_decodedata","title":"<code>encoding/base64/url_decode(data)</code>","text":"<p>Decodes the given unpadded alternate base64 encoding defined in RFC 4648 and return it as a string</p>"},{"location":"howto/hooks/lua/#encodinghexencodevalue","title":"<code>encoding/hex/encode(value)</code>","text":"<p>Encode the given value string to hexadecimal values (string)</p>"},{"location":"howto/hooks/lua/#encodinghexdecodevalue","title":"<code>encoding/hex/decode(value)</code>","text":"<p>Decode the given hexadecimal string back to the string it represents (UTF-8)</p>"},{"location":"howto/hooks/lua/#encodingjsonmarshaltable","title":"<code>encoding/json/marshal(table)</code>","text":"<p>Encodes the given table into a JSON string</p>"},{"location":"howto/hooks/lua/#encodingjsonunmarshalstring","title":"<code>encoding/json/unmarshal(string)</code>","text":"<p>Decodes the given string into the equivalent Lua structure</p>"},{"location":"howto/hooks/lua/#encodingyamlmarshaltable","title":"<code>encoding/yaml/marshal(table)</code>","text":"<p>Encodes the given table into a YAML string</p>"},{"location":"howto/hooks/lua/#encodingyamlunmarshalstring","title":"<code>encoding/yaml/unmarshal(string)</code>","text":"<p>Decodes the given YAML encoded string into the equivalent Lua structure</p>"},{"location":"howto/hooks/lua/#encodingparquetget_schemapayload","title":"<code>encoding/parquet/get_schema(payload)</code>","text":"<p>Read the payload (string) as the contents of a Parquet file and return its schema in the following table structure:</p> <pre><code>{\n  { [\"name\"] = \"column_a\", [\"type\"] = \"INT32\" },\n  { [\"name\"] = \"column_b\", [\"type\"] = \"BYTE_ARRAY\" }\n}\n</code></pre>"},{"location":"howto/hooks/lua/#formats","title":"<code>formats</code>","text":""},{"location":"howto/hooks/lua/#formatsdelta_clientkey-secret-region","title":"<code>formats/delta_client(key, secret, region)</code>","text":"<p>Creates a new Delta Lake client used to interact with the lakeFS server. * <code>key</code>: lakeFS access key id * <code>secret</code>: lakeFS secret access key * <code>region</code>: The region in which your lakeFS server is configured at.</p>"},{"location":"howto/hooks/lua/#formatsdelta_clientget_tablerepository_id-reference_id-prefix","title":"<code>formats/delta_client.get_table(repository_id, reference_id, prefix)</code>","text":"<p>Returns a representation of a Delta Lake table under the given repository, reference, and prefix. The format of the response is two tables:</p> <ol> <li>the first is a table of the format <code>{number, {string}}</code> where <code>number</code> is a version in the Delta Log, and the mapped <code>{string}</code> array contains JSON strings of the different Delta Lake log operations listed in the mapped version entry. e.g.:</li> </ol> <pre><code>{\n  0 = {\n    \"{\\\"commitInfo\\\":...}\",\n    \"{\\\"add\\\": ...}\",\n    \"{\\\"remove\\\": ...}\"\n  },\n  1 = {\n    \"{\\\"commitInfo\\\":...}\",\n    \"{\\\"add\\\": ...}\",\n    \"{\\\"remove\\\": ...}\"\n  }\n}\n</code></pre> <ol> <li>the second is a table of the metadata of the current table snapshot. The metadata table can be used to initialize the Delta Lake table in an external Catalog. It consists of the following fields:<ul> <li><code>id</code>: The table's ID</li> <li><code>name</code>: The table's name</li> <li><code>description</code>: The table's description</li> <li><code>schema_string</code>: The table's schema string</li> <li><code>partition_columns</code>: The table's partition columns</li> <li><code>configuration</code>: The table's configuration</li> <li><code>created_time</code>: The table's creation time</li> </ul> </li> </ol>"},{"location":"howto/hooks/lua/#gcloud","title":"<code>gcloud</code>","text":""},{"location":"howto/hooks/lua/#gcloudgs_clientgcs_credentials_json_string","title":"<code>gcloud/gs_client(gcs_credentials_json_string)</code>","text":"<p>Create a new Google Cloud Storage client using a string that contains a valid <code>credentials.json</code> file content.</p>"},{"location":"howto/hooks/lua/#gcloudgswrite_fuse_symlinksource-destination-mount_info","title":"<code>gcloud/gs.write_fuse_symlink(source, destination, mount_info)</code>","text":"<p>Will create a gcsfuse symlink from the source (typically a lakeFS physical address for an object) to a given destination.</p> <p><code>mount_info</code> is a Lua table with <code>\"from\"</code> and <code>\"to\"</code> keys - since symlinks don't work for <code>gs://...</code> URIs, they need to point to the mounted location instead. <code>from</code> will be removed from the beginning of <code>source</code>, and <code>destination</code> will be added instead.</p> <p>Example</p> <pre><code>source = \"gs://bucket/lakefs/data/abc/def\"\ndestination = \"gs://bucket/exported/path/to/object\"\nmount_info = {\n    [\"from\"] = \"gs://bucket\",\n    [\"to\"] = \"/home/user/gcs-mount\"\n}\ngs.write_fuse_symlink(source, destination, mount_info)\n-- Symlink: \"/home/user/gcs-mount/exported/path/to/object\" -&gt; \"/home/user/gcs-mount/lakefs/data/abc/def\"\n</code></pre>"},{"location":"howto/hooks/lua/#hook","title":"<code>hook</code>","text":"<p>A set of utilities to aide in writing user friendly hooks.</p>"},{"location":"howto/hooks/lua/#hookfailmessage","title":"<code>hook/fail(message)</code>","text":"<p>Will abort the current hook's execution with the given message. This is similar to using <code>error()</code>, but is typically used to separate generic runtime errors (an API call that returned an unexpected response) and explicit failure of the calling hook.</p> <p>When called, errors will appear without a stack-trace, and the error message will be directly the one given as <code>message</code>.</p> <p>Example</p> <pre><code>&gt; hook = require(\"hook\")\n&gt; hook.fail(\"this hook shall not pass because of: \" .. reason)\n</code></pre>"},{"location":"howto/hooks/lua/#lakefs","title":"<code>lakefs</code>","text":"<p>The Lua Hook library allows calling back to the lakeFS API using the identity of the user that triggered the action. For example, if user A tries to commit and triggers a <code>pre-commit</code> hook - any call made inside that hook to the lakeFS API, will automatically use user A's identity for authorization and auditing purposes.</p>"},{"location":"howto/hooks/lua/#lakefscreate_tagrepository_id-reference_id-tag_id","title":"<code>lakefs/create_tag(repository_id, reference_id, tag_id)</code>","text":"<p>Create a new tag for the given reference</p>"},{"location":"howto/hooks/lua/#lakefsdiff_refsrepository_id-left_reference_id-right_reference_id-after-prefix-delimiter-amount","title":"<code>lakefs/diff_refs(repository_id, left_reference_id, right_reference_id [, after, prefix, delimiter, amount])</code>","text":"<p>Returns an object-wise diff between <code>left_reference_id</code> and <code>right_reference_id</code>.</p>"},{"location":"howto/hooks/lua/#lakefslist_objectsrepository_id-reference_id-after-prefix-delimiter-amount","title":"<code>lakefs/list_objects(repository_id, reference_id [, after, prefix, delimiter, amount])</code>","text":"<p>List objects in the specified repository and reference (branch, tag, commit ID, etc.). If delimiter is empty, will default to a recursive listing. Otherwise, common prefixes up to <code>delimiter</code> will be shown as a single entry.</p>"},{"location":"howto/hooks/lua/#lakefsget_objectrepository_id-reference_id-path","title":"<code>lakefs/get_object(repository_id, reference_id, path)</code>","text":"<p>Returns 2 values:</p> <ol> <li>The HTTP status code returned by the lakeFS API</li> <li>The content of the specified object as a Lua string</li> </ol>"},{"location":"howto/hooks/lua/#lakefsdiff_branchrepository_id-branch_id-after-amount-prefix-delimiter","title":"<code>lakefs/diff_branch(repository_id, branch_id [, after, amount, prefix, delimiter])</code>","text":"<p>Returns an object-wise diff of uncommitted changes on <code>branch_id</code>.</p>"},{"location":"howto/hooks/lua/#lakefsstat_objectrepository_id-ref_id-path-user_metadata","title":"<code>lakefs/stat_object(repository_id, ref_id, path[, user_metadata])</code>","text":"<p>Returns a stat object for the given path under the given reference and repository. Returns 2 values:</p> <ol> <li>The HTTP status code returned by the lakeFS API</li> <li>The stat response object as a JSON string</li> </ol> <p>Parameters:</p> <ul> <li><code>repository_id</code>: The repository ID</li> <li><code>ref_id</code>: The reference to stat from (branch, tag, commit ID)</li> <li><code>path</code>: Path to the object to stat</li> <li><code>user_metadata</code>: (Optional) Boolean flag to include user metadata in response</li> </ul>"},{"location":"howto/hooks/lua/#lakefsupdate_object_user_metadatarepository_id-branch_id-path-metadata","title":"<code>lakefs/update_object_user_metadata(repository_id, branch_id, path, metadata)</code>","text":"<p>Update user metadata for an object.</p> <p>Parameters:</p> <ul> <li><code>repository_id</code>: The repository ID</li> <li><code>branch_id</code>: The branch containing the object</li> <li><code>path</code>: Path to the object to update</li> <li><code>metadata</code>: A table containing key-value pairs to set as user metadata</li> </ul> <p>Returns 2 values:</p> <ol> <li>The HTTP status code returned by the lakeFS API (204 on success)</li> <li>Empty on success, error on failure</li> </ol>"},{"location":"howto/hooks/lua/#lakefscatalogexportglue_exporterget_full_table_namedescriptor-action_info","title":"<code>lakefs/catalogexport/glue_exporter.get_full_table_name(descriptor, action_info)</code>","text":"<p>Generate glue table name.</p> <p>Parameters:</p> <ul> <li><code>descriptor(Table)</code>: Object from (e.g. _lakefs_tables/my_table.yaml).</li> <li><code>action_info(Table)</code>: The global action object.</li> </ul>"},{"location":"howto/hooks/lua/#lakefscatalogexportdelta_exporter","title":"<code>lakefs/catalogexport/delta_exporter</code>","text":"<p>A package used to export Delta Lake tables from lakeFS to an external cloud storage.</p>"},{"location":"howto/hooks/lua/#lakefscatalogexportdelta_exporterexport_delta_logaction-table_def_names-write_object-delta_client-table_descriptors_path-path_transformer","title":"<code>lakefs/catalogexport/delta_exporter.export_delta_log(action, table_def_names, write_object, delta_client, table_descriptors_path, path_transformer)</code>","text":"<p>The function used to export Delta Lake tables. The return value is a table with mapping of table names to external table location (from which it is possible to query the data) and latest Delta table version's metadata. The response is of the form: <code>{&lt;table_name&gt; = {path = \"s3://mybucket/mypath/mytable\", metadata = {id = \"table_id\", name = \"table_name\", ...}}}</code>.</p> <p>Parameters:</p> <ul> <li><code>action</code>: The global action object</li> <li><code>table_def_names</code>: Delta tables name list (e.g. <code>{\"table1\", \"table2\"}</code>)</li> <li><code>write_object</code>: A writer function with <code>function(bucket, key, data)</code> signature, used to write the exported Delta Log (e.g. <code>aws/s3_client.put_object</code> or <code>azure/blob_client.put_object</code>)</li> <li><code>delta_client</code>: A Delta Lake client that implements <code>get_table: function(repo, ref, prefix)</code></li> <li><code>table_descriptors_path</code>: The path under which the table descriptors of the provided <code>table_def_names</code> reside</li> <li><code>path_transformer</code>: (Optional) A function(path) used for transforming the path of the saved delta logs path fields as well as the saved table physical path (used to support Azure Unity catalog use cases)</li> </ul> <p>Delta export example for AWS S3</p> <pre><code>---\nname: delta_exporter\non:\npost-commit: null\nhooks:\n- id: delta_export\n    type: lua\n    properties:\n    script: |\n        local aws = require(\"aws\")\n        local formats = require(\"formats\")\n        local delta_exporter = require(\"lakefs/catalogexport/delta_exporter\")\n        local json = require(\"encoding/json\")\n\n        local table_descriptors_path = \"_lakefs_tables\"\n        local sc = aws.s3_client(args.aws.access_key_id, args.aws.secret_access_key, args.aws.region)\n        local delta_client = formats.delta_client(args.lakefs.access_key_id, args.lakefs.secret_access_key, args.aws.region)\n        local delta_table_details = delta_export.export_delta_log(action, args.table_defs, sc.put_object, delta_client, table_descriptors_path)\n\n        for t, details in pairs(delta_table_details) do\n        print(\"Delta Lake exported table \\\"\" .. t .. \"\\\"'s location: \" .. details[\"path\"] .. \"\\n\")\n        print(\"Delta Lake exported table \\\"\" .. t .. \"\\\"'s metadata:\\n\")\n        for k, v in pairs(details[\"metadata\"]) do\n            if type(v) == \"table\" then\n            print(\"\\t\" .. k .. \" = \" .. json.marshal(v) .. \"\\n\")\n            else\n            print(\"\\t\" .. k .. \" = \" .. v .. \"\\n\")\n            end\n        end\n        end\n    args:\n        aws:\n        access_key_id: &lt;AWS_ACCESS_KEY_ID&gt;\n        secret_access_key: &lt;AWS_SECRET_ACCESS_KEY&gt;\n        region: us-east-1\n        lakefs:\n        access_key_id: &lt;LAKEFS_ACCESS_KEY_ID&gt; \n        secret_access_key: &lt;LAKEFS_SECRET_ACCESS_KEY&gt;\n        table_defs:\n        - mytable\n</code></pre> <p>For the table descriptor under the <code>_lakefs_tables/mytable.yaml</code>:</p> <pre><code>---\nname: myTableActualName\ntype: delta\npath: a/path/to/my/delta/table\n</code></pre> <p>Delta export example for Azure Blob Storage:</p> <pre><code>name: Delta Exporter\non:\npost-commit:\n    branches: [\"{% raw %}{{ .Branch }}{% endraw %}*\"]\nhooks:\n- id: delta_exporter\n    type: lua\n    properties:\n    script: |\n        local azure = require(\"azure\")\n        local formats = require(\"formats\")\n        local delta_exporter = require(\"lakefs/catalogexport/delta_exporter\")\n\n        local table_descriptors_path = \"_lakefs_tables\"\n        local sc = azure.blob_client(args.azure.storage_account, args.azure.access_key)\n        local function write_object(_, key, buf)\n        return sc.put_object(key,buf)\n        end\n        local delta_client = formats.delta_client(args.lakefs.access_key_id, args.lakefs.secret_access_key)\n        local delta_table_details = delta_export.export_delta_log(action, args.table_defs, sc.put_object, delta_client, table_descriptors_path)\n\n        for t, details in pairs(delta_table_details) do\n        print(\"Delta Lake exported table \\\"\" .. t .. \"\\\"'s location: \" .. details[\"path\"] .. \"\\n\")\n        print(\"Delta Lake exported table \\\"\" .. t .. \"\\\"'s metadata:\\n\")\n        for k, v in pairs(details[\"metadata\"]) do\n            if type(v) == \"table\" then\n            print(\"\\t\" .. k .. \" = \" .. json.marshal(v) .. \"\\n\")\n            else\n            print(\"\\t\" .. k .. \" = \" .. v .. \"\\n\")\n            end\n        end\n        end\n    args:\n        azure:\n        storage_account: \"{% raw %}{{ .AzureStorageAccount }}{% endraw %}\"\n        access_key: \"{% raw %}{{ .AzureAccessKey }}{% endraw %}\"\n        lakefs: # provide credentials of a user that has access to the script and Delta Table\n        access_key_id: \"{% raw %}{{ .LakeFSAccessKeyID }}{% endraw %}\"\n        secret_access_key: \"{% raw %}{{ .LakeFSSecretAccessKey }}{% endraw %}\"\n        table_defs:\n        - mytable\n</code></pre>"},{"location":"howto/hooks/lua/#lakefscatalogexportdelta_exporterchanged_table_defstable_def_names-table_descriptors_path-repository_id-ref-compare_ref","title":"<code>lakefs/catalogexport/delta_exporter.changed_table_defs(table_def_names, table_descriptors_path, repository_id, ref, compare_ref)</code>","text":"<p>Utility function to filter list of table defs based on those that have changed. Returns the subset of the tables in the table_def_names parameter that have changed.</p> <p>Parameters:</p> <ul> <li><code>table_def_names(table of strings)</code>: List of table names to filter based on the diff</li> <li><code>table_descriptors_path(string)</code>: The path under which the table descriptors of the provided <code>table_def_names</code> reside</li> <li><code>repository_id(string)</code>: The repository ID</li> <li><code>ref(string)</code>: base reference pointing at a specific version of the data i.e. a branch, commit ID, or tag</li> <li><code>compare_ref(string)</code>: compared-to reference for the diff to determine which tables changed</li> </ul> <p>Example</p> <pre><code>local delta_export = require(\"lakefs/catalogexport/delta_exporter\")\nlocal ref = action.commit.parents[1]\nlocal compare_ref = action.commit_id\nlocal changed_table_defs = delta_export.changed_table_defs(args.table_defs, args.table_descriptors_path, action.repository_id, ref, compare_ref)\nfor i = 1, #changed_table_defs do\n    print(changed_table_defs[i])\nend\n</code></pre>"},{"location":"howto/hooks/lua/#lakefscatalogexporttable_extractor","title":"<code>lakefs/catalogexport/table_extractor</code>","text":"<p>Utility package to parse <code>_lakefs_tables/</code> descriptors.</p>"},{"location":"howto/hooks/lua/#lakefscatalogexporttable_extractorlist_table_descriptor_entriesclient-repo_id-commit_id","title":"<code>lakefs/catalogexport/table_extractor.list_table_descriptor_entries(client, repo_id, commit_id)</code>","text":"<p>List all YAML files under <code>_lakefs_tables/*</code> and return a list of type <code>[{physical_address, path}]</code>, ignores hidden files. The <code>client</code> is <code>lakefs</code> client.</p>"},{"location":"howto/hooks/lua/#lakefscatalogexporttable_extractorget_table_descriptorclient-repo_id-ref-logical_path","title":"<code>lakefs/catalogexport/table_extractor.get_table_descriptor(client, repo_id, ref, logical_path)</code>","text":"<p>Read a table descriptor and parse YAML object. Will set <code>partition_columns</code> to <code>{}</code> if no partitions are defined.</p> <p>Parameters: * <code>client</code>: <code>lakefs</code> client * <code>repo_id(string)</code>: The repository ID * <code>ref(string)</code>: reference pointing at a specific version of the data i.e. a branch, commit ID, or tag * <code>logical_path(string)</code>: logical path of the table descriptor file within the repo</p>"},{"location":"howto/hooks/lua/#lakefscatalogexporthiveextract_partition_pagerclient-repo_id-commit_id-base_path-partition_cols-page_size","title":"<code>lakefs/catalogexport/hive.extract_partition_pager(client, repo_id, commit_id, base_path, partition_cols, page_size)</code>","text":"<p>Hive format partition iterator each result set is a collection of files under the same partition in lakeFS.</p> <p>Example</p> <pre><code>local lakefs = require(\"lakefs\")\nlocal pager = hive.extract_partition_pager(lakefs, repo_id, commit_id, prefix, partitions, 10)\nfor part_key, entries in pager do\n    print(\"partition: \" .. part_key)\n    for _, entry in ipairs(entries) do\n        print(\"path: \" .. entry.path .. \" physical: \" .. entry.physical_address)\n    end\nend\n</code></pre>"},{"location":"howto/hooks/lua/#lakefscatalogexportsymlink_exporter","title":"<code>lakefs/catalogexport/symlink_exporter</code>","text":"<p>Writes metadata for a table using Hive's SymlinkTextInputFormat. Currently only <code>S3</code> is supported.</p> <p>The default export paths per commit:</p> <pre><code>${storageNamespace}\n_lakefs/\n    exported/\n        ${ref}/\n            ${commitId}/\n                ${tableName}/\n                    p1=v1/symlink.txt\n                    p1=v2/symlink.txt\n                    p1=v3/symlink.txt\n                    ...\n</code></pre>"},{"location":"howto/hooks/lua/#lakefscatalogexportsymlink_exporterexport_s3s3_client-table_src_path-action_info-options","title":"<code>lakefs/catalogexport/symlink_exporter.export_s3(s3_client, table_src_path, action_info [, options])</code>","text":"<p>Export Symlink files that represent a table to S3 location.</p> <p>Parameters:</p> <ul> <li><code>s3_client</code>: Configured client.</li> <li><code>table_src_path(string)</code>: Path to the table spec YAML file in <code>_lakefs_tables</code> (e.g. _lakefs_tables/my_table.yaml).</li> <li><code>action_info(table)</code>: The global action object.</li> <li><code>options(table)</code>:</li> <li><code>debug(boolean)</code>: Print extra info.</li> <li><code>export_base_uri(string)</code>: Override the prefix in S3 e.g. <code>s3://other-bucket/path/</code>.</li> <li><code>writer(function(bucket, key, data))</code>: If passed then will not use s3 client, helpful for debug.</li> </ul> <p>Example</p> <pre><code>local exporter = require(\"lakefs/catalogexport/symlink_exporter\")\nlocal aws = require(\"aws\")\n-- args are user inputs from a lakeFS action.\nlocal s3 = aws.s3_client(args.aws.aws_access_key_id, args.aws.aws_secret_access_key, args.aws.aws_region)\nexporter.export_s3(s3, args.table_descriptor_path, action, {debug=true})\n</code></pre>"},{"location":"howto/hooks/lua/#lakefscatalogexportglue_exporter","title":"<code>lakefs/catalogexport/glue_exporter</code>","text":"<p>A Package for automating the export process from lakeFS stored tables into Glue catalog.</p>"},{"location":"howto/hooks/lua/#lakefscatalogexportglue_exporterexport_glueglue-db-table_src_path-create_table_input-action_info-options","title":"<code>lakefs/catalogexport/glue_exporter.export_glue(glue, db, table_src_path, create_table_input, action_info, options)</code>","text":"<p>Represent lakeFS table in Glue Catalog. This function will create a table in Glue based on configuration. It assumes that there is a symlink location that is already created and only configures it by default for the same commit.</p> <p>Parameters:</p> <ul> <li><code>glue</code>: AWS glue client</li> <li><code>db(string)</code>: glue database name</li> <li><code>table_src_path(string)</code>: path to table spec (e.g. _lakefs_tables/my_table.yaml)</li> <li><code>create_table_input(table)</code>: Input equal mapping to table_input in AWS, the same as we use for <code>glue.create_table</code>. should contain inputs describing the data format (e.g. InputFormat, OutputFormat, SerdeInfo) since the exporter is agnostic to this. by default this function will configure table location and schema.</li> <li><code>action_info(table)</code>: the global action object.</li> <li><code>options(table)</code>:</li> <li><code>table_name(string)</code>: Override default glue table name</li> <li><code>debug(boolean</code></li> <li><code>export_base_uri(string)</code>: Override the default prefix in S3 for symlink location e.g. s3://other-bucket/path/</li> <li><code>create_db_input(table)</code>: if this is specified, then it indicates we want to create a new database for the table export. The parameter expects a table that is converted to JSON and passed \"as is\" to AWS and is parallel to the AWS SDK CreateDatabaseInput</li> </ul> <p>When creating a glue table, the final table input will consist of the <code>create_table_input</code> input parameter and lakeFS computed defaults that will override it:</p> <ul> <li><code>Name</code> Gable table name <code>get_full_table_name(descriptor, action_info)</code>.</li> <li><code>PartitionKeys</code> Partition columns usually deduced from <code>_lakefs_tables/${table_src_path}</code>.</li> <li><code>TableType</code> = \"EXTERNAL_TABLE\"</li> <li><code>StorageDescriptor</code>: Columns usually deduced from <code>_lakefs_tables/${table_src_path}</code>.</li> <li><code>StorageDescriptor.Location</code> = symlink_location</li> </ul> <p>Example</p> <pre><code>local aws = require(\"aws\")\nlocal exporter = require(\"lakefs/catalogexport/glue_exporter\")\nlocal glue = aws.glue_client(args.aws_access_key_id, args.aws_secret_access_key, args.aws_region)\n-- table_input can be passed as a simple Key-Value object in YAML as an argument from an action, this is inline example:\nlocal table_input = {\nStorageDescriptor:\n    InputFormat: \"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat\"\n    OutputFormat: \"org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat\"\n    SerdeInfo:\n    SerializationLibrary: \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\"\nParameters:\n    classification: \"parquet\"\n    EXTERNAL: \"TRUE\"\n    \"parquet.compression\": \"SNAPPY\"\n}\nexporter.export_glue(glue, \"my-db\", \"_lakefs_tables/animals.yaml\", table_input, action, {debug=true, create_db_input = {DatabaseInput = {Description=\"DB exported from LakeFS\"}, Tags = {Owner = \"Joe\"}}})\n</code></pre>"},{"location":"howto/hooks/lua/#lakefscatalogexportglue_exporterget_full_table_namedescriptor-action_info_1","title":"<code>lakefs/catalogexport/glue_exporter.get_full_table_name(descriptor, action_info)</code>","text":"<p>Generate glue table name.</p> <p>Parameters:</p> <ul> <li><code>descriptor(Table)</code>: Object from (e.g. _lakefs_tables/my_table.yaml).</li> <li><code>action_info(Table)</code>: The global action object.</li> </ul>"},{"location":"howto/hooks/lua/#lakefscatalogexportunity_exporter","title":"<code>lakefs/catalogexport/unity_exporter</code>","text":"<p>A package used to register exported Delta Lake tables to Databricks' Unity catalog.</p>"},{"location":"howto/hooks/lua/#lakefscatalogexportunity_exporterregister_tablesaction-table_descriptors_path-delta_table_details-databricks_client-warehouse_id","title":"<code>lakefs/catalogexport/unity_exporter.register_tables(action, table_descriptors_path, delta_table_details, databricks_client, warehouse_id)</code>","text":"<p>The function used to register exported Delta Lake tables in Databricks' Unity Catalog. The registration will use the following paths to register the table: <code>&lt;catalog&gt;.&lt;branch name&gt;.&lt;table_name&gt;</code> where the branch name will be used as the schema name. The return value is a table with mapping of table names to registration request status.</p> <p>Note: (Azure users) Databricks catalog external locations is supported only for ADLS Gen2 storage accounts. When exporting Delta tables using the <code>lakefs/catalogexport/delta_exporter.export_delta_log</code> function, the <code>path_transformer</code> must be used to convert the paths scheme to <code>abfss</code>. The built-in <code>azure</code> Lua library provides this functionality with <code>transformPathToAbfss</code>.</p> <p>Parameters:</p> <ul> <li><code>action(table)</code>: The global action table</li> <li><code>table_descriptors_path(string)</code>: The path under which the table descriptors of the provided <code>table_paths</code> reside.</li> <li><code>delta_table_details(table)</code>: Table names to physical paths mapping and table metadata (e.g. <code>{table1 = {path = \"s3://mybucket/mytable1\", metadata = {id = \"table_1_id\", name = \"table1\", ...}}, table2 = {path = \"s3://mybucket/mytable2\", metadata = {id = \"table_2_id\", name = \"table2\", ...}}}</code>.)</li> <li><code>databricks_client(table)</code>: A Databricks client that implements <code>create_or_get_schema: function(id, catalog_name)</code> and <code>register_external_table: function(table_name, physical_path, warehouse_id, catalog_name, schema_name)</code></li> <li><code>warehouse_id(string)</code>: Databricks warehouse ID.</li> </ul> <p>Example</p> <p>The following registers an exported Delta Lake table to Unity Catalog.</p> <pre><code>local databricks = require(\"databricks\")\nlocal unity_export = require(\"lakefs/catalogexport/unity_exporter\")\n\nlocal delta_table_locations = {\n[\"table1\"] = \"s3://mybucket/mytable1\",\n}\n-- Register the exported table in Unity Catalog:\nlocal action_details = {\nrepository_id = \"my-repo\",\ncommit_id = \"commit_id\",\nbranch_id = \"main\",\n}\nlocal databricks_client = databricks.client(\"&lt;DATABRICKS_HOST&gt;\", \"&lt;DATABRICKS_TOKEN&gt;\")\nlocal registration_statuses = unity_export.register_tables(action_details, \"_lakefs_tables\", delta_table_locations, databricks_client, \"&lt;WAREHOUSE_ID&gt;\")\n\nfor t, status in pairs(registration_statuses) do\nprint(\"Unity catalog registration for table \\\"\" .. t .. \"\\\" completed with status: \" .. status .. \"\\n\")\nend\n</code></pre> <p>For the table descriptor under the <code>_lakefs_tables/delta-table-descriptor.yaml</code>: <pre><code>---\nname: my_table_name\ntype: delta\npath: path/to/delta/table/data\ncatalog: my-catalog\n</code></pre></p> <p>For detailed step-by-step guide on how to use <code>unity_exporter.register_tables</code> as a part of a lakeFS action refer to the Unity Catalog docs.</p>"},{"location":"howto/hooks/lua/#pathparsepath_string","title":"<code>path/parse(path_string)</code>","text":"<p>Returns a table for the given path string with the following structure:</p> <p>Example</p> <pre><code>&gt; require(\"path\")\n&gt; path.parse(\"a/b/c.csv\")\n{\n    [\"parent\"] = \"a/b/\"\n    [\"base_name\"] = \"c.csv\"\n}\n</code></pre>"},{"location":"howto/hooks/lua/#pathjoinpath_parts","title":"<code>path/join(*path_parts)</code>","text":"<p>Receives a variable number of strings and returns a joined string that represents a path:</p> <p>Example</p> <pre><code>&gt; path = require(\"path\")\n&gt; path.join(\"/\", \"path/\", \"to\", \"a\", \"file.data\")\npath/o/a/file.data\n</code></pre>"},{"location":"howto/hooks/lua/#pathis_hiddenpath_string-seperator-prefix","title":"<code>path/is_hidden(path_string [, seperator, prefix])</code>","text":"<p>returns a boolean - <code>true</code> if the given path string is hidden (meaning it starts with <code>prefix</code>) - or if any of its parents start with <code>prefix</code>.</p> <p>Example</p> <pre><code>&gt; require(\"path\")\n&gt; path.is_hidden(\"a/b/c\") -- false\n&gt; path.is_hidden(\"a/b/_c\") -- true\n&gt; path.is_hidden(\"a/_b/c\") -- true\n&gt; path.is_hidden(\"a/b/_c/\") -- true\n</code></pre>"},{"location":"howto/hooks/lua/#pathdefault_separator","title":"<code>path/default_separator()</code>","text":"<p>Returns a constant string (<code>/</code>)</p>"},{"location":"howto/hooks/lua/#regexpmatchpattern-s","title":"<code>regexp/match(pattern, s)</code>","text":"<p>Returns true if the string <code>s</code> matches <code>pattern</code>. This is a thin wrapper over Go's regexp.MatchString.</p>"},{"location":"howto/hooks/lua/#regexpquote_metas","title":"<code>regexp/quote_meta(s)</code>","text":"<p>Escapes any meta-characters in string <code>s</code> and returns a new string</p>"},{"location":"howto/hooks/lua/#regexpcompilepattern","title":"<code>regexp/compile(pattern)</code>","text":"<p>Returns a regexp match object for the given pattern</p>"},{"location":"howto/hooks/lua/#regexpcompiled_patternfind_alls-n","title":"<code>regexp/compiled_pattern.find_all(s, n)</code>","text":"<p>Returns a table list of all matches for the pattern, (up to <code>n</code> matches, unless <code>n == -1</code> in which case all possible matches will be returned)</p>"},{"location":"howto/hooks/lua/#regexpcompiled_patternfind_all_submatchs-n","title":"<code>regexp/compiled_pattern.find_all_submatch(s, n)</code>","text":"<p>Returns a table list of all sub-matches for the pattern, (up to <code>n</code> matches, unless <code>n == -1</code> in which case all possible matches will be returned). Submatches are matches of parenthesized subexpressions (also known as capturing groups) within the regular expression, numbered from left to right in order of opening parenthesis. Submatch 0 is the match of the entire expression, submatch 1 is the match of the first parenthesized subexpression, and so on</p>"},{"location":"howto/hooks/lua/#regexpcompiled_patternfinds","title":"<code>regexp/compiled_pattern.find(s)</code>","text":"<p>Returns a string representing the left-most match for the given pattern in string <code>s</code></p>"},{"location":"howto/hooks/lua/#regexpcompiled_patternfind_submatchs","title":"<code>regexp/compiled_pattern.find_submatch(s)</code>","text":"<p>find_submatch returns a table of strings holding the text of the leftmost match of the regular expression in <code>s</code> and the matches, if any, of its submatches</p>"},{"location":"howto/hooks/lua/#stringssplits-sep","title":"<code>strings/split(s, sep)</code>","text":"<p>returns a table of strings, the result of splitting <code>s</code> with <code>sep</code>.</p>"},{"location":"howto/hooks/lua/#stringstrims","title":"<code>strings/trim(s)</code>","text":"<p>Returns a string with all leading and trailing white space removed, as defined by Unicode</p>"},{"location":"howto/hooks/lua/#stringsreplaces-old-new-n","title":"<code>strings/replace(s, old, new, n)</code>","text":"<p>Returns a copy of the string s with the first n non-overlapping instances of <code>old</code> replaced by <code>new</code>. If <code>old</code> is empty, it matches at the beginning of the string and after each UTF-8 sequence, yielding up to k+1 replacements for a k-rune string.</p> <p>If n &lt; 0, there is no limit on the number of replacements</p>"},{"location":"howto/hooks/lua/#stringshas_prefixs-prefix","title":"<code>strings/has_prefix(s, prefix)</code>","text":"<p>Returns <code>true</code> if <code>s</code> begins with <code>prefix</code></p>"},{"location":"howto/hooks/lua/#stringshas_suffixs-suffix","title":"<code>strings/has_suffix(s, suffix)</code>","text":"<p>Returns <code>true</code> if <code>s</code> ends with <code>suffix</code></p>"},{"location":"howto/hooks/lua/#stringscontainss-substr","title":"<code>strings/contains(s, substr)</code>","text":"<p>Returns <code>true</code> if <code>substr</code> is contained anywhere in <code>s</code></p>"},{"location":"howto/hooks/lua/#timenow","title":"<code>time/now()</code>","text":"<p>Returns a <code>float64</code> representing the amount of nanoseconds since the unix epoch (01/01/1970 00:00:00).</p>"},{"location":"howto/hooks/lua/#timeformatepoch_nano-layout-zone","title":"<code>time/format(epoch_nano, layout, zone)</code>","text":"<p>Returns a string representation of the given epoch_nano timestamp for the given Timezone (e.g. <code>\"UTC\"</code>, <code>\"America/Los_Angeles\"</code>, ...) The <code>layout</code> parameter should follow Go's time layout format.</p>"},{"location":"howto/hooks/lua/#timeformat_isoepoch_nano-zone","title":"<code>time/format_iso(epoch_nano, zone)</code>","text":"<p>Returns a string representation of the given <code>epoch_nano</code> timestamp for the given Timezone (e.g. <code>\"UTC\"</code>, <code>\"America/Los_Angeles\"</code>, ...) The returned string will be in ISO8601 format.</p>"},{"location":"howto/hooks/lua/#timesleepduration_ns","title":"<code>time/sleep(duration_ns)</code>","text":"<p>Sleep for <code>duration_ns</code> nanoseconds</p>"},{"location":"howto/hooks/lua/#timesinceepoch_nano","title":"<code>time/since(epoch_nano)</code>","text":"<p>Returns the amount of nanoseconds elapsed since <code>epoch_nano</code></p>"},{"location":"howto/hooks/lua/#timeaddepoch_time-duration_table","title":"<code>time/add(epoch_time, duration_table)</code>","text":"<p>Returns a new timestamp (in nanoseconds passed since 01/01/1970 00:00:00) for the given <code>duration</code>. The <code>duration</code> should be a table with the following structure:</p> <p>Example</p> <pre><code>&gt; require(\"time\")\n&gt; time.add(time.now(), {\n    [\"hour\"] = 1,\n    [\"minute\"] = 20,\n    [\"second\"] = 50\n})\n</code></pre> <p>You may omit any of the fields from the table, resulting in a default value of <code>0</code> for omitted fields</p>"},{"location":"howto/hooks/lua/#timeparselayout-value","title":"<code>time/parse(layout, value)</code>","text":"<p>Returns a <code>float64</code> representing the amount of nanoseconds since the unix epoch (01/01/1970 00:00:00). This timestamp will represent date <code>value</code> parsed using the <code>layout</code> format.</p> <p>The <code>layout</code> parameter should follow Go's time layout format</p>"},{"location":"howto/hooks/lua/#timeparse_isovalue","title":"<code>time/parse_iso(value)</code>","text":"<p>Returns a <code>float64</code> representing the amount of nanoseconds since the unix epoch (01/01/1970 00:00:00 for <code>value</code>. The <code>value</code> string should be in ISO8601 format</p>"},{"location":"howto/hooks/lua/#uuidnew","title":"<code>uuid/new()</code>","text":"<p>Returns a new 128-bit RFC 4122 UUID in string representation.</p>"},{"location":"howto/hooks/lua/#neturl","title":"<code>net/url</code>","text":"<p>Provides a <code>parse</code> function parse a URL string into parts, returns a table with the URL's host, path, scheme, query and fragment.</p> <p>Example</p> <pre><code>&gt; local url = require(\"net/url\")\n&gt; url.parse(\"https://example.com/path?p1=a#section\")\n{\n    [\"host\"] = \"example.com\"\n    [\"path\"] = \"/path\"\n    [\"scheme\"] = \"https\"\n    [\"query\"] = \"p1=a\"\n    [\"fragment\"] = \"section\"\n}\n</code></pre>"},{"location":"howto/hooks/lua/#nethttp-optional","title":"<code>net/http</code> (optional)","text":"<p>Provides a <code>request</code> function that performs an HTTP request. For security reasons, this package is not available by default as it enables http requests to be sent out from the lakeFS instance network. The feature should be enabled under <code>actions.lua.net_http_enabled</code> configuration. Request will time out after 30 seconds.</p> <p>Example</p> <pre><code>http.request(url [, body])\nhttp.request{\nurl = string,\n[method = string,]\n[headers = header-table,]\n[body = string,]\n}\n</code></pre> <p>Returns a code (number), body (string), headers (table) and status (string).</p> <ul> <li>code - status code number</li> <li>body - string with the response body</li> <li>headers - table with the response request headers (key/value or table of values)</li> <li>status - status code text</li> </ul> <p>The first form of the call will perform GET requests or POST requests if the body parameter is passed.</p> <p>The second form accepts a table and allows you to customize the request method and headers.</p> <p>Example of a GET request</p> <p>Example</p> <pre><code>local http = require(\"net/http\")\nlocal code, body = http.request(\"&lt;https://example.com&gt;\")\nif code == 200 then\n    print(body)\nelse\n    print(\"Failed to get example.com - status code: \" .. code)\nend\n</code></pre> <p>Example of a POST request</p> <pre><code>local http = require(\"net/http\")\nlocal code, body = http.request{\n    url=\"https://httpbin.org/post\",\n    method=\"POST\",\n    body=\"custname=tester\",\n    headers={[\"Content-Type\"]=\"application/x-www-form-urlencoded\"},\n}\nif code == 200 then\n    print(body)\nelse\n    print(\"Failed to post data - status code: \" .. code)\nend\n</code></pre>"},{"location":"howto/hooks/webhooks/","title":"Webhooks","text":"<p>A Webhook is a Hook type that sends an HTTP POST request to the configured URL. Any non <code>2XX</code> response by the responding endpoint will fail the Hook, cancel the execution of the following Hooks under the same Action. For <code>pre-*</code> hooks, the triggering operation will also be aborted.</p> <p>Warning</p> <p>You should not use <code>pre-*</code> webhooks for long-running tasks, since they block the performed operation. Moreover, the branch is locked during the execution of <code>pre-*</code> hooks, so the webhook server cannot perform any write operations on the branch (like uploading or commits).</p>"},{"location":"howto/hooks/webhooks/#action-file-webhook-properties","title":"Action File Webhook properties","text":"<p>See the Action configuration for overall configuration schema and details.</p> Property Description Data Type Required Default Value Env Vars Support url The URL address of the request String true no timeout Time to wait for response before failing the hook String (golang's Duration representation) false 1 minute no query_params List of query params that will be added to the request Dictionary(String:String or String:List(String) false yes headers Headers to add to the request Dictionary(String:String) false yes <p>Secrets &amp; Environment Variables</p> <p>lakeFS Actions supports secrets by using environment variables. The format <code>{% raw %}{{{% endraw %} ENV.SOME_ENV_VAR {% raw %}}}{% endraw %}</code> will be replaced with the value of <code>$SOME_ENV_VAR</code> during the execution of the action. If that environment variable doesn't exist in the lakeFS server environment, the action run will fail.</p> <p>Info</p> <p>All environment variables need to begin with \"LAKEFSACTIONS_\". Otherwise, they will be blocked. Additionally, the <code>actions.env.enabled</code> configuration parameter can be set to <code>false</code> to block access to all environment variables.</p> <p>Example</p> <pre><code>...\nhooks:\n- id: prevent_user_columns\n    type: webhook\n    description: Ensure no user_* columns under public/\n    properties:\n    url: \"http://&lt;host:port&gt;/webhooks/schema\"\n    timeout: 1m30s\n    query_params:\n        disallow: [\"user_\", \"private_\"]\n        prefix: public/\n    headers:\n        secret_header: \"{% raw %}{{{% endraw %} ENV.MY_SECRET {% raw %}}}{% endraw %}\"\n...\n</code></pre>"},{"location":"howto/hooks/webhooks/#request-body-schema","title":"Request body schema","text":"<p>Upon execution, a webhook will send a request containing a JSON object with the following fields:</p> Field Description Type event_type Type of the event that triggered the Action string event_time Time of the event that triggered the Action (RFC3339 formatted) string action_name Containing Hook Action's Name string hook_id ID of the Hook string repository_id ID of the Repository string branch_id<sup>1</sup> ID of the Branch string source_ref Reference to the source on which the event was triggered string commit_message<sup>2</sup> The message for the commit (or merge) that is taking place string committer<sup>2</sup> Name of the committer string commit_metadata<sup>2</sup> The metadata for the commit that is taking place string commit_id<sup>3</sup> The ID of the commit that is being created string tag_id The ID of the created/deleted tag (available for Tag events) string merge_source The source branch/tag/ref on merge (available for Merge events) string <p>Example</p> <pre><code>{\n    \"event_type\": \"pre-merge\",\n    \"event_time\": \"2021-02-28T14:03:31Z\",\n    \"action_name\": \"test action\",\n    \"hook_id\": \"prevent_user_columns\",\n    \"repository_id\": \"repo1\",\n    \"branch_id\": \"feature-1\",\n    \"source_ref\": \"feature-1\",\n    \"commit_message\": \"merge commit message\",\n    \"commit_id\": \"5891b5b522d5df086d0ff0b110fbd9d21bb4fc7163af34d08286a2e846f6be03\",\n    \"committer\": \"committer\",\n    \"commit_metadata\": {\n        \"key\": \"value\"\n    }\n}\n</code></pre> <ol> <li> <p>N/A for Tag events\u00a0\u21a9</p> </li> <li> <p>N/A for Tag and Create/Delete Branch events\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Available for Commit/Merge events only. In Merge, this represents the merge commit ID to be created if the merge operation succeeds.\u00a0\u21a9</p> </li> </ol>"},{"location":"integrations/","title":"Integrations with lakeFS","text":"<p>You can use lakeFS with a wide range of tools and frameworks.</p> <p>lakeFS provides several clients directly, as well as an S3-compatible gateway. This gateway means that if you want to use something with lakeFS, so long as that technology can interface with S3, it can interface with lakeFS.</p> <p>See below for detailed instructions for using different technologies with lakeFS.</p> <p>Missing Something?</p> <p>If there is a technology not listed here that you would like to use with lakeFS, please drop by our Slack and we'll help you get started with it.</p> Airbyte Amazon Athena Amazon SageMaker Apache Airflow Apache Hive Apache Iceberg Apache Kafka Apache Spark AWS CLI Cloudera Databricks Delta Lake Dremio DuckDB Git Glue / Hive metastore HuggingFace Datasets Kubeflow Presto / Trino Python R Red Hat OpenShift AI Vertex AI MLflow"},{"location":"integrations/airbyte/","title":"Airbyte","text":"<p>Airbyte is an open-source platform for syncing data from applications, APIs, and databases to warehouses, lakes, and other destinations. You can use Airbyte's connectors to get your data pipelines to consolidate many input sources.</p> <p>The integration between Airbyte and lakeFS brings resilience and manageability when you use Airbyte connectors to sync data to your S3 buckets by leveraging lakeFS branches and atomic commits and merges.</p>"},{"location":"integrations/airbyte/#use-cases","title":"Use cases","text":"<p>You can take advantage of lakeFS consistency guarantees and Data Lifecycle Management when ingesting data to S3 using lakeFS:</p> <ol> <li>Consolidate many data sources to a single branch and expose them to consumers simultaneously when merging to the <code>main</code> branch.</li> <li>Test incoming data for breaking schema changes using lakeFS hooks.</li> <li>Prevent consumers from reading partial data from connectors which failed half-way through sync.</li> <li>Experiment with ingested data on a branch before exposing it.</li> </ol>"},{"location":"integrations/airbyte/#s3-connector","title":"S3 Connector","text":"<p>lakeFS exposes an S3 Gateway that enables applications to communicate with lakeFS the same way they would with Amazon S3. You can use Airbyte's S3 Connector to upload data to lakeFS.</p> <p>Warning</p> <p>If using Airbyte OSS, please ensure you are using S3 destination connector version 0.3.17 or higher. Previous connector versions are not supported.</p>"},{"location":"integrations/airbyte/#configuring-lakefs-using-the-connector","title":"Configuring lakeFS using the connector","text":"<p>Set the following parameters when creating a new Destination of type S3:</p> Name Value Example Endpoint The lakeFS S3 gateway URL <code>https://cute-axolotol.lakefs-demo.io</code> S3 Bucket Name The lakeFS repository where the data will be written <code>example-repo</code> S3 Bucket Path The branch and the path where the data will be written <code>main/data/from/airbyte</code> Where <code>main</code> is the branch name, and <code>data/from/airbyte</code> is the path under the branch. S3 Bucket Region Not applicable to lakeFS, use <code>us-east-1</code> <code>us-east-1</code> S3 Key ID The lakeFS access key id used to authenticate to lakeFS. <code>AKIAlakefs12345EXAMPLE</code> S3 Access Key The lakeFS secret access key used to authenticate to lakeFS. <code>abc/lakefs/1234567bPxRfiCYEXAMPLEKEY</code> <p>The UI configuration will look as follows:</p> <p></p>"},{"location":"integrations/airflow/","title":"Using lakeFS with Apache Airflow","text":"<p>Apache Airflow is a platform that allows users to programmatically author, schedule, and monitor workflows.</p> <p>To run Airflow with lakeFS, you need to follow a few steps.</p>"},{"location":"integrations/airflow/#create-a-lakefs-connection-on-airflow","title":"Create a lakeFS connection on Airflow","text":"<p>To access the lakeFS server and authenticate with it, create a new Airflow Connection of type HTTP and add it to your DAG.  You can do that using the Airflow UI or the CLI. Here\u2019s an example Airflow command that does just that:</p> <pre><code>airflow connections add conn_lakefs --conn-type=HTTP --conn-host=http://&lt;LAKEFS_ENDPOINT&gt; \\\n    --conn-extra='{\"access_key_id\":\"&lt;LAKEFS_ACCESS_KEY_ID&gt;\",\"secret_access_key\":\"&lt;LAKEFS_SECRET_ACCESS_KEY&gt;\"}'\n</code></pre>"},{"location":"integrations/airflow/#install-the-lakefs-airflow-package","title":"Install the lakeFS Airflow package","text":"<p>You can use <code>pip</code> to install the package</p> <pre><code>pip install airflow-provider-lakefs\n</code></pre>"},{"location":"integrations/airflow/#use-the-package","title":"Use the package","text":""},{"location":"integrations/airflow/#operators","title":"Operators","text":"<p>The package exposes several operations to interact with a lakeFS server:</p> <ol> <li><code>CreateBranchOperator</code> creates a new lakeFS branch from the source branch (<code>main</code> by default).     <pre><code>task_create_branch = CreateBranchOperator(\n    task_id='create_branch',\n    repo='example-repo',\n    branch='example-branch',\n    source_branch='main'\n)\n</code></pre></li> <li><code>CommitOperator</code> commits uncommitted changes to a branch.     <pre><code>task_commit = CommitOperator(\n    task_id='commit',\n    repo='example-repo',\n    branch='example-branch',\n    msg='committing to lakeFS using airflow!',\n    metadata={'committed_from\": \"airflow-operator'}\n)\n</code></pre></li> <li><code>MergeOperator</code> merges 2 lakeFS branches.     <pre><code>task_merge = MergeOperator(\n    task_id='merge_branches',\n    source_ref='example-branch',\n    destination_branch='main',\n    msg='merging job outputs',\n    metadata={'committer': 'airflow-operator'}\n)\n</code></pre></li> </ol>"},{"location":"integrations/airflow/#sensors","title":"Sensors","text":"<p>Sensors are also available that allow synchronizing a running DAG with external operations:</p> <ol> <li><code>CommitSensor</code> waits until a commit has been applied to the branch     <pre><code>task_sense_commit = CommitSensor(\n    repo='example-repo',\n    branch='example-branch',\n    task_id='sense_commit'\n)\n</code></pre></li> <li><code>FileSensor</code> waits until a given file is present on a branch.     <pre><code>task_sense_file = FileSensor(\n    task_id='sense_file',\n    repo='example-repo',\n    branch='example-branch',\n    path=\"file/to/sense\"\n)\n</code></pre></li> </ol>"},{"location":"integrations/airflow/#example","title":"Example","text":"<p>This example DAG in the airflow-provider-lakeFS repository shows how to use all of these.</p>"},{"location":"integrations/airflow/#performing-other-operations","title":"Performing other operations","text":"<p>Sometimes an operator might not be supported by airflow-provider-lakeFS yet. You can access lakeFS directly by using:</p> <ul> <li><code>SimpleHttpOperator</code> to send API requests to lakeFS. </li> <li><code>BashOperator</code> with lakectl commands.</li> </ul> <p>For example, deleting a branch using <code>BashOperator</code>:</p> <pre><code>commit_extract = BashOperator(\n    task_id='delete_branch',\n    bash_command='lakectl branch delete lakefs://example-repo/example-branch',\n    dag=dag,\n)\n</code></pre>"},{"location":"integrations/athena/","title":"Using lakeFS with Amazon Athena","text":"<p>Deprecated Feature</p> <p>Having heard the feedback from the community, we are planning to replace the below manual steps with an automated process. You can read more about it here.</p> <p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL.</p> <p>Amazon Athena works directly above S3 and can't access lakeFS. Tables created using Athena aren't readable by lakeFS. However, tables stored in lakeFS (that were created with glue/hive) can be queried by Athena.</p> <p>To support querying data from lakeFS with Amazon Athena, we will use <code>create-symlink</code>, one of the metastore commands in lakectl. <code>create-symlink</code> receives a source table, destination table, and the table location. It performs two actions:</p> <ol> <li>It creates partitioned directories with symlink files in the underlying S3 bucket.</li> <li>It creates a table in Glue catalog with symlink format type and location pointing to the created symlinks.</li> </ol> <p>Note</p> <p><code>.lakectl.yaml</code> file should be configured with the proper hive/glue credentials. For more information </p> <p>create-symlink receives a table in glue or hive pointing to lakeFS and creates a copy of the table in glue.</p> <p>The table data will use the <code>SymlinkTextInputFormat</code>, which will point to the lakeFS repository storage namespace. You will be able to query your data with Athena without copying any data. However, the symlinks table will only show the data that existed during  the copy. If the table changed in lakeFS, you need to run <code>create-symlink</code> again for your changed to be reflected in Athena.</p>"},{"location":"integrations/athena/#example","title":"Example:","text":"<p>Let's assume that some time ago, we created a hive table <code>my_table</code> that is stored in lakeFS repo <code>example</code> under branch <code>main</code>, using the command:</p> <pre><code>CREATE EXTERNAL TABLE `my_table`(\n   `id` bigint, \n   `key` string \n)\nPARTITIONED BY (YEAR INT, MONTH INT)\nLOCATION 's3://example/main/my_table';\nWITH (format = 'PARQUET', external_location 's3a://example/main/my_table' );\n</code></pre> <p>The repository <code>example</code> has the S3 storage space <code>s3://my-bucket/my-repo-prefix/</code>.  After inserting some data into it, the object structure under <code>lakefs://example/main/my_table</code> looks as follows:</p> <p></p> <p>To query that table with Athena, you need to use the <code>create-symlink</code> command as follows:</p> <pre><code>lakectl metastore create-symlink \\\n    --repo example \\\n    --branch main \\\n    --path my_table \\\n    --from-client-type hive \\\n    --from-schema default \\\n    --from-table my_table \\\n    --to-schema default \\ \n    --to-table my_table  \n</code></pre> <p>The command will generate two notable outputs:</p> <ol> <li>For each partition, the command will create a symlink file:</li> </ol> <pre><code>aws s3 ls s3://my-bucket/my-repo-prefix/my_table/ --recursive\n2021-11-23 17:46:29          0 my-repo-prefix/my_table/symlinks/example/main/my_table/year=2021/month=11/symlink.txt\n2021-11-23 17:46:29         60 my-repo-prefix/my_table/symlinks/example/main/my_table/year=2021/month=12/symlink.txt\n2021-11-23 17:46:30         60 my-repo-prefix/my_table/symlinks/example/main/my_table/year=2022/month=1/symlink.txt\n</code></pre> <p>An example content of a symlink file, where each line represents a single object of the specific partition:</p> <pre><code>s3://my-bucket/my-repo-prefix/5bdc62da516944b49889770d98274227\ns3://my-bucket/my-repo-prefix/64262fbf3d6347a79ead641d2b2baee6\ns3://my-bucket/my-repo-prefix/64486c8de6484de69f12d7d26804c93e\ns3://my-bucket/my-repo-prefix/b0165d5c5b13473d8a0f460eece9eb26\n</code></pre> <ol> <li>A glue table pointing to the symlink directories structure:</li> </ol> <pre><code>aws glue get-table --name my_table --database-name default\n\n{\n  \"Table\": {\n    \"Name\": \"my_table\",\n    \"DatabaseName\": \"default\",\n    \"Owner\": \"anonymous\",\n    \"CreateTime\": \"2021-11-23T17:46:30+02:00\",\n    \"UpdateTime\": \"2021-11-23T17:46:30+02:00\",\n    \"LastAccessTime\": \"1970-01-01T02:00:00+02:00\",\n    \"Retention\": 0,\n    \"StorageDescriptor\": {\n      \"Columns\": [\n        {\n          \"Name\": \"id\",\n          \"Type\": \"bigint\",\n          \"Comment\": \"\"\n        },\n        {\n          \"Name\": \"key\",\n          \"Type\": \"string\",\n          \"Comment\": \"\"\n        }\n      ],\n      \"Location\": \"s3://my-bucket/my-repo-prefix/symlinks/example/main/my_table\",\n      \"InputFormat\": \"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat\",\n      \"OutputFormat\": \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\",\n      \"Compressed\": false,\n      \"NumberOfBuckets\": -1,\n      \"SerdeInfo\": {\n        \"Name\": \"default\",\n        \"SerializationLibrary\": \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\",\n        \"Parameters\": {\n          \"serialization.format\": \"1\"\n        }\n      },\n      \"StoredAsSubDirectories\": false\n    },\n    \"PartitionKeys\": [\n      {\n        \"Name\": \"year\",\n        \"Type\": \"int\",\n        \"Comment\": \"\"\n      },\n      {\n        \"Name\": \"month\",\n        \"Type\": \"int\",\n        \"Comment\": \"\"\n      }\n    ],\n    \"ViewOriginalText\": \"\",\n    \"ViewExpandedText\": \"\",\n    \"TableType\": \"EXTERNAL_TABLE\",\n    \"Parameters\": {\n      \"EXTERNAL\": \"TRUE\",\n      \"bucketing_version\": \"2\",\n      \"transient_lastDdlTime\": \"1637681750\"\n    },\n    \"CreatedBy\": \"arn:aws:iam::************:user/********\",\n    \"IsRegisteredWithLakeFormation\": false,\n    \"CatalogId\": \"*********\"\n  }\n}\n</code></pre> <p>You can now safely use Athena to query <code>my_table</code>.</p>"},{"location":"integrations/aws_cli/","title":"Using lakeFS with AWS CLI","text":"<p>lakeFS exposes an S3-compatible API, so you can use the AWS S3 CLI to interact with objects in your repositories.</p>"},{"location":"integrations/aws_cli/#configuration","title":"Configuration","text":"<p>You would like to configure an AWS profile for lakeFS.</p> <p>To configure the lakeFS credentials, run:</p> <pre><code>aws configure --profile lakefs\n</code></pre> <p>You will be prompted to enter the AWS Access Key ID and the AWS Secret Access Key.</p> <p>It should look like this:</p> <pre><code>aws configure --profile lakefs\n# output:  \n# AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE    \n# AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n# Default region name [None]: \n# Default output format [None]:\n</code></pre>"},{"location":"integrations/aws_cli/#path-convention","title":"Path convention","text":"<p>When accessing objects in S3, you will need to use the lakeFS path convention:</p> <pre><code>s3://[REPOSITORY]/[BRANCH]/PATH/TO/OBJECT\n</code></pre>"},{"location":"integrations/aws_cli/#usage","title":"Usage","text":"<p>After configuring the credentials, this is what a command should look:</p> <pre><code>aws s3 --profile lakefs \\\n    --endpoint-url https://lakefs.example.com \\\n    ls s3://example-repo/main/example-directory\n</code></pre> <p>You can use an alias to make it shorter and more convenient.</p>"},{"location":"integrations/aws_cli/#examples","title":"Examples","text":""},{"location":"integrations/aws_cli/#list-directory","title":"List directory","text":"<pre><code>aws --profile lakefs \\\n    --endpoint-url https://lakefs.example.com \\\n    s3 ls s3://example-repo/main/example-directory\n</code></pre>"},{"location":"integrations/aws_cli/#copy-from-lakefs-to-lakefs","title":"Copy from lakeFS to lakeFS","text":"<pre><code>aws --profile lakefs \\\n    --endpoint-url https://lakefs.example.com \\\n    s3 cp s3://example-repo/main/example-file-1 s3://example-repo/main/example-file-2\n</code></pre>"},{"location":"integrations/aws_cli/#copy-from-lakefs-to-a-local-path","title":"Copy from lakeFS to a local path","text":"<pre><code>aws --profile lakefs \\\n    --endpoint-url https://lakefs.example.com \\\n    s3 cp s3://example-repo/main/example-file-1 /path/to/local/file\n</code></pre>"},{"location":"integrations/aws_cli/#copy-from-a-local-path-to-lakefs","title":"Copy from a local path to lakeFS","text":"<pre><code>aws --profile lakefs \\\n    --endpoint-url https://lakefs.example.com \\\n    s3 cp /path/to/local/file s3://example-repo/main/example-file-1\n</code></pre>"},{"location":"integrations/aws_cli/#delete-file","title":"Delete file","text":"<pre><code>aws --profile lakefs \\\n    --endpoint-url https://lakefs.example.com \\\n    s3 rm s3://example-repo/main/example-directory/example-file\n</code></pre>"},{"location":"integrations/aws_cli/#delete-directory","title":"Delete directory","text":"<pre><code>aws --profile lakefs \\\n    --endpoint-url https://lakefs.example.com \\\n    s3 rm s3://example-repo/main/example-directory/ --recursive\n</code></pre>"},{"location":"integrations/aws_cli/#adding-an-alias","title":"Adding an alias","text":"<p>To make the command shorter and more convenient, you can create an alias:</p> <pre><code>alias awslfs='aws --endpoint https://lakefs.example.com --profile lakefs'\n</code></pre> <p>Now, the ls command using the alias will be as follows:</p> <pre><code>awslfs s3 ls s3://example-repo/main/example-directory\n</code></pre>"},{"location":"integrations/cloudera/","title":"Using lakeFS with Cloudera Spark","text":"<p>Use the lakeFS Hadoop FileSystem to integrate lakeFS with Cloudera Spark.</p> <p>Review Cloudera Partner Listing for the Cloudera certification of lakeFS integration with Cloudera Data Platform (CDP) and Cloudera Spark.</p>"},{"location":"integrations/databricks/","title":"Using lakeFS with Databricks","text":""},{"location":"integrations/databricks/#overview","title":"Overview","text":"<p>Databricks is a unified, open analytics platform for building, deploying, sharing, and maintaining enterprise-grade data, analytics, and AI solutions at scale.</p> <p>In this document, we will cover the various Databricks products and how they integrate with lakeFS.</p> <p></p>"},{"location":"integrations/databricks/#databricks-compute-options","title":"Databricks Compute Options","text":"<p>Databricks offers several compute options for running workloads. All of these options can be used with lakeFS.  At a basic level, Databricks compute products are Spark clusters that run on top of cloud infrastructure and offer different configuration options.  From a lakeFS integration perspective, the main difference is how you configure the storage operations that perform read/writes to lakeFS.</p> <p>lakeFS storage operations will either use the lakeFS Hadoop Filesystem, which utilizes the lakeFS OpenAPI, or the s3a Filesystem, which uses the lakeFS S3 Gateway. In short, the lakeFS S3 Gateway is the fastest way to get started, but it routes all traffic through the lakeFS server. The lakeFS Hadoop Filesystem requires more setup, but all data transfers will occur directly on the bucket. The lakeFS Hadoop Filesystem can write to storage using the s3a filesystem or using pre-signed URLs generated by the lakeFS server. To read more about the alternatives, see the Spark integration page.</p>"},{"location":"integrations/databricks/#all-purpose-compute","title":"All-Purpose Compute","text":"<p>Provisioned compute used to analyze data in notebooks.</p> <p>When you create a Databricks compute cluster, you can configure it to use lakeFS with the lakeFS Hadoop Filesystem (see Databricks installation guide) or the lakeFS S3 Gateway. The lakeFS S3 Gateway can be configured in the notebook or during cluster setup (Advanced Options -&gt; Spark -&gt; Spark config).</p>"},{"location":"integrations/databricks/#jobs-compute","title":"Jobs Compute","text":"<p>Provisioned compute used to run automated jobs. The Databricks job scheduler automatically creates a job compute whenever a job is configured to run on new compute.</p> <p>To use lakeFS with Databricks jobs, a compute cluster needs to be configured in the cluster setup, just like with All-Purpose compute. </p> <p>Note</p> <p>Serverless compute for Databricks jobs is currently not supported.</p>"},{"location":"integrations/databricks/#sql-warehouses","title":"SQL Warehouses","text":"<p>Classic &amp; Pro warehouses are used to run SQL commands on data objects in the SQL editor or interactive notebooks. Serverless warehouses do the same, except that they are on-demand elastic compute.</p> <p>All warehouses do not allow the installation of external jars, such as the lakeFS Hadoop Filesystem. To use SQL warehouses with lakeFS, utilize the lakeFS S3 Gateway.</p>"},{"location":"integrations/databricks/#unity-catalog","title":"Unity Catalog","text":"<p>Unity Catalog is Databricks' metastore that provides centralized access control, auditing, lineage, and data discovery capabilities across Databricks workspaces.</p> <p>lakeFS can be used with Unity Catalog to provide a versioned view of the data and the Unity Catalog metadata.</p> <p>lakeFS support for Unity Catalog differs between lakeFS OSS and lakeFS Enterprise &amp; Cloud.</p>"},{"location":"integrations/databricks/#catalog-exports-lakefs","title":"Catalog Exports lakeFS","text":"<p>Leveraging the external tables feature within Unity Catalog, you can register a Delta Lake table exported from lakeFS and access it through the unified catalog.</p> <p>Note</p> <p>lakeFS Catalog exporters offer read-only table exports.</p> <p>Catalog Exports relies on lakeFS Actions and offers a way to export changes from lakeFS to Unity Catalog.</p> <p>For the full guide on how to use Catalog Exports with Unity Catalog, see the documentation.</p>"},{"location":"integrations/databricks/#delta-lake","title":"Delta Lake","text":"<p>lakeFS supports Delta Lake tables, and it provides a versioned view of the data and the Delta Lake metadata. Read the Delta Lake docs for more information.</p>"},{"location":"integrations/delta/","title":"Using lakeFS with Delta Lake","text":"<p>Delta Lake is an open-source storage framework designed to improve performance and provide transactional guarantees to data lake tables.</p> <p>Because lakeFS is format-agnostic, you can save data in Delta format within a lakeFS repository and benefit from the advantages of both technologies. Specifically:</p> <ol> <li>ACID operations can span across multiple Delta tables.</li> <li>CI/CD hooks can validate Delta table contents, schema, or even referential integrity.</li> <li>lakeFS supports zero-copy branching for quick experimentation with full isolation.</li> </ol>"},{"location":"integrations/delta/#delta-lake-tables-from-the-lakefs-perspective","title":"Delta Lake Tables from the lakeFS Perspective","text":"<p>lakeFS is a data versioning tool, functioning at the object level. This implies that, by default, lakeFS remains agnostic to whether the objects within a Delta table location represent a table, table metadata, or data. As per the Delta Lake protocol, any modification to a table\u2014whether it involves adding data or altering table metadata\u2014results in the creation of a new object in the table's transaction log. Typically, residing under the <code>_delta_log</code> path, relative to the root of the table's directory. This new object has an incremented version compared to its predecessor.</p> <p>Consequently, when making changes to a Delta table within the lakeFS environment, these changes are reflected as changes to objects within the table location. For instance, inserting a record into a table named \"my-table,\" which is partitioned by 'category' and 'country,' is represented in lakeFS as added objects within the table prefix (i.e., the table data) and the table transaction log. </p> <p>Similarly, when performing a metadata operation such as renaming a table column, new objects are appended to the table transaction log, indicating the schema change. </p>"},{"location":"integrations/delta/#using-delta-lake-with-lakefs-from-apache-spark","title":"Using Delta Lake with lakeFS from Apache Spark","text":"<p>Note</p> <p>Given the native integration between Delta Lake and Spark, it's most common that you'll interact with Delta tables in a Spark environment</p> <p>To configure a Spark environment to read from and write to a Delta table within a lakeFS repository, you need to set the proper credentials and endpoint in the S3 Hadoop configuration, like you'd do with any Spark environment.</p> <p>Once set, you can interact with Delta tables using regular Spark path URIs. Make sure that you include the lakeFS repository and branch name:</p> <pre><code>df.write.format(\"delta\").save(\"s3a://&lt;repo-name&gt;/&lt;branch-name&gt;/path/to/delta-table\")\n</code></pre> <p>Info</p> <p>If using the Databricks Analytics Platform, see the integration guide for configuring a Databricks cluster to use lakeFS.</p> <p>To see the integration in action see this notebook in the lakeFS Samples Repository.</p>"},{"location":"integrations/delta/#using-delta-lake-with-lakefs-from-python","title":"Using Delta Lake with lakeFS from Python","text":"<p>The delta-rs library provides bindings for Python. This means that you can use Delta Lake and lakeFS directly from Python without needing Spark. Integration is done through the lakeFS S3 Gateway</p> <p>The documentation for the <code>deltalake</code> Python module details how to read, write, and query Delta Lake tables. To use it with lakeFS use an <code>s3a</code> path for the table based on your repository and branch (for example, <code>s3a://delta-lake-demo/main/my_table/</code>) and specify the following <code>storage_options</code>:</p> <pre><code>storage_options = {\n    \"AWS_ENDPOINT\": &lt;your lakeFS endpoint&gt;,\n    \"AWS_ACCESS_KEY_ID\": &lt;your lakeFS access key&gt;,\n    \"AWS_SECRET_ACCESS_KEY\": &lt;your lakeFS secret key&gt;,\n    \"AWS_REGION\": \"us-east-1\",\n    \"AWS_S3_ALLOW_UNSAFE_RENAME\": \"true\"\n}\n</code></pre> <p>If your lakeFS is not using HTTPS (for example, you're just running it locally) then add the option</p> <pre><code>\"AWS_STORAGE_ALLOW_HTTP\": \"true\"\n</code></pre> <p>To see the integration in action see this notebook in the lakeFS Samples Repository.</p>"},{"location":"integrations/delta/#exporting-delta-lake-tables-from-lakefs-into-unity-catalog","title":"Exporting Delta Lake tables from lakeFS into Unity Catalog","text":"<p>This option is for users who are managing Delta Lake tables with lakeFS and access them through Databricks Unity Catalog. lakeFS offers a Data Catalog Export functionality that provides read-only access to your Delta tables from within Unity catalog. Using the data catalog exporters, you can work on Delta tables in isolation and easily explore them within the Unity Catalog.</p> <p>Once exported, you can query the versioned table data with:</p> <pre><code>SELECT * FROM my_catalog.main.my_delta_table\n</code></pre> <p>Here, <code>main</code> is the name of the lakeFS branch from which the delta table was exported.</p> <p>To enable Delta table exports to Unity catalog use the Unity catalog integration guide.</p>"},{"location":"integrations/delta/#limitations","title":"Limitations","text":"Multi-Writer Support in lakeFS for Delta Lake Tables <p>lakeFS currently supports a single writer for Delta Lake tables. Attempting to utilize multiple writers for writing to a Delta table may result in two types of issues:</p> <ol> <li>Merge Conflicts: These conflicts arise when multiple writers modify a Delta table on different branches, and an attempt is made to merge these branches.     </li> <li>Concurrent File Overwrite: This issue occurs when multiple writers concurrently modify a Delta table on the same branch.     </li> </ol> <p>Note</p> <p>lakeFS currently lacks its own implementation for a LogStore, and the default Log store used does not control concurrency.</p> <p>To address these limitations, consider following best practices for implementing multi-writer support.</p>"},{"location":"integrations/delta/#best-practices","title":"Best Practices","text":""},{"location":"integrations/delta/#implementing-multi-writer-support-through-lakefs-branches-and-merges","title":"Implementing Multi-Writer Support through lakeFS Branches and Merges","text":"<p>To achieve safe multi-writes to a Delta Lake table on lakeFS, we recommend following these best practices:</p> <ol> <li>Isolate Changes: Make modifications to your table in isolation. Each set of changes should be associated with a dedicated lakeFS branch, branching off from the main branch.</li> <li>Merge Atomically: After making changes in isolation, try to merge them back into the main branch. This approach guarantees that the integration of changes is cohesive.</li> </ol> <p>The workflow involves:</p> <ul> <li>Creating a new lakeFS branch from the main branch for any table change.</li> <li>Making modifications in isolation.</li> <li>Attempting to merge the changes back into the main branch.</li> <li>Iterating the process in case of a merge failure due to conflicts.</li> </ul> <p>The diagram below provides a visual representation of how branches and merges can be utilized to manage concurrency effectively: </p>"},{"location":"integrations/delta/#follow-vacuum-by-garbage-collection","title":"Follow Vacuum by Garbage Collection","text":"<p>To delete unused files from a table directory while working with Delta Lake over lakeFS you need to first use Delta lake Vacuum to soft-delete the files, and then use lakeFS Garbage Collection to hard-delete them from the storage.</p> <p>Tip</p> <p>lakeFS enables you to recover from undesired vacuum runs by reverting the changes done by a vacuum run before running Garbage Collection.</p>"},{"location":"integrations/delta/#when-running-lakefs-inside-your-vpc-on-aws","title":"When running lakeFS inside your VPC (on AWS)","text":"<p>When lakeFS runs inside your private network, your Databricks cluster needs to be able to access it. This can be done by setting up a VPC peering between the two VPCs (the one where lakeFS runs and the one where Databricks runs). For this to work on Delta Lake tables, you would also have to disable multi-cluster writes with:</p> <pre><code>spark.databricks.delta.multiClusterWrites.enabled false\n</code></pre>"},{"location":"integrations/delta/#using-multi-cluster-writes-on-aws","title":"Using multi cluster writes (on AWS)","text":"<p>When using multi-cluster writes, Databricks overrides Delta\u2019s S3-commit action.</p> <p>The new action tries to contact lakeFS from servers on Databricks\u2019 own AWS account, which of course won\u2019t be able to access your private network. So, if you must use multi-cluster writes, you\u2019ll have to allow access from Databricks\u2019 AWS account to lakeFS. If you are trying to achieve that, please reach out on Slack and the community will try to assist.</p>"},{"location":"integrations/delta/#further-reading","title":"Further Reading","text":"<p>See Guaranteeing Consistency in Your Delta Lake Tables With lakeFS post on the lakeFS blog to learn how to guarantee data quality in a Delta table by utilizing lakeFS branches.</p>"},{"location":"integrations/dremio/","title":"Using lakeFS with Dremio","text":"<p>Dremio is a next-generation data lake engine that liberates your data with live,  interactive queries directly on cloud data lake storage, including S3 and lakeFS.</p>"},{"location":"integrations/dremio/#configuration","title":"Configuration","text":"<p>Starting from version 3.2.3, Dremio supports Minio as an experimental S3-compatible plugin. Similarly, you can connect lakeFS with Dremio.</p> <p>Suppose you already have both lakeFS and Dremio deployed, and want to use Dremio to query your data in the lakeFS repositories. You can follow the steps listed below to configure on Dremio UI:</p> <ol> <li>click Add Data Lake.</li> <li>Under File Stores, choose Amazon S3.</li> <li>Under Advanced Options, check Enable compatibility mode (experimental).</li> <li>Under Advanced Options &gt; Connection Properties, add <code>fs.s3a.path.style.access</code> and set the value to <code>true</code>.</li> <li>Under Advanced Options &gt; Connection Properties, add <code>fs.s3a.endpoint</code> and set lakeFS S3 endpoint to the value. </li> <li>Under the General tab, specify the access_key_id and secret_access_key provided by lakeFS server.</li> <li>Click Save, and now you should be able to browse lakeFS repositories on Dremio.</li> </ol>"},{"location":"integrations/duckdb/","title":"Using lakeFS with DuckDB","text":"<p>DuckDB is an in-process SQL OLAP database management system. You can access data in lakeFS from DuckDB, as well as use DuckDB from within the web interface of lakeFS</p>"},{"location":"integrations/duckdb/#accessing-lakefs-from-duckdb","title":"Accessing lakeFS from DuckDB","text":""},{"location":"integrations/duckdb/#configuration","title":"Configuration","text":"<p>Querying data in lakeFS from DuckDB is similar to querying data in S3 from DuckDB. It is done using the httpfs extension connecting to the S3 Gateway that lakeFS provides.</p> <p>If not loaded already, install and load the <code>HTTPFS</code> extension: </p> <pre><code>INSTALL httpfs;\nLOAD httpfs;\n</code></pre> <p>Then run the following to configure the connection. </p> <pre><code>-- \"s3_region\" is the S3 region on which your bucket resides. If local storage, or not S3, then just set it to \"us-east-1\".\nSET s3_region='us-east-1';\n-- the host (and port, if necessary) of your lakeFS server\nSET s3_endpoint='lakefs.example.com';\n-- the access credentials for your lakeFS user\nSET s3_access_key_id='AKIAIOSFODNN7EXAMPLE'; \n-- the access credentials for your lakeFS user\nSET s3_secret_access_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'; \nSET s3_url_style='path';\n\n-- Uncomment in case the endpoint listen on non-secure, for example running lakeFS locally.\n-- SET s3_use_ssl=false;\n</code></pre>"},{"location":"integrations/duckdb/#querying-data","title":"Querying Data","text":"<p>Once configured, you can query data using the lakeFS S3 Gateway using the following URI pattern:</p> <pre><code>s3://&lt;REPOSITORY NAME&gt;/&lt;REFERENCE ID&gt;/&lt;PATH TO DATA&gt;\n</code></pre> <p>Since the S3 Gateway implemenets all S3 functionality required by DuckDB, you can query using globs and patterns, including support for Hive-partitioned data.</p> <p>Example:</p> <pre><code>SELECT * \nFROM parquet_scan('s3://example-repo/main/data/population/by-region/*.parquet', HIVE_PARTITIONING=1) \nORDER BY name;\n</code></pre>"},{"location":"integrations/duckdb/#writing-data","title":"Writing Data","text":"<p>No special configuration required for writing to a branch. Assuming the configuration above and write permissions to a <code>dev</code> branch, a write operation would look like any DuckDB write:</p> <pre><code>CREATE TABLE sampled_population AS SELECT * \nFROM parquet_scan('s3://example-repo/main/data/population/by-region/*.parquet', HIVE_PARTITIONING=1) \nUSING SAMPLE reservoir(50000 ROWS) REPEATABLE (100);\n\nCOPY sampled_population TO 's3://example-repo/main/data/population/sample.parquet'; -- actual write happens here\n</code></pre>"},{"location":"integrations/duckdb/#using-duckdb-in-python-with-lakefs-spec","title":"Using DuckDB in Python with lakefs-spec","text":"<p>Python users can use DuckDB by leveraging the lakefs-spec package. </p> <p>Note</p> <p>This library is a third-party package and not maintained by the lakeFS developers; please file issues and bug reports directly in the lakefs-spec repository.</p> <p>Using lakefs-spec, querying lakeFS could be done using pre-signed URLs, allowing for efficient and secure I/O, where the data files are read directly from the underlying object store.</p> <pre><code>import duckdb\nfrom fsspec import filesystem\n\nduckdb.register_filesystem(filesystem('lakefs'))\n\nduckdb.sql(\"SELECT * FROM 'lakefs://example-repo/main/data/population/sample.parquet'\")\n</code></pre>"},{"location":"integrations/duckdb/#using-duckdb-in-the-lakefs-web-ui","title":"Using DuckDB in the lakeFS web UI","text":"<p>The lakeFS web UI includes DuckDB in the Object viewer page. </p> <p></p> <p>Using this you can query objects in lakeFS directly using a <code>lakefs</code> path: </p> <pre><code>lakefs://&lt;repository&gt;/&lt;branch&gt;/object/path/foo.parquet\n</code></pre> <p>The DuckDB query editor is provided by DuckDB WASM. It renders and provides querying capabilities for any objects of the following types:</p> <ul> <li>Parquet</li> <li>CSV</li> <li>TSV</li> </ul>"},{"location":"integrations/git/","title":"Integrating lakeFS with Git","text":"<p>Integrating code and data version control systems allows you to associate code versions with data versions, facilitating the reproduction of complex environments with multiple components. Consequently, lakeFS, a data version control system,  seamlessly integrates with Git. This combination establishes a robust foundation for versioning both your code and data,  fostering a streamlined and reproducible development process.</p>"},{"location":"integrations/git/#use-cases","title":"Use Cases","text":""},{"location":"integrations/git/#develop-reproducible-ml-models","title":"Develop Reproducible ML Models","text":"<p>Maintain a comprehensive record of both model code versions and input data versions to ensure the reproducibility of ML  model results.</p> <p>The common way to develop reproducible ML models with lakeFS is to use the  lakectl local command. See Working with lakeFS Data Locally  to understand how to use lakectl local in conjunction with Git to develop reproducible ML models.    </p>"},{"location":"integrations/git/#develop-reproducible-etl-pipelines","title":"Develop Reproducible ETL Pipelines","text":"<p>Track code versions for each step in ETL pipelines along with the corresponding data versions of their inputs and outputs. This approach allows straight forward troubleshooting and reproduction of data errors. </p> <p>Check out this lakeFS sample  that demonstrates how Git, Airflow, and lakeFS can be integrated to result in reproducible ETL pipelines.   </p>"},{"location":"integrations/glue_hive_metastore/","title":"Using lakeFS with the Glue/Hive Metastore","text":"<p>Deprecated Feature</p> <p>Having heard the feedback from the community, we are planning to replace the below manual steps with an automated process. You can read more about it here.</p>"},{"location":"integrations/glue_hive_metastore/#about-glue-hive-metastore","title":"About Glue / Hive Metastore","text":"<p>This part explains about how Glue/Hive Metastore work with lakeFS.</p> <p>Glue and Hive Metastore store metadata related to Hive and other services (such as Spark and Trino). They contain metadata such as the location of the table, information about columns, partitions and many more.</p>"},{"location":"integrations/glue_hive_metastore/#without-lakefs","title":"Without lakeFS","text":"<p>To query the table <code>my_table</code>, Spark will: * Request the metadata from Hive metastore (steps 1,2), * Use the location from the metadata to access the data in S3 (steps 3,4). </p>"},{"location":"integrations/glue_hive_metastore/#with-lakefs","title":"With lakeFS","text":"<p>When using lakeFS, the flow stays exactly the same. Note that the location of the table <code>my_table</code> now contains the branch <code>s3://example/main/path/to/table</code> </p>"},{"location":"integrations/glue_hive_metastore/#managing-tables-with-lakefs-branches","title":"Managing Tables With lakeFS Branches","text":""},{"location":"integrations/glue_hive_metastore/#motivation","title":"Motivation","text":"<p>When creating a table in Glue/Hive metastore (using a client such as Spark, Hive, Presto), we specify the table location. Consider the table <code>my_table</code> that was created with the location <code>s3://example/main/path/to/table</code>.</p> <p>Suppose you created a new branch called <code>DEV</code> with <code>main</code> as the source branch. The data from <code>s3://example/main/path/to/table</code> is now accessible in <code>s3://example/DEV/path/to/table</code>. The metadata is not managed in lakeFS, meaning you don't have any table pointing to <code>s3://example/DEV/path/to/table</code>.</p> <p>To address this, lakeFS introduces <code>lakectl metastore</code> commands. The case above can be handled using the copy command: it creates a copy of <code>my_table</code> with data located in <code>s3://example/DEV/path/to/table</code>. Note that this is a fast, metadata-only operation.</p>"},{"location":"integrations/glue_hive_metastore/#configurations","title":"Configurations","text":"<p>The <code>lakectl metastore</code> commands can run on Glue or Hive metastore. Add the following to the lakectl configuration file (by default <code>~/.lakectl.yaml</code>):</p>"},{"location":"integrations/glue_hive_metastore/#hive","title":"Hive","text":"<pre><code>metastore:\n  type: hive\n  hive:\n    uri: hive-metastore:9083\n</code></pre>"},{"location":"integrations/glue_hive_metastore/#glue","title":"Glue","text":"<pre><code>metastore:\n  type: glue\n  glue:\n    catalog_id: 123456789012\n    region: us-east-1\n    profile: default # optional, implies using a credentials file\n    credentials:\n      access_key_id: AKIAIOSFODNN7EXAMPLE\n      secret_access_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n</code></pre> <p>Note</p> <p>It's recommended to set type and catalog-id/metastore-uri in the lakectl configuration file.</p>"},{"location":"integrations/glue_hive_metastore/#suggested-model","title":"Suggested Model","text":"<p>For simplicity, we recommend creating a schema for each branch. That way, you can use the same table name across different schemas.</p> <p>For example:</p> <p>after creating branch <code>example_branch</code>, also create a schema named <code>example_branch</code>. For a table named <code>my_table</code> under the schema <code>main</code>, create a new table under the same name and under the schema <code>example_branch</code>. You now have two <code>my_table</code>, one in the main schema and one in the branch schema.</p>"},{"location":"integrations/glue_hive_metastore/#commands","title":"Commands","text":"<p>Metastore tools support three commands: <code>copy</code>, <code>diff</code>, and <code>create-symlink</code>. copy and diff can work both on Glue and on Hive. create-symlink works only on Glue.</p> <p>Note</p> <p>If <code>to-schema</code> or <code>to-table</code> are not specified, the destination branch and source table names will be used as per the suggested model.</p> <p>Metastore commands can only run on tables located in lakeFS. You should not use tables that aren't located in lakeFS.</p>"},{"location":"integrations/glue_hive_metastore/#copy","title":"Copy","text":"<p>The <code>copy</code> command creates a copy of a table pointing to the defined branch. In case the destination table already exists, the command will only merge the changes.</p> <p>Example:</p> <p>Suppose we created the table <code>inventory</code> on branch <code>main</code> on schema <code>default</code>.</p> <pre><code>CREATE EXTERNAL TABLE `inventory`(\n        `inv_item_sk` int,\n        `inv_warehouse_sk` int,\n        `inv_quantity_on_hand` int)\n    PARTITIONED BY (\n        `inv_date_sk` int) STORED AS ORC\n    LOCATION\n        's3a://my_repo/main/path/to/table';\n</code></pre> <p>We create a new lakeFS branch <code>example_branch</code>:</p> <pre><code>lakectl branch create lakefs://my_repo/example_branch --source lakefs://my_repo/main\n</code></pre> <p>The data from <code>s3://my_repo/main/path/to/table</code> is now accessible in <code>s3://my_repo/DEV/path/to/table</code>. To query the data in <code>s3://my_repo/DEV/path/to/table</code>, you would like to create a copy of the table <code>inventory</code> in schema <code>example_branch</code> pointing to the new branch.</p> <pre><code>lakectl metastore copy --from-schema default --from-table inventory --to-schema example_branch --to-table inventory --to-branch example_branch\n</code></pre> <p>After running this command, query the table <code>example_branch.inventory</code> to get the data from <code>s3://my_repo/DEV/path/to/table</code></p>"},{"location":"integrations/glue_hive_metastore/#copy-partition","title":"Copy Partition","text":"<p>After adding a partition to the branch table, you may want to copy the partition to the main table. For example, for the new partition <code>2020-08-01</code>, run the following to copy the partition to the main table:</p> <pre><code>lakectl metastore copy --type hive --from-schema example_branch --from-table inventory --to-schema default --to-table inventory --to-branch main -p 2020-08-01\n</code></pre> <p>For a table partitioned by more than one column, specify the partition flag for every column. For example, for the partition <code>(year='2020',month='08',day='01')</code>:</p> <pre><code>lakectl metastore copy --from-schema example_branch --from-table branch_inventory --to-schema default --to-branch main -p 2020 -p 08 -p 01\n</code></pre>"},{"location":"integrations/glue_hive_metastore/#diff","title":"Diff","text":"<p>Provides a two-way diff between two tables. Shows added<code>+</code> , removed<code>-</code> and changed<code>~</code> partitions and columns.</p> <p>Example:</p> <p>Suppose you made some changes on the copied table <code>inventory</code> on schema <code>example_branch</code> and now want to view the changes before merging back to <code>inventory</code> on schema <code>default</code>.</p> <p>Hive: <pre><code>lakectl metastore diff --type hive --address thrift://hive-metastore:9083 --from-schema example_branch --from-table branch --to-schema default --to-table inventory\n</code></pre></p> <p>The output will look like this:</p> <pre><code>Columns are identical\nPartitions\n- 2020-07-04\n+ 2020-07-05\n+ 2020-07-06\n~ 2020-07-08\n</code></pre>"},{"location":"integrations/glue_metastore/","title":"Using lakeFS with the Glue Catalog","text":""},{"location":"integrations/glue_metastore/#overview","title":"Overview","text":"<p>The integration between Glue and lakeFS is based on Data Catalog Exports.</p> <p>This guide describes how to use lakeFS with the Glue Data Catalog. You'll be able to query your lakeFS data by specifying the repository, branch and commit in your SQL query. Currently, only read operations are supported on the tables. You will set up the automation required to work with lakeFS on top of the Glue Data Catalog, including:</p> <ol> <li>Create a table descriptor under <code>_lakefs_tables/&lt;your-table&gt;.yaml</code>. This will represent your table schema.</li> <li>Write an exporter script that will:</li> <li>Mirror your branch's state into Hive Symlink files readable by Athena.</li> <li>Export the table descriptors from your branch to the Glue Catalog.</li> <li>Set up lakeFS hooks to trigger the above script when specific events occur.</li> </ol>"},{"location":"integrations/glue_metastore/#example-using-athena-to-query-lakefs-data","title":"Example: Using Athena to query lakeFS data","text":""},{"location":"integrations/glue_metastore/#prerequisites","title":"Prerequisites","text":"<p>Before starting, make sure you have: 1. An active lakeFS installation with S3 as the backing storage, and a repository in this installation. 2. A database in Glue Data Catalog (lakeFS does not create one). 3. AWS Credentials with permission to manage Glue, Athena Query and S3 access.</p>"},{"location":"integrations/glue_metastore/#add-table-descriptor","title":"Add table descriptor","text":"<p>Let's define a table, and commit it to lakeFS.  Save the YAML below as <code>animals.yaml</code> and upload it to lakeFS. </p> <pre><code>lakectl fs upload lakefs://catalogs/main/_lakefs_tables/animals.yaml -s ./animals.yaml &amp;&amp; \\\nlakectl commit lakefs://catalogs/main -m \"added table\"\n</code></pre> <pre><code>name: animals\ntype: hive\n# data location root in lakeFS\npath: tables/animals\n# partitions order\npartition_columns: ['type', 'weight']\nschema:\n  type: struct\n  # all the columns spec\n  fields:\n    - name: type\n      type: string\n      nullable: true\n      metadata:\n        comment: axolotl, cat, dog, fish etc\n    - name: weight\n      type: integer\n      nullable: false\n      metadata: {}\n    - name: name\n      type: string\n      nullable: false\n      metadata: {}\n</code></pre>"},{"location":"integrations/glue_metastore/#write-some-table-data","title":"Write some table data","text":"<p>Insert data into the table path, using your preferred method (e.g. Spark), and commit upon completion. This example uses CSV files, and the files added to lakeFS should look like this:</p> <p></p>"},{"location":"integrations/glue_metastore/#the-exporter-script","title":"The exporter script","text":"<p>Upload the following script to your main branch under <code>scripts/animals_exporter.lua</code> (or a path of your choice).</p> <p>Note</p> <p>For code references check symlink_exporter and glue_exporter docs.</p> <pre><code>local aws = require(\"aws\")\nlocal symlink_exporter = require(\"lakefs/catalogexport/symlink_exporter\")\nlocal glue_exporter = require(\"lakefs/catalogexport/glue_exporter\")\n-- settings \nlocal access_key = args.aws.aws_access_key_id\nlocal secret_key = args.aws.aws_secret_access_key\nlocal region = args.aws.aws_region\nlocal table_path = args.table_source -- table descriptor \nlocal db = args.catalog.db_name -- glue db\nlocal table_input = args.catalog.table_input -- table input (AWS input spec) for Glue\n-- export symlinks \nlocal s3 = aws.s3_client(access_key, secret_key, region)\nlocal result = symlink_exporter.export_s3(s3, table_path, action, {debug=true})\n-- register glue table\nlocal glue = aws.glue_client(access_key, secret_key, region)\nlocal res = glue_exporter.export_glue(glue, db, table_path, table_input, action, {debug=true})\n</code></pre>"},{"location":"integrations/glue_metastore/#configure-action-hooks","title":"Configure Action Hooks","text":"<p>Hooks serve as the mechanism that triggers the execution of the exporter. For more detailed information on how to configure exporter hooks, you can refer to Running an Exporter.</p> <p>Info</p> <p>The <code>args.catalog.table_input</code> argument in the Lua script is assumed to be passed from the action arguments, that way the same script can be reused for different tables. </p> <p>heck the example to construct the table input in the lua code.</p> Hook CSV Glue TableHook Parquet Glue TableMultiple Hooks / Inline script <p>Upload to <code>_lakefs_actions/animals_glue.yaml</code>: </p> <pre><code>name: Glue Exporter\non:\n  post-commit:\n    branches: [\"main\"]\nhooks:\n  - id: animals_table_glue_exporter\n    type: lua\n    properties:\n      script_path: \"scripts/animals_exporter.lua\"\n      args:\n        aws:\n          aws_access_key_id: \"&lt;AWS_ACCESS_KEY_ID&gt;\"\n          aws_secret_access_key: \"&lt;AWS_SECRET_ACCESS_KEY&gt;\"\n          aws_region: \"&lt;AWS_REGION&gt;\"\n        table_source: '_lakefs_tables/animals.yaml'\n        catalog:\n          db_name: \"my-glue-db\"\n          table_input:\n            StorageDescriptor: \n              InputFormat: \"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat\"\n              OutputFormat: \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\"\n              SerdeInfo:\n                SerializationLibrary: \"org.apache.hadoop.hive.serde2.OpenCSVSerde\"\n                Parameters:\n                  separatorChar: \",\"\n            Parameters: \n              classification: \"csv\"\n              \"skip.header.line.count\": \"1\"\n</code></pre> <p>When working with Parquet files, upload the following to <code>_lakefs_actions/animals_glue.yaml</code>:</p> <pre><code>name: Glue Exporter\non:\npost-commit:\n    branches: [\"main\"]\nhooks:\n- id: animals_table_glue_exporter\n    type: lua\n    properties:\n        script_path: \"scripts/animals_exporter.lua\"\n        args:\n        aws:\n            aws_access_key_id: \"&lt;AWS_ACCESS_KEY_ID&gt;\"\n            aws_secret_access_key: \"&lt;AWS_SECRET_ACCESS_KEY&gt;\"\n            aws_region: \"&lt;AWS_REGION&gt;\"\n        table_source: '_lakefs_tables/animals.yaml'\n        catalog:\n            db_name: \"my-glue-db\"\n            table_input:\n                StorageDescriptor:\n                InputFormat: \"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat\"\n                OutputFormat: \"org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat\"\n                SerdeInfo:\n                    SerializationLibrary: \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\"\n                Parameters:\n                classification: \"parquet\"\n                EXTERNAL: \"TRUE\"\n                \"parquet.compression\": \"SNAPPY\"\n</code></pre> <p>The following example demonstrates how to separate the symlink and glue exporter into building blocks running in separate hooks. It also shows how to run the lua script inline instead of a file, depending on user preference.</p> <pre><code>name: Animal Table Exporter\non:\npost-commit:\n    branches: [\"main\"]\nhooks:\n- id: symlink_exporter\n    type: lua\n    properties:\n    args:\n        aws:\n        aws_access_key_id: \"&lt;AWS_ACCESS_KEY_ID&gt;\"\n        aws_secret_access_key: \"&lt;AWS_SECRET_ACCESS_KEY&gt;\"\n        aws_region: \"&lt;AWS_REGION&gt;\"\n        table_source: '_lakefs_tables/animals.yaml'\n    script: |\n        local exporter = require(\"lakefs/catalogexport/symlink_exporter\")\n        local aws = require(\"aws\")\n        local table_path = args.table_source\n        local s3 = aws.s3_client(args.aws.aws_access_key_id, args.aws.aws_secret_access_key, args.aws.aws_region)\n        exporter.export_s3(s3, table_path, action, {debug=true})\n- id: glue_exporter\n    type: lua\n    properties:\n    args:\n        aws:\n        aws_access_key_id: \"&lt;AWS_ACCESS_KEY_ID&gt;\"\n        aws_secret_access_key: \"&lt;AWS_SECRET_ACCESS_KEY&gt;\"\n        aws_region: \"&lt;AWS_REGION&gt;\"\n        table_source: '_lakefs_tables/animals.yaml'\n        catalog:\n        db_name: \"my-glue-db\"\n        table_input: # add glue table input here \n    script: |\n        local aws = require(\"aws\")\n        local exporter = require(\"lakefs/catalogexport/glue_exporter\")\n        local glue = aws.glue_client(args.aws.aws_access_key_id, args.aws.aws_secret_access_key, args.aws.aws_region)\n        exporter.export_glue(glue, args.catalog.db_name, args.table_source, args.catalog.table_input, action, {debug=true})  \n</code></pre> <p>Adding the script and the action files to the repository and commit it. This is a post-commit action, meaning it will be executed after the commit operation has taken place. </p> <pre><code>lakectl fs upload lakefs://catalogs/main/scripts/animals_exporter.lua -s ./animals_exporter.lua\nlakectl fs upload lakefs://catalogs/main/_lakefs_actions/animals_glue.yaml -s ./animals_glue.yaml\nlakectl commit lakefs://catalogs/main -m \"trigger first export hook\"\n</code></pre> <p>Once the action has completed its execution, you can review the results in the action logs.</p> <p></p>"},{"location":"integrations/glue_metastore/#use-athena","title":"Use Athena","text":"<p>We can use the exported Glue table with any tool that supports Glue Catalog (or Hive compatible) such as Athena, Trino, Spark and others. To use Athena we can simply run <code>MSCK REPAIR TABLE</code> and then query the tables.</p> <p>In Athena, make sure that the correct database (<code>my-glue-db</code> in the example above) is configured, then run: </p> <pre><code>MSCK REPAIR TABLE `animals_catalogs_main_9255e5`; -- load partitions for the first time \nSELECT * FROM `animals_catalogs_main_9255e5` limit 50;\n</code></pre> <p></p>"},{"location":"integrations/glue_metastore/#cleanup","title":"Cleanup","text":"<p>Users can use additional hooks / actions to implement a custom cleanup logic to delete the symlink in S3 and Glue Tables. </p> <pre><code>glue.delete_table(db, '&lt;glue table name&gt;')\ns3.delete_recursive('bucket', 'path/to/symlinks/of/a/commit/')\n</code></pre>"},{"location":"integrations/hive/","title":"Using lakeFS with Apache Hive","text":"<p>The Apache Hive \u2122 data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive.</p>"},{"location":"integrations/hive/#configuration","title":"Configuration","text":"<p>To configure Hive to work with lakeFS, you need to set the lakeFS credentials in the corresponding S3 credential fields.</p> <p>lakeFS endpoint: <code>fs.s3a.endpoint</code> </p> <p>lakeFS access key: <code>fs.s3a.access.key</code></p> <p>lakeFS secret key: <code>fs.s3a.secret.key</code></p> <p>Note</p> <p>In the following examples, we set AWS credentials at runtime for clarity. In production, these properties should be set using one of Hadoop's standard ways of Authenticating with S3. </p> <p>For example, you can add the configurations to the file <code>hdfs-site.xml</code>:</p> <pre><code>&lt;configuration&gt;\n    ...\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.secret.key&lt;/name&gt;\n        &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.access.key&lt;/name&gt;\n        &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.endpoint&lt;/name&gt;\n        &lt;value&gt;https://lakefs.example.com&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n       &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt;\n       &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Info</p> <p>In this example, we set <code>fs.s3a.path.style.access</code> to true to remove the need for additional DNS records for virtual hosting <code>fs.s3a.path.style.access</code> that was introduced in Hadoop 2.8.0</p>"},{"location":"integrations/hive/#examples","title":"Examples","text":""},{"location":"integrations/hive/#example-with-schema","title":"Example with schema","text":"<pre><code>CREATE  SCHEMA example LOCATION 's3a://example/main/' ;\nCREATE TABLE example.request_logs (\n    request_time timestamp,\n    url string,\n    ip string,\n    user_agent string\n);\n</code></pre>"},{"location":"integrations/hive/#example-with-an-external-table","title":"Example with an external table","text":"<pre><code>CREATE EXTERNAL TABLE request_logs (\n    request_time timestamp,\n    url string,\n    ip string,\n    user_agent string\n) LOCATION 's3a://example/main/request_logs' ;\n</code></pre>"},{"location":"integrations/huggingface_datasets/","title":"Versioning HuggingFace Datasets with lakeFS","text":"<p>HuggingFace \ud83e\udd17 Datasets is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks.</p> <p>\ud83e\udd17 Datasets supports access to cloud storage providers through fsspec FileSystem implementations.</p> <p>lakefs-spec is a community implementation of an fsspec Filesystem that fully leverages lakeFS' capabilities. Let's start by installing it:</p>"},{"location":"integrations/huggingface_datasets/#installation","title":"Installation","text":"<pre><code>pip install lakefs-spec\n</code></pre>"},{"location":"integrations/huggingface_datasets/#configuration","title":"Configuration","text":"<p>If you've already configured the lakeFS python SDK and/or lakectl, you should have a <code>$HOME/.lakectl.yaml</code> file that contains your access credentials and endpoint for your lakeFS environment.</p> <p>Otherwise, install <code>lakectl</code> and run <code>lakectl config</code> to set up your access credentials.</p>"},{"location":"integrations/huggingface_datasets/#reading-a-dataset","title":"Reading a Dataset","text":"<p>To read a dataset, all we have to do is use a <code>lakefs://...</code> URI when calling <code>load_dataset</code>:</p> <pre><code>&gt;&gt;&gt; from datasets import load_dataset\n&gt;&gt;&gt; \n&gt;&gt;&gt; dataset = load_dataset('csv', data_files='lakefs://example-repository/my-branch/data/example.csv')\n</code></pre> <p>That's it! this should automatically load the lakefs-spec implementation that we've installed, which will use the <code>$HOME/.lakectl.yaml</code> file to read its credentials, so we don't need to pass additional configuration.</p>"},{"location":"integrations/huggingface_datasets/#savingloading","title":"Saving/Loading","text":"<p>Once we've loaded a Dataset, we can save it using the <code>save_to_disk</code> method as normal:</p> <pre><code>&gt;&gt;&gt; dataset.save_to_disk('lakefs://example-repository/my-branch/datasets/example/')\n</code></pre> <p>At this point, we might want to commit that change to lakeFS, and tag it, so we could share it with our colleagues.</p> <p>We can do it through the UI or lakectl, but let's do it with the lakeFS Python SDK:</p> <pre><code>&gt;&gt;&gt; import lakefs\n&gt;&gt;&gt;\n&gt;&gt;&gt; repo = lakefs.repository('example-repository')\n&gt;&gt;&gt; commit = repo.branch('my-branch').commit(\n...     'saved my first huggingface Dataset!',\n...     metadata={'using': '\ud83e\udd17'})\n&gt;&gt;&gt; repo.tag('alice_experiment1').create(commit)\n</code></pre> <p>Now, others on our team can load our exact dataset by using the tag we created:</p> <pre><code>&gt;&gt;&gt; from datasets import load_from_disk\n&gt;&gt;&gt;\n&gt;&gt;&gt; dataset = load_from_disk('lakefs://example-repository/alice_experiment1/datasets/example/')\n</code></pre>"},{"location":"integrations/iceberg/","title":"Using lakeFS with Apache Iceberg","text":""},{"location":"integrations/iceberg/#lakefs-iceberg-rest-catalog","title":"lakeFS Iceberg REST Catalog","text":"<p>Info</p> <p>Available in lakeFS Enterprise</p> <p>Tip</p> <p>lakeFS Iceberg REST Catalog is currently in private preview for lakeFS Enterprise customers. Contact us to get started!</p>"},{"location":"integrations/iceberg/#what-is-lakefs-iceberg-rest-catalog","title":"What is lakeFS Iceberg REST Catalog?","text":"<p>lakeFS Iceberg REST Catalog allow you to use lakeFS as a spec-compliant Apache Iceberg REST catalog,  allowing Iceberg clients to manage and access tables using a standard REST API. </p> <p></p> <p>Using lakeFS Iceberg REST Catalog, you can use lakeFS a drop-in replacement for other Iceberg catalogs like AWS Glue, Nessie, Hive Metastore - or the lakeFS HadoopCatalog (see below)</p> <p>With lakeFS Iceberg REST Catalog, you can:</p> <ul> <li>Manage Iceberg tables with full version control capabilities.</li> <li>Use standard Iceberg clients and tools without modification.</li> <li>Leverage lakeFS's branching and merging features for managing table's lifecycle.</li> <li>Maintain data consistency across different environments.</li> </ul>"},{"location":"integrations/iceberg/#use-cases","title":"Use Cases","text":"<ol> <li>Version-Controlled Data Development:<ul> <li>Create feature branches for table schema changes or data migrations</li> <li>Test modifications in isolation, across multiple tables</li> <li>Merge changes safely with conflict detection</li> </ul> </li> <li>Multi-Environment Management:<ul> <li>Use branches to represent different environments (dev, staging, prod)</li> <li>Promote changes between environments through merges, with automated testing</li> <li>Maintain consistent table schemas across environments</li> </ul> </li> <li>Collaborative Data Development:<ul> <li>Multiple teams can work on different table features simultaneously</li> <li>Maintain data quality through pre-merge validations</li> <li>Collaborate using pull requests on changes to data and schema</li> </ul> </li> <li>Manage and Govern Access to data:<ul> <li>Use the detailed built-in commit log capturing who, what and how data is changed</li> <li>Manage access using fine grained access control to users and groups using RBAC policies</li> <li>Rollback changes atomically and safely to reduce time-to-recover and increase system stability</li> </ul> </li> </ol>"},{"location":"integrations/iceberg/#configuration","title":"Configuration","text":"<p>The Iceberg REST catalog API is exposed at <code>/iceberg/api</code> in your lakeFS server. </p> <p>To use it:</p> <ol> <li>Enable the feature (contact us for details).</li> <li>Configure your Iceberg clients to use the lakeFS REST catalog endpoint.</li> <li>Use your lakeFS access key and secret for authentication.</li> </ol>"},{"location":"integrations/iceberg/#catalog-initialization-example-using-pyiceberg","title":"Catalog Initialization Example (using <code>pyiceberg</code>)","text":"<pre><code>from pyiceberg.catalog.rest import RestCatalog\n\ncatalog = RestCatalog(name = \"my_catalog\", **{\n    'prefix': 'lakefs',\n    'uri': f'{lakefs_endpoint}/iceberg/api',\n    'oauth2-server-uri': f'{lakefs_endpoint}/iceberg/api/v1/oauth/tokens',\n    'credential': f'{lakefs_client_key}:{lakefs_client_secret}',\n})\n</code></pre>"},{"location":"integrations/iceberg/#example-client-code","title":"Example Client code","text":"Python (PyIceberg)TrinoSpark <pre><code>import lakefs\nfrom pyiceberg.catalog import load_catalog\n\n# Initialize the catalog\ncatalog = RestCatalog(name = \"my_catalog\", **{\n    'prefix': 'lakefs',\n    'uri': 'https://lakefs.example.com/iceberg/api',\n    'oauth2-server-uri': 'https://lakefs.example.com/iceberg/api/iceberg/api/v1/oauth/tokens',\n    'credential': f'AKIAlakefs12345EXAMPLE:abc/lakefs/1234567bPxRfiCYEXAMPLEKEY',\n})\n\n# List namespaces in a branch\ncatalog.list_namespaces(('repo', 'main'))\n\n# Query a table\ncatalog.list_tables('repo.main.inventory')\ntable = catalog.load_table('repo.main.inventory.books')\narrow_df = table.scan().to_arrow()\n</code></pre> <pre><code>-- List tables in the iceberg catalog\nUSE \"repo.main.inventory\"; -- &lt;repository&gt;.&lt;branch or reference&gt;.&lt;namespace&gt;\nSHOW TABLES;\n\n-- Query a table\nSELECT * FROM books LIMIT 100;\n\n-- Switch to a different branch\nUSE \"repo.new_branch.inventory\";\nSELECT * FROM books;\n</code></pre> <pre><code>// Configure Spark to use the lakeFS REST catalog\nspark.sql(\"USE my_repo.main.inventory\")\n\n// List available tables\nspark.sql(\"SHOW TABLES\").show()\n\n// Query data with branch isolation\nspark.sql(\"SELECT * FROM books\").show()\n\n// Switch to a feature branch\nspark.sql(\"USE my_repo.new_branch.inventory\")\nspark.sql(\"SELECT * FROM books\").show()\n</code></pre>"},{"location":"integrations/iceberg/#namespaces-and-tables","title":"Namespaces and Tables","text":""},{"location":"integrations/iceberg/#namespace-operations","title":"Namespace Operations","text":"<p>The Iceberg Catalog supports Iceberg namespace operations:</p> <ul> <li>Create namespaces</li> <li>List namespaces</li> <li>Drop namespaces</li> <li>List tables within namespaces</li> </ul>"},{"location":"integrations/iceberg/#namespace-usage","title":"Namespace Usage","text":"<p>Namespaces in the Iceberg Catalog follow the pattern <code>\"&lt;repository&gt;.&lt;branch&gt;.&lt;namespace&gt;(.&lt;namespace&gt;...)\"</code> where:</p> <ul> <li><code>&lt;repository&gt;</code> must be a valid lakeFS repository name.</li> <li><code>&lt;branch&gt;</code> must be a valid lakeFS branch name.</li> <li><code>&lt;namespace&gt;</code> components can be nested using unit separator (e.g., <code>inventory.books</code>).</li> </ul> <p>Examples:</p> <ul> <li><code>my-repo.main.inventory</code></li> <li><code>my-repo.feature-branch.inventory.books</code></li> </ul> <p>The repository and branch components must already exist in lakeFS before using them in the Iceberg catalog.</p>"},{"location":"integrations/iceberg/#namespace-restrictions","title":"Namespace Restrictions","text":"<ul> <li>Repository and branch names must follow lakeFS naming conventions.</li> <li>Namespace components cannot contain special characters except dots (.) for nesting.</li> <li>The total namespace path length must be less than 255 characters.</li> <li>Namespaces are case-sensitive.</li> <li>Empty namespace components are not allowed.</li> </ul>"},{"location":"integrations/iceberg/#table-operations","title":"Table Operations","text":"<p>The Iceberg Catalog supports all standard Iceberg table operations:</p> <ul> <li>Create tables with schemas and partitioning.</li> <li>Update table schemas and partitioning.</li> <li>Commit changes to tables.</li> <li>Delete tables.</li> <li>List tables in namespaces.</li> </ul>"},{"location":"integrations/iceberg/#version-control-features","title":"Version Control Features","text":"<p>The Iceberg Catalog integrates with lakeFS's version control system, treating each table change as a commit.  This provides a complete history of table modifications and enables branching and merging workflows.</p>"},{"location":"integrations/iceberg/#catalog-changes-as-commits","title":"Catalog Changes as Commits","text":"<p>Each modification to a table (schema changes, data updates, etc.) creates a new commit in lakeFS.  Creating or deleting a namespace or a table results in a lakeFS commit on the relevant branch, as well as table data updates (\"Iceberg table commit\").</p>"},{"location":"integrations/iceberg/#branching-and-merging","title":"Branching and Merging","text":"<p>Create a new branch to work on table changes:</p> <pre><code># Create a lakeFS branch using lakeFS Python SDK\nbranch = lakefs.repository('repo').branch('new_branch').create(source_reference='main')\n\n# The table is now accessible in the new branch\nnew_table = catalog.load_table(f'repo.{branch.id}.inventory.books')\n</code></pre> <p>Merge changes between branches:</p> <pre><code># Merge the branch using lakeFS Python SDK\nbranch.merge_into('main')\n\n# Changes are now visible in main\nmain_table = catalog.load_table('repo.main.inventory.books')\n</code></pre> <p>Info</p> <p>Currently, lakeFS handles table changes as file operations during merges.</p> <p>This means that when merging branches with table changes, lakeFS treats the table metadata files as regular files.</p> <p>No special merge logic is applied to handle conflicting table changes, and if there are conflicting changes to the same table in different branches,  the merge will fail with a conflict that needs to be resolved manually.</p>"},{"location":"integrations/iceberg/#authentication","title":"Authentication","text":"<p>lakeFS provides an OAuth2 token endpoint at <code>/catalog/iceberg/v1/oauth/tokens</code> that clients need to configure.  To authenticate, clients must provide their lakeFS access key and secret in the format <code>access_key:secret</code> as the credential.</p> <p>The authorization requirements are managed at the lakeFS level, meaning:</p> <ul> <li>Users need appropriate lakeFS permissions to access repositories and branches</li> <li>Table operations require lakeFS permissions on the underlying objects</li> <li>The same lakeFS RBAC policies apply to Iceberg catalog operations</li> </ul>"},{"location":"integrations/iceberg/#limitations","title":"Limitations","text":"<ol> <li>Table Maintenance:<ul> <li>See Table Maintenance section for details</li> </ul> </li> <li>Advanced Features:<ul> <li>Views (all view operations are unsupported)</li> <li>Transactional DML (<code>stage-create</code>)</li> <li>Server-side query planning</li> <li>Table renaming</li> <li>Updating table's location (using Commit)</li> <li>Table statistics (<code>set-statistics</code> and <code>remove-statistics</code> operations are currently a no-op)</li> </ul> </li> <li>lakeFS Iceberg REST Catalog is currently tested to work with Amazon S3 and Google Cloud Storage. Other storage backends, such as Azure or Local storage are currently not supported, but will be in future releases.</li> <li>Currently only Iceberg <code>v2</code> table format is supported</li> </ol>"},{"location":"integrations/iceberg/#table-maintenance","title":"Table Maintenance","text":"<p>The following table maintenance operations are not supported in the current version:</p> <ul> <li>Drop table with purge</li> <li>Compact data files</li> <li>Rewrite manifests</li> <li>Expire snapshots</li> <li>Remove old metadata files</li> <li>Delete orphan files</li> </ul> <p>Danger</p> <p>To prevent data loss, clients should disable their own cleanup operations by:</p> <ul> <li>Disabling orphan file deletion.</li> <li>Setting <code>remove-dangling-deletes</code> to false when rewriting.</li> <li>Disabling snapshot expiration.</li> <li>Setting a very high value for <code>min-snapshots-to-keep</code> parameter.</li> </ul>"},{"location":"integrations/iceberg/#roadmap","title":"Roadmap","text":"<p>The following features are planned for future releases:</p> <ol> <li>Catalog Sync:<ul> <li>Support for pushing/pulling tables to/from other catalogs</li> <li>Integration with AWS Glue and other Iceberg-compatible catalogs</li> </ul> </li> <li>Table Import:<ul> <li>Support for importing existing Iceberg tables from other catalogs</li> <li>Bulk import capabilities for large-scale migrations</li> </ul> </li> <li>Azure Storage Support</li> <li>Advanced Features:<ul> <li>Views API support</li> <li>Table transactions</li> </ul> </li> <li>Advanced versioning capabilities<ul> <li>merge non-conflicting table updates</li> </ul> </li> </ol>"},{"location":"integrations/iceberg/#how-it-works","title":"How it works","text":"<p>Under the hood, the lakeFS Iceberg REST Catalog keeps track of each table's metadata file. This is typically referred to as the table pointer.</p> <p>This pointer is stored inside the repository's storage namespace. </p> <p>When a request is made, the catalog would examine the table's fully qualified namespace: <code>&lt;repository&gt;.&lt;reference&gt;.&lt;namespace&gt;.&lt;table_name&gt;</code> to read that special pointer file from the given reference specified, and returns the underlying object store location of the metadata file to the client. When a table is created or updated, lakeFS would make sure to generate a new metadata file inside the storage namespace, and register that metadata file as the current pointer for the requested branch.</p> <p>This approach leverages Iceberg's existing metadata and the immutability of its snapshots: a commit in lakeFS captures a metadata file, which in turn captures manifest lists, manifest files and all related data files.</p> <p>Besides simply avoiding \"double booking\" where both Iceberg and lakeFS would need to keep track of which files belong to which version, it also greatly improves the scalability and compatibility of the catalog with the existing Iceberg tool ecosystem.</p>"},{"location":"integrations/iceberg/#example-reading-an-iceberg-table","title":"Example: Reading an Iceberg Table","text":"<p>Here's a simplified example of what reading from an Iceberg table would look like:</p> <pre><code>sequenceDiagram\n    Actor Iceberg Client\n    participant lakeFS Catalog API\n    participant lakeFS\n    participant Object Store\n\n    Iceberg Client-&gt;&gt;lakeFS Catalog API: get table metadata(\"repo.branch.table\")\n    lakeFS Catalog API-&gt;&gt;lakeFS: get('repo', 'branch', 'table')\n    lakeFS-&gt;&gt;lakeFS Catalog API: physical_address\n    lakeFS Catalog API-&gt;&gt;Iceberg Client: object location (\"s3://.../metadata.json\")\n    Iceberg Client-&gt;&gt;Object Store: GetObject\n    Object Store-&gt;&gt;Iceberg Client: table data</code></pre>"},{"location":"integrations/iceberg/#example-writing-an-iceberg-table","title":"Example: Writing an Iceberg Table","text":"<p>Here's a simplified example of what writing to an Iceberg table would look like:</p> <pre><code>sequenceDiagram\n    Actor Iceberg Client\n    participant lakeFS Catalog API\n    participant lakeFS\n    participant Object Store\n    Iceberg Client-&gt;&gt;Object Store: table data\n    Iceberg Client-&gt;&gt;lakeFS Catalog API: commit\n    lakeFS Catalog API-&gt;&gt;lakeFS: create branch(\"branch-tx-uuid\")\n    lakeFS Catalog API-&gt;&gt;lakeFS: put('new table pointer')\n    lakeFS-&gt;&gt;Object Store: PutObject(\"metdata.json\")\n    lakeFS Catalog API-&gt;&gt;lakeFS: merge('branch-tx-uuid', 'branch)\n    lakeFS Catalog API-&gt;&gt;Iceberg Client: done</code></pre>"},{"location":"integrations/iceberg/#related-resources","title":"Related Resources","text":"<p>Further Reading</p> <ul> <li>Iceberg REST Catalog API Specification</li> <li>Iceberg Official Documentation</li> <li>lakeFS Enterprise Features</li> </ul>"},{"location":"integrations/iceberg/#deprecated-iceberg-hadoopcatalog","title":"Deprecated: Iceberg HadoopCatalog","text":"<p>Warning</p> <p><code>HadoopCatalog</code> and other filesystem-based catalogs are currently not recommended by the Apache Iceberg community and come with several limitations around concurrency and tooling.</p> <p>As such, the <code>HadoopCatalog</code> described in this section is now deprecated and will not receive further updates</p> Setup MavenPySpark <p>Use the following Maven dependency to install the lakeFS custom catalog:</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.lakefs&lt;/groupId&gt;\n&lt;artifactId&gt;lakefs-iceberg&lt;/artifactId&gt;\n&lt;version&gt;0.1.4&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Include the <code>lakefs-iceberg</code> jar in your package list along with Iceberg. For example: </p> <pre><code>.config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.3.0,io.lakefs:lakefs-iceberg:0.1.4\")\n</code></pre> Configuration PySparkSpark Shell <p>Set up the Spark SQL catalog:  <pre><code>.config(\"spark.sql.catalog.lakefs\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n.config(\"spark.sql.catalog.lakefs.catalog-impl\", \"io.lakefs.iceberg.LakeFSCatalog\") \\\n.config(\"spark.sql.catalog.lakefs.warehouse\", f\"lakefs://{repo_name}\") \\ \n.config(\"spark.sql.catalog.lakefs.cache-enabled\", \"false\")\n</code></pre></p> <p>Configure the S3A Hadoop FileSystem with your lakeFS connection details. Note that these are your lakeFS endpoint and credentials, not your S3 ones.</p> <pre><code>.config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n.config(\"spark.hadoop.fs.s3a.endpoint\", \"https://example-org.us-east-1.lakefscloud.io\") \\\n.config(\"spark.hadoop.fs.s3a.access.key\", \"AKIAIO5FODNN7EXAMPLE\") \\\n.config(\"spark.hadoop.fs.s3a.secret.key\", \"wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY\") \\\n.config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n</code></pre> <pre><code>spark-shell --conf spark.sql.catalog.lakefs=\"org.apache.iceberg.spark.SparkCatalog\" \\\n    --conf spark.sql.catalog.lakefs.catalog-impl=\"io.lakefs.iceberg.LakeFSCatalog\" \\\n    --conf spark.sql.catalog.lakefs.warehouse=\"lakefs://example-repo\" \\\n    --conf spark.sql.catalog.lakefs.cache-enabled=\"false\" \\\n    --conf spark.hadoop.fs.s3.impl=\"org.apache.hadoop.fs.s3a.S3AFileSystem\" \\\n    --conf spark.hadoop.fs.s3a.endpoint=\"https://example-org.us-east-1.lakefscloud.io\" \\\n    --conf spark.hadoop.fs.s3a.access.key=\"AKIAIO5FODNN7EXAMPLE\" \\\n    --conf spark.hadoop.fs.s3a.secret.key=\"wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY\" \\\n    --conf spark.hadoop.fs.s3a.path.style.access=\"true\"\n</code></pre> Using Iceberg tables with HadoopCatalog Create a table <p>To create a table on your main branch, use the following syntax:</p> <pre><code>CREATE TABLE lakefs.main.db1.table1 (id int, data string);\n</code></pre> Insert data into the table <pre><code>INSERT INTO lakefs.main.db1.table1 VALUES (1, 'data1');\nINSERT INTO lakefs.main.db1.table1 VALUES (2, 'data2');\n</code></pre> Create a branch <p>We can now commit the creation of the table to the main branch:</p> <pre><code>lakectl commit lakefs://example-repo/main -m \"my first iceberg commit\"\n</code></pre> <p>Then, create a branch:</p> <pre><code>lakectl branch create lakefs://example-repo/dev -s lakefs://example-repo/main\n</code></pre> Make changes on the branch <p>We can now make changes on the branch:</p> <pre><code>INSERT INTO lakefs.dev.db1.table1 VALUES (3, 'data3');\n</code></pre> Query the table <p>If we query the table on the branch, we will see the data we inserted:</p> <pre><code>SELECT * FROM lakefs.dev.db1.table1;\n</code></pre> <p>Results in:</p> <pre><code>+----+------+\n| id | data |\n+----+------+\n| 1  | data1|\n| 2  | data2|\n| 3  | data3|\n+----+------+\n</code></pre> <p>However, if we query the table on the main branch, we will not see the new changes:</p> <pre><code>SELECT * FROM lakefs.main.db1.table1;\n</code></pre> <p>Results in:</p> <pre><code>+----+------+\n| id | data |\n+----+------+\n| 1  | data1|\n| 2  | data2|\n+----+------+\n</code></pre> Migrating an existing Iceberg table to the Hadoop Catalog <p>This is done through an incremental copy from the original table into lakeFS. </p> <ol> <li>Create a new lakeFS repository <code>lakectl repo create lakefs://example-repo &lt;base storage path&gt;</code></li> <li> <p>Initiate a spark session that can interact with the source iceberg table and the target lakeFS catalog.      Here's an example of Hadoop and S3 session and lakeFS catalog with per-bucket config: </p> <p><pre><code>SparkConf conf = new SparkConf();\nconf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\");\n\n// set hadoop on S3 config (source tables we want to copy) for spark\nconf.set(\"spark.sql.catalog.hadoop_prod\", \"org.apache.iceberg.spark.SparkCatalog\");\nconf.set(\"spark.sql.catalog.hadoop_prod.type\", \"hadoop\");\nconf.set(\"spark.sql.catalog.hadoop_prod.warehouse\", \"s3a://my-bucket/warehouse/hadoop/\");\nconf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\");\nconf.set(\"spark.hadoop.fs.s3a.bucket.my-bucket.access.key\", \"&lt;AWS_ACCESS_KEY&gt;\");\nconf.set(\"spark.hadoop.fs.s3a.bucket.my-bucket.secret.key\", \"&lt;AWS_SECRET_KEY&gt;\");\n\n// set lakeFS config (target catalog and repository)\nconf.set(\"spark.sql.catalog.lakefs\", \"org.apache.iceberg.spark.SparkCatalog\");\nconf.set(\"spark.sql.catalog.lakefs.catalog-impl\", \"io.lakefs.iceberg.LakeFSCatalog\");\nconf.set(\"spark.sql.catalog.lakefs.warehouse\", \"lakefs://example-repo\");\nconf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\");\nconf.set(\"spark.hadoop.fs.s3a.bucket.example-repo.access.key\", \"&lt;LAKEFS_ACCESS_KEY&gt;\");\nconf.set(\"spark.hadoop.fs.s3a.bucket.example-repo.secret.key\", \"&lt;LAKEFS_SECRET_KEY&gt;\");\nconf.set(\"spark.hadoop.fs.s3a.bucket.example-repo.endpoint\"  , \"&lt;LAKEFS_ENDPOINT&gt;\");\n</code></pre> 3. Create Schema in lakeFS and copy the data</p> <p>Example of copy with spark-sql: </p> <pre><code>-- Create Iceberg Schema in lakeFS\nCREATE SCHEMA IF NOT EXISTS &lt;lakefs-catalog&gt;.&lt;branch&gt;.&lt;db&gt;\n-- Create new iceberg table in lakeFS from the source table (pre-lakeFS)\nCREATE TABLE IF NOT EXISTS &lt;lakefs-catalog&gt;.&lt;branch&gt;.&lt;db&gt; USING iceberg AS SELECT * FROM &lt;iceberg-original-table&gt;\n</code></pre> </li> </ol>"},{"location":"integrations/kafka/","title":"Using lakeFS with Apache Kafka","text":"<p>Apache Kafka provides a unified, high-throughput, low-latency platform for handling real-time data feeds.</p> <p>Different distributions of Kafka offer different methods for exporting data to S3 called Kafka Sink Connectors.</p> <p>The most commonly used Connector for S3 is Confluent's S3 Sink Connector.</p> <p>Add the following to <code>connector.properties</code> file for lakeFS support:</p> <pre><code># Your lakeFS repository\ns3.bucket.name=example-repo\n\n# Your lakeFS S3 endpoint and credentials\nstore.url=https://lakefs.example.com\naws.access.key.id=AKIAIOSFODNN7EXAMPLE\naws.secret.access.key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\n# main being the branch we want to write to\ntopics.dir=main/topics \n</code></pre> <p>Example</p> <p>For usage examples, see the lakeFS Kafka sample repo. </p>"},{"location":"integrations/kubeflow/","title":"Using lakeFS with Kubeflow pipelines","text":"<p>Kubeflow is a project dedicated to making deployments of ML workflows on Kubernetes simple, portable, and scalable. A Kubeflow pipeline is a portable and scalable definition of an ML workflow composed of steps. Each step in the pipeline is an instance of a component represented as an instance of ContainerOp.</p>"},{"location":"integrations/kubeflow/#add-pipeline-steps-for-lakefs-operations","title":"Add pipeline steps for lakeFS operations","text":"<p>To integrate lakeFS into your Kubeflow pipeline, you need to create Kubeflow components that perform lakeFS operations. Currently, there are two methods to create lakeFS <code>ContainerOps</code>:</p> <ol> <li>Implement a function-based ContainerOp that uses the lakeFS Python API to invoke lakeFS operations.</li> <li>Implement a ContainerOp that uses the <code>lakectl</code> CLI docker image to invoke lakeFS operations.</li> </ol>"},{"location":"integrations/kubeflow/#function-based-containerops","title":"Function-based ContainerOps","text":"<p>To implement a function-based component that invokes lakeFS operations, you should use the Python OpenAPI client lakeFS provides. See the example below that demonstrates how to make the client's package available to your ContainerOp.</p>"},{"location":"integrations/kubeflow/#example-operations","title":"Example operations","text":"<p>Create a new branch: A function-based ContainerOp that creates a branch called <code>example-branch</code> based on the <code>main</code> branch of <code>example-repo</code>.</p> <pre><code>from kfp import components\n\ndef create_branch(repo_name, branch_name, source_branch):\n    import lakefs\n    from lakefs.client import Client\n    client = Client(\n        host=\"https://lakefs.example.com\",\n        username=\"AKIAIOSFODNN7EXAMPLE\",\n        password=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n    )\n    lakefs.repository(repo_name, client=client).branch(branch_name).create(source_reference=source_branch)\n\n# Convert the function to a lakeFS pipeline step.\ncreate_branch_op = components.func_to_container_op(\n    func=create_branch,\n    packages_to_install=['lakefs'])\n</code></pre> <p>You can invoke any lakeFS operation supported by lakeFS OpenAPI. For example, you could implement a commit and merge function-based ContainerOps. Check out the Python documentation and the full API reference.</p>"},{"location":"integrations/kubeflow/#non-function-based-containerops","title":"Non-function-based ContainerOps","text":"<p>To implement a non-function based ContainerOp, you should use the <code>treeverse/lakectl</code> docker image. With this image, you can run lakectl commands to execute the desired lakeFS operation.</p> <p>For <code>lakectl</code> to work with Kubeflow, you will need to pass your lakeFS configurations as environment variables named:</p> <ul> <li><code>LAKECTL_CREDENTIALS_ACCESS_KEY_ID: AKIAIOSFODNN7EXAMPLE</code></li> <li><code>LAKECTL_SECRET_ACCESS_KEY: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY</code></li> <li><code>LAKECTL_SERVER_ENDPOINT_URL: https://lakefs.example.com</code></li> </ul>"},{"location":"integrations/kubeflow/#example-operations_1","title":"Example operations","text":"<ol> <li> <p>Commit changes to a branch: A ContainerOp that commits uncommitted changes to <code>example-branch</code> on <code>example-repo</code>.</p> <p><pre><code>from kubernetes.client.models import V1EnvVar\n\ndef commit_op():\n    return dsl.ContainerOp(\n    name='commit',\n    image='treeverse/lakectl',\n    arguments=['commit', 'lakefs://example-repo/example-branch', '-m', 'commit message']).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_ACCESS_KEY_ID',value='AKIAIOSFODNN7EXAMPLE')).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY',value='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')).add_env_variable(V1EnvVar(name='LAKECTL_SERVER_ENDPOINT_URL',value='https://lakefs.example.com'))\n</code></pre> 1. Merge two lakeFS branches: A ContainerOp that merges <code>example-branch</code> into the <code>main</code> branch of <code>example-repo</code>. <pre><code>def merge_op():\n    return dsl.ContainerOp(\n    name='merge',\n    image='treeverse/lakectl',\n    arguments=['merge', 'lakefs://example-repo/example-branch', 'lakefs://example-repo/main']).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_ACCESS_KEY_ID',value='AKIAIOSFODNN7EXAMPLE')).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY',value='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')).add_env_variable(V1EnvVar(name='LAKECTL_SERVER_ENDPOINT_URL',value='https://lakefs.example.com'))\n</code></pre></p> </li> </ol> <p>You can invoke any lakeFS operation supported by <code>lakectl</code> by implementing it as a ContainerOp. Check out the complete CLI reference for the list of supported operations.</p> <p>Note</p> <p>The lakeFS Kubeflow integration that uses <code>lakectl</code> is supported on lakeFS version &gt;= v0.43.0.</p>"},{"location":"integrations/kubeflow/#add-the-lakefs-steps-to-your-pipeline","title":"Add the lakeFS steps to your pipeline","text":"<p>Add the steps created in the previous step to your pipeline before compiling it.</p>"},{"location":"integrations/kubeflow/#example-pipeline","title":"Example pipeline","text":"<p>A pipeline that implements a simple ETL that has steps for branch creation and commits.</p> <pre><code>def lakectl_pipeline():\n    create_branch_task = create_branch_op('example-repo', 'example-branch', 'main') # A function-based component\n    extract_task = example_extract_op()\n    commit_task = commit_op()\n    transform_task = example_transform_op()\n    commit_task = commit_op()\n    load_task = example_load_op()\n</code></pre> <p>Info</p> <p>It's recommended to store credentials as Kubernetes secrets and pass them as environment variables to Kubeflow operations using V1EnvVarSource.</p>"},{"location":"integrations/mlflow/","title":"Using MLflow with lakeFS","text":"<p>MLflow is a comprehensive tool designed to manage the machine learning lifecycle, assisting practitioners and teams in handling the complexities of ML processes. It focuses on the full lifecycle of machine learning projects, ensuring that each phase is manageable, traceable, and reproducible.</p> <p>MLflow comprises multiple core components, and lakeFS seamlessly integrates with the MLflow Tracking component. MLflow tracking enables experiment tracking that accounts for both inputs and outputs, allowing for visualization and comparison of experiment results.</p>"},{"location":"integrations/mlflow/#benefits-of-integrating-mlflow-with-lakefs","title":"Benefits of integrating MLflow with lakeFS","text":"<p>Integrating MLflow with lakeFS offers several advantages that enhance the machine learning workflow:</p> <ol> <li>Experiment Reproducibility: By leveraging MLflow's input logging capabilities alongside lakeFS's data versioning, you can precisely track the specific dataset version used in each experiment run. This ensures that experiments remain reproducible over time, even as datasets evolve.</li> <li>Parallel Experiments with Zero Data Copy: lakeFS enables efficient branching without duplicating data. This allows for multiple experiments to be conducted in parallel, with each branch providing an isolated environment for dataset modifications. Changes in one branch do not affect others, promoting safe collaboration among team members. Once an experiment is complete, the branch can be seamlessly merged back into the main dataset, incorporating new insights.</li> </ol>"},{"location":"integrations/mlflow/#how-to-use-mlflow-with-lakefs","title":"How to use MLflow with lakeFS","text":"<p>To harness the combined capabilities of MLflow and lakeFS for safe experimentation and accurate result reproduction, consider the workflow below and review the practical examples provided on the next section.  </p>"},{"location":"integrations/mlflow/#recommended-workflow","title":"Recommended workflow","text":"<ol> <li>Create a branch for each experiment: Start each experiment by creating a dedicated lakeFS branch for it. This approach  allows you to safely make changes to your input dataset without duplicating it. You will later load data from this branch  to your MLflow experiment runs. </li> <li>Read datasets from the experiment branch: Conduct your experiments by reading data directly from the dedicated  branch. We recommend to read the dataset from the head commit of the branch to ensure precise version tracking.</li> <li>Create an MLflow Dataset pointing to lakeFS: Use MLflow's Dataset ensuring that the dataset source points to lakeFS. </li> <li>Log your input: Use MLflow's log_input  function to log the versioned dataset stored in lakeFS.    </li> <li>Commit dataset changes: Machine learning development is inherently iterative. When you make changes to your input dataset, commit them to the experiment branch in lakeFS with a meaningful commit message. During an experiment run, load the dataset version corresponding to the branch's head commit and track this reference to facilitate future result reproduction.</li> <li>Merge experiment results: After concluding your experimentation, merge the branch used for the selected experiment  run back into the main branch.</li> </ol> <p>Branch per experiment Vs. Branch per experiment run</p> <p>While it's possible to create a lakeFS branch for each experiment run, given that lakeFS branches are both quick and  cost-effective to create, it's often more efficient to create a branch per experiment. By reading directly from the head commit of the experiment branch, you can distinguish between dataset versions without creating excessive branches. This practice maintains branch hygiene within lakeFS.</p>"},{"location":"integrations/mlflow/#example-using-pandas","title":"Example: Using Pandas","text":"<pre><code>import lakefs \nimport mlflow\nimport pandas as pd\n\nrepo = lakefs.Repository(\"my-repo\")\nrepo_id = repo.id\n\nexp_branch = repo.branch(\"experiment-1\").create(source_reference=\"main\", exist_ok=True)\nbranch_id = exp_branch.id\nhead_commit_id = exp_branch.head.id\n\ntable_path = \"famous_people.csv\"\n\ndataset_source_url = f\"s3://{repo_id}/{head_commit_id}/{table_path}\"\n\n# Use Pandas to read from lakeFS, at its most updated version to which the head commit id is pointing\nraw_data = pd.read_csv(dataset_source_url, delimiter=\";\", storage_options={\n        \"key\": \"AKIAIOSFOLKFSSAMPLES\",\n        \"secret\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"client_kwargs\": {\"endpoint_url\": \"http://localhost:8000\"}\n    })\n\n# Create an instance of a PandasDataset\ndataset = mlflow.data.from_pandas(\n    raw_data, source=dataset_source_url, name=\"famous_people\"\n)\n\n# View some of the recorded Dataset information\nprint(f\"Dataset name: {dataset.name}\")\nprint(f\"Dataset source URI: {dataset.source.uri}\")\n\n# Use mlflow input logging to track the dataset versioned by lakeFS\nwith mlflow.start_run() as run:\n    mlflow.log_input(dataset, context=\"training\")\n    mlflow.set_tag(\"lakefs_repo\", repo_id)\n    mlflow.set_tag(\"lakefs_branch\", branch_id) \n    mlflow.set_tag(\"lakefs_commit\", head_commit_id) \n\n# Inspect run's dataset\nlogged_run = mlflow.get_run(run.info.run_id) # \n\n# Retrieve the Dataset object\nlogged_dataset = logged_run.inputs.dataset_inputs[0].dataset\n\n# View some of the recorded Dataset information\nprint(f\"Logged dataset name: {logged_dataset.name}\")\nprint(f\"Logged dataset source URI: {logged_dataset.source}\")\n</code></pre> <p>Output <pre><code>Dataset name: famous_people\nDataset source URI: s3://my-repo/3afddad4fef987b4919f5e82f16682c018f59ed2ff003a6a81adf72edaad23c3/fp.csv\nLogged dataset name: famous_people\nLogged dataset source URI: {\"uri\": \"s3://my-repo/3afddad4fef987b4919f5e82f16682c018f59ed2ff003a6a81adf72edaad23c3/fp.csv\"}\n</code></pre></p>"},{"location":"integrations/mlflow/#example-using-spark","title":"Example: Using Spark","text":"<p>The example below configures Spark to access lakeFS' S3-compatible API and load a Delta Lake tables to the experiment. </p> <pre><code>import lakefs \nimport mlflow\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"lakeFS / Mlflow\") \\\n    .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\ \n    .config(\"spark.hadoop.fs.s3a.endpoint\", 'http://localhost:8000') \\ \n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\ \n    .config(\"spark.hadoop.fs.s3a.access.key\", 'AKIAlakefs12345EXAMPLE') \\ \n    .config(\"spark.hadoop.fs.s3a.secret.key\", 'abc/lakefs/1234567bPxRfiCYEXAMPLEKEY') \\ \n    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\") \\ \n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\ \n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\ \n    .getOrCreate()\n\nrepo = lakefs.Repository(\"my-repo\")\nrepo_id = repo.id\n\nexp_branch = repo.branch(\"experiment-1\").create(source_reference=\"main\", exist_ok=True)\nbranch_id = exp_branch.id\nhead_commit_id = exp_branch.head.id\n\ntable_path = \"gold/train_v2/\"\n\ndataset_source_url = f\"s3://{repo_id}/{head_commit_id}/{table_path}\"\n\n# Load delta lake table from lakeFS, at its most updated version to which the head commit id is pointing\ndataset = mlflow.data.load_delta(path=dataset_source_url, name=\"boat-images\")\n\n# View some of the recorded Dataset information\nprint(f\"Dataset name: {dataset.name}\")\nprint(f\"Dataset source URI: {dataset.source.path}\")\n\n# Use mlflow input logging to track the dataset versioned by lakeFS\nwith mlflow.start_run() as run:\n    mlflow.log_input(dataset, context=\"training\")\n    mlflow.set_tag(\"lakefs_repo\", repo_id)\n    mlflow.set_tag(\"lakefs_branch\", branch_id) # Log the branch id, to have a friendly lakeFS reference to search the input dataset in\n    mlflow.set_tag(\"lakefs_commit\", head_commit_id)\n\n# Inspect run's dataset\nlogged_run = mlflow.get_run(run.info.run_id) # \n\n# Retrieve the Dataset object\nlogged_dataset = logged_run.inputs.dataset_inputs[0].dataset\n\n# View some of the recorded Dataset information\nprint(f\"Logged dataset name: {logged_dataset.name}\")\nprint(f\"Logged dataset source URI: {logged_dataset.source}\")\n</code></pre> <p>Output:</p> <pre><code>Dataset name: boat-images\nDataset source URI: s3://my-repo/3afddad4fef987b4919f5e82f16682c018f59ed2ff003a6a81adf72edaad23c3/gold/train_v2/\nLogged dataset name: boat-images\nLogged dataset source URI: {\"path\": \"s3://my-repo/3afddad4fef987b4919f5e82f16682c018f59ed2ff003a6a81adf72edaad23c3/gold/train_v2/\"}\n</code></pre>"},{"location":"integrations/mlflow/#reproduce-experiment-results","title":"Reproduce experiment results","text":"<p>To reproduce the results of a specific experiment run in MLflow, it's essential to retrieve the exact dataset and associated metadata used during that run. While the MLflow Tracking UI provides a general overview, detailed dataset information  and its source are best accessed programmatically.</p> <ol> <li> <p>Obtain the Run ID: Navigate to the MLflow UI and copy the Run ID of the experiment you're interested in.</p> <p></p> </li> <li> <p>Extract Dataset Information Using MLflow's Python SDK:</p> <pre><code>import mlflow\n\n# Inspect run's dataset and tags\nrun_id = \"c0f8fbb1b63748abaa0a6479115e272c\"\nrun = mlflow.get_run(run_id) \n\n# Retrieve the Dataset object\nlogged_dataset = run.inputs.dataset_inputs[0].dataset\n\n# View some of the recorded Dataset information\nprint(f\"Run ID: {run_id} Dataset name: {logged_dataset.name}\")\nprint(f\"Run ID: {run_id} Dataset source URI: {logged_dataset.source}\")\n\n# Retrieve run's tags \nlogged_tags = run.data.tags\nprint(f\"Run ID: {run_id} tags: {logged_tags}\")\n</code></pre> <p>Output</p> <pre><code>Run ID: c0f8fbb1b63748abaa0a6479115e272c Dataset name: boat-images\nRun ID: c0f8fbb1b63748abaa0a6479115e272c Dataset source URI: {\"path\": \"s3://my-repo/3afddad4fef987b4919f5e82f16682c018f59ed2ff003a6a81adf72edaad23c3/gold/train_v2/\"}\nRun ID: c0f8fbb1b63748abaa0a6479115e272c tags: {'lakefs_branch': 'experiment-1', 'lakefs_repo': 'my-repo', 'lakefs_commit': '3afddad4fef987b4919f5e82f16682c018f59ed2ff003a6a81adf72edaad23c3'}\n</code></pre> </li> </ol> <p>Notes</p> <ul> <li>The Dataset Source URI provides the location of the dataset at the exact version used in the run. </li> <li>Run tags, such as 'lakefs_branch' and 'lakefs_repo', offer additional context about the dataset's origin within lakeFS.</li> </ul>"},{"location":"integrations/mlflow/#compare-runs-input","title":"Compare runs input","text":"<p>To determine whether two distinct MLflow runs utilized the same input dataset, you can compare specific attributes of  their logged Dataset objects. The source attribute, which contains the versioned dataset's URI, is a common choice for  this comparison. Here's an example:</p> <pre><code>import mlflow\n\nfirst_run_id = \"4c0464d665944dc5bb90587d455948b8\"\nfirst_run = mlflow.get_run(first_run_id)\n\n# Retrieve the Dataset object\nfirst_dataset = first_run.inputs.dataset_inputs[0].dataset\nfirst_dataset_src = first_dataset.source\n\nsec_run_id = \"12b91e073a8b40df97ea8d570534de31\"\nsec_run = mlflow.get_run(sec_run_id)\n\n# Retrieve the Dataset object\nsec_dataset = sec_run.inputs.dataset_inputs[0].dataset\nsec_dataset_src = sec_dataset.source\n\nassert first_dataset_src == sec_dataset_src, \"Dataset sources are not equal.\"\n\nprint(f\"First dataset src: {first_dataset_src}\")\nprint(f\"Second dataset src: {sec_dataset_src}\")\n</code></pre> <p>Output</p> <pre><code>First dataset src: {\"uri\": \"s3://mlflow-tracking/f16682c0186a81adf72edaad23c3f59ed2ff3afddad4fef987b4919f5e82003a/gold/train_v2/\"}\nSecond dataset src: {\"uri\": \"s3://mlflow-tracking/3afddad4fef987b4919f5e82f16682c018f59ed2ff003a6a81adf72edaad23c3/gold/train_v2/\"}\n</code></pre> <p>In this example, the source attribute of each Dataset object is compared to determine if the input datasets are identical.  If they differ, you can further inspect them. With the dataset source URI in hand, you can use lakeFS to gain more insights about the changes made to your dataset:</p> <ul> <li>Inspect the lakeFS Commit ID: By examining the commit ID within the URI, you can retrieve detailed information about  the commit, including the author and the purpose of the changes.</li> <li>Use lakeFS Diff: lakeFS offers a diff function that allows you to compare different versions of your data. </li> </ul> <p>By leveraging these tools, you can effectively track and understand the evolution of your datasets across different MLflow runs.</p>"},{"location":"integrations/presto_trino/","title":"Using lakeFS with Presto/Trino","text":"<p>Presto and Trino are a distributed SQL query engines designed to query large data sets distributed over one or more heterogeneous data sources.</p> <p>Querying data in lakeFS from Presto/Trino is similar to querying data in S3 from Presto/Trino. It is done using the Presto Hive connector or Trino Hive connector.</p> <p>Credentials</p> <p>In the following examples, we set AWS credentials at runtime for clarity. In production, these properties should be set using one of Hadoop's standard ways of Authenticating with S3. </p>"},{"location":"integrations/presto_trino/#configuration","title":"Configuration","text":""},{"location":"integrations/presto_trino/#configure-the-hive-connector","title":"Configure the Hive connector","text":"<p>Create <code>/etc/catalog/hive.properties</code> with the following contents to mount the <code>hive-hadoop2</code> connector as the Hive catalog, replacing <code>example.net:9083</code> with the correct host and port for your Hive Metastore Thrift service:</p> <pre><code>connector.name=hive-hadoop2\nhive.metastore.uri=thrift://example.net:9083\n</code></pre> <p>Add the lakeFS configurations to <code>/etc/catalog/hive.properties</code> in the corresponding S3 configuration properties:</p> <pre><code>hive.s3.aws-access-key=AKIAIOSFODNN7EXAMPLE\nhive.s3.aws-secret-key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nhive.s3.endpoint=https://lakefs.example.com\nhive.s3.path-style-access=true\n</code></pre>"},{"location":"integrations/presto_trino/#configure-hive","title":"Configure Hive","text":"<p>Presto/Trino uses Hive Metastore Service (HMS) or a compatible implementation of the Hive Metastore such as AWS Glue Data Catalog to write data to S3. In case you are using Hive Metastore, you will need to configure Hive as well.</p> <p>In file <code>hive-site.xml</code> add to the configuration:</p> <pre><code>&lt;configuration&gt;\n    ...\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.access.key&lt;/name&gt;\n        &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt;&lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.secret.key&lt;/name&gt;\n        &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.endpoint&lt;/name&gt;\n        &lt;value&gt;https://lakefs.example.com&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"integrations/presto_trino/#examples","title":"Examples","text":"<p>Here are some examples based on examples from the Presto Hive connector examples and Trino Hive connector examples</p>"},{"location":"integrations/presto_trino/#example-with-schema","title":"Example with schema","text":"<p>Create a new schema named <code>main</code> that will store tables in a lakeFS repository named <code>example</code> branch: <code>master</code>:</p> <pre><code>CREATE SCHEMA main\nWITH (location = 's3a://example/main')\n</code></pre> <p>Create a new Hive table named <code>page_views</code> in the <code>web</code> schema stored using the ORC file format,  partitioned by date and country, and bucketed by user into <code>50</code> buckets (note that Hive requires the partition columns to be the last columns in the table):</p> <pre><code>CREATE TABLE main.page_views (\n  view_time timestamp,\n  user_id bigint,\n  page_url varchar,\n  ds date,\n  country varchar\n)\nWITH (\n  format = 'ORC',\n  partitioned_by = ARRAY['ds', 'country'],\n  bucketed_by = ARRAY['user_id'],\n  bucket_count = 50\n)\n</code></pre>"},{"location":"integrations/presto_trino/#example-with-external-table","title":"Example with External table","text":"<p>Create an external Hive table named <code>request_logs</code> that points at existing data in lakeFS:</p> <pre><code>CREATE TABLE main.request_logs (\n  request_time timestamp,\n  url varchar,\n  ip varchar,\n  user_agent varchar\n)\nWITH (\n  format = 'TEXTFILE',\n  external_location = 's3a://example/main/data/logs/'\n)\n</code></pre>"},{"location":"integrations/presto_trino/#example-of-copying-a-table-with-metastore-tools","title":"Example of copying a table with metastore tools:","text":"<p>Deprecated Feature</p> <p>Having heard the feedback from the community, we are planning to replace the below manual steps with an automated process. You can read more about it here.</p> <p>Copy the created table <code>page_views</code> on schema <code>main</code> to schema <code>example_branch</code> with location <code>s3a://example/example_branch/page_views/</code> </p> <pre><code>lakectl metastore copy --from-schema main --from-table page_views --to-branch example_branch \n</code></pre>"},{"location":"integrations/python/","title":"Use Python to interact with your objects on lakeFS","text":"<p>Warning</p> <p>If your project is currently using the legacy Python <code>lakefs-client</code>, please be aware that this version has been deprecated. As of release v1.44.0, it's no longer supported for new updates or features.</p> <p>High Level Python SDK</p> <p>We've just released a new High Level Python SDK library, and we're super excited to tell you about it! Continue reading to get the full story! Though our previous SDK client is still supported and maintained, we highly recommend using the new High Level SDK.</p> <p>For previous Python SDKs follow these links</p> <p>legacy-sdk (Deprecated)</p>"},{"location":"integrations/python/#references-resources","title":"References &amp; Resources","text":"<ul> <li>High Level Python SDK Documentation: https://pydocs-lakefs.lakefs.io</li> <li>Generated Python SDK Documentation: https://pydocs-sdk.lakefs.io</li> <li>Boto S3 Router: https://github.com/treeverse/boto-s3-router</li> <li>lakefs-spec API Reference: https://lakefs-spec.org/latest/reference/lakefs_spec/</li> </ul>"},{"location":"integrations/python/#python-integration-options","title":"Python Integration Options","text":"<ul> <li>Use the High Level lakeFS SDK to perform object operations, versioning and other lakeFS-specific operations.</li> <li>Use the generated lakefs-sdk for direct API access based on the OpenAPI specification of lakeFS.</li> <li>Using lakefs-spec to perform high-level file operations through a file-system-like API.</li> <li>Use Boto to perform object operations through the lakeFS S3 gateway.</li> </ul>"},{"location":"integrations/python/#using-the-lakefs-sdk","title":"Using the lakeFS SDK","text":""},{"location":"integrations/python/#installing","title":"Installing","text":"<p>Install the Python client using pip:</p> <pre><code>pip install lakefs\n</code></pre>"},{"location":"integrations/python/#initializing","title":"Initializing","text":"<p>The High Level SDK by default will try to collect authentication parameters from the environment and attempt to create a default client. When working in an environment where lakectl is configured it is not necessary to instantiate a lakeFS client or provide it for creating the lakeFS objects. In case no authentication parameters exist, it is also possible to explicitly create a lakeFS client</p> <p>Here's how to instantiate a client:</p> <p>Info</p> <p>See here for instructions on how to log in with Python using your AWS role. This is applicable for enterprise users.</p> <pre><code>from lakefs.client import Client\n\nclt = Client(\n    host=\"http://localhost:8000\",\n    username=\"AKIAIOSFODNN7EXAMPLE\",\n    password=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n)\n</code></pre> <p>You can use TLS with a CA that is not trusted on the host by configuring the client with a CA cert bundle file.  It should contain concatenated CA certificates in PEM format:</p> <pre><code>clt = Client(\n    host=\"http://localhost:8000\",\n    username=\"AKIAIOSFODNN7EXAMPLE\",\n    password=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n    # Customize the CA certificates used to verify the peer.\n    ssl_ca_cert=\"path/to/concatenated_CA_certificates.PEM\",\n)\n</code></pre> <p>For testing SSL endpoints you may wish to use a self-signed certificate.  If you do this and receive an <code>SSL: CERTIFICATE_VERIFY_FAILED</code> error message you might add the following configuration to your client:</p> <pre><code>clt = Client(\n    host=\"http://localhost:8000\",\n    username=\"AKIAIOSFODNN7EXAMPLE\",\n    password=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n    verify_ssl=False,\n)\n</code></pre> <p>{: .warning } This setting allows well-known \"man-in-the-middle\", impersonation, and credential stealing attacks.  Never use this in any production setting.</p> <p>Optionally, to enable communication via proxies, add a proxy configuration:</p> <pre><code>clt = Client(\n    host=\"http://localhost:8000\",\n    username=\"AKIAIOSFODNN7EXAMPLE\",\n    password=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n    ssl_ca_cert=\"(if needed)\",\n    proxy=\"&lt;proxy server URL&gt;\",\n)\n</code></pre>"},{"location":"integrations/python/#usage-examples","title":"Usage Examples","text":"<p>Lets see how we can interact with lakeFS using the High Level SDK.</p> Creating a repository <pre><code>import lakefs\n\nrepo = lakefs.repository(\"example-repo\").create(storage_namespace=\"s3://storage-bucket/repos/example-repo\")\nprint(repo)\n</code></pre> <p>If using an explicit client, create the Repository object and pass the client to it (note the changed syntax).</p> <pre><code>import lakefs\nfrom lakefs.client import Client\n\nclt = Client(\n    host=\"http://localhost:8000\",\n    username=\"AKIAIOSFODNN7EXAMPLE\",\n    password=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n)\n\nrepo = lakefs.Repository(\"example-repo\", client=clt).create(storage_namespace=\"s3://storage-bucket/repos/example-repo\")\nprint(repo)\n</code></pre> <p>Output</p> <pre><code>{id: 'example-repo', creation_date: 1697815536, default_branch: 'main', storage_namespace: 's3://storage-bucket/repos/example-repo'}\n</code></pre> List repositories <pre><code>import lakefs\n\nprint(\"Listing repositories:\")\nfor repo in lakefs.repositories():\n    print(repo)\n</code></pre> <p>Output</p> <pre><code>Listing repositories:\n{id: 'example-repo', creation_date: 1697815536, default_branch: 'main', storage_namespace: 's3://storage-bucket/repos/example-repo'}\n</code></pre> Creating a branch <pre><code>import lakefs\n\nbranch1 = lakefs.repository(\"example-repo\").branch(\"experiment1\").create(source_reference=\"main\")\nprint(\"experiment1 ref:\", branch1.get_commit().id)\n\nbranch2 = lakefs.repository(\"example-repo\").branch(\"experiment2\").create(source_reference=\"main\")\nprint(\"experiment2 ref:\", branch2.get_commit().id)\n</code></pre> <p>Output</p> <pre><code>experiment1 ref: 7a300b41a8e1ca666c653171a364c08f640549c24d7e82b401bf077c646f8859\nexperiment2 ref: 7a300b41a8e1ca666c653171a364c08f640549c24d7e82b401bf077c646f8859\n</code></pre> List branches <pre><code>import lakefs\n\nfor branch in lakefs.repository(\"example-repo\").branches():\n    print(branch)\n</code></pre> <p>Output</p> <pre><code>experiment1\nexperiment2\nmain\n</code></pre>"},{"location":"integrations/python/#io","title":"IO","text":"<p>Great, now lets see some IO operations in action!</p> <p>The new High Level SDK provide IO semantics which allow to work with lakeFS objects as if they were files in your filesystem. This is extremely useful when working with data transformation packages that accept file descriptors and streams.</p>"},{"location":"integrations/python/#upload","title":"Upload","text":"<p>A simple way to upload data is to use the <code>upload</code> method which accepts contents as <code>str/bytes</code></p> <pre><code>obj = branch1.object(path=\"text/sample_data.txt\").upload(content_type=\"text/plain\", data=\"This is my object data\")\nprint(obj.stat())\n</code></pre> <p>Output</p> <pre><code>{'path': 'text/sample_data.txt', 'physical_address': 's3://storage-bucket/repos/example-repo/data/gke0ignnl531fa6k90p0/ckpfk4fnl531fa6k90pg', 'physical_address_expiry': None, 'checksum': '4a09d10820234a95bb548f14e4435bba', 'size_bytes': 15, 'mtime': 1701865289, 'metadata': {}, 'content_type': 'text/plain'}\n</code></pre> <p>Reading the data is just as simple:</p> <pre><code>print(obj.reader(mode='r').read())\n</code></pre> <p>Output</p> <pre><code>This is my object data\n</code></pre> <p>Now let's generate a \"sample_data.csv\" file and write it directly to a lakeFS writer object</p> <pre><code>import csv\n\nsample_data = [\n    [1, \"Alice\", \"alice@example.com\"],\n    [2, \"Bob\", \"bob@example.com\"],\n    [3, \"Carol\", \"carol@example.com\"],\n]\n\nobj = branch1.object(path=\"csv/sample_data.csv\")\n\nwith obj.writer(mode='w', pre_sign=True, content_type=\"text/csv\") as fd:\n    writer = csv.writer(fd)\n    writer.writerow([\"ID\", \"Name\", \"Email\"])\n    for row in sample_data:\n        writer.writerow(row)\n</code></pre> <p>On context exit the object will be uploaded to lakeFS</p> <pre><code>print(obj.stat())\n</code></pre> <p>Output</p> <pre><code>{'path': 'csv/sample_data.csv', 'physical_address': 's3://storage-bucket/repos/example-repo/data/gke0ignnl531fa6k90p0/ckpfk4fnl531fa6k90pg', 'physical_address_expiry': None, 'checksum': 'f181262c138901a74d47652d5ea72295', 'size_bytes': 88, 'mtime': 1701865939, 'metadata': {}, 'content_type': 'text/csv'}\n</code></pre> <p>We can also upload raw byte contents:</p> <pre><code>obj = branch1.object(path=\"raw/file1.data\").upload(data=b\"Hello Object World\", pre_sign=True)\nprint(obj.stat())\n</code></pre> <p>Output</p> <pre><code>{'path': 'raw/file1.data', 'physical_address': 's3://storage-bucket/repos/example-repo/data/gke0ignnl531fa6k90p0/ckpfltvnl531fa6k90q0', 'physical_address_expiry': None, 'checksum': '0ef432f8eb0305f730b0c57bbd7a6b08', 'size_bytes': 18, 'mtime': 1701866323, 'metadata': {}, 'content_type': 'application/octet-stream'}\n</code></pre>"},{"location":"integrations/python/#uncommitted-changes","title":"Uncommitted changes","text":"<p>Using the branch <code>uncommmitted</code> method will show all the uncommitted changes on that branch:</p> <pre><code>for diff in branch1.uncommitted():\n    print(diff)\n</code></pre> <p>Output</p> <pre><code>{'type': 'added', 'path': 'text/sample_data.txt', 'path_type': 'object', 'size_bytes': 15}\n{'type': 'added', 'path': 'csv/sample_data.csv', 'path_type': 'object', 'size_bytes': 88}\n{'type': 'added', 'path': 'raw/file1.data', 'path_type': 'object', 'size_bytes': 18}\n</code></pre> <p>As expected, our change appears here. Let's commit it and attach some arbitrary metadata:</p> <pre><code>ref = branch1.commit(message='Add some data!', metadata={'using': 'python_sdk'})\nprint(ref.get_commit())\n</code></pre> <p>Output</p> <pre><code>{'id': 'c4666db80d2a984b4eab8ce02b6a60830767eba53995c26350e0ad994e15fedb', 'parents': ['a7a092a5a32a2cd97f22abcc99414f6283d29f6b9dd2725ce89f90188c5901e5'], 'committer': 'admin', 'message': 'Add some data!', 'creation_date': 1701866838, 'meta_range_id': '999bedeab1b740f83d2cf8c52548d55446f9038c69724d399adc4438412cade2', 'metadata': {'using': 'python_sdk'}}\n</code></pre> <p>Calling <code>uncommitted</code> again on the same branch, this time there should be no uncommitted files:</p> <pre><code>print(len(list(branch1.uncommitted())))\n</code></pre> <p>Output</p> <pre><code>0\n</code></pre>"},{"location":"integrations/python/#merging-changes-from-a-branch-into-main","title":"Merging changes from a branch into main","text":"<p>Let's diff between your branch and the main branch:</p> <pre><code>main = repo.branch(\"main\")\nfor diff in main.diff(other_ref=branch1):\n    print(diff)\n</code></pre> <p>Output</p> <pre><code>{'type': 'added', 'path': 'text/sample_data.txt', 'path_type': 'object', 'size_bytes': 15}\n{'type': 'added', 'path': 'csv/sample_data.csv', 'path_type': 'object', 'size_bytes': 88}\n{'type': 'added', 'path': 'raw/file1.data', 'path_type': 'object', 'size_bytes': 18}\n</code></pre> <p>Looks like we have some changes. Let's merge them:</p> <pre><code>res = branch1.merge_into(main)\nprint(res)\n# output:\n# cfddb68b7265ae0b17fafa1a2068f8414395e0a8b8bc0f8d741cbcce1e67e394\n</code></pre> <p>Let's diff again - there should be no changes as all changes are on our main branch already:</p> <pre><code>print(len(list(main.diff(other_ref=branch1))))\n</code></pre> <p>Output</p> <pre><code>0\n</code></pre>"},{"location":"integrations/python/#read-data-from-main-branch","title":"Read data from main branch","text":"<pre><code>import csv\n\nobj = main.object(path=\"csv/sample_data.csv\")\n\nfor row in csv.reader(obj.reader(mode='r')):\n    print(row)\n</code></pre> <p>Output</p> <pre><code>['ID', 'Name', 'Email']\n['1', 'Alice', 'alice@example.com']\n['2', 'Bob', 'bob@example.com']\n['3', 'Carol', 'carol@example.com']\n</code></pre>"},{"location":"integrations/python/#importing-data-into-lakefs","title":"Importing data into lakeFS","text":"<p>The new SDK makes it much easier to import existing data from the object store into lakeFS, using the new ImportManager</p> <pre><code>import lakefs\n\nbranch = lakefs.repository(\"example-repo\").repo.branch(\"experiment3\")\n\n# We can import data from multiple sources in a single import process\n# The following example initializes a new ImportManager and adds 2 source types; A prefix and an object.\nimporter = branch.import_data(commit_message=\"added public S3 data\") \\\n    .prefix(\"s3://example-bucket1/path1/\", destination=\"datasets/path1/\") \\\n    .object(\"s3://example-bucket1/path2/imported_obj\", destination=\"datasets/path2/imported_obj\")\n\n# run() is a convenience method that blocks until the import is reported as done, raising an exception if it fails.\nimporter.run()\n</code></pre> <p>Alternatively we can call <code>start()</code> and <code>status()</code> ourselves for an async version of the above</p> <pre><code>import time\n\n# Async version\nimporter.start()\nstatus = importer.start()\n\nwhile not status.completed or status.error is None:\n        time.sleep(3)  # or whatever interval you choose\n        status = importer.status()\n\nif status.error:\n    # handle!\n\nprint(f\"imported a total of {status.ingested_objects} objects!\")\n</code></pre> <p>Output</p> <pre><code>imported a total of 25478 objects!\n</code></pre>"},{"location":"integrations/python/#transactions","title":"Transactions","text":"<p>Transactions is a new feature in the High Level SDK. It allows performing a sequence of operations on a branch as an atomic unit, similarly to how database transactions work. Under the hood, the transaction creates an ephemeral branch from the source branch, performs all the operation on that branch, and merges it back to the source branch once the transaction is completed. Transactions are currently supported as a context manager only.</p> <pre><code>import lakefs\n\nbranch = lakefs.repository(\"example-repo\").repo.branch(\"experiment3\")\n\nwith branch.transact(commit_message=\"my transaction\") as tx:\n    for obj in tx.objects(prefix=\"prefix_to_delete/\"):  # Delete some objects\n        obj.delete()\n\n    # Create new object\n    tx.object(\"new_object\").upload(\"new object data\")\n\nprint(len(list(branch.objects(prefix=\"prefix_to_delete/\"))))\nprint(branch.object(\"new_object\").exists())\n</code></pre> <p>Output</p> <pre><code>0\nTrue\n</code></pre>"},{"location":"integrations/python/#python-sdk-documentation-and-api-reference","title":"Python SDK documentation and API reference","text":"<p>For the documentation of lakeFS's Python package and full api reference, see https://pydocs-lakefs.lakefs.io</p>"},{"location":"integrations/python/#using-lakefs-spec-for-higher-level-file-operations","title":"Using lakefs-spec for higher-level file operations","text":"<p>The lakefs-spec project provides higher-level file operations on lakeFS objects with a filesystem API, built on the fsspec project.</p> <p>Note</p> <p>This library is a third-party package and not maintained by the lakeFS developers; please file issues and bug reports directly in the lakefs-spec repository.</p>"},{"location":"integrations/python/#installation","title":"Installation","text":"<p>Install <code>lakefs-spec</code> directly with <code>pip</code>:</p> <pre><code>python -m pip install --upgrade lakefs-spec\n</code></pre>"},{"location":"integrations/python/#interacting-with-lakefs-through-a-file-system","title":"Interacting with lakeFS through a file system","text":"<p>To write a file directly to a branch in a lakeFS repository, consider the following example:</p> <pre><code>from pathlib import Path\n\nfrom lakefs_spec import LakeFSFileSystem\n\nREPO, BRANCH = \"example-repo\", \"main\"\n\n# Prepare a local example file.\nlpath = Path(\"demo.txt\")\nlpath.write_text(\"Hello, lakeFS!\")\n\nfs = LakeFSFileSystem()  # will auto-discover credentials from ~/.lakectl.yaml\nrpath = f\"{REPO}/{BRANCH}/{lpath.name}\"\nfs.put(lpath, rpath)\n</code></pre> <p>Reading it again from remote is as easy as the following:</p> <pre><code>f = fs.open(rpath, \"rt\")\nprint(f.readline())  # prints \"Hello, lakeFS!\"\n</code></pre> <p>Many more operations like retrieving an object's metadata or checking an object's existence on the lakeFS server are also supported. For a full list, see the API reference.</p>"},{"location":"integrations/python/#integrations-with-popular-data-science-packages","title":"Integrations with popular data science packages","text":"<p>A number of Python data science projects support fsspec, with pandas being a prominent example. Reading a Parquet file from a lakeFS repository into a Pandas data frame for analysis is very easy, demonstrated on the quickstart repository sample data:</p> <pre><code>import pandas as pd\n\n# Read into pandas directly by supplying the lakeFS URI...\nlakes = pd.read_parquet(f\"lakefs://quickstart/main/lakes.parquet\")\ngerman_lakes = lakes.query('Country == \"Germany\"')\n# ... and store directly, again with a raw lakeFS URI.\ngerman_lakes.to_csv(f\"lakefs://quickstart/main/german_lakes.csv\")\n</code></pre> <p>A list of integrations with popular data science libraries can be found in the lakefs-spec documentation.</p>"},{"location":"integrations/python/#using-transactions-for-atomic-versioning-operations","title":"Using transactions for atomic versioning operations","text":"<p>As with the high-level SDK (see above), lakefs-spec also supports transactions for conducting versioning operations on newly modified files. The following is an example of creating a commit on the repository's main branch directly after a file upload:</p> <pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\n# assumes you have a local train-test split as two text files:\n# train-data.txt, and test-data.txt.\nwith fs.transaction(\"example-repo\", \"main\") as tx:\n    fs.put_file(\"train-data.txt\", f\"example-repo/{tx.branch.id}/train-data.txt\")\n    tx.commit(message=\"Add training data\")\n    fs.put_file(\"test-data.txt\", f\"example-repo/{tx.branch.id}/test-data.txt\")\n    sha = tx.commit(message=\"Add test data\")\n    tx.tag(sha, name=\"My train-test split\")\n</code></pre> <p>Transactions are atomic - if an exception happens at any point of the transaction, the repository remains unchanged.</p>"},{"location":"integrations/python/#further-information","title":"Further information","text":"<p>For more user guides, tutorials on integrations with data science tools like pandas, and more, check out the lakefs-spec documentation.</p>"},{"location":"integrations/python/#using-boto","title":"Using Boto","text":"<p>Info</p> <p>To use Boto with lakeFS alongside S3, check out Boto S3 Router. It will route requests to either S3 or lakeFS according to the provided bucket name.</p> <p>lakeFS exposes an S3-compatible API, so you can use Boto to interact with your objects on lakeFS.</p>"},{"location":"integrations/python/#initializing_1","title":"Initializing","text":"<p>Create a Boto3 S3 client with your lakeFS endpoint and key-pair:</p> <pre><code>import boto3\ns3 = boto3.client('s3',\n    endpoint_url='https://lakefs.example.com',\n    aws_access_key_id='AKIAIOSFODNN7EXAMPLE',\n    aws_secret_access_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')\n</code></pre> <p>The client is now configured to operate on your lakeFS installation.</p>"},{"location":"integrations/python/#configuring-boto3-s3-client-with-checksum-settings","title":"Configuring Boto3 S3 Client with Checksum Settings","text":"<p>In newer versions of Boto3, when connecting to lakeFS using HTTPS, you might encounter an AccessDenied error on upload, while the lakeFS logs display an error <code>encoding/hex: invalid byte: U+0053 'S'</code>.</p> <p>To avoid this issue, explicitly configure the Boto3 client with the following checksum settings:</p> <ul> <li><code>request_checksum_calculation: 'when_required'</code></li> <li><code>response_checksum_validation: 'when_required'</code></li> </ul> <p>Example of how to configure it:</p> <pre><code>import boto3\nfrom botocore.config import Config\n\n# Configure checksum settings\nconfig = Config(\n    request_checksum_calculation='when_required',\n    response_checksum_validation='when_required'\n)\n\ns3_client = boto3.client(\n    's3',\n    endpoint_url='https://lakefs.example.com',\n    aws_access_key_id='AKIAIOSFODNN7EXAMPLE',\n    aws_secret_access_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',\n    config=config,\n)\n</code></pre>"},{"location":"integrations/python/#usage-examples_1","title":"Usage Examples","text":""},{"location":"integrations/python/#put-an-object-into-lakefs","title":"Put an object into lakeFS","text":"<p>Use a branch name and a path to put an object in lakeFS:</p> <pre><code>with open('/local/path/to/file_0', 'rb') as f:\n    s3.put_object(Body=f, Bucket='example-repo', Key='main/example-file.parquet')\n</code></pre> <p>You can now commit this change using the lakeFS UI or CLI.</p>"},{"location":"integrations/python/#list-objects","title":"List objects","text":"<p>List the branch objects starting with a prefix:</p> <pre><code>list_resp = s3.list_objects_v2(Bucket='example-repo', Prefix='main/example-prefix')\nfor obj in list_resp['Contents']:\n    print(obj['Key'])\n</code></pre> <p>Or, use a lakeFS commit ID to list objects for a specific commit:</p> <pre><code>list_resp = s3.list_objects_v2(Bucket='example-repo', Prefix='c7a632d74f/example-prefix')\nfor obj in list_resp['Contents']:\n    print(obj['Key'])\n</code></pre>"},{"location":"integrations/python/#get-object-metadata","title":"Get object metadata","text":"<p>Get object metadata using branch and path:</p> <pre><code>s3.head_object(Bucket='example-repo', Key='main/example-file.parquet')\n# output:\n# {'ResponseMetadata': {'RequestId': '72A9EBD1210E90FA',\n#  'HostId': '',\n#  'HTTPStatusCode': 200,\n#  'HTTPHeaders': {'accept-ranges': 'bytes',\n#   'content-length': '1024',\n#   'etag': '\"2398bc5880e535c61f7624ad6f138d62\"',\n#   'last-modified': 'Sun, 24 May 2020 10:42:24 GMT',\n#   'x-amz-request-id': '72A9EBD1210E90FA',\n#   'date': 'Sun, 24 May 2020 10:45:42 GMT'},\n#  'RetryAttempts': 0},\n# 'AcceptRanges': 'bytes',\n# 'LastModified': datetime.datetime(2020, 5, 24, 10, 42, 24, tzinfo=tzutc()),\n# 'ContentLength': 1024,\n# 'ETag': '\"2398bc5880e535c61f7624ad6f138d62\"',\n# 'Metadata': {}}\n</code></pre>"},{"location":"integrations/r/","title":"Using R with lakeFS","text":"<p>R is a powerful language used widely in data science. lakeFS interfaces with R in two ways: </p> <ul> <li>To read and write data in lakeFS use standard S3 tools such as the <code>aws.s3</code> library. lakeFS has a S3 gateway which presents a lakeFS repository as an S3 bucket. </li> <li>For working with lakeFS operations such as branches and commits use the API for which can be accessed from R using the <code>httr</code> library. </li> </ul> <p>Examples</p> <p>To see examples of R in action with lakeFS please visit the lakeFS-samples repository and the sample notebooks</p>"},{"location":"integrations/r/#reading-and-writing-from-lakefs-with-r","title":"Reading and Writing from lakeFS with R","text":"<p>Working with data stored in lakeFS from R is the same as you would with an S3 bucket, via the S3 Gateway that lakeFS provides.</p> <p>You can use any library that interfaces with S3. In this example we'll use the aws.s3 library.</p> <pre><code>install.packages(c(\"aws.s3\"))\nlibrary(aws.s3)\n</code></pre>"},{"location":"integrations/r/#configuration","title":"Configuration","text":"<p>The R S3 client documentation includes full details of the configuration options available. A good approach for using it with lakeFS set the endpoint and authentication details as environment variables: </p> <pre><code>Sys.setenv(\"AWS_ACCESS_KEY_ID\" = \"AKIAIOSFODNN7EXAMPLE\",\n           \"AWS_SECRET_ACCESS_KEY\" = \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n           \"AWS_S3_ENDPOINT\" = \"lakefs.mycorp.com:8000\")\n</code></pre> <p>Note: it is generally best practice to set these environment variables outside of the R script; it is done so here for convenience of the example.</p> <p>In conjunction with this you must also specify <code>region</code> and <code>use_https</code> in each call of an <code>aws.s3</code> function as these cannot be set globally. For example: </p> <pre><code>bucketlist(\n    region = \"\",\n    use_https = FALSE\n    )\n</code></pre> <ul> <li><code>region</code> should always be empty</li> <li><code>use_https</code> should be set to <code>TRUE</code> or <code>FALSE</code> depending on whether your lakeFS endpoint uses HTTPS.</li> </ul>"},{"location":"integrations/r/#listing-repositories","title":"Listing repositories","text":"<p>The S3 gateway exposes a repository as a bucket, and so using the <code>aws.s3</code> function <code>bucketlist</code> will return a list of available repositories on lakeFS: </p> <pre><code>bucketlist(\n    region = \"\",\n    use_https = FALSE\n    )\n</code></pre>"},{"location":"integrations/r/#writing-to-lakefs-from-r","title":"Writing to lakeFS from R","text":"<p>Assuming you're using the <code>aws.s3</code> library there various functions available including <code>s3save</code>, <code>s3saveRDS</code>, and <code>put_object</code>. Here's an example of writing an R object to lakeFS: </p> <pre><code>repo_name &lt;- \"example\"\nbranch &lt;- \"development\"\n\ns3saveRDS(x=my_df, \n          bucket = repo_name, \n          object = paste0(branch,\"/my_df.R\"), \n          region = \"\",\n          use_https = FALSE)\n</code></pre> <p>You can also upload local files to lakeFS using R and the <code>put_object</code> function: </p> <pre><code>repo_name &lt;- \"example\"\nbranch &lt;- \"development\"\nlocal_file &lt;- \"/tmp/never.gonna\"\n\nput_object(file = local_file, \n           bucket = repo_name, \n           object = paste0(branch,\"/give/you/up\"),\n           region = \"\",\n           use_https = FALSE)\n</code></pre>"},{"location":"integrations/r/#reading-from-lakefs-with-r","title":"Reading from lakeFS with R","text":"<p>As with writing data from R to lakeFS, there is a similar set of functions for reading data. These include <code>s3load</code>, <code>s3readRDS</code>, and <code>get_object</code>. Here's an example of reading an R object from lakeFS: </p> <pre><code>repo_name &lt;- \"example\"\nbranch &lt;- \"development\"\n\nmy_df &lt;- s3readRDS(bucket = repo_name, \n                   object = paste0(branch,\"/my_data.R\"),\n                   region = \"\",\n                   use_https = FALSE)\n</code></pre>"},{"location":"integrations/r/#listing-objects","title":"Listing Objects","text":"<p>In general you should always specify a branch prefix when listing objects. Here's an example to list the <code>main</code> branch in the <code>quickstart</code> repository: </p> <pre><code>get_bucket_df(bucket = \"quickstart\",\n              prefix = \"main/\",\n              region = \"\",\n              use_https = FALSE)\n</code></pre> <p>When listing objects in lakeFS there is a special case which is the repository/bucket level. When you list at this level you will get the branches returned as folders. These are not listed recursively, unless you list something under the branch. To understand more about this please refer to #5441</p>"},{"location":"integrations/r/#working-with-arrow","title":"Working with Arrow","text":"<p>Arrow's R library includes powerful support for data analysis, including reading and writing multiple file formats including Parquet, Arrow, CSV, and JSON. It has functionality for connecting to S3, and thus integrates perfectly with lakeFS. </p> <p>To start with install and load the library</p> <pre><code>install.packages(\"arrow\")\nlibrary(arrow)\n</code></pre> <p>Then create an S3FileSystem object to connect to your lakeFS instance</p> <pre><code>lakefs &lt;- S3FileSystem$create(\n    endpoint_override = \"lakefs.mycorp.com:8000\",\n    scheme = \"http\"\n    access_key = \"AKIAIOSFODNN7EXAMPLE\", \n    secret_key = \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \n    region = \"\",\n)\n</code></pre> <p>From here you can list the contents of a particular lakeFS repository and branch</p> <pre><code>lakefs$ls(path = \"quickstart/main\")\n</code></pre> <p>To read a Parquet from lakeFS with R use the <code>read_parquet</code> function</p> <pre><code>lakes &lt;- read_parquet(lakefs$path(\"quickstart/main/lakes.parquet\"))\n</code></pre> <p>Writing a file follows a similar pattern. Here is rewriting the same file as above but in Arrow format</p> <pre><code>write_feather(x = lakes,\n              sink = lakefs$path(\"quickstart/main/lakes.arrow\"))\n</code></pre>"},{"location":"integrations/r/#performing-lakefs-operations-using-the-lakefs-api-from-r","title":"Performing lakeFS Operations using the lakeFS API from R","text":"<p>As well as reading and writing data, you will also want to carry out lakeFS operations from R including creating branches, committing data, and more. </p> <p>To do this call the lakeFS API from the <code>httr</code> library. You should refer to the API documentation for full details of the endpoints and their behaviour. Below are a few examples to illustrate the usage. </p>"},{"location":"integrations/r/#check-the-lakefs-server-version","title":"Check the lakeFS Server Version","text":"<p>This is a useful API call to establish connectivity and test authentication. </p> <pre><code>library(httr)\nlakefs_api_url &lt;- \"lakefs.mycorp.com:8000/api/v1\"\nlakefsAccessKey &lt;- \"AKIAIOSFODNN7EXAMPLE\"\nlakefsSecretKey &lt;- \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\n\nr=GET(url=paste0(lakefs_api_url, \"/config/version\"), \n      authenticate(lakefsAccessKey, lakefsSecretKey))\n</code></pre> <p>The returned object <code>r</code> can be inspected to determine the outcome of the operation by comparing it to the status codes specified in the API. Here is some example R code to demonstrate the idea: </p> <pre><code>if (r$status_code == 200) {\n    print(paste0(\"\u2705 lakeFS credentials and connectivity verified. \u2139\ufe0f lakeFS version \",content(r)$version))   \n} else {\n    print(\"\ud83d\uded1 failed to get lakeFS version\")\n    print(content(r)$message)\n}\n</code></pre>"},{"location":"integrations/r/#create-a-repository","title":"Create a Repository","text":"<pre><code>library(httr)\nlakefs_api_url &lt;- \"lakefs.mycorp.com:8000/api/v1\"\nlakefsAccessKey &lt;- \"AKIAIOSFODNN7EXAMPLE\"\nlakefsSecretKey &lt;- \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\nrepo_name &lt;- \"my_new_repo\"\n\n# Define the payload\nbody=list(name=repo_name, \n          storage_namespace=\"s3://example-bucket/foo\")\n\n# Call the API\nr=POST(url=paste0(lakefs_api_url, \"/repositories\"), \n        authenticate(lakefsAccessKey, lakefsSecretKey),\n        body=body, encode=\"json\")\n</code></pre>"},{"location":"integrations/r/#commit-data","title":"Commit Data","text":"<pre><code>library(httr)\nlakefs_api_url &lt;- \"lakefs.mycorp.com:8000/api/v1\"\nlakefsAccessKey &lt;- \"AKIAIOSFODNN7EXAMPLE\"\nlakefsSecretKey &lt;- \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\nrepo_name &lt;- \"my_new_repo\"\nbranch &lt;- \"example\"\n\n# Define the payload\nbody=list(message=\"add some data and charts\", \n          metadata=list(\n              client=\"httr\", \n              author=\"rmoff\"))\n\n# Call the API\nr=POST(url=paste0(lakefs_api_url, \"/repositories/\", repo_name, \"/branches/\", branch, \"/commits\"), \n       authenticate(lakefsAccessKey, lakefsSecretKey),\n       body=body, encode=\"json\")\n</code></pre>"},{"location":"integrations/red_hat_openshift_ai/","title":"Using lakeFS with Red Hat OpenShift AI","text":"<p>Red Hat\u00ae OpenShift\u00ae is an enterprise-ready Kubernetes container platform with full-stack automated operations to manage hybrid cloud, multi-cloud, and edge deployments. OpenShift includes an enterprise-grade Linux operating system, container runtime, networking, monitoring, registry, and authentication and authorization solutions.</p> <p>Red Hat\u00ae OpenShift\u00ae AI is a flexible, scalable artificial intelligence (AI) and machine learning (ML) platform that enables enterprises to create and deliver AI-enabled applications at scale across hybrid cloud environments. Built using open source technologies, OpenShift AI provides trusted, operationally consistent capabilities for teams to experiment, serve models, and deliver innovative apps.</p> <p>OpenShift AI and lakeFS can be deployed in OpenShift cluster in 3 different architectures: 1. OpenShift AI, lakeFS and object storage are delpoyed in OpenShift cluster 2. OpenShift AI and lakeFS are deployed in OpenShift cluster while object storage is external 3. OpenShift AI is deployed in OpenShift cluster while lakeFS and object storage are external</p> <p></p> <p>Refer to an example in lakeFS-samples to deploy lakeFS, MinIO and OpenShift AI tutorial (Fraud Detection demo) in OpenShift cluster. Fraud detection demo is a step-by-step guide for using OpenShift AI to train an example model in JupyterLab, deploy the model, and refine the model by using automated pipelines.</p> <p>In this example, OpenShift AI is configured to connect over S3 interace to lakeFS, which will version the data in a backend MinIO instance. This is the architecture to run Fraud Detection demo with or without lakeFS:</p> <p></p> <p>lakeFS-samples also includes multiple Helm chart examples to deploy lakeFS and MinIO in different scenarios:</p> <ol> <li>lakefs-local.yaml: Bring up lakeFS using local object storage. This would be useful for a quick demo where MinIO is not included.</li> <li>lakefs-minio.yaml: Bring up lakeFS configured to use MinIO as backend object storage. This will be used in the lakeFS demo.</li> <li>minio-direct.yaml: This file would only be used if lakeFS is not in the picture and OpenShift AI will communicate directly with MinIO. It will bring up MinIO as it is in the default Fraud Detection demo, complete with configuring MinIO storage buckets and the OpenShift AI data connections. It may serve useful in debugging an issue.</li> <li>minio-via-lakefs.yaml: Bring up MinIO for the modified Fraud Detection demo that includes lakeFS, complete with configuring MinIO storage buckets, but do NOT configure the OpenShift AI data connections. This will be used in the lakeFS demo.</li> </ol>"},{"location":"integrations/sagemaker/","title":"Using lakeFS with Amazon SageMaker","text":"<p>Amazon SageMaker helps to prepare, build, train and deploy ML models quickly by bringing together a broad set of capabilities purpose-built for ML.</p>"},{"location":"integrations/sagemaker/#initializing-session-and-client","title":"Initializing session and client","text":"<p>Initialize a Sagemaker session and an S3 client with lakeFS as the endpoint:</p> <pre><code>import sagemaker\nimport boto3\n\nendpoint_url = '&lt;LAKEFS_ENDPOINT&gt;'\naws_access_key_id = '&lt;LAKEFS_ACCESS_KEY_ID&gt;'\naws_secret_access_key = '&lt;LAKEFS_SECRET_ACCESS_KEY&gt;'\nrepo = 'example-repo'\n\nsm = boto3.client('sagemaker',\n    endpoint_url=endpoint_url,\n    aws_access_key_id=aws_access_key_id,\n    aws_secret_access_key=aws_secret_access_key)\n\ns3_resource = boto3.resource('s3',\n    endpoint_url=endpoint_url,\n    aws_access_key_id=aws_access_key_id,\n    aws_secret_access_key=aws_secret_access_key)\n\nsession = sagemaker.Session(boto3.Session(), sagemaker_client=sm, default_bucket=repo)\nsession.s3_resource = s3_resource\n</code></pre>"},{"location":"integrations/sagemaker/#usage-examples","title":"Usage Examples","text":""},{"location":"integrations/sagemaker/#upload-train-and-test-data","title":"Upload train and test data","text":"<p>Let's use the created session for uploading data to the 'main' branch:</p> <pre><code>prefix = \"/prefix-within-branch\"\nbranch = 'main'\n\ntrain_file = 'train_data.csv';\ntrain_data.to_csv(train_file, index=False, header=True)\ntrain_data_s3_path = session.upload_data(path=train_file, key_prefix=branch + prefix + \"/train\")\n\ntest_file = 'test_data.csv';\ntest_data_no_target.to_csv(test_file, index=False, header=False)\ntest_data_s3_path = session.upload_data(path=test_file, key_prefix=branch + prefix + \"/test\")\n</code></pre>"},{"location":"integrations/sagemaker/#download-objects","title":"Download objects","text":"<p>You can use the integration with lakeFS to download a portion of the data you see fit:</p> <pre><code>repo = 'example-repo'\nprefix = \"/prefix-to-download\"\nbranch = 'main'\nlocalpath = './' + branch\n\nsession.download_data(path=localpath, bucket=repo, key_prefix = branch + prefix)\n</code></pre> <p>Note</p> <p>Advanced AWS SageMaker features, like Autopilot jobs, are encapsulated and don't have the option to override the S3 endpoint. However, it is possible to export the required inputs from lakeFS to S3.</p> <p>If you're using SageMaker features that aren't supported by lakeFS, we'd love to hear from you.</p>"},{"location":"integrations/spark/","title":"Using lakeFS with Apache Spark","text":"<p>There are several ways to use lakeFS with Spark:</p> <ul> <li>The lakeFS FileSystem: Direct data flow from client to storage, highly scalable. AWS S3<ul> <li>lakeFS FileSystem in Presigned mode: Best of both worlds. AWS S3Azure Blob</li> </ul> </li> <li>The S3-compatible API:  All Storage Vendors</li> </ul> <p>Tip</p> <p>See how SimilarWeb is using lakeFS with Spark to manage algorithm changes in data pipelines.</p>"},{"location":"integrations/spark/#lakefs-hadoop-filesystem","title":"lakeFS Hadoop FileSystem","text":"<p>In this mode, Spark will read and write objects directly from the underlying object store, reducing the load on the lakeFS server. It will only access the lakeFS server for metadata operations.</p> <p>After configuring the lakeFS Hadoop FileSystem below, use URIs of the form <code>lakefs://example-repo/ref/path/to/data</code> to interact with your data on lakeFS.</p>"},{"location":"integrations/spark/#installation","title":"Installation","text":"Spark StandaloneDatabricksCloudera Spark <p>Add the package to your <code>spark-submit</code> command:</p> <pre><code>--packages io.lakefs:hadoop-lakefs-assembly:0.2.5\n</code></pre> <p>In  your cluster settings, under the Libraries tab, add the following Maven package:</p> <pre><code>io.lakefs:hadoop-lakefs-assembly:0.2.5\n</code></pre> <p>Once installed, it should look something like this:</p> <p></p> <p>Add the package to your <code>pyspark</code> or <code>spark-submit</code> command:</p> <pre><code>--packages io.lakefs:hadoop-lakefs-assembly:0.2.5\n</code></pre> <p>Add the configuration to access the S3 bucket used by lakeFS to your <code>pyspark</code> or <code>spark-submit</code> command or add this configuration at the Cloudera cluster level (see below):</p> <pre><code>--conf spark.yarn.access.hadoopFileSystems=s3a://bucket-name\n</code></pre> <p>Add the configuration to access the S3 bucket used by lakeFS at the Cloudera cluster level:</p> <ol> <li>Log in to the CDP (Cloudera Data Platform) web interface.</li> <li>From the CDP home screen, click the <code>Management Console</code> icon.</li> <li>In the Management Console, select <code>Data Hub Clusters</code> from the navigation pane.</li> <li>Select the cluster you want to configure. Click on <code>CM-UI</code> link under Services:     </li> <li> <p>In Cloudera Manager web interface, click on <code>Clusters</code> from the navigation pane and click on <code>spark_on_yarn</code> option:</p> <p></p> </li> <li> <p>Click on <code>Configuration</code> tab and search for <code>spark.yarn.access.hadoopFileSystems</code> in the search box:</p> <p></p> </li> <li> <p>Add S3 bucket used by lakeFS <code>s3a://bucket-name</code> in the <code>spark.yarn.access.hadoopFileSystems</code> list:</p> <p></p> </li> </ol>"},{"location":"integrations/spark/#configuration","title":"Configuration","text":"<p>Set the <code>fs.lakefs.*</code> Hadoop configurations to point to your lakeFS installation:</p> <ul> <li><code>fs.lakefs.impl</code>: <code>io.lakefs.LakeFSFileSystem</code></li> <li><code>fs.lakefs.access.key</code>: lakeFS access key</li> <li><code>fs.lakefs.secret.key</code>: lakeFS secret key</li> <li><code>fs.lakefs.endpoint</code>: lakeFS API URL (e.g. <code>https://example-org.us-east-1.lakefscloud.io/api/v1</code>)</li> </ul> <p>Configure the lakeFS client to use a temporary token instead of static credentials:</p> <ul> <li><code>fs.lakefs.auth.provider</code>: The default is <code>basic_auth</code> with <code>fs.lakefs.access.key</code> and <code>fs.lakefs.secret.key</code> for basic authentication.</li> </ul> <p>Can be set to <code>io.lakefs.auth.TemporaryAWSCredentialsLakeFSTokenProvider</code> for using temporary AWS credentials, you can read more about it here.</p> <p>When using <code>io.lakefs.auth.TemporaryAWSCredentialsLakeFSTokenProvider</code> as the auth provider the following configuration are relevant:</p> <ul> <li><code>fs.lakefs.token.aws.access.key</code>: AWS assumed role access key</li> <li><code>fs.lakefs.token.aws.secret.key</code>: AWS assumed role secret key</li> <li><code>fs.lakefs.token.aws.session.token</code>: AWS assumed role temporary session token</li> <li><code>fs.lakefs.token.aws.sts.endpoint</code>: AWS STS regional endpoint for generated the presigned-url (i.e <code>https://sts.us-west-2.amazonaws.com</code>)</li> <li><code>fs.lakefs.token.aws.sts.duration_seconds</code>: Optional, the duration in seconds for the initial identity token (default is 60)</li> <li><code>fs.lakefs.token.duration_seconds</code>: Optional, the duration in seconds for the lakeFS token (default is set in the lakeFS configuration auth.login_duration)</li> <li><code>fs.lakefs.token.sts.additional_headers</code>: Optional, comma separated list of <code>header:value</code> to attach when generating presigned sts request. Default is <code>X-Lakefs-Server-ID:fs.lakefs.endpoint</code>.</li> </ul> <p>Configure the S3A FileSystem to access your S3 storage, for example using the <code>fs.s3a.*</code> configurations (these are not your lakeFS credentials):</p> <ul> <li><code>fs.s3a.access.key</code>: AWS S3 access key</li> <li><code>fs.s3a.secret.key</code>: AWS S3 secret key</li> </ul> <p>Here are some configuration examples:</p> CLIScalaPySparkXML ConfigurationDatabricks <pre><code>spark-shell --conf spark.hadoop.fs.s3a.access.key='AKIAIOSFODNN7EXAMPLE' \\\n            --conf spark.hadoop.fs.s3a.secret.key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' \\\n            --conf spark.hadoop.fs.s3a.endpoint='https://s3.eu-central-1.amazonaws.com' \\\n            --conf spark.hadoop.fs.lakefs.impl=io.lakefs.LakeFSFileSystem \\\n            --conf spark.hadoop.fs.lakefs.access.key=AKIAlakefs12345EXAMPLE \\\n            --conf spark.hadoop.fs.lakefs.secret.key=abc/lakefs/1234567bPxRfiCYEXAMPLEKEY \\\n            --conf spark.hadoop.fs.lakefs.endpoint=https://example-org.us-east-1.lakefscloud.io/api/v1 \\\n            --packages io.lakefs:hadoop-lakefs-assembly:0.2.5 \\\n            io.example.ExampleClass\n</code></pre> <pre><code>spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", \"AKIAIOSFODNN7EXAMPLE\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.s3a.endpoint\", \"https://s3.eu-central-1.amazonaws.com\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.access.key\", \"AKIAlakefs12345EXAMPLE\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.endpoint\", \"https://example-org.us-east-1.lakefscloud.io/api/v1\")\n</code></pre> <pre><code>sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIAIOSFODNN7EXAMPLE\")\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\")\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"https://s3.eu-central-1.amazonaws.com\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.access.key\", \"AKIAlakefs12345EXAMPLE\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.endpoint\", \"https://example-org.us-east-1.lakefscloud.io/api/v1\")\n</code></pre> <p>Make sure that you load the lakeFS FileSystem into Spark by running it with <code>--packages</code> or <code>--jars</code>, and then add these into a configuration file, e.g., <code>$SPARK_HOME/conf/hdfs-site.xml</code>:</p> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.access.key&lt;/name&gt;\n        &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n            &lt;name&gt;fs.s3a.secret.key&lt;/name&gt;\n            &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.endpoint&lt;/name&gt;\n        &lt;value&gt;https://s3.eu-central-1.amazonaws.com&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.lakefs.impl&lt;/name&gt;\n        &lt;value&gt;io.lakefs.LakeFSFileSystem&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.lakefs.access.key&lt;/name&gt;\n        &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.lakefs.secret.key&lt;/name&gt;\n        &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.lakefs.endpoint&lt;/name&gt;\n        &lt;value&gt;https://example-org.us-east-1.lakefscloud.io/api/v1&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Add the following the cluster's configuration under <code>Configuration \u27a1\ufe0f Advanced options</code>:</p> <pre><code>spark.hadoop.fs.lakefs.impl io.lakefs.LakeFSFileSystem\nspark.hadoop.fs.lakefs.access.key AKIAlakefs12345EXAMPLE\nspark.hadoop.fs.lakefs.secret.key abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\nspark.hadoop.fs.s3a.access.key AKIAIOSFODNN7EXAMPLE\nspark.hadoop.fs.s3a.secret.key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nspark.hadoop.fs.s3a.impl shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem\nspark.hadoop.fs.lakefs.endpoint https://example-org.us-east-1.lakefscloud.io/api/v1\n</code></pre> <p>Alternatively, follow this step by step Databricks integration tutorial, including lakeFS Hadoop File System, Python client and lakeFS SPARK client.</p> <p>Note</p> <p>If your bucket is on a region other than us-east-1, you may also need to configure <code>fs.s3a.endpoint</code> with the correct region. Amazon provides S3 endpoints you can use.</p>"},{"location":"integrations/spark/#usage-with-temporaryawscredentialslakefstokenprovider","title":"Usage with TemporaryAWSCredentialsLakeFSTokenProvider","text":"<p>An initial setup is required - you must have AWS Auth configured with lakeFS. The <code>TemporaryAWSCredentialsLakeFSTokenProvider</code> depends on the caller to provide AWS credentials (e.g Assumed Role Key,Secret and Token) as input to the lakeFS client.</p> <p>Warning</p> <p>Configure <code>sts.endpoint</code> with a valid sts regional service endpoint and it must be be equal to the region that is used for authentication first place. The only exception is <code>us-east-1</code> which is the default region for STS.</p> <p>Warning</p> <p>Using the current provider the lakeFS token will not renew upon expiry and the user will need to re-authenticate.</p> <p>PySpark example using <code>TemporaryAWSCredentialsLakeFSTokenProvider</code> with boto3 and AWS session credentials:</p> <pre><code>import boto3 \n\nsession = boto3.session.Session()\n\n# AWS credentials used s3a to access lakeFS bucket\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIAIOSFODNN7EXAMPLE\")\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\")\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"https://s3.us-west-2.amazonaws.com\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.endpoint\", \"https://example-org.us-west-2.lakefscloud.io/api/v1\")\nsc._jsc.hadoopConfiguration().set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.auth.provider\", \"io.lakefs.auth.TemporaryAWSCredentialsLakeFSTokenProvider\")\n# AWS tempporary session credentials to use with lakeFS\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.token.aws.access.key\", session.get_credentials().access_key)\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.token.aws.secret.key\", session.get_credentials().secret_key)\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.token.aws.session.token\", session.get_credentials().token)\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.token.aws.sts.endpoint\", \"https://sts.us-west-2.amazonaws.com\")\n</code></pre>"},{"location":"integrations/spark/#usage","title":"Usage","text":"<p>Hadoop FileSystem paths use the <code>lakefs://</code> protocol, with paths taking the form <code>lakefs://&lt;repository&gt;/&lt;ref&gt;/path/to/object</code>. <code>&lt;ref&gt;</code> can be a branch, tag, or commit ID in lakeFS. Here's an example for reading a Parquet file from lakeFS to a Spark DataFrame:</p> <pre><code>val repo = \"example-repo\"\nval branch = \"main\"\nval df = spark.read.parquet(s\"lakefs://${repo}/${branch}/example-path/example-file.parquet\")\n</code></pre> <p>Here's how to write some results back to a lakeFS path:</p> <pre><code>df.write.partitionBy(\"example-column\").parquet(s\"lakefs://${repo}/${branch}/output-path/\")\n</code></pre> <p>The data is now created in lakeFS as new changes in your branch. You can now commit these changes or revert them.</p>"},{"location":"integrations/spark/#hadoop-filesystem-in-presigned-mode","title":"Hadoop FileSystem in Presigned mode","text":"<p>Info</p> <p>Available starting version 0.1.13 of the FileSystem</p> <p>In this mode, the lakeFS server is responsible for authenticating with your storage. The client will still perform data operations directly on the storage. To do so, it will use pre-signed storage URLs provided by the lakeFS server.</p> <p>When using this mode, you don't need to configure the client with access to your storage:</p> CLIScalaPySparkXML ConfigurationDatabricks <pre><code>spark-shell --conf spark.hadoop.fs.lakefs.access.mode=presigned \\\n            --conf spark.hadoop.fs.lakefs.impl=io.lakefs.LakeFSFileSystem \\\n            --conf spark.hadoop.fs.lakefs.access.key=AKIAlakefs12345EXAMPLE \\\n            --conf spark.hadoop.fs.lakefs.secret.key=abc/lakefs/1234567bPxRfiCYEXAMPLEKEY \\\n            --conf spark.hadoop.fs.lakefs.endpoint=https://example-org.us-east-1.lakefscloud.io/api/v1 \\\n            --packages io.lakefs:hadoop-lakefs-assembly:0.2.5\n</code></pre> <pre><code>spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.access.mode\", \"presigned\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.access.key\", \"AKIAlakefs12345EXAMPLE\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.endpoint\", \"https://example-org.us-east-1.lakefscloud.io/api/v1\")\n</code></pre> <pre><code>sc._jsc.hadoopConfiguration().set(\"fs.lakefs.access.mode\", \"presigned\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.access.key\", \"AKIAlakefs12345EXAMPLE\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\")\nsc._jsc.hadoopConfiguration().set(\"fs.lakefs.endpoint\", \"https://example-org.us-east-1.lakefscloud.io/api/v1\")\n</code></pre> <p>Make sure that you load the lakeFS FileSystem into Spark by running it with <code>--packages</code> or <code>--jars</code>, and then add these into a configuration file, e.g., <code>$SPARK_HOME/conf/hdfs-site.xml</code>:</p> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.lakefs.access.mode&lt;/name&gt;\n        &lt;value&gt;presigned&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.lakefs.impl&lt;/name&gt;\n        &lt;value&gt;io.lakefs.LakeFSFileSystem&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.lakefs.access.key&lt;/name&gt;\n        &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.lakefs.secret.key&lt;/name&gt;\n        &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.lakefs.endpoint&lt;/name&gt;\n        &lt;value&gt;https://example-org.us-east-1.lakefscloud.io/api/v1&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Add the following the cluster's configuration under <code>Configuration \u27a1\ufe0f Advanced options</code>:</p> <pre><code>spark.hadoop.fs.lakefs.access.mode presigned\nspark.hadoop.fs.lakefs.impl io.lakefs.LakeFSFileSystem\nspark.hadoop.fs.lakefs.access.key AKIAlakefs12345EXAMPLE\nspark.hadoop.fs.lakefs.secret.key abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\nspark.hadoop.fs.lakefs.endpoint https://example-org.us-east-1.lakefscloud.io/api/v1\n</code></pre>"},{"location":"integrations/spark/#s3-compatible-api","title":"S3-compatible API","text":"<p>lakeFS has an S3-compatible endpoint which you can point Spark at to get started quickly.</p> <p>You will access your data using S3-style URIs, e.g. <code>s3a://example-repo/example-branch/example-table</code>.</p> <p>You can use the S3-compatible API regardless of where your data is hosted.</p>"},{"location":"integrations/spark/#configuration_1","title":"Configuration","text":"<p>To configure Spark to work with lakeFS, we set S3A Hadoop configuration to the lakeFS endpoint and credentials:</p> <ul> <li><code>fs.s3a.access.key</code>: lakeFS access key</li> <li><code>fs.s3a.secret.key</code>: lakeFS secret key</li> <li><code>fs.s3a.endpoint</code>: lakeFS S3-compatible API endpoint (e.g. https://example-org.us-east-1.lakefscloud.io)</li> <li><code>fs.s3a.path.style.access</code>: <code>true</code></li> </ul> <p>Here is how to do it:</p> CLIScalaXML ConfigurationEMR <pre><code>spark-shell --conf spark.hadoop.fs.s3a.access.key='AKIAlakefs12345EXAMPLE' \\\n            --conf spark.hadoop.fs.s3a.secret.key='abc/lakefs/1234567bPxRfiCYEXAMPLEKEY' \\\n            --conf spark.hadoop.fs.s3a.path.style.access=true \\\n            --conf spark.hadoop.fs.s3a.endpoint='https://example-org.us-east-1.lakefscloud.io' ...\n</code></pre> <pre><code>spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", \"AKIAlakefs12345EXAMPLE\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.s3a.endpoint\", \"https://example-org.us-east-1.lakefscloud.io\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.s3a.path.style.access\", \"true\")\n</code></pre> <p>Add these into a configuration file, e.g. <code>$SPARK_HOME/conf/hdfs-site.xml</code>:</p> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.access.key&lt;/name&gt;\n        &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n            &lt;name&gt;fs.s3a.secret.key&lt;/name&gt;\n            &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.endpoint&lt;/name&gt;\n        &lt;value&gt;https://example-org.us-east-1.lakefscloud.io&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Use the below configuration when creating the cluster. You may delete any app configuration that is not suitable for your use case:</p> <pre><code>[\n{\n    \"Classification\": \"spark-defaults\",\n    \"Properties\": {\n    \"spark.sql.catalogImplementation\": \"hive\"\n    }\n},\n{\n    \"Classification\": \"core-site\",\n    \"Properties\": {\n        \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3.path.style.access\": \"true\",\n        \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3a.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3a.path.style.access\": \"true\"\n    }\n},\n{\n    \"Classification\": \"emrfs-site\",\n    \"Properties\": {\n        \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3.path.style.access\": \"true\",\n        \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3a.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3a.path.style.access\": \"true\"\n    }\n},\n{\n    \"Classification\": \"presto-connector-hive\",\n    \"Properties\": {\n        \"hive.s3.aws-access-key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"hive.s3.aws-secret-key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"hive.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"hive.s3.path-style-access\": \"true\",\n        \"hive.s3-file-system-type\": \"PRESTO\"\n    }\n},\n{\n    \"Classification\": \"hive-site\",\n    \"Properties\": {\n        \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3.path.style.access\": \"true\",\n        \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3a.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3a.path.style.access\": \"true\"\n    }\n},\n{\n    \"Classification\": \"hdfs-site\",\n    \"Properties\": {\n        \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3.path.style.access\": \"true\",\n        \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3a.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3a.path.style.access\": \"true\"\n    }\n},\n{\n    \"Classification\": \"mapred-site\",\n    \"Properties\": {\n        \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3.path.style.access\": \"true\",\n        \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3a.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3a.path.style.access\": \"true\"\n    }\n}\n]\n</code></pre> <p>Alternatively, you can pass these configuration values when adding a step.</p> <p>For example:</p> <pre><code>aws emr add-steps --cluster-id j-197B3AEGQ9XE4 \\\n--steps=\"Type=Spark,Name=SparkApplication,ActionOnFailure=CONTINUE, \\\nArgs=[--conf,spark.hadoop.fs.s3a.access.key=AKIAIOSFODNN7EXAMPLE, \\\n--conf,spark.hadoop.fs.s3a.secret.key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY, \\\n--conf,spark.hadoop.fs.s3a.endpoint=https://example-org.us-east-1.lakefscloud.io, \\\n--conf,spark.hadoop.fs.s3a.path.style.access=true, \\\ns3a://&lt;lakefs-repo&gt;/&lt;lakefs-branch&gt;/path/to/jar]\"\n</code></pre>"},{"location":"integrations/spark/#per-bucket-configuration","title":"Per-bucket configuration","text":"<p>The above configuration will use lakeFS as the sole S3 endpoint. To use lakeFS in parallel with S3, you can configure Spark to use lakeFS only for specific bucket names. For example, to configure only <code>example-repo</code> to use lakeFS, set the following configurations:</p> CLIScalaXML ConfigurationEMR <pre><code>spark-shell --conf spark.hadoop.fs.s3a.bucket.example-repo.access.key='AKIAlakefs12345EXAMPLE' \\\n            --conf spark.hadoop.fs.s3a.bucket.example-repo.secret.key='abc/lakefs/1234567bPxRfiCYEXAMPLEKEY' \\\n            --conf spark.hadoop.fs.s3a.bucket.example-repo.endpoint='https://example-org.us-east-1.lakefscloud.io' \\\n            --conf spark.hadoop.fs.s3a.path.style.access=true\n</code></pre> <pre><code>spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.access.key\", \"AKIAlakefs12345EXAMPLE\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.endpoint\", \"https://example-org.us-east-1.lakefscloud.io\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.s3a.path.style.access\", \"true\")\n</code></pre> <p>Add these into a configuration file, e.g. <code>$SPARK_HOME/conf/hdfs-site.xml</code>:</p> <p>```xml  fs.s3a.bucket.example-repo.access.key AKIAlakefs12345EXAMPLE fs.s3a.bucket.example-repo.secret.key abc/lakefs/1234567bPxRfiCYEXAMPLEKEY fs.s3a.bucket.example-repo.endpoint https://example-org.us-east-1.lakefscloud.io fs.s3a.path.style.access true </p> <p>Use the below configuration when creating the cluster. You may delete any app configuration that is not suitable for your use case:</p> <pre><code>[\n{\n    \"Classification\": \"spark-defaults\",\n    \"Properties\": {\n    \"spark.sql.catalogImplementation\": \"hive\"\n    }\n},\n{\n    \"Classification\": \"core-site\",\n    \"Properties\": {\n        \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3.bucket.example-repo.path.style.access\": \"true\",\n        \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3a.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3a.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3a.bucket.example-repo.path.style.access\": \"true\"\n    }\n},\n{\n    \"Classification\": \"emrfs-site\",\n    \"Properties\": {\n        \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3.bucket.example-repo.path.style.access\": \"true\",\n        \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3a.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3a.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3a.bucket.example-repo.path.style.access\": \"true\"\n    }\n},\n{\n    \"Classification\": \"presto-connector-hive\",\n    \"Properties\": {\n        \"hive.s3.aws-access-key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"hive.s3.aws-secret-key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"hive.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"hive.s3.path-style-access\": \"true\",\n        \"hive.s3-file-system-type\": \"PRESTO\"\n    }\n},\n{\n    \"Classification\": \"hive-site\",\n    \"Properties\": {\n        \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3.bucket.example-repo.path.style.access\": \"true\",\n        \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3a.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3a.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3a.bucket.example-repo.path.style.access\": \"true\"\n    }\n},\n{\n    \"Classification\": \"hdfs-site\",\n    \"Properties\": {\n        \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3.bucket.example-repo.path.style.access\": \"true\",\n        \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3a.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3a.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3a.bucket.example-repo.path.style.access\": \"true\"\n    }\n},\n{\n    \"Classification\": \"mapred-site\",\n    \"Properties\": {\n        \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3.bucket.example-repo.path.style.access\": \"true\",\n        \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"fs.s3a.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"fs.s3a.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\",\n        \"fs.s3a.bucket.example-repo.path.style.access\": \"true\"\n    }\n}\n]\n</code></pre> <p>Alternatively, you can pass these configuration values when adding a step.</p> <p>For example:</p> <pre><code>aws emr add-steps --cluster-id j-197B3AEGQ9XE4 \\\n--steps=\"Type=Spark,Name=SparkApplication,ActionOnFailure=CONTINUE, \\\nArgs=[--conf,spark.hadoop.fs.s3a.bucket.example-repo.access.key=AKIAIOSFODNN7EXAMPLE, \\\n--conf,spark.hadoop.fs.s3a.bucket.example-repo.secret.key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY, \\\n--conf,spark.hadoop.fs.s3a.bucket.example-repo.endpoint=https://example-org.us-east-1.lakefscloud.io, \\\n--conf,spark.hadoop.fs.s3a.path.style.access=true, \\\ns3a://&lt;lakefs-repo&gt;/&lt;lakefs-branch&gt;/path/to/jar]\"\n</code></pre> <p>With this configuration set, you read S3A paths with <code>example-repo</code> as the bucket will use lakeFS, while all other buckets will use AWS S3.</p>"},{"location":"integrations/spark/#usage_1","title":"Usage","text":"<p>Here's an example for reading a Parquet file from lakeFS to a Spark DataFrame:</p> <pre><code>val repo = \"example-repo\"\nval branch = \"main\"\nval df = spark.read.parquet(s\"s3a://${repo}/${branch}/example-path/example-file.parquet\")\n</code></pre> <p>Here's how to write some results back to a lakeFS path: <pre><code>df.write.partitionBy(\"example-column\").parquet(s\"s3a://${repo}/${branch}/output-path/\")\n</code></pre></p> <p>The data is now created in lakeFS as new changes in your branch. You can now commit these changes or revert them.</p>"},{"location":"integrations/spark/#configuring-azure-databricks-with-the-s3-compatible-api","title":"Configuring Azure Databricks with the S3-compatible API","text":"<p>If you use Azure Databricks, you can take advantage of the lakeFS S3-compatible API with your Azure account and the S3A FileSystem.  This will require installing the <code>hadoop-aws</code> package (with the same version as your <code>hadoop-azure</code> package) to your Databricks cluster.</p> <p>Define your FileSystem configurations in the following way:</p> <pre><code>spark.hadoop.fs.lakefs.impl=org.apache.hadoop.fs.s3a.S3AFileSystem\nspark.hadoop.fs.lakefs.access.key=\u2018AKIAlakefs12345EXAMPLE\u2019                   // The access key to your lakeFS server\nspark.hadoop.fs.lakefs.secret.key=\u2018abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\u2019     // The secret key to your lakeFS server\nspark.hadoop.fs.lakefs.path.style.access=true\nspark.hadoop.fs.lakefs.endpoint=\u2018https://example-org.us-east-1.lakefscloud.io\u2019                 // The endpoint of your lakeFS server\n</code></pre> <p>For more details about Mounting cloud object storage on Databricks.</p>"},{"location":"integrations/spark/#configuring-databricks-sql-warehouse-with-the-s3-compatible-api","title":"Configuring Databricks SQL Warehouse with the S3-compatible API","text":"<p>A SQL warehouse is a compute resource that lets you run SQL commands on data  objects within Databricks SQL.</p> <p>If you use Databricks SQL warehouse, you can take advantage of the lakeFS  S3-compatible API with the S3A FileSystem. </p> <p>Define your SQL Warehouse configurations in the following way:</p> <ol> <li> <p>In the top right, select <code>Admin Settings</code> and then <code>Compute</code>, and <code>SQL warehouses</code>.</p> </li> <li> <p>Under <code>Data Access Properties</code> add the following key-value pairs for     each lakeFS repository you want to access:</p> </li> </ol> <pre><code>spark.hadoop.fs.s3a.impl shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem\nspark.hadoop.fs.s3a.bucket.example-repo.access.key AKIAIOSFODNN7EXAMPLE // The access key to your lakeFS server\nspark.hadoop.fs.s3a.bucket.example-repo.secret.key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY // The secret key to your lakeFS server\nspark.hadoop.fs.s3a.bucket.example-repo.endpoint https://example-org.us-east-1.lakefscloud.io // The endpoint of your lakeFS server\nspark.hadoop.fs.s3a.bucket.example-repo.path.style.access true               \n</code></pre> <ol> <li>Changes are applied automatically after the SQL Warehouse restarts.</li> <li>You can now use the lakeFS S3-compatible API with your SQL Warehouse, e.g.:</li> </ol> <pre><code>SELECT * FROM delta.`s3a://example-repo/main/datasets/delta-table/` LIMIT 100\n</code></pre>"},{"location":"integrations/spark/#experimental-pre-signed-mode-for-s3a","title":"\u26a0\ufe0f Experimental: Pre-signed mode for S3A","text":"<p>In Hadoop 3.1.4 version and above (as tested using our lakeFS Hadoop FS), it is possible to use pre-signed URLs as return values from the lakeFS S3 Gateway.</p> <p>This has the immediate benefit of reducing the amount of traffic that has to go through the lakeFS server thus improving IO performance.  To read more about pre-signed URLs, see this guide.</p> <p>Here's an example Spark configuration to enable this support:</p> <pre><code>spark.hadoop.fs.s3a.impl shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem\nspark.hadoop.fs.s3a.bucket.example-repo.access.key AKIAIOSFODNN7EXAMPLE // The access key to your lakeFS server\nspark.hadoop.fs.s3a.bucket.example-repo.secret.key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY // The secret key to your lakeFS server\nspark.hadoop.fs.s3a.path.style.access true\nspark.hadoop.fs.s3a.bucket.example-repo.signing-algorithm QueryStringSignerType\nspark.hadoop.fs.s3a.bucket.example-repo.user.agent.prefix s3RedirectionSupport\n</code></pre> <p>Note</p> <p><code>user.agent.prefix</code> should contain the string <code>s3RedirectionSupport</code> but does not have to match the string exactly.</p> <p>Once configured, requests will include the string <code>s3RedirectionSupport</code> in the <code>User-Agent</code> HTTP header sent with GetObject requests, resulting in lakeFS responding with a pre-signed URL. Setting the <code>signing-algorithm</code> to <code>QueryStringSignerType</code> is required to stop S3A from signing a pre-signed URL, since the existence of more than one signature method will return an error from S3.</p> <p>Info</p> <p>This feature requires a lakeFS server of version &gt;1.18.0</p>"},{"location":"integrations/unity-catalog/","title":"Using lakeFS with the Unity Catalog","text":""},{"location":"integrations/unity-catalog/#overview","title":"Overview","text":"<p>Databricks Unity Catalog serves as a centralized data governance platform for your data lakes. Through the Unity Catalog, you can search for and locate data assets across workspaces via a unified catalog. Leveraging the external tables feature within Unity Catalog, you can register a Delta Lake table exported from lakeFS and access it through the unified catalog. </p> <p>The subsequent step-by-step guide will lead you through the process of configuring a Lua hook that exports Delta Lake tables from lakeFS, and subsequently registers them in Unity Catalog.</p> <p>Note</p> <p>Currently, Unity Catalog export feature exclusively supports AWS S3 and Azure ADLS Gen2 as the underlying storage solution. It's planned to support other cloud providers soon.</p> <p>Reference Guide: lakeFS + Unity Catalog Integration: Step-by-Step Tutorial </p>"},{"location":"integrations/unity-catalog/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have the following:</p> <ol> <li>Access to Unity Catalog</li> <li>An active lakeFS installation with S3 as the backing storage, and a repository in this installation.</li> <li>A Databricks SQL warehouse.</li> <li>AWS Credentials with S3 access.</li> <li>lakeFS credentials with access to your Delta Tables.</li> </ol> <p>Info</p> <p>Supported from lakeFS v1.4.0</p>"},{"location":"integrations/unity-catalog/#databricks-authentication","title":"Databricks authentication","text":"<p>Given that the hook will ultimately register a table in Unity Catalog, authentication with Databricks is imperative. Make sure that:</p> <ol> <li>You have a Databricks Service Principal.</li> <li>The Service principal has token usage permissions,    and an associated token    configured.</li> <li>The service principal has the <code>Service principal: Manager</code> privilege over itself (Workspace: Admin console -&gt; Service principals -&gt; <code>&lt;service principal&gt;</code> -&gt; Permissions -&gt; Grant access (<code>&lt;service principal&gt;</code>:    <code>Service principal: Manager</code>), with <code>Workspace access</code> and <code>Databricks SQL access</code> checked (Admin console -&gt; Service principals -&gt; <code>&lt;service principal&gt;</code> -&gt; Configurations).</li> <li>Your SQL warehouse allows the service principal to use it (SQL Warehouses -&gt; <code>&lt;SQL warehouse&gt;</code> -&gt; Permissions -&gt; <code>&lt;service principal&gt;</code>: <code>Can use</code>).</li> <li>The catalog grants the <code>USE CATALOG</code>, <code>USE SCHEMA</code>, <code>CREATE SCHEMA</code> permissions to the service principal(Catalog -&gt; <code>&lt;catalog name&gt;</code> -&gt; Permissions -&gt; Grant -&gt; <code>&lt;service principal&gt;</code>: <code>USE CATALOG</code>, <code>USE SCHEMA</code>, <code>CREATE SCHEMA</code>).</li> <li>You have an External Location configured, and the service principal has the <code>CREATE EXTERNAL TABLE</code> permission over it (Catalog -&gt; External Data -&gt; External Locations -&gt; Create location).</li> </ol>"},{"location":"integrations/unity-catalog/#guide","title":"Guide","text":""},{"location":"integrations/unity-catalog/#table-descriptor-definition","title":"Table descriptor definition","text":"<p>To guide the Unity Catalog exporter in configuring the table in the catalog, define its properties in the Delta Lake table descriptor.  The table descriptor should include (at minimum) the following fields:</p> <ol> <li><code>name</code>: The table name.</li> <li><code>type</code>: Should be <code>delta</code>.</li> <li><code>catalog</code>: The name of the catalog in which the table will be created.</li> <li><code>path</code>: The path in lakeFS (starting from the root of the branch) in which the Delta Lake table's data is found.</li> </ol> <p>Let's define the table descriptor and upload it to lakeFS:</p> <p>Save the following as <code>famous-people-td.yaml</code>:</p> <pre><code>---\nname: famous_people\ntype: delta\ncatalog: my-catalog-name\npath: tables/famous-people\n</code></pre> <p>Tip</p> <p>It's recommended to create a Unity catalog with the same name as your repository</p> <p>Upload the table descriptor to <code>_lakefs_tables/famous-people-td.yaml</code> and commit:</p> <pre><code>lakectl fs upload lakefs://repo/main/_lakefs_tables/famous-people-td.yaml -s ./famous-people-td.yaml &amp;&amp; \\\nlakectl commit lakefs://repo/main -m \"add famous people table descriptor\"\n</code></pre>"},{"location":"integrations/unity-catalog/#write-some-data","title":"Write some data","text":"<p>Insert data into the table path, using your preferred method (e.g. Spark), and commit upon completion.</p> <p>We shall use Spark and lakeFS's S3 gateway to write some data as a Delta table:</p> <pre><code>pyspark --packages \"io.delta:delta-spark_2.12:3.0.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262\" \\\n  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\\n  --conf spark.hadoop.fs.s3a.aws.credentials.provider='org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider' \\\n  --conf spark.hadoop.fs.s3a.endpoint='&lt;LAKEFS_SERVER_URL&gt;' \\\n  --conf spark.hadoop.fs.s3a.access.key='&lt;LAKEFS_ACCESS_KEY&gt;' \\\n  --conf spark.hadoop.fs.s3a.secret.key='&lt;LAKEFS_SECRET_ACCESS_KEY&gt;' \\\n  --conf spark.hadoop.fs.s3a.path.style.access=true\n</code></pre> <pre><code>data = [\n   ('James','Bond','England','intelligence'),\n   ('Robbie','Williams','England','music'),\n   ('Hulk','Hogan','USA','entertainment'),\n   ('Mister','T','USA','entertainment'),\n   ('Rafael','Nadal','Spain','professional athlete'),\n   ('Paul','Haver','Belgium','music'),\n]\ncolumns = [\"firstname\",\"lastname\",\"country\",\"category\"]\ndf = spark.createDataFrame(data=data, schema = columns)\ndf.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"category\", \"country\").save(\"s3a://repo/main/tables/famous-people\")\n</code></pre>"},{"location":"integrations/unity-catalog/#the-unity-catalog-exporter-script","title":"The Unity Catalog exporter script","text":"<p>Example</p> <p>For code references check delta_exporter and  unity_exporter docs.</p> <p>Create <code>unity_exporter.lua</code>:</p> <pre><code>local aws = require(\"aws\")\nlocal formats = require(\"formats\")\nlocal databricks = require(\"databricks\")\nlocal delta_export = require(\"lakefs/catalogexport/delta_exporter\")\nlocal unity_export = require(\"lakefs/catalogexport/unity_exporter\")\n\nlocal sc = aws.s3_client(args.aws.access_key_id, args.aws.secret_access_key, args.aws.region)\n\n-- Export Delta Lake tables export:\nlocal delta_client = formats.delta_client(args.lakefs.access_key_id, args.lakefs.secret_access_key, args.aws.region)\nlocal delta_table_details = delta_export.export_delta_log(action, args.table_defs, sc.put_object, delta_client, \"_lakefs_tables\")\n\n-- Register the exported table in Unity Catalog:\nlocal databricks_client = databricks.client(args.databricks_host, args.databricks_token)\nlocal registration_statuses = unity_export.register_tables(action, \"_lakefs_tables\", delta_table_details, databricks_client, args.warehouse_id)\n\nfor t, status in pairs(registration_statuses) do\n    print(\"Unity catalog registration for table \\\"\" .. t .. \"\\\" completed with commit schema status : \" .. status .. \"\\n\")\nend\n</code></pre> <p>Upload the lua script to the <code>main</code> branch under <code>scripts/unity_exporter.lua</code> and commit:</p> <pre><code>lakectl fs upload lakefs://repo/main/scripts/unity_exporter.lua -s ./unity_exporter.lua &amp;&amp; \\\nlakectl commit lakefs://repo/main -m \"upload unity exporter script\"\n</code></pre>"},{"location":"integrations/unity-catalog/#action-configuration","title":"Action configuration","text":"<p>Define an action configuration that will run the above script after a commit is completed (<code>post-commit</code>) over the <code>main</code> branch.</p> <p>Create <code>unity_exports_action.yaml</code>:</p> <pre><code>---\nname: unity_exports\non:\n  post-commit:\n     branches: [\"main\"]\nhooks:\n  - id: unity_export\n    type: lua\n    properties:\n      script_path: scripts/unity_exporter.lua\n      args:\n        aws:\n          access_key_id: &lt;AWS_ACCESS_KEY_ID&gt;\n          secret_access_key: &lt;AWS_SECRET_ACCESS_KEY&gt;\n          region: &lt;AWS_REGION&gt;\n        lakefs: # provide credentials of a user that has access to the script and Delta Table\n          access_key_id: &lt;LAKEFS_ACCESS_KEY_ID&gt; \n          secret_access_key: &lt;LAKEFS_SECRET_ACCESS_KEY&gt;\n        table_defs: # an array of table descriptors used to be defined in Unity Catalog\n          - famous-people-td\n        databricks_host: &lt;DATABRICKS_HOST_URL&gt;\n        databricks_token: &lt;DATABRICKS_SERVICE_PRINCIPAL_TOKEN&gt;\n        warehouse_id: &lt;WAREHOUSE_ID&gt;\n</code></pre> <p>Upload the action configurations to <code>_lakefs_actions/unity_exports_action.yaml</code> and commit:</p> <p>Note</p> <p>Once the commit will finish its run, the action will start running since we've configured it to run on <code>post-commit</code>  events on the <code>main</code> branch.</p> <pre><code>lakectl fs upload lakefs://repo/main/_lakefs_actions/unity_exports_action.yaml -s ./unity_exports_action.yaml &amp;&amp; \\\nlakectl commit lakefs://repo/main -m \"upload action and run it\"\n</code></pre> <p>The action has run and exported the <code>famous_people</code> Delta Lake table to the repo's storage namespace, and has register  the table as an external table in Unity Catalog under the catalog <code>my-catalog-name</code>, schema <code>main</code> (as the branch's name) and  table name <code>famous_people</code>: <code>my-catalog-name.main.famous_people</code>.</p> <p></p>"},{"location":"integrations/unity-catalog/#databricks-integration","title":"Databricks Integration","text":"<p>After registering the table in Unity, you can leverage your preferred method to query the data  from the exported table under <code>my-catalog-name.main.famous_people</code>, and view it in the Databricks's Catalog Explorer, or retrieve it using the Databricks CLI with the following command: </p> <pre><code>databricks tables get my-catalog-name.main.famous_people\n</code></pre> <p></p>"},{"location":"integrations/vertex_ai/","title":"Using Vertex AI with lakeFS","text":"<p>Vertex AI lets Google Cloud users Build, deploy, and scale machine learning (ML) models faster, with fully managed ML tools for any use case.</p> <p>lakeFS Works with Vertex AI by allowing users to create repositories on GCS Buckets, then use either the Dataset API to create managed Datasets on top of lakeFS version, or by automatically exporting lakeFS object versions in a way readable by Cloud Storage Mounts. </p>"},{"location":"integrations/vertex_ai/#using-lakefs-with-vertex-managed-datasets","title":"Using lakeFS with Vertex Managed Datasets","text":"<p>Vertex's ImageDataset and VideoDataset allow creating a dataset by importing a CSV file from gcs (see <code>gcs_source</code>).</p> <p>This CSV file contains GCS addresses of image files and their corresponding labels.</p> <p>Since the lakeFS API supports exporting the underlying GCS address of versioned objects, we can generate such a CSV file when creating the dataset:  </p> <pre><code>#!/usr/bin/env python\n\n# Requirements:\n# google-cloud-aiplatform&gt;=1.31.0\n# lakefs&gt;=1.0.0\n\nimport csv\nfrom pathlib import PosixPath\nfrom io import StringIO\n\nimport lakefs\nfrom google.cloud import storage\nfrom google.cloud import aiplatform\n\n# Dataset configuration\nlakefs_repo = 'my-repository'\nlakefs_ref = 'main'\nimg_dataset = 'datasets/my-images/'\n\n# Vertex configuration\nimport_bucket = 'underlying-gcs-bucket'\n\n# produce import file for Vertex's SDK\nbuf = StringIO()\ncsv_writer = csv.writer(buf)\nfor obj in lakefs.repository(lakefs_repo).ref(lakefs_ref).objects(prefix=img_dataset):\n    p = PosixPath(obj.path)\n    csv_writer.writerow((obj.physical_address, p.parent.name))\n\n# spit out CSV\nprint('Generated path and labels CSV')\nbuf.seek(0)\n\n# Write it to storage\nstorage_client = storage.Client()\nbucket = storage_client.bucket(import_bucket)\nblob = bucket.blob(f'vertex/imports/{lakefs_repo}/{lakefs_ref}/labels.csv')\nwith blob.open('w') as out:\n    out.write(buf.read())\n\nprint(f'Wrote CSV to gs://{import_bucket}/vertex/imports/{lakefs_repo}/{lakefs_ref}/labels.csv')\n\n# import in Vertex, as dataset\nprint('Importing dataset...')\nds = aiplatform.ImageDataset.create(\n    display_name=f'{lakefs_repo}_{lakefs_ref}_imgs',\n    gcs_source=f'gs://{import_bucket}/vertex/imports/{lakefs_repo}/{lakefs_ref}/labels.csv',\n    import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n    sync=True\n)\nds.wait()\nprint(f'Done! {ds.display_name} ({ds.resource_name})')\n</code></pre>"},{"location":"integrations/vertex_ai/#using-lakefs-with-cloud-storage-fuse","title":"Using lakeFS with Cloud Storage Fuse","text":"<p>Vertex allows using Google Cloud Storage mounted as a Fuse Filesystem as custom input for training jobs.</p> <p>Instead of having to copy lakeFS files for each version we want to consume, we can create symlinks by using gcsfuse's native symlink inodes.</p> <p>This process can be fully automated by using the example gcsfuse_symlink_exporter.lua Lua hook.</p> <p>Here's what we need to do:</p> <ol> <li>Upload the example <code>.lua</code> file into our lakeFS repository. For this example, we'll put it under <code>scripts/gcsfuse_symlink_exporter.lua</code>.</li> <li>Create a new hook definition file and upload to <code>_lakefs_actions/export_images.yaml</code>:</li> </ol> <pre><code>---\n# Example hook declaration: (_lakefs_actions/export_images.yaml):\nname: export_images\n\non:\n  post-commit:\n    branches: [\"main\"]\n  post-merge:\n    branches: [\"main\"]\n  post-create-tag:\n\nhooks:\n- id: gcsfuse_export_images\n  type: lua\n  properties:\n    script_path: scripts/export_gcs_fuse.lua  # Path to the script we uploaded in the previous step\n    args:\n      prefix: \"datasets/images/\"  # Path we want to export every commit\n      destination: \"gs://my-bucket/exports/my-repo/\"  # Where should we create the symlinks?\n      mount:\n        from: \"gs://my-bucket/repos/my-repo/\"  # Symlinks are to a unix-mounted file\n        to: \"/gcs/my-bucket/repos/my-repo/\"    #  This will ensure they point to a location that exists.\n\n      # Should be the contents of a valid credentials.json file\n      # See: https://developers.google.com/workspace/guides/create-credentials\n      # Will be used to write the symlink files\n      gcs_credentials_json_string: |\n        {\n          \"client_id\": \"...\",\n          \"client_secret\": \"...\",\n          \"refresh_token\": \"...\",\n          \"type\": \"...\"\n        }\n</code></pre> <p>Done! On the next tag creation or update to the <code>main</code> branch, we'll automatically export the lakeFS version of <code>datasets/images/</code> to a mountable location.</p> <p>To consume the symlink-ed files, we can read them normally from the mount:</p> <pre><code>with open('/gcs/my-bucket/exports/my-repo/branches/main/datasets/images/001.jpg') as f:\n    image_data = f.read()\n</code></pre> <p>Previously exported commits are also readable, if we exported them in the past:</p> <pre><code>commit_id = 'abcdef123deadbeef567'\nwith open(f'/gcs/my-bucket/exports/my-repo/commits/{commit_id}/datasets/images/001.jpg') as f:\n    image_data = f.read()\n</code></pre>"},{"location":"integrations/vertex_ai/#considerations-when-using-lakefs-with-cloud-storage-fuse","title":"Considerations when using lakeFS with Cloud Storage Fuse","text":"<p>For lakeFS paths to be readable by gcsfuse, the mount option <code>--implicit-dirs</code> must be specified.</p>"},{"location":"project/","title":"The lakeFS Project","text":"<p>lakeFS provides version control over the data lake and lakehouse, and uses Git-like semantics to create and access those versions. If you know git, you\u2019ll be right at home with lakeFS.</p> <p>lakeFS is an open-source project under the Apache 2.0 license.</p> <p>The project was created and is supported by Treeverse, a commercial company founded by engineers passionate about providing solutions to the evolving world of data engineering.</p>"},{"location":"project/#our-commitment-to-open-source","title":"Our commitment to open source","text":"<p>lakeFS is an open-source project under the Apache 2.0 license. The project was created and is supported by Treeverse, a commercial company founded by engineers passionate about providing solutions to the evolving world of data engineering.</p> <p>Why did we choose to open the source of our core capabilities?</p> <p>We believe in the bottom-up adoption of technologies. We believe collaborative communities have the power to bring the best solutions to the community. We believe that every engineer should be able to use, contribute to, and influence cutting edge technologies, so they can innovate in their domain.</p> <p>What is our commitment to open source?</p> <p>We created lakeFS, our open-source project, to provide a Git-like interface on top of object stores - so that you can fully take advantage of with any data application at any scale.</p> <p>For that reason, we commit that the following capabilities are and will remain open-source as part of lakeFS:</p> <ul> <li>All versioning capabilities,</li> <li>Git-Like interface for the versioning operations,</li> <li>Support for public object store APIs,</li> <li>Integrations with publicly available applications accessing an object store,</li> <li>CLI, API, and GUI interfaces.</li> </ul> <p>We also commit to keeping lakeFS scalable in throughput and performance.</p> <p>We are deeply committed to our community of engineers who use and contribute to the project. We are and will continue to be highly responsive and shape lakeFS together to provide the data versioning capabilities we are all looking for.</p> <p>What is lakeFS Cloud?</p> <p>Treeverse offers lakeFS Cloud, which provides all the same benefits of the Git-like interface on top of object stores as a fully-managed service.</p> <p>The vision behind lakeFS Cloud is to provide a managed data versioning and management solution for data practitioners. lakeFS Cloud will leverage the lakeFS open-source technology, integrate capabilities and unique features, and lead its users to implement best practices.</p> <p>As part of our commitment to the open source values of transparency and interoperability, we believe everyone should be able to enjoy these benefits, regardless of whether or not they choose to use the managed offering. </p> <p>Because of that, we will not intentionally make it harder to build these features independently on top of the open source solution.</p>"},{"location":"project/code-migrate-1.0-sdk/","title":"lakeFS 1.0 - Code Migration Guide","text":"<p>Version 1.0.0 promises API and SDK stability. By \"API\" we mean any access to a lakeFS REST endpoint. By \"SDK\" we mean auto-generated lakeFS clients: <code>lakefs-sdk</code> for Python and <code>io.lakefs:sdk</code> for Java. This guide details the steps to allow you to upgrade your code to enjoy this stability.</p> <p>Avoid using APIs and SDKs labeled as <code>experimental</code>, <code>internal</code>, or <code>deprecated</code>. If you must use them, be prepared to adjust your application to align with any lakeFS server updates.</p> <p>Your software developed without such APIs should be compatible with all minor version updates of the lakeFS server from the version you originally developed with.</p> <p>If you rely on a publicly released API and SDK, it will adhere to semantic versioning. Transitioning your application to a minor SDK version update should be smooth.</p> <p>The operation names and tags from the <code>api/swagger.yml</code> specification might differ based on the SDK or coding language in use.</p>"},{"location":"project/code-migrate-1.0-sdk/#deleted-api-operations","title":"Deleted API Operations","text":"<p>The following API operations have been removed:</p> <ul> <li><code>updatePassword</code></li> <li><code>forgotPassword</code></li> <li><code>logBranchCommits</code></li> <li><code>expandTemplate</code></li> <li><code>createMetaRange</code></li> <li><code>ingestRange</code></li> <li><code>updateBranchToken</code></li> </ul>"},{"location":"project/code-migrate-1.0-sdk/#internal-api-operations","title":"Internal API Operations","text":"<p>The following operations are for <code>internal</code> use only and should not be used in your application code. Some deprecated operations have alternatives provided.</p> <ul> <li><code>setupCommPrefs</code></li> <li><code>getSetupState</code></li> <li><code>setup</code></li> <li><code>getAuthCapabilities</code></li> <li><code>uploadObjectPreflight</code></li> <li><code>setGarbageCollectionRulesPreflight</code></li> <li><code>createBranchProtectionRulePreflight</code></li> <li><code>postStatsEvents</code></li> <li><code>dumpRefs</code> (will be replaced with a long-running API later)</li> <li><code>restoreRefs</code> (will be replaced with a long-running API later)</li> <li><code>createSymlinkFile</code> (Deprecated)</li> <li><code>getStorageConfig</code> (Deprecated. Alternative: <code>getConfig</code>)</li> <li><code>getLakeFSVersion</code> (Deprecated. Alternative: <code>getConfig</code>)</li> <li><code>stageObject</code> (Deprecated. Alternatives: <code>get/link physical address</code> or <code>import</code>)</li> <li><code>internalDeleteBranchProtectionRule</code> (Deprecated. Temporary backward support. Alternative: <code>setBranchProtectionRules</code>)</li> <li><code>internalCreateBranchProtectionRule</code> (Deprecated. Temporary backward support. Alternative: <code>setBranchProtectionRules</code>)</li> <li><code>internalGetBranchProtectionRule</code> (Deprecated. Temporary backward support. Alternative: <code>getBranchProtectionRules</code>)</li> <li><code>internalDeleteGarbageCollectionRules</code> (Deprecated. Temporary backward support. Alternative: <code>deleteGCRules</code>)</li> <li><code>internalSetGarbageCollectionRules</code> (Deprecated. Temporary backward support. Alternative: <code>setGCRules</code>)</li> <li><code>internalGetGarbageCollectionRules</code> (Deprecated. Temporary backward support. Alternative: <code>getGCRules</code>)</li> <li><code>prepareGarbageCollectionCommits</code></li> <li><code>getGarbageCollectionConfig</code></li> </ul>"},{"location":"project/code-migrate-1.0-sdk/#newupdated-api-operations","title":"New/Updated API Operations","text":"<p>Here are the newly added or updated operations:</p> <ul> <li><code>getConfig</code> (Retrieve lakeFS version and storage info)</li> <li><code>setBranchProtectionRules</code> (Route updated)</li> <li><code>getBranchProtectionRules</code> (Route updated)</li> <li><code>getGCRules</code> (New route introduced)</li> <li><code>setGCRules</code> (New route introduced)</li> <li><code>deleteGCRules</code> (New route introduced)</li> <li><code>importStatus</code> (Response structure updated: 'ImportStatusResp' to 'ImportStatus')</li> <li><code>uploadObject</code> (Parameters 'if-none-match' and 'storageClass' are now deprecated)</li> <li><code>prepareGarbageCollectionCommits</code> (Request body removed)</li> <li><code>getOtfDiffs</code> &amp; <code>otfDiff</code> (Removed from 'otf diff' tag; retained in 'experimental' tag)</li> </ul>"},{"location":"project/code-migrate-1.0-sdk/#migrating-sdk-code-for-java-and-jvm-based-languages","title":"Migrating SDK Code for Java and JVM-based Languages","text":""},{"location":"project/code-migrate-1.0-sdk/#introduction","title":"Introduction","text":"<p>If you are using the lakeFS client for Java or for any other JVM-based language, be aware that the current package is not stable with respect to minor version upgrades. Transitioning from <code>io.lakefs:lakefs-client</code> to <code>io.lakefs:sdk</code> will necessitate rewriting your API calls to fit the new design paradigm.</p>"},{"location":"project/code-migrate-1.0-sdk/#problem-with-the-old-style","title":"Problem with the Old Style","text":"<p>Previously, API calls required developers to pass all parameters, including optional ones, in a single function call. As demonstrated in this older style:</p> <pre><code>ObjectStats objectStat = objectsApi.statObject(\n    objectLoc.getRepository(), objectLoc.getRef(), objectLoc.getPath(),\n    false, false);\n</code></pre> <p>This method posed a couple of challenges:</p> <ol> <li>Inflexibility with Upgrades: If an optional parameter were introduced in newer versions, existing code would fail to compile.</li> <li>Maintenance Difficulty: Long argument lists can be challenging to manage and understand, leading to potential mistakes and readability issues.</li> </ol> <p>Adopting the Fluent Style</p> <p>In the revised SDK, API calls adopt a fluent style, making the code more modular and adaptive to changes.</p> <p>Here's an example of the new style:</p> <pre><code>ObjectStats objectStat = objectsApi\n    .statObject(\n        objectLoc.getRepository(), objectLoc.getRef(), objectLoc.getPath()\n    )\n    .userMetadata(true)\n    .execute();\n</code></pre>"},{"location":"project/code-migrate-1.0-sdk/#heres-a-breakdown-of-the-changes","title":"Here's a breakdown of the changes:","text":"<ol> <li>Initial Function Call: Begin by invoking the desired function with all required parameters.</li> <li>Modifying Optional Parameters: Chain any modifications to optional parameters after the initial function. For instance, <code>userMetadata</code> is changed in the example above.</li> <li>Unused Optional Parameters: You can safely ignore these. For instance, this code ignores the <code>presign</code> optional parameter because it never uses it.</li> <li>Execution: Complete the call with the <code>.execute()</code> method.</li> </ol> <p>This new design offers several advantages:</p> <ul> <li>Compatibility with Upgrades: When a new optional parameter is introduced, existing code will use its default value, preserving compatibility with minor server version upgrades.</li> <li>Improved Readability: The fluent style makes it evident which parameters are required and which ones are optional, enhancing code clarity.</li> </ul> <p>When migrating your code, ensure you refactor all your API calls to adopt the new fluent style. This ensures that your application remains maintainable and is safeguarded against potential issues arising from minor SDK version upgrades.</p> <p>For an illustrative example of the transition between styles, you can view the changes made in this pull request: lakeFS pull request #6529.</p>"},{"location":"project/code-migrate-1.0-sdk/#migrating-sdk-code-for-python","title":"Migrating SDK Code for Python","text":""},{"location":"project/code-migrate-1.0-sdk/#introduction_1","title":"Introduction","text":"<p>If you continue using the Python <code>lakefs-client</code> package for lakeFS, it's important to note that the package has reached its end of support with minor version updates. You need to switch from <code>lakefs-client</code> to <code>lakefs-sdk</code>, which will require rewriting your API calls.</p>"},{"location":"project/code-migrate-1.0-sdk/#heres-a-breakdown-of-the-changes_1","title":"Here's a breakdown of the changes:","text":"<ol> <li>Modules change</li> <li>The previous <code>model</code> module was renamed to <code>models</code>, meaning that <code>lakefs_client.model</code> imports should be replaced with <code>lakefs_sdk.models</code> imports.</li> <li>The <code>apis</code> module in <code>lakefs_client</code> is deprecated and no longer supported. To migrate to the new <code>api</code> module in <code>lakefs_sdk</code>, you should replace all imports of <code>lakefs_client.apis</code> with imports of <code>lakefs_sdk.api</code>. We still recommend using the <code>lakefs_sdk.LakeFSClient</code> class instead of using the <code>api</code> module directly. The <code>LakeFSClient</code> class provides a higher-level interface to the LakeFS API and makes it easier to use LakeFS in your applications.</li> <li><code>upload_object</code> API call: The <code>content</code> parameter value passed to the <code>objects_api.upload_object</code> method call should be either a <code>string</code> containing the path to the uploaded file, or <code>bytes</code> of data to be uploaded.</li> <li><code>get_object</code> API call: The return value of <code>client.get_object(...)</code> is a <code>bytearray</code> containing the content of the object.</li> <li><code>**client.{operation}_api**</code>: The <code>lakefs-client</code> package\u2019s <code>LakeFSClient</code> class\u2019s deprecation-marked operations (<code>client.{operation}</code>) will no longer be available in the <code>lakefs-sdk</code> package\u2019s <code>LakeFSClient</code> class. In their place, the <code>client.{operation}_api</code> should be used.</li> <li>Minimum Python Version: 3.7</li> <li>Fetching results from response objects: Instead of fetching the required results properties from a dictionary using <code>response_result.get_property(prop_name)</code>, the response objects will include domain specific entities, thus referring to the properties in the <code>results</code> of the response - <code>response_result.prop_name</code>.</li> </ol> <p>For example, instead of:</p> <pre><code>response = lakefs_client.branches.diff_branch(repository='repo', branch='main')\ndiff = response.results[0] # 'results' is a 'DiffList' object\npath = diff.get_property('path') # 'diff' is a dictionary\n</code></pre> <p>You should use:</p> <pre><code>response = lakefs_client.branches_api.diff_branch(repository='repo', branch='main')\ndiff = response.results[0] # 'results' is a 'DiffList' object\npath = diff.path # 'diff' is a 'Diff' object\n</code></pre>"},{"location":"project/contributing/","title":"Contributing to lakeFS","text":"<p>Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community.</p> <p>Please read through this document before submitting any issues or pull requests to ensure that we have all the necessary information to effectively respond to your bug report or contribution.</p> <p>Don't know where to start?</p> <p>Reach out on the #dev channel on our Slack and we will help you get started. We also recommend this free series about contributing to OSS projects.</p>"},{"location":"project/contributing/#getting-started","title":"Getting Started","text":"<p>Before you get started, we kindly ask that you:</p> <ul> <li>Check out the code of conduct.</li> <li>Sign the lakeFS CLA when making your first pull request (individual / corporate)</li> <li>Submit any security issues directly to security@treeverse.io.</li> <li>Contributions should have an associated GitHub issue. </li> <li>Before making major contributions, please reach out to us on the #dev channel on Slack.   We will make sure no one else is working on the same feature. </li> </ul>"},{"location":"project/contributing/#setting-up-an-environment","title":"Setting up an Environment","text":"<p>This section was tested on macOS and Linux (Fedora 32, Ubuntu 20.04) - Your mileage may vary</p> <p>Our Go release workflow holds the Go and Node.js versions we currently use under go-version and node-version compatibly. The Java workflows use Maven 3.8.x (but any recent version of Maven should work).</p> <ol> <li>Install the required dependencies for your OS:<ol> <li>Git</li> <li>GNU make (probably best to install from your OS package manager such as apt or brew)</li> <li>Docker</li> <li>Go</li> <li>Node.js &amp; npm</li> <li>Java 8<ul> <li>Apple M1 users can install this from Azul Zulu Builds for Java JDK. Builds for Intel-based Macs are available from java.com.</li> </ul> </li> <li>Maven <ul> <li>Required for building and testing Spark client code, as well as the hadoopfs client.</li> </ul> </li> <li>Optional - PostgreSQL 11 (useful for running and debugging locally)</li> <li>Optional - Rust &amp; Cargo (useful for building the Rust SDK)</li> <li>Optional - Buf CLI (only needed if you like to update Protocol Buffer files)</li> </ol> </li> <li> <p>Clone the repository from GitHub.</p> <p>This gives you read-only access to the repository. To contribute, see the next section.</p> </li> <li> <p>Build the project:     <pre><code>make build\n</code></pre></p> <p>Warning</p> <p><code>make build</code> won't work for Windows users.</p> </li> <li> <p>Make sure tests are passing. The following should not return any errors:     <pre><code>make test\n</code></pre></p> </li> </ol>"},{"location":"project/contributing/#before-creating-a-pull-request","title":"Before creating a pull request","text":"<ol> <li>Review this document in full.</li> <li>Make sure there's an open issue on GitHub that this pull request addresses, and that it isn't labeled <code>x/wontfix</code>.</li> <li>Fork the lakeFS repository.</li> <li>If you're adding new functionality, create a new branch named <code>feature/&lt;DESCRIPTIVE NAME&gt;</code>.</li> <li>If you're fixing a bug, create a new branch named <code>fix/&lt;DESCRIPTIVE NAME&gt;-&lt;ISSUE NUMBER&gt;</code>.</li> </ol>"},{"location":"project/contributing/#testing-your-change","title":"Testing your change","text":"<p>Once you've made the necessary changes to the code, make sure the tests pass:</p> <p>Run unit tests:</p> <pre><code>make test\n</code></pre> <p>Check that linting rules are passing. </p> <pre><code>make checks-validator\n</code></pre> <p>Note</p> <p>You will need GNU diff to run this. On the macOS it can be installed with <code>brew install diffutils</code></p> <p>lakeFS uses go fmt as a style guide for Go code.</p> <p>Run system-tests:</p> <pre><code>make system-tests\n</code></pre> <p>Info</p> <p>Want to dive deeper into our system tests infrastructure? Need to debug the tests? Follow this documentation.</p>"},{"location":"project/contributing/#submitting-a-pull-request","title":"Submitting a pull request","text":"<p>Open a GitHub pull request with your change. The PR description should include a brief explanation of your change. You should also mention the related GitHub issue. If the issue should be automatically closed after the merge, please link it to the PR.</p> <p>After submitting your pull request, GitHub Actions will automatically run tests on your changes and make sure that your updated code builds and runs on Go 1.19.x.</p> <p>Check back shortly after submitting your pull request to make sure that your code passes these checks. If any of the checks come back with a red X, then do your best to address the errors.</p> <p>A developer from our team will review your pull request, and may request some changes to it. After the request is approved, it will be merged to our main branch.</p>"},{"location":"project/contributing/#documentation","title":"Documentation","text":"<p>Any contribution to the docs, whether it is in conjunction with a code contribution or as a standalone, is appreciated.</p> <p>Documentation of features and changes in behaviour should be included in the pull request. You can create separate pull requests for documentation changes only.</p> <p>To learn how to contribute to the lakeFS documentation see this page, which also includes details on how to build the documentation locally. </p>"},{"location":"project/contributing/#changelogmd","title":"CHANGELOG.md","text":"<p>Any user-facing change should be labeled with <code>include-changelog</code>.</p> <p>The PR title should contain a concise summary of the feature or fix and the description should have the GitHub issue number. When we publish a new version of lakeFS, we will add this to the relevant version section of the changelog. If the change should not be included in the changelog, label it with <code>exclude-changelog</code>.</p>"},{"location":"project/contributing/#user-facing-changes-examples","title":"User-Facing Changes Examples","text":"<ol> <li>UI/UX modifications: Changes to the layout, color scheme, or navigation structure.</li> <li>New features: Adding functionality that users can directly interact with, unless defined as internal.</li> <li>Configuration changes: Updates to settings that users can adjust.</li> <li>Performance improvements: Enhancements that noticeably speed up the application.</li> <li>Bug fixes.</li> <li>Security updates: Changes that address vulnerabilities or privacy concerns.</li> </ol>"},{"location":"project/contributing/#non-user-facing-changes","title":"Non-User-Facing Changes:","text":"<ol> <li>Code refactoring: Restructuring the codebase without changing its external behavior.</li> <li>Backend optimizations: Improvements to server-side processes that don't noticeably affect performance.</li> <li>Database schema changes: Modifications to the data structure that don't alter the user interface and do not require data migration.</li> <li>Development tooling updates: Changes to build processes or development environments.</li> <li>Internal API: Adding/Altering APIs tagged as internal.</li> <li>Documentation updates.</li> <li>Imported libraries: Updates to third-party libraries that don't introduce security updates.</li> </ol>"},{"location":"project/docs/","title":"lakeFS Documentation","text":"<p>Any contribution to the docs, whether it is in conjunction with a code contribution or as a standalone, is appreciated.</p> <p>Please see the contributing guide for details on contributing to lakeFS in general. </p> <p>Notice</p> <p>lakeFS documentation is written using Markdown.  Make sure to familiarize yourself with the Markdown Guide.</p>"},{"location":"project/docs/#lakefs-documentation-philosophy","title":"lakeFS Documentation Philosophy","text":"<p>We are heavily inspired by the Di\u00e1taxis approach to documentation. </p> <p>At a very high-level, it defines documentation as falling into one of four categories: </p> <ul> <li>How To</li> <li>Tutorial</li> <li>Reference</li> <li>Explanation</li> </ul> <p>There is a lot more to it than this, and you are encouraged to read the Di\u00e1taxis website for more details. Its application to lakeFS was discussed in #6197</p>"},{"location":"project/docs/#lakefs-style-guide","title":"lakeFS Style Guide:","text":"<ul> <li>Don't use unnecessary tech jargon or vague/wordy constructions - keep it friendly, not condescending.</li> <li>Be inclusive and welcoming - use gender-neutral words and pronouns when talking about abstract people like users and developers.</li> <li>Replace complex expressions with simpler ones.</li> <li>Keep it short - 25-30 words max per sentence.  Otherwise, your readers might get lost on the way. </li> <li>Use active voice instead of passive. For example: This feature can be used to do task X. vs. You can use this feature to do task X. The second one reads much better, right?</li> <li>You can explain things better by including examples. Show, not tell. Use illustrations, images, gifs, code snippets, etc.</li> <li>Establish a visual hierarchy to help people quickly find the information they need. Use text formatting to create levels of title and subtitle (such as <code>#</code> to <code>######</code> markdown headings).  The title of every page should use the topmost heading <code>#</code>; all other headings on the page should use lower headers <code>##</code> to <code>######</code>.</li> </ul>"},{"location":"project/docs/#headings","title":"Headings","text":"<p>The title of the page should be H1 (<code>#</code> in markdown). Use headings in descending order and do not skip any. </p>"},{"location":"project/docs/#test-your-changes-locally","title":"Test your changes locally","text":"<p>If you have the necessary dependencies installed, you can run mkdocs to build and serve the documentation from your machine using the provided Makefile target: </p> <pre><code>make docs-serve\n</code></pre>"},{"location":"project/docs/#link-checking-locally","title":"Link Checking locally","text":"<p>When making a pull request to lakeFS that involves a <code>docs/*</code> file, a GitHub action will automagically check the links. You can also run this link checker manually on your local machine: </p> <ol> <li> <p>Build the site: </p> <pre><code>mkdocs build\n</code></pre> </li> <li> <p>Check the links: </p> <pre><code>docker run --rm \\\n        --name lakefs_docs_lychee \\\n        --volume \"$PWD:/data\"\\\n        --volume \"/tmp:/output\"\\\n        --tty \\\n        lycheeverse/lychee:master \\\n        --exclude-file /data/.lycheeignore \\\n        --output /output/lychee_report.md \\\n        --format markdown \\\n        /data/docs/site\n</code></pre> </li> <li> <p>Review the <code>lychee_report.md</code> in your local <code>/tmp</code> folder</p> </li> </ol>"},{"location":"quickstart/","title":"lakeFS Quickstart","text":"<p>Welcome to lakeFS!</p> <p>Tip</p> <p>You can use the 30-day free trial of lakeFS Cloud if you want to try out lakeFS without installing anything. </p> <p>lakeFS provides a \"Git for data\" platform enabling you to implement best practices from software engineering on your data lake, including branching and merging, CI/CD, and production-like dev/test environments. </p> <p>This quickstart will introduce you to some of the core ideas in lakeFS and show what you can do by illustrating the concept of branching, merging, and rolling back changes to data. It's laid out in five short sections: </p> <p>Start Here \ud83d\udc49</p>"},{"location":"quickstart/actions-and-hooks/","title":"Actions and Hooks in lakeFS","text":"<p>When we interact with lakeFS it can be useful to have certain checks performed at stages along the way. Let's see how actions in lakeFS can be of benefit here. </p> <p>We're going to enforce a rule that when a commit is made to any branch that begins with <code>etl</code>: </p> <ul> <li>the commit message must not be blank</li> <li>there must be <code>job_name</code> and <code>version</code> metadata</li> <li>the <code>version</code> must be numeric</li> </ul> <p>To do this we'll create an action. In lakeFS, an action specifies one or more events that will trigger it, and references one or more hooks to run when triggered. Actions are YAML files written to lakeFS under the <code>_lakefs_actions/</code> folder of the lakeFS repository.</p> <p>Hooks can be either a Lua script that lakeFS will execute itself, an external web hook, or an Airflow DAG. In this example, we're using a Lua hook.</p>"},{"location":"quickstart/actions-and-hooks/#configuring-the-action","title":"Configuring the Action","text":"<ol> <li>In lakeFS create a new branch called <code>add_action</code>. You can do this through the UI or with <code>lakectl</code>:      <pre><code>lakectl branch create lakefs://quickstart/add_action --source lakefs://quickstart/main\n</code></pre></li> <li>Open up your favorite text editor (or emacs), and paste the following YAML:      <pre><code>name: Check Commit Message and Metadata\non:\n    pre-commit:\n    branches:\n    - etl**\nhooks:\n- id: check_metadata\n    type: lua\n    properties:\n    script: |\n        commit_message=action.commit.message\n        if commit_message and #commit_message&gt;0 then\n            print(\"\u2705 The commit message exists and is not empty: \" .. commit_message)\n        else\n            error(\"\\n\\n\u274c A commit message must be provided\")\n        end\n\n        job_name=action.commit.metadata[\"job_name\"]\n        if job_name == nil then\n            error(\"\\n\u274c Commit metadata must include job_name\")\n        else\n            print(\"\u2705 Commit metadata includes job_name: \" .. job_name)\n        end\n\n        version=action.commit.metadata[\"version\"]\n        if version == nil then\n            error(\"\\n\u274c Commit metadata must include version\")\n        else\n            print(\"\u2705 Commit metadata includes version: \" .. version)\n            if tonumber(version) then\n                print(\"\u2705 Commit metadata version is numeric\")\n            else\n                error(\"\\n\u274c Version metadata must be numeric: \" .. version)\n            end\n        end\n</code></pre></li> <li> <p>Save this file as <code>/tmp/check_commit_metadata.yml</code></p> <ul> <li>You can save it elsewhere, but make sure you change the path below when uploading</li> </ul> </li> <li> <p>Upload the <code>check_commit_metadata.yml</code> file to the <code>add_action</code> branch under <code>_lakefs_actions/</code>. As above, you can use the UI (make sure you select the correct branch when you do), or with <code>lakectl</code>:</p> <pre><code>lakectl fs upload lakefs://quickstart/add_action/_lakefs_actions/check_commit_metadata.yml --source /tmp/check_commit_metadata.yml\n</code></pre> </li> <li> <p>Go to the Uncommitted Changes tab in the UI, and make sure that you see the new file in the path shown: </p> <p></p> <p>Click Commit Changes and enter a suitable message to commit this new file to the branch. </p> </li> <li> <p>Now we'll merge this new branch into <code>main</code>. From the Compare tab in the UI compare the <code>main</code> branch with <code>add_action</code> and click Merge</p> <p></p> </li> </ol>"},{"location":"quickstart/actions-and-hooks/#testing-the-action","title":"Testing the Action","text":"<p>Let's remind ourselves what the rules are that the action is going to enforce.</p> <p>When a commit is made to any branch that begins with <code>etl</code>: </p> <ul> <li>the commit message must not be blank</li> <li>there must be <code>job_name</code> and <code>version</code> metadata</li> <li>the <code>version</code> must be numeric</li> </ul> <p>We'll start by creating a branch that's going to match the <code>etl</code> pattern, and then go ahead and commit a change and see how the action works. </p> <ol> <li> <p>Create a new branch (see above instructions on how to do this if necessary) called <code>etl_20230504</code>. Make sure you use <code>main</code> as the source branch. </p> <p>In your new branch you should see the action that you created and merged above: </p> <p></p> </li> <li> <p>To simulate an ETL job we'll use the built-in DuckDB editor to run some SQL and write the result back to the lakeFS branch. </p> <p>Open the <code>lakes.parquet</code> file on the <code>etl_20230504</code> branch from the Objects tab. Replace the SQL statement with the following: </p> <pre><code>COPY (\n    WITH src AS (\n        SELECT lake_name, country, depth_m,\n            RANK() OVER ( ORDER BY depth_m DESC) AS lake_rank\n        FROM READ_PARQUET('lakefs://quickstart/etl_20230504/lakes.parquet'))\n    SELECT * FROM SRC WHERE lake_rank &lt;= 10\n) TO 'lakefs://quickstart/etl_20230504/top10_lakes.parquet'    \n</code></pre> </li> <li> <p>Head to the Uncommitted Changes tab in the UI and notice that there is now a file called <code>top10_lakes.parquet</code> waiting to be committed. </p> <p></p> <p>Now we're ready to start trying out the commit rules, and seeing what happens if we violate them.</p> </li> <li> <p>Click on Commit Changes, leave the Commit message blank, and click Commit Changes to confirm. </p> <p>Note that the commit fails because the hook did not succeed</p> <p><code>pre-commit hook aborted</code></p> <p>with the output from the hook's code displayed</p> <p><code>\u274c A commit message must be provided</code></p> <p></p> </li> <li> <p>Do the same as the previous step, but provide a message this time: </p> <p></p> <p>The commit still fails as we need to include metadata too, which is what the error tells us</p> <p><code>\u274c Commit metadata must include job_name</code></p> </li> <li> <p>Repeat the Commit Changes dialog and use the Add Metadata field to add the required metadata: </p> <p></p> <p>We're almost there, but this still fails (as it should), since the version is not entirely numeric but includes <code>v</code> and <code>\u00df</code>: </p> <p><code>\u274c Version metadata must be numeric: v1.00\u00df</code></p> <p>Repeat the commit attempt specify the version as <code>1.00</code> this time, and rejoice as the commit succeeds</p> <p></p> </li> </ol> <p>You can view the history of all action runs from the Action tab: </p> <p></p> <p>\u2190 Rollback the changes Work with lakeFS data on your local environment \u2192</p>"},{"location":"quickstart/branch/","title":"Create a Branch","text":"<p>lakeFS uses branches in a similar way to Git. It's a great way to isolate changes until, or if, we are ready to re-integrate them. lakeFS uses a zero-copy branching technique which means that it's very efficient to create branches of your data. </p> <p>Having seen the lakes data in the previous step we're now going to create a new dataset to hold data only for lakes in Denmark. Why? Well, because :)</p> <p>The first thing we'll do is create a branch for us to do this development against. We'll use the <code>lakectl</code> tool to create the branch, which we first need to configure with our credentials.  In a new terminal window run the following:</p> <pre><code>lakectl config\n</code></pre> <p>Tip</p> <p>If for some reason you get a command not found error, you can call lakectl by using <code>python -m lakectl</code> instead.</p> <p>Follow the prompts to enter the credentials that you got in the first step. Leave the Server endpoint URL as <code>http://127.0.0.1:8000</code>. </p> <p>Now that lakectl is configured, we can use it to create the branch. Run the following:</p> <pre><code>lakectl branch create lakefs://quickstart/denmark-lakes --source lakefs://quickstart/main\n</code></pre> <p>You should get a confirmation message like this:</p> <pre><code>Source ref: lakefs://quickstart/main\ncreated branch 'denmark-lakes' 3384cd7cdc4a2cd5eb6249b52f0a709b49081668bb1574ce8f1ef2d956646816\n</code></pre>"},{"location":"quickstart/branch/#transforming-the-data","title":"Transforming the Data","text":"<p>Now we'll make a change to the data. lakeFS has several native clients, as well as an S3-compatible endpoint. This means that anything that can use S3 will work with lakeFS. Pretty neat.</p> <p>We're going to use DuckDB which is embedded within the web interface of lakeFS. </p> <p>From the lakeFS Objects page select the <code>lakes.parquet</code> file to open the DuckDB editor: </p> <p></p> <p>To start with, we'll load the lakes data into a DuckDB table so that we can manipulate it. Replace the previous text in the DuckDB editor with this: </p> <pre><code>CREATE OR REPLACE TABLE lakes AS \n    SELECT * FROM READ_PARQUET('lakefs://quickstart/denmark-lakes/lakes.parquet');\n</code></pre> <p>You'll see a row count of 100,000 to confirm that the DuckDB table has been created. </p> <p>Just to check that it's the same data that we saw before we'll run the same query. Note that we are querying a DuckDB table (<code>lakes</code>), rather than using a function to query a parquet file directly. </p> <pre><code>SELECT   country, COUNT(*)\nFROM     lakes\nGROUP BY country\nORDER BY COUNT(*) \nDESC LIMIT 5;\n</code></pre> <p></p>"},{"location":"quickstart/branch/#making-a-change-to-the-data","title":"Making a Change to the Data","text":"<p>Now we can change our table, which was loaded from the original <code>lakes.parquet</code>, to remove all rows not for Denmark:</p> <pre><code>DELETE FROM lakes WHERE Country != 'Denmark';\n</code></pre> <p></p> <p>We can verify that it's worked by reissuing the same query as before:</p> <pre><code>SELECT   country, COUNT(*)\nFROM     lakes\nGROUP BY country\nORDER BY COUNT(*) \nDESC LIMIT 5;\n</code></pre> <p></p>"},{"location":"quickstart/branch/#write-the-data-back-to-lakefs","title":"Write the Data back to lakeFS","text":"<p>The changes so far have only been to DuckDB's copy of the data. Let's now push it back to lakeFS. Note the path is different this time as we're writing it to the <code>denmark-lakes</code> branch, not <code>main</code>: </p> <pre><code>COPY lakes TO 'lakefs://quickstart/denmark-lakes/lakes.parquet';\n</code></pre> <p></p>"},{"location":"quickstart/branch/#verify-that-the-datas-changed-on-the-branch","title":"Verify that the Data's Changed on the Branch","text":"<p>Let's just confirm for ourselves that the parquet file itself has the new data. We'll drop the <code>lakes</code> table just to be sure, and then query the parquet file directly:</p> <pre><code>DROP TABLE lakes;\n\nSELECT   country, COUNT(*)\nFROM     READ_PARQUET('lakefs://quickstart/denmark-lakes/lakes.parquet')\nGROUP BY country\nORDER BY COUNT(*) \nDESC LIMIT 5;\n</code></pre> <p></p>"},{"location":"quickstart/branch/#what-about-the-data-in-main","title":"What about the data in <code>main</code>?","text":"<p>So we've changed the data in our <code>denmark-lakes</code> branch, deleting swathes of the dataset. What's this done to our original data in the <code>main</code> branch? Absolutely nothing! See for yourself by running the same query as above, but against the <code>main</code> branch:</p> <p><pre><code>SELECT   country, COUNT(*)\nFROM     READ_PARQUET('lakefs://quickstart/main/lakes.parquet')\nGROUP BY country\nORDER BY COUNT(*) \nDESC LIMIT 5;\n</code></pre> </p> <p>In the next step we'll see how to commit our changes and merge our branch back into main. </p> <p>\u2190 Query the pre-populated data Merge the branch back into main \u2192</p>"},{"location":"quickstart/commit-and-merge/","title":"Committing Changes in lakeFS","text":"<p>Info</p> <p>In the previous step we branched our data from <code>main</code> into a new <code>denmark-lakes</code> branch, and overwrote the <code>lakes.parquet</code> to hold solely information about lakes in Denmark. Now we're going to commit that change (just like Git) and merge it back to main (just like git).</p> <p>Having make the change to the datafile in the <code>denmark-lakes</code> branch, we now want to commit it. There are various options for interacting with the lakeFS API, including the web interface, a Python client, and <code>lakectl</code> which is what we'll use here. Run the following from a terminal window:</p> <pre><code>lakectl commit lakefs://quickstart/denmark-lakes -m \"Create a dataset of just the lakes in Denmark\"\n</code></pre> <p>You will get confirmation of the commit including its hash. <pre><code>Branch: lakefs://quickstart/denmark-lakes\nCommit for branch \"denmark-lakes\" completed.\n\nID: ba6d71d0965fa5d97f309a17ce08ad006c0dde15f99c5ea0904d3ad3e765bd74\nMessage: Create a dataset of just the lakes in Denmark\nTimestamp: 2023-03-15 08:09:36 +0000 UTC\nParents: 3384cd7cdc4a2cd5eb6249b52f0a709b49081668bb1574ce8f1ef2d956646816\n</code></pre></p> <p>With our change committed, it's now time to merge it to back to the <code>main</code> branch. </p>"},{"location":"quickstart/commit-and-merge/#merging-branches-in-lakefs","title":"Merging Branches in lakeFS \ud83d\udd00","text":"<p>As above, we'll use <code>lakectl</code> to do this too. The syntax just requires us to specify the source and target of the merge. Run this from a terminal window.</p> <pre><code>lakectl merge lakefs://quickstart/denmark-lakes lakefs://quickstart/main\n</code></pre> <p>We can confirm that this has worked by returning to the same object view of <code>lakes.parquet</code> as before and clicking on Execute to rerun the same query. You'll see that the country row counts have changed, and only Denmark is left in the data: </p> <p></p> <p>But\u2026oh no! A slow chill creeps down your spine, and the bottom drops out of your stomach. What have you done! \ud83d\ude31 You were supposed to create a separate file of Denmark's lakes - not replace the original one</p> <p>Is all lost? Will our hero overcome the obstacles? No, and yes respectively!</p> <p>Have no fear; lakeFS can revert changes. Tune in for the final part of the quickstart to see how. </p> <p>\u2190 Create a branch of the data Rollback the changes \u2192</p>"},{"location":"quickstart/launch/","title":"Spin up the environment","text":"<p>Tip</p> <p>If you don't want to install lakeFS locally, you can use the 30-day free trial of lakeFS Cloud. Once you launch the free trial you will have access to the same content as this quickstart within the provided repository once you login.</p> <p>install and launch lakeFS:</p> <pre><code>pip install lakefs\npython -m lakefs.quickstart\n</code></pre> <p>After a few moments you should see the lakeFS container ready to use: </p> <pre><code>\u2502\n\u2502 lakeFS running in quickstart mode.\n\u2502     Login at http://127.0.0.1:8000/\n\u2502\n\u2502     Access Key ID    : AKIAIOSFOLQUICKSTART\n\u2502     Secret Access Key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\u2502\n</code></pre> <p>You're now ready to dive into lakeFS! </p> <ol> <li> <p>Open lakeFS's web interface at http://127.0.0.1:8000/</p> </li> <li> <p>Login with the quickstart credentials. </p> <ul> <li>Access Key ID: <code>AKIAIOSFOLQUICKSTART</code></li> <li>Secret Access Key: <code>wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY</code></li> </ul> </li> <li> <p>You'll notice that there aren't any repositories created yet. Click the Create Sample Repository button. </p> </li> </ol> <p></p> <p>You will see the sample repository created and the quickstart guide within it. You can follow along there, or here - it's the same :) </p> <p></p> <p>\u2190 Quickstart introduction Query the pre-populated data \u2192</p>"},{"location":"quickstart/learning-more-lakefs/","title":"Learn more about lakeFS","text":"<p>The lakeFS quickstart is just the beginning of your lakeFS journey \ud83d\udee3\ufe0f</p> <p>Here are some more resources to help you find out more about lakeFS.</p>"},{"location":"quickstart/learning-more-lakefs/#connecting-lakefs-to-your-own-object-storage","title":"Connecting lakeFS to your own object storage","text":"<p>Enjoyed the quickstart and want to try out lakeFS against your own data? Here's how to run lakeFS locally, connecting to an object store.</p> <p>Note</p> <p>Make sure the Quickstart server from the previous steps isn't also running as you'll get a port conflict.</p> AWS S3Azure Blob StorageGoogle Cloud StorageMinIO <pre><code>export LAKEFS_BLOCKSTORE_TYPE=\"s3\"\nexport AWS_ACCESS_KEY_ID=\"YourAccessKeyValue\"\nexport AWS_SECRET_ACCESS_KEY=\"YourSecretKeyValue\"\nlakefs run --local-settings\n</code></pre> <pre><code>export LAKEFS_BLOCKSTORE_TYPE=\"azure\"\nexport LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCOUNT=\"YourAzureStorageAccountName\"\nexport LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCESS_KEY=\"YourAzureStorageAccessKey\"\nlakefs run --local-settings\n</code></pre> <p><pre><code>export LAKEFS_BLOCKSTORE_TYPE=\"gs\"\nexport LAKEFS_BLOCKSTORE_GS_CREDENTIALS_JSON=\"YourGoogleServiceAccountKeyJSON\"\nlakefs run --local-settings\n</code></pre> where you will replace <code>YourGoogleServiceAccountKeyJSON</code> with JSON string that contains your Google service account key.</p> <p>If you want to use the JSON file that contains your Google service account key instead of JSON string (as in the previous command) then go to the directory where JSON file is stored and run the command with local parameters:</p> <pre><code>export LAKEFS_BLOCKSTORE_TYPE=\"gs\"\nexport LAKEFS_BLOCKSTORE_GS_CREDENTIALS_FILE=\"/myfiles/YourGoogleServiceAccountKey.json\"\nlakefs run --local-settings\n</code></pre> <p>This command will mount your present working directory (PWD) within the container and will read the JSON file from your PWD.</p> <p>To use lakeFS with MinIO (or other S3-compatible object storage), use the following example:</p> <pre><code>export LAKEFS_BLOCKSTORE_TYPE=\"s3\"\nexport LAKEFS_BLOCKSTORE_S3_FORCE_PATH_STYLE=\"true\"\nexport LAKEFS_BLOCKSTORE_S3_ENDPOINT=\"http://&lt;minio_endpoint&gt;\"\nexport LAKEFS_BLOCKSTORE_S3_DISCOVER_BUCKET_REGION=\"false\"\nexport LAKEFS_BLOCKSTORE_S3_CREDENTIALS_ACCESS_KEY_ID=\"&lt;minio_access_key&gt;\"\nexport LAKEFS_BLOCKSTORE_S3_CREDENTIALS_SECRET_ACCESS_KEY=\"&lt;minio_secret_key&gt;\"\nlakefs run --local-settings\n</code></pre>"},{"location":"quickstart/learning-more-lakefs/#deploying-lakefs","title":"Deploying lakeFS","text":"<p>Ready to do this thing for real? The deployment guides show you how to deploy lakeFS locally (including on Kubernetes) or on AWS, Azure, or GCP.</p> <p>Alternatively you might want to have a look at lakeFS Cloud which provides a fully-managed, SOC-2 compliant, lakeFS service.</p>"},{"location":"quickstart/learning-more-lakefs/#lakefs-samples","title":"lakeFS Samples","text":"<p>The lakeFS Samples GitHub repository includes some excellent examples including:</p> <ul> <li>How to implement multi-table transaction on multiple Delta Tables</li> <li>Notebooks to show integration of lakeFS with Spark, Python, Delta Lake, Airflow and Hooks.</li> <li>Examples of using lakeFS webhooks to run automated data quality checks on different branches.</li> <li>Using lakeFS branching features to create dev/test data environments for ETL testing and experimentation.</li> <li>Reproducing ML experiments with certainty using lakeFS tags.</li> </ul>"},{"location":"quickstart/learning-more-lakefs/#lakefs-community","title":"lakeFS Community","text":"<p>The lakeFS community is important to us. Our guiding principles are:</p> <ul> <li>Fully open, in code and conversation</li> <li>We learn and grow together</li> <li>Compassion and respect in every interaction</li> </ul> <p>We'd love for you to join our Slack group and come and introduce yourself on <code>#announcements-and-more</code>. Or just lurk and soak up the vibes \ud83d\ude0e</p> <p>If you're interested in getting involved in the development of lakeFS, head over our the GitHub repo to look at the code and peruse the issues. The comprehensive contributing document should have you covered on next steps but if you've any questions the <code>#dev</code> channel on Slack will be delighted to help.</p> <p>We love speaking at meetups and chatting to community members at them - you can find a list of these here.</p> <p>Finally, make sure to drop by to say hi on Twitter or LinkedIn \ud83d\udc4b\ud83c\udffb</p>"},{"location":"quickstart/learning-more-lakefs/#lakefs-concepts-and-internals","title":"lakeFS Concepts and Internals","text":"<p>We describe lakeFS as \"Git for data\" but what does that actually mean? Have a look at the concepts and architecture guides, as well as the explanation of how merges are handled. To go deeper you might be interested in the internals of versioning and our internal database structure.</p> <p>\u2190 Work with lakeFS data on your local environment</p>"},{"location":"quickstart/query/","title":"Let's Query Something","text":"<p>The lakeFS server has been loaded with a sample parquet datafile. Fittingly enough for a piece of software to help users of data lakes, the <code>lakes.parquet</code> file holds data about lakes around the world. </p> <p>You'll notice that the branch is set to <code>main</code>. This is conceptually the same as your main branch in Git against which you develop software code. </p> <p></p> <p>Let's have a look at the data, ahead of making some changes to it on a branch in the following steps. </p> <p>Click on <code>lakes.parquet</code> and notice that the built-it DuckDB runs a query to show a preview of the file's contents. </p> <p></p> <p>Now we'll run our own query on it to look at the top five countries represented in the data. </p> <p>Copy and paste the following SQL statement into the DuckDB query panel and click on Execute.</p> <pre><code>SELECT   country, COUNT(*)\nFROM     READ_PARQUET('lakefs://quickstart/main/lakes.parquet')\nGROUP BY country\nORDER BY COUNT(*) \nDESC LIMIT 5;\n</code></pre> <p></p> <p>Next we're going to make some changes to the data\u2014but on a development branch so that the data in the main branch remains untouched. </p> <p>\u2190 Launch the quickstart environment Create a branch of the data \u2192</p>"},{"location":"quickstart/rollback/","title":"Rolling back Changes in lakeFS","text":"<p>Our intrepid user (you) merged a change back into the <code>main</code> branch and realised that they had made a mistake \ud83e\udd26\ud83c\udffb. </p> <p>The good news for them (you) is that lakeFS can revert changes made, similar to how you would in Git \ud83d\ude05. </p> <p>From your terminal window run <code>lakectl</code> with the <code>revert</code> command:</p> <p><pre><code>lakectl branch revert lakefs://quickstart/main main --parent-number 1 --yes\n</code></pre> You should see a confirmation of a successful rollback: <pre><code>Branch: lakefs://quickstart/main\ncommit main successfully reverted\n</code></pre></p> <p>Back in the object page and the DuckDB query we can see that the original file is now back to how it was:  </p> <p>\u2190 Merge the branch back into main Using Actions and Hooks in lakeFS \u2192</p>"},{"location":"quickstart/work-with-data-locally/","title":"Work with lakeFS Data Locally","text":"<p>When working with lakeFS, there are scenarios where we need to access and manipulate data locally. An example use case for working locally is machine learning model development. Machine learning model development is dynamic and iterative. To optimize this process, experiments need to be conducted with speed, tracking ease, and reproducibility. Localizing model data during development accelerates the process by enabling interactive and offline development and reducing data access latency.</p> <p>lakeFS provides 2 ways to expose versioned data locally</p>"},{"location":"quickstart/work-with-data-locally/#lakefs-mount","title":"lakeFS Mount","text":"<p>Info</p> <p>lakeFS Mount is available for lakeFS Enterprise and lakeFS Cloud customers. You can try it out by signing up</p>"},{"location":"quickstart/work-with-data-locally/#getting-started-with-lakefs-mount","title":"Getting started with lakeFS Mount","text":"<p>Prerequisites:</p> <ul> <li>A working lakeFS Server running either lakeFS Enterprise or lakeFS Cloud</li> <li>You\u2019ve installed the lakectl command line utility: this is the official lakeFS command line interface, on top of which lakeFS Mount is built.</li> <li>lakectl is configured properly to access your lakeFS server as detailed in the configuration instructions</li> </ul>"},{"location":"quickstart/work-with-data-locally/#mounting-a-path-to-a-local-directory","title":"Mounting a path to a local directory:","text":"<ol> <li> <p>In lakeFS create a new branch called <code>my-experiment</code>. You can do this through the UI or with <code>lakectl</code>:</p> <pre><code>lakectl branch create \\\n    lakefs://quickstart/my-experiment \\\n    --source lakefs://quickstart/main\n</code></pre> </li> <li> <p>Mount images from your quickstart repository into a local directory named <code>my_local_dir</code></p> <pre><code>everest mount lakefs://quickstart/my-experiment/images my_local_dir\n</code></pre> <p>Once complete, <code>my_local_dir</code> should be mounted with the specified path.</p> </li> <li> <p>Verify that <code>my_local_dir</code> is linked to the correct path in your lakeFS remote:     <pre><code>ls -l my_local_dir\n</code></pre></p> </li> <li> <p>To unmount the directory, simply run:</p> <pre><code>everest umount ./my_local_dir\n</code></pre> <p>Which will unmount the path and terminate the local mount-server.</p> </li> </ol>"},{"location":"quickstart/work-with-data-locally/#lakectl-local","title":"lakectl local","text":"<p>Alternatively, we can use lakectl local to bring a subset of our lakeFS data to a local directory within the lakeFS container and edit an image dataset used for ML model development. Unlike lakeFS Mount, using <code>lakectl local</code> requires copying data to/from lakeFS and your local machine.</p> <p>Reference Guide: lakeFS lakectl local for machine learning</p>"},{"location":"quickstart/work-with-data-locally/#cloning-a-subset-of-lakefs-data-into-a-local-directory","title":"Cloning a Subset of lakeFS Data into a Local Directory","text":"<ol> <li>In lakeFS create a new branch called <code>my-experiment</code>. You can do this through the UI or with <code>lakectl</code>:     <pre><code>lakectl branch create lakefs://quickstart/my-experiment --source lakefs://quickstart/main\n</code></pre></li> <li>Clone images from your quickstart repository into a local directory named <code>my_local_dir</code> within your container:     <pre><code>lakectl local clone lakefs://quickstart/my-experiment/images my_local_dir\n</code></pre></li> <li>Verify that <code>my_local_dir</code> is linked to the correct path in your lakeFS remote:     <pre><code>lakectl local list\n</code></pre>    You should see confirmation that my_local_dir is tracking the desired lakeFS path.:    <pre><code>    my_local_dir lakefs://quickstart/my-experiment/images/8614575b5488b47a094163bd17a12ed0b82e0bcbfd22ed1856151c671f1faa53\n</code></pre></li> <li> <p>Verify that your local environment is up-to-date with its remote path:</p> <pre><code>lakectl local status my_local_dir\n</code></pre> <p>You should get a confirmation message like this showing that there is no difference between your local environment and the lakeFS remote:</p> <pre><code>diff 'local:///home/lakefs/my_local_dir' &lt;--&gt; 'lakefs://quickstart/8614575b5488b47a094163bd17a12ed0b82e0bcbfd22ed1856151c671f1faa53/images/'...\ndiff 'lakefs://quickstart/8614575b5488b47a094163bd17a12ed0b82e0bcbfd22ed1856151c671f1faa53/images/' &lt;--&gt; 'lakefs://quickstart/my-experiment/images/'...\n\nNo diff found.\n</code></pre> </li> </ol>"},{"location":"quickstart/work-with-data-locally/#making-changes-to-data-locally","title":"Making Changes to Data Locally","text":"<ol> <li>Clean the dataset by removing images larger than 225 KB:     <pre><code>find my_local_dir -type f -size +225k -delete\n</code></pre></li> <li> <p>Check the status of your local changes compared to the lakeFS remote path:     <pre><code>lakectl local status my_local_dir\n</code></pre></p> <p>You should get a confirmation message like this, showing the modifications you made locally: <pre><code>diff 'local:///home/lakefs/my_local_dir' &lt;--&gt; 'lakefs://quickstart/8614575b5488b47a094163bd17a12ed0b82e0bcbfd22ed1856151c671f1faa53/images/'...\ndiff 'lakefs://quickstart/8614575b5488b47a094163bd17a12ed0b82e0bcbfd22ed1856151c671f1faa53/images/' &lt;--&gt; 'lakefs://quickstart/my-experiment/images/'...\n\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 SOURCE \u2551 CHANGE   \u2551 PATH                \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 local  \u2551 modified \u2551 axolotl.png         \u2551\n\u2551 local  \u2551 removed  \u2551 duckdb-main-02.png  \u2551\n\u2551 local  \u2551 removed  \u2551 empty-repo-list.png \u2551\n\u2551 local  \u2551 removed  \u2551 repo-contents.png   \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n</code></pre></p> </li> </ol>"},{"location":"quickstart/work-with-data-locally/#pushing-local-changes-to-lakefs","title":"Pushing Local Changes to lakeFS","text":"<p>Once we are done with editing the image dataset in our local environment, we will push our changes to the lakeFS remote so that the improved dataset is shared and versioned.</p> <ol> <li> <p>Commit your local changes to lakeFS:</p> <pre><code>lakectl local commit -m 'Deleted images larger than 225KB in size and changed the Axolotl image' my_local_dir\n</code></pre> <p>In your branch, you should see the commit including your local changes:</p> <p></p> </li> <li> <p>Compare <code>my-experiment</code> branch to the <code>main</code> branch to visualize your changes:</p> <p></p> </li> </ol> <p>Bonus Challenge</p> <p>And so with that, this quickstart for lakeFS draws to a close. If you're simply having too much fun to stop then here's an exercise for you.</p> <p>Implement the requirement from the beginning of this quickstart correctly, such that you write <code>denmark-lakes.parquet</code> in the respective branch and successfully merge it back into main. Look up how to list the contents of the <code>main</code> branch and verify that it looks like this:</p> <pre><code>object          2023-03-21 17:33:51 +0000 UTC    20.9 kB         denmark-lakes.parquet\nobject          2023-03-21 14:45:38 +0000 UTC    916.4 kB        lakes.parquet\n</code></pre> <p>\u2190 Using Actions and Hooks in lakeFS Learn more about lakeFS \u2192</p>"},{"location":"reference/","title":"lakeFS Reference","text":""},{"location":"reference/#api","title":"API","text":"<ul> <li>lakeFS API</li> <li>S3 Gateway API</li> </ul>"},{"location":"reference/#components","title":"Components","text":"<ul> <li>Server Configuration</li> <li>lakeFS command-line tool lakectl</li> </ul>"},{"location":"reference/#clients","title":"Clients","text":"<ul> <li>Spark Metadata Client</li> <li>lakeFS Hadoop FileSystem</li> <li>Python Client</li> </ul>"},{"location":"reference/#security","title":"Security","text":"<ul> <li>Authentication</li> <li>Remote Authenticator</li> <li>Role-Based Access Control (RBAC)</li> <li>Presigned URL</li> <li>Access Control Lists (ACLs)</li> <li>Single Sign On (SSO)</li> <li>Login to lakeFS with AWS IAM</li> </ul>"},{"location":"reference/#other-reference-documentation","title":"Other Reference Documentation","text":"<ul> <li>Monitoring using Prometheus</li> <li>Auditing</li> </ul>"},{"location":"reference/api/","title":"lakeFS API","text":""},{"location":"reference/auditing/","title":"Auditing","text":"<p>Info</p> <p>This feature is available on lakeFS Cloud and lakeFS Enterprise</p> <p>The lakeFS audit log allows you to view all relevant user action information in a clear and organized table, including when the action was performed, by whom, and what it was they did. </p> <p>This can be useful for several purposes, including: </p> <ol> <li> <p>Compliance - Audit logs can be used to show what data users accessed, as well as any changes they made to user management.</p> </li> <li> <p>Troubleshooting - If something changes on your underlying object store that you weren't expecting, such as a big file suddenly breaking into thousands of smaller files, you can use the audit log to find out what action led to this change. </p> </li> </ol>"},{"location":"reference/auditing/#setting-up-access-to-audit-logs-on-aws-s3","title":"Setting up access to Audit Logs on AWS S3","text":"<p>The access to the Audit Logs is done via AWS S3 Access Point.</p> <p>There are different ways to interact with an access point (see Using access points in AWS).</p> <p>The initial setup:</p> <ol> <li>Take note of the IAM Role ARN that will be used to access the data. This should be the user or role used by e.g. Athena.</li> <li>Reach out to customer success and provide this ARN. Once receiving the ARN role, an access point will be created and you should get in response the following details:<ol> <li>S3 Bucket (e.g. <code>arn:aws:s3:::lakefs-audit-logs-us-east-1-production</code>)</li> <li>S3 URI to an access point (e.g. <code>s3://arn:aws:s3:us-east-1:&lt;treeverse-id&gt;:accesspoint/lakefs-logs-&lt;organization&gt;</code>)</li> <li>Access Point alias. You can use this alias instead of the bucket name or Access Point ARN to access data through the Access Point. (e.g. <code>lakefs-logs-&lt;generated&gt;-s3alias</code>)</li> <li>Update your IAM Role policy and trust policy if required</li> </ol> </li> </ol> <p>A minimal example for IAM policy with 2 lakeFS installations in 2 regions (<code>us-east-1</code>, <code>us-west-2</code>):</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::lakefs-audit-logs-us-east-1-production\",\n                \"arn:aws:s3:::lakefs-audit-logs-us-east-1-production/*\",\n                \"arn:aws:s3:::lakefs-logs-&lt;generated&gt;-s3alias/*\",\n                \"arn:aws:s3:us-east-1:&lt;treeverse-id&gt;:accesspoint/lakefs-logs-&lt;organization&gt;\",\n                \"arn:aws:s3:us-east-1:&lt;treeverse-id&gt;:accesspoint/lakefs-logs-&lt;organization&gt;/*\"\n            ],\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"etl/v1/data/region=&lt;region_a&gt;/organization=org-&lt;organization&gt;/*\",\n                        \"etl/v1/data/region=&lt;region_b&gt;/organization=org-&lt;organization&gt;/*\"\n                    ]\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::lakefs-audit-logs-us-east-1-production\",\n                \"arn:aws:s3:::lakefs-audit-logs-us-east-1-production/etl/v1/data/region=&lt;region_a&gt;/organization=org-&lt;organization&gt;/*\",\n                \"arn:aws:s3:::lakefs-audit-logs-us-east-1-production/etl/v1/data/region=&lt;region_b&gt;/organization=org-&lt;organization&gt;/*\",\n                \"arn:aws:s3:::lakefs-logs-&lt;generated&gt;-s3alias/*\",\n                \"arn:aws:s3:us-east-1:&lt;treeverse-id&gt;:accesspoint/lakefs-logs-&lt;organization&gt;/object/etl/v1/data/region=&lt;region_a&gt;/organization=org-&lt;organization&gt;/*\",\n                \"arn:aws:s3:us-east-1:&lt;treeverse-id&gt;:accesspoint/lakefs-logs-&lt;organization&gt;/object/etl/v1/data/region=&lt;region_b&gt;/organization=org-&lt;organization&gt;/*\"\n            ]\n        },\n        {\n            \"Action\": [\n                \"kms:Decrypt\"\n            ],\n            \"Resource\": [\n                \"arn:aws:kms:us-east-1:&lt;treeverse-id&gt;:key/&lt;encryption-key-id&gt;\"\n            ],\n            \"Effect\": \"Allow\"\n        }\n    ]\n}\n</code></pre> <p>Trust Policy example that allows anyone in your account to assume the role above:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::&lt;YOUR_ACCOUNT_ID&gt;:root\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {}\n        }\n    ]\n}\n</code></pre> <p>Authentication is done by assuming an IAM Role:</p> <pre><code># Assume role use AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN:\naws sts assume-role --role-arn arn:aws:iam::&lt;your-aws-account&gt;:role/&lt;reader-role&gt; --role-session-name &lt;name&gt; \n\n# verify role assumed\naws sts get-caller-identity \n\n# list objects (can be used with --recursive) with access point ARN\naws s3 ls arn:aws:s3:us-east-1:&lt;treeverse-id&gt;:accesspoint/lakefs-logs-&lt;organization&gt;/etl/v1/data/region=&lt;region&gt;/organization=org-&lt;organization&gt;/\n\n# get object locally via s3 access point alias \naws s3api get-object --bucket lakefs-logs-&lt;generated&gt;-s3alias --key etl/v1/data/region=&lt;region&gt;/organization=org-&lt;organization&gt;/year=&lt;YY&gt;/month=&lt;MM&gt;/day=&lt;DD&gt;/hour=&lt;HH&gt;/&lt;file&gt;-snappy.parquet sample.parquet \n</code></pre>"},{"location":"reference/auditing/#data-layout","title":"Data layout","text":"<p>Tip</p> <p>The bucket name is important when creating the IAM policy but, the Access Point ARN and Alias will be the ones that are used to access the data (i.e AWS CLI, Spark etc).</p> <p>Bucket Name: <code>lakefs-audit-logs-us-east-1-production</code></p> <p>Root prefix: <code>etl/v1/data/region=&lt;region&gt;/organization=org-&lt;organization-name&gt;/</code></p> <p>Files Path pattern: All the audit logs files are in parquet format and their pattern is: <code>etl/v1/data/region=&lt;region&gt;/organization=org-&lt;organization-name&gt;/year=&lt;YY&gt;/month=&lt;MM&gt;/day=&lt;DD&gt;/hour=&lt;HH&gt;/*-snappy.parquet</code></p>"},{"location":"reference/auditing/#path-values","title":"Path Values","text":"<p>region: lakeFS installation region (e.g the region in lakeFS URL: https://..lakefscloud.io/) <p>organization: Found in the lakeFS URL <code>https://&lt;organization-name&gt;.&lt;region&gt;.lakefscloud.io/</code>. The value in the S3 path must be prefixed with <code>org-&lt;organization-name&gt;</code></p>"},{"location":"reference/auditing/#partitions","title":"Partitions","text":"<ul> <li><code>year</code></li> <li><code>month</code></li> <li><code>day</code></li> <li><code>hour</code></li> </ul>"},{"location":"reference/auditing/#example","title":"Example","text":"<p>As an example paths for \"Acme\" organization with 2 lakeFS installations:</p> <pre><code># ACME in us-east-1 \netl/v1/data/region=us-east-1/organization=org-acme/year=2024/month=02/day=12/hour=13/log_abc-snappy.parquet\n\n# ACME in us-west-2 \netl/v1/data/region=us-west-2/organization=org-acme/year=2024/month=02/day=12/hour=13/log_xyz-snappy.parquet\n</code></pre>"},{"location":"reference/auditing/#schema","title":"Schema","text":"<p>The files are in parquet format and can be accessed directly from Spark or any client that can read parquet files. Using Spark's <code>printSchema()</code> we can inspect the values, that\u2019s the latest schema with comments on important columns:</p> column type description <code>data_user</code> string the internal user ID for the user making the request. if using an external IdP (i.e SSO, Microsoft Entra, etc) it will be the UID represented by the IdP. (see below an  example how to extract the info of external IDs in python) <code>data_repository</code> string the repository ID relevant for this request. Currently only returned for s3_gateway requests <code>data_ref</code> string the reference ID (tag, branch, ...) relevant for this request. Currently only returned for s3_gateway requests <code>data_status_code</code> int HTTP status code returned for this request <code>data_service_name</code> string Service name for the request. Could be either \"rest_api\" or \"s3_gateway\" <code>data_request_id</code> string Unique ID representing this request <code>data_path</code> string HTTP path used for this request <code>data_operation_id</code> string Logical operation ID for this request. E.g. <code>list_objects</code>, <code>delete_repository</code>, ... <code>data_method</code> string HTTP method for the request <code>data_time</code> string datetime representing the start time of this request, in ISO 8601 format"},{"location":"reference/auditing/#idp-users-map-user-ids-from-audit-logs-to-an-email-in-lakefs","title":"IdP users: map user IDs from audit logs to an email in lakeFS","text":"<p>The <code>data_user</code> column in each log represents the user id that performed it.</p> <ul> <li>It might be empty in cases where authentication is not required (e.g login attempt).</li> <li>If the user is an API user created internally in lakeFS that id is also the name it was given.</li> <li><code>data_user</code> might contain an ID to an external IdP (i.e. SSO system), usually it is not human friendly, we can correlate the ID to a lakeFS email used, see an example using the Python lakefs-sdk.</li> </ul> <pre><code>import lakefs_sdk\n\n# Configure HTTP basic authorization: basic_auth\nconfiguration = lakefs_sdk.Configuration(\n    host = \"https://&lt;org&gt;.&lt;region&gt;.lakefscloud.io/api/v1\",\n    username = 'AKIA...',\n    password = '...'\n)\n\n# Print all user email and uid in lakeFS \n# the uid is equal to the user id in the audit logs.\nwith lakefs_sdk.ApiClient(configuration) as api_client:\n    auth_api = lakefs_sdk.AuthApi(api_client)\n    has_more = True\n    next_offset = ''\n    page_size = 100 \n    while has_more: \n        resp = auth_api.list_users(prefix='', after=next_offset, amount=page_size)\n        for u in resp.results:\n            email = u.email\n            uid = u.id\n            print(f'Email: {email}, UID: {uid}')\n\n        has_more = resp.pagination.has_more \n        next_offset = resp.pagination.next_offset\n</code></pre> <p>Example: Glue Notebook with Spark</p> <pre><code>from awsglue.transforms import *\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n\n# connect to s3 access point \nalias = 's3://&lt;bucket-alias-name&gt;'\ns3_dyf = glueContext.create_dynamic_frame.from_options(\n    format_options={},\n    connection_type=\"s3\",\n    format=\"parquet\",\n    connection_options={\n        \"paths\": [alias + \"/etl/v1/data/region=&lt;region&gt;/organization=org-&lt;org&gt;/year=&lt;YY&gt;/month=&lt;MM&gt;/day=&lt;DD&gt;/hour=&lt;HH&gt;/\"],\n        \"recurse\": True,\n    },\n    transformation_ctx=\"sample-ctx\",\n)\n\ns3_dyf.show()\ns3_dyf.printSchema()\n</code></pre>"},{"location":"reference/cli/","title":"lakectl (lakeFS command-line tool)","text":"<p>Note</p> <p>This file (cli.md) is automatically generated from the Go code files under <code>cmd/lakectl</code>.  Any changes made directly to the Markdown file will be overwritten, and should instead be made to the relevant Go files. </p>"},{"location":"reference/cli/#installing-lakectl-locally","title":"Installing lakectl locally","text":"<p><code>lakectl</code> is available for Linux, macOS, and Windows. You can also run it using Docker.</p> <p> Download lakectl</p> <p>Or using Homebrew for Linux/macOS:</p> <pre><code>brew tap treeverse/lakefs\nbrew install lakefs\n</code></pre>"},{"location":"reference/cli/#configuring-credentials-and-api-endpoint","title":"Configuring credentials and API endpoint","text":"<p>Once you've installed the lakectl command, run:</p> <pre><code>lakectl config\n# output:\n# Config file /home/janedoe/.lakectl.yaml will be used\n# Access key ID: AKIAIOSFODNN7EXAMPLE\n# Secret access key: ****************************************\n# Server endpoint URL: http://localhost:8000\n</code></pre> <p>This will setup a <code>$HOME/.lakectl.yaml</code> file with the credentials and API endpoint you've supplied. When setting up a new installation and creating initial credentials (see Quickstart), the UI will provide a link to download a preconfigured configuration file for you.</p> <p><code>lakectl</code> configuration items can each be controlled by an environment variable. The variable name will have a prefix of LAKECTL_, followed by the name of the configuration, replacing every '.' with a '_'. Example: <code>LAKECTL_SERVER_ENDPOINT_URL</code>  controls <code>server.endpoint_url</code>.</p>"},{"location":"reference/cli/#running-lakectl-from-docker","title":"Running lakectl from Docker","text":"<p>If you'd rather run <code>lakectl</code> from a Docker container you can do so by passing configuration elements as environment variables.  Here is an example: </p> <pre><code>docker run --rm --pull always \\\n          -e LAKECTL_CREDENTIALS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE \\\n          -e LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY=xxxxx\n          -e LAKECTL_SERVER_ENDPOINT_URL=https://host.us-east-2.lakefscloud.io/ \\\n          --entrypoint lakectl treeverse/lakefs \\\n          repo list\n</code></pre> <p>Bear in mind that if you are running lakeFS itself locally you will need to account for this in your networking configuration of  the Docker container. That is to say, <code>localhost</code> to a Docker container is itself, not the host machine on which it is running.</p>"},{"location":"reference/cli/#command-reference","title":"Command Reference","text":""},{"location":"reference/cli/#lakectl","title":"lakectl","text":"<p>A cli tool to explore manage and work with lakeFS</p> Synopsis <p>lakectl is a CLI tool allowing exploration and manipulation of a lakeFS environment</p> <pre><code>lakectl [flags]\n</code></pre> Options <pre><code>      --base-uri string      base URI used for lakeFS address parse\n  -c, --config string        config file (default is $HOME/.lakectl.yaml)\n  -h, --help                 help for lakectl\n      --log-format string    set logging output format\n      --log-level string     set logging level (default \"none\")\n      --log-output strings   set logging output(s)\n      --no-color             don't use fancy output colors (default value can be set by NO_COLOR environment variable)\n      --verbose              run in verbose mode\n  -v, --version              version for lakectl\n</code></pre> <p>Note</p> <p>The <code>base-uri</code> option can be controlled with the <code>LAKECTL_BASE_URI</code> environment variable.</p> Example usage <pre><code>$ export LAKECTL_BASE_URI=\"lakefs://my-repo/my-branch\"\n# Once set, use relative lakefs uri's:\n$ lakectl fs ls /path\n</code></pre>"},{"location":"reference/cli/#lakectl-actions","title":"lakectl actions","text":"<p>Manage Actions commands</p> Options <pre><code>  -h, --help   help for actions\n</code></pre>"},{"location":"reference/cli/#lakectl-actions-help","title":"lakectl actions help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type actions help [path to command] for full details.</p> <pre><code>lakectl actions help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-actions-runs","title":"lakectl actions runs","text":"<p>Explore runs information</p> Options <pre><code>  -h, --help   help for runs\n</code></pre>"},{"location":"reference/cli/#lakectl-actions-runs-describe","title":"lakectl actions runs describe","text":"<p>Describe run results</p> Synopsis <p>Show information about the run and all the hooks that were executed as part of the run</p> <pre><code>lakectl actions runs describe &lt;repository URI&gt; &lt;run_id&gt; [flags]\n</code></pre> Examples <pre><code>lakectl actions runs describe lakefs://my-repo 20230719152411arS0z6I\n</code></pre> Options <pre><code>      --after string   show results after this value (used for pagination)\n      --amount int     number of results to return. By default, all results are returned.\n  -h, --help           help for describe\n</code></pre>"},{"location":"reference/cli/#lakectl-actions-runs-help","title":"lakectl actions runs help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type runs help [path to command] for full details.</p> <pre><code>lakectl actions runs help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-actions-runs-list","title":"lakectl actions runs list","text":"<p>List runs</p> Synopsis <p>List all runs on a repository optional filter by branch or commit</p> <pre><code>lakectl actions runs list &lt;repository URI&gt; [--branch &lt;branch&gt;] [--commit &lt;commit_id&gt;] [flags]\n</code></pre> Examples <pre><code>lakectl actions runs list lakefs://my-repo --branch my-branch --commit 600dc0ffee\n</code></pre> Options <pre><code>      --branch string   show results for specific branch\n      --commit string   show results for specific commit ID\n      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-actions-validate","title":"lakectl actions validate","text":"<p>Validate action file</p> Synopsis <p>Tries to parse the input action file as lakeFS action file</p> <pre><code>lakectl actions validate [flags]\n</code></pre> Examples <pre><code>lakectl actions validate path/to/my/file\n</code></pre> Options <pre><code>  -h, --help   help for validate\n</code></pre>"},{"location":"reference/cli/#lakectl-annotate","title":"lakectl annotate","text":"<p>List entries under a given path, annotating each with the latest modifying commit</p> <pre><code>lakectl annotate &lt;path URI&gt; [flags]\n</code></pre> Options <pre><code>      --first-parent   follow only the first parent commit upon seeing a merge commit\n  -h, --help           help for annotate\n  -r, --recursive      recursively annotate all entries under a given path or prefix\n</code></pre>"},{"location":"reference/cli/#lakectl-auth","title":"lakectl auth","text":"<p>Manage authentication and authorization</p> Synopsis <p>Manage authentication and authorization including users, groups and ACLs This functionality is supported with an external auth service only.</p> Options <pre><code>  -h, --help   help for auth\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups","title":"lakectl auth groups","text":"<p>Manage groups</p> Options <pre><code>  -h, --help   help for groups\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-acl","title":"lakectl auth groups acl","text":"<p>Manage ACLs</p> Synopsis <p>manage ACLs of groups</p> Options <pre><code>  -h, --help   help for acl\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-acl-get","title":"lakectl auth groups acl get","text":"<p>Get ACL of group</p> <pre><code>lakectl auth groups acl get [flags]\n</code></pre> Options <pre><code>  -h, --help        help for get\n      --id string   Group identifier\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-acl-help","title":"lakectl auth groups acl help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type acl help [path to command] for full details.</p> <pre><code>lakectl auth groups acl help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-acl-set","title":"lakectl auth groups acl set","text":"<p>Set ACL of group</p> Synopsis <p>Set ACL of group. permission will be attached to all repositories.</p> <pre><code>lakectl auth groups acl set [flags]\n</code></pre> Options <pre><code>  -h, --help                help for set\n      --id string           Group identifier\n      --permission string   Permission, typically one of \"Read\", \"Write\", \"Super\" or \"Admin\"\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-create","title":"lakectl auth groups create","text":"<p>Create a group</p> <pre><code>lakectl auth groups create [flags]\n</code></pre> Options <pre><code>  -h, --help        help for create\n      --id string   Group identifier\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-delete","title":"lakectl auth groups delete","text":"<p>Delete a group</p> <pre><code>lakectl auth groups delete [flags]\n</code></pre> Options <pre><code>  -h, --help        help for delete\n      --id string   Group identifier\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-help","title":"lakectl auth groups help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type groups help [path to command] for full details.</p> <pre><code>lakectl auth groups help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-list","title":"lakectl auth groups list","text":"<p>List groups</p> <pre><code>lakectl auth groups list [flags]\n</code></pre> Options <pre><code>      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-members","title":"lakectl auth groups members","text":"<p>Manage group user memberships</p> Options <pre><code>  -h, --help   help for members\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-members-add","title":"lakectl auth groups members add","text":"<p>Add a user to a group</p> <pre><code>lakectl auth groups members add [flags]\n</code></pre> Options <pre><code>  -h, --help          help for add\n      --id string     Group identifier\n      --user string   Username (email for password-based users, default: current user)\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-members-help","title":"lakectl auth groups members help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type members help [path to command] for full details.</p> <pre><code>lakectl auth groups members help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-members-list","title":"lakectl auth groups members list","text":"<p>List users in a group</p> <pre><code>lakectl auth groups members list [flags]\n</code></pre> Options <pre><code>      --id string       Group identifier\n      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-members-remove","title":"lakectl auth groups members remove","text":"<p>Remove a user from a group</p> <pre><code>lakectl auth groups members remove [flags]\n</code></pre> Options <pre><code>  -h, --help          help for remove\n      --id string     Group identifier\n      --user string   Username (email for password-based users, default: current user)\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-policies","title":"lakectl auth groups policies","text":"<p>Manage group policies</p> Synopsis <p>Manage group policies.  Requires an external authorization server with matching support.</p> Options <pre><code>  -h, --help   help for policies\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-policies-attach","title":"lakectl auth groups policies attach","text":"<p>Attach a policy to a group</p> <pre><code>lakectl auth groups policies attach [flags]\n</code></pre> Options <pre><code>  -h, --help            help for attach\n      --id string       User identifier\n      --policy string   Policy identifier\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-policies-detach","title":"lakectl auth groups policies detach","text":"<p>Detach a policy from a group</p> <pre><code>lakectl auth groups policies detach [flags]\n</code></pre> Options <pre><code>  -h, --help            help for detach\n      --id string       User identifier\n      --policy string   Policy identifier\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-policies-help","title":"lakectl auth groups policies help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type policies help [path to command] for full details.</p> <pre><code>lakectl auth groups policies help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-groups-policies-list","title":"lakectl auth groups policies list","text":"<p>List policies for the given group</p> <pre><code>lakectl auth groups policies list [flags]\n</code></pre> Options <pre><code>      --id string       Group identifier\n      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-help","title":"lakectl auth help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type auth help [path to command] for full details.</p> <pre><code>lakectl auth help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-policies","title":"lakectl auth policies","text":"<p>Manage policies</p> Options <pre><code>  -h, --help   help for policies\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-policies-create","title":"lakectl auth policies create","text":"<p>Create a policy</p> <pre><code>lakectl auth policies create [flags]\n</code></pre> Options <pre><code>  -h, --help                        help for create\n      --id string                   Policy identifier\n      --statement-document string   JSON statement document path (or \"-\" for stdin)\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-policies-delete","title":"lakectl auth policies delete","text":"<p>Delete a policy</p> <pre><code>lakectl auth policies delete [flags]\n</code></pre> Options <pre><code>  -h, --help        help for delete\n      --id string   Policy identifier\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-policies-help","title":"lakectl auth policies help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type policies help [path to command] for full details.</p> <pre><code>lakectl auth policies help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-policies-list","title":"lakectl auth policies list","text":"<p>List policies</p> <pre><code>lakectl auth policies list [flags]\n</code></pre> Options <pre><code>      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-policies-show","title":"lakectl auth policies show","text":"<p>Show a policy</p> <pre><code>lakectl auth policies show [flags]\n</code></pre> Options <pre><code>  -h, --help        help for show\n      --id string   Policy identifier\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users","title":"lakectl auth users","text":"<p>Manage users</p> Options <pre><code>  -h, --help   help for users\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-create","title":"lakectl auth users create","text":"<p>Create a user</p> <pre><code>lakectl auth users create [flags]\n</code></pre> Options <pre><code>  -h, --help        help for create\n      --id string   Username\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-credentials","title":"lakectl auth users credentials","text":"<p>Manage user credentials</p> Options <pre><code>  -h, --help   help for credentials\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-credentials-create","title":"lakectl auth users credentials create","text":"<p>Create user credentials</p> <pre><code>lakectl auth users credentials create [flags]\n</code></pre> Options <pre><code>  -h, --help        help for create\n      --id string   Username (email for password-based users, default: current user)\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-credentials-delete","title":"lakectl auth users credentials delete","text":"<p>Delete user credentials</p> <pre><code>lakectl auth users credentials delete [flags]\n</code></pre> Options <pre><code>      --access-key-id string   Access key ID to delete\n  -h, --help                   help for delete\n      --id string              Username (email for password-based users, default: current user)\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-credentials-help","title":"lakectl auth users credentials help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type credentials help [path to command] for full details.</p> <pre><code>lakectl auth users credentials help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-credentials-list","title":"lakectl auth users credentials list","text":"<p>List user credentials</p> <pre><code>lakectl auth users credentials list [flags]\n</code></pre> Options <pre><code>      --id string       Username (email for password-based users, default: current user)\n      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-delete","title":"lakectl auth users delete","text":"<p>Delete a user</p> <pre><code>lakectl auth users delete [flags]\n</code></pre> Options <pre><code>  -h, --help        help for delete\n      --id string   Username (email for password-based users)\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-groups","title":"lakectl auth users groups","text":"<p>Manage user groups</p> Options <pre><code>  -h, --help   help for groups\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-groups-help","title":"lakectl auth users groups help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type groups help [path to command] for full details.</p> <pre><code>lakectl auth users groups help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-groups-list","title":"lakectl auth users groups list","text":"<p>List groups for the given user</p> <pre><code>lakectl auth users groups list [flags]\n</code></pre> Options <pre><code>      --id string       Username (email for password-based users)\n      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-help","title":"lakectl auth users help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type users help [path to command] for full details.</p> <pre><code>lakectl auth users help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-list","title":"lakectl auth users list","text":"<p>List users</p> <pre><code>lakectl auth users list [flags]\n</code></pre> Options <pre><code>      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-policies","title":"lakectl auth users policies","text":"<p>Manage user policies</p> Synopsis <p>Manage user policies.  Requires an external authorization server with matching support.</p> Options <pre><code>  -h, --help   help for policies\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-policies-attach","title":"lakectl auth users policies attach","text":"<p>Attach a policy to a user</p> <pre><code>lakectl auth users policies attach [flags]\n</code></pre> Options <pre><code>  -h, --help            help for attach\n      --id string       Username (email for password-based users)\n      --policy string   Policy identifier\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-policies-detach","title":"lakectl auth users policies detach","text":"<p>Detach a policy from a user</p> <pre><code>lakectl auth users policies detach [flags]\n</code></pre> Options <pre><code>  -h, --help            help for detach\n      --id string       Username (email for password-based users)\n      --policy string   Policy identifier\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-policies-help","title":"lakectl auth users policies help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type policies help [path to command] for full details.</p> <pre><code>lakectl auth users policies help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-auth-users-policies-list","title":"lakectl auth users policies list","text":"<p>List policies for the given user</p> <pre><code>lakectl auth users policies list [flags]\n</code></pre> Options <pre><code>      --effective       List all distinct policies attached to the user, including by group memberships\n      --id string       Username (email for password-based users)\n      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-branch","title":"lakectl branch","text":"<p>Create and manage branches within a repository</p> Synopsis <p>Create delete and list branches within a lakeFS repository</p> Options <pre><code>  -h, --help   help for branch\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-create","title":"lakectl branch create","text":"<p>Create a new branch in a repository</p> <pre><code>lakectl branch create &lt;branch URI&gt; -s &lt;source ref URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl branch create lakefs://example-repo/new-branch -s lakefs://example-repo/main\n</code></pre> Options <pre><code>  -h, --help            help for create\n  -s, --source string   source branch uri\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-delete","title":"lakectl branch delete","text":"<p>Delete a branch in a repository, along with its uncommitted changes (CAREFUL)</p> <pre><code>lakectl branch delete &lt;branch URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl branch delete lakefs://my-repo/my-branch\n</code></pre> Options <pre><code>  -h, --help   help for delete\n  -y, --yes    Automatically say yes to all confirmations\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-help","title":"lakectl branch help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type branch help [path to command] for full details.</p> <pre><code>lakectl branch help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-list","title":"lakectl branch list","text":"<p>List branches in a repository</p> <pre><code>lakectl branch list &lt;repository URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl branch list lakefs://my-repo\n</code></pre> Options <pre><code>      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-reset","title":"lakectl branch reset","text":"<p>Reset uncommitted changes - all of them, or by path</p> Synopsis <p>reset changes.  There are four different ways to reset changes:   1. reset all uncommitted changes - reset lakefs://myrepo/main    2. reset uncommitted changes under specific path - reset lakefs://myrepo/main --prefix path   3. reset uncommitted changes for specific object - reset lakefs://myrepo/main --object path</p> <pre><code>lakectl branch reset &lt;branch URI&gt; [--prefix|--object] [flags]\n</code></pre> Examples <pre><code>lakectl branch reset lakefs://my-repo/my-branch\n</code></pre> Options <pre><code>  -h, --help            help for reset\n      --object string   path to object to be reset\n      --prefix string   prefix of the objects to be reset\n  -y, --yes             Automatically say yes to all confirmations\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-revert","title":"lakectl branch revert","text":"<p>Given a commit, record a new commit to reverse the effect of this commit</p> Synopsis <p>The commits will be reverted in left-to-right order</p> <pre><code>lakectl branch revert &lt;branch URI&gt; &lt;commit ref to revert&gt; [&lt;more commits&gt;...] [flags]\n</code></pre> Examples <pre><code>lakectl branch revert lakefs://example-repo/example-branch commitA\n              Revert the changes done by commitA in example-branch\n              branch revert lakefs://example-repo/example-branch HEAD~1 HEAD~2 HEAD~3\n              Revert the changes done by the second last commit to the fourth last commit in example-branch\n</code></pre> Options <pre><code>      --allow-empty-commit   allow empty commit (revert without changes)\n  -h, --help                 help for revert\n  -m, --parent-number int    the parent number (starting from 1) of the mainline. The revert will reverse the change relative to the specified parent.\n  -y, --yes                  Automatically say yes to all confirmations\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-show","title":"lakectl branch show","text":"<p>Show branch latest commit reference</p> <pre><code>lakectl branch show &lt;branch URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl branch show lakefs://my-repo/my-branch\n</code></pre> Options <pre><code>  -h, --help   help for show\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-protect","title":"lakectl branch-protect","text":"<p>Create and manage branch protection rules</p> Synopsis <p>Define branch protection rules to prevent direct changes. Changes to protected branches can only be done by merging from other branches.</p> Options <pre><code>  -h, --help   help for branch-protect\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-protect-add","title":"lakectl branch-protect add","text":"<p>Add a branch protection rule</p> Synopsis <p>Add a branch protection rule for a given branch name pattern</p> <pre><code>lakectl branch-protect add &lt;repository URI&gt; &lt;pattern&gt; [flags]\n</code></pre> Examples <pre><code>lakectl branch-protect add lakefs://my-repo 'stable_*'\n</code></pre> Options <pre><code>  -h, --help   help for add\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-protect-delete","title":"lakectl branch-protect delete","text":"<p>Delete a branch protection rule</p> Synopsis <p>Delete a branch protection rule for a given branch name pattern</p> <pre><code>lakectl branch-protect delete &lt;repository URI&gt; &lt;pattern&gt; [flags]\n</code></pre> Examples <pre><code>lakectl branch-protect delete lakefs://my-repo stable_*\n</code></pre> Options <pre><code>  -h, --help   help for delete\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-protect-help","title":"lakectl branch-protect help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type branch-protect help [path to command] for full details.</p> <pre><code>lakectl branch-protect help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-branch-protect-list","title":"lakectl branch-protect list","text":"<p>List all branch protection rules</p> <pre><code>lakectl branch-protect list &lt;repository URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl branch-protect list lakefs://my-repo\n</code></pre> Options <pre><code>  -h, --help   help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-cherry-pick","title":"lakectl cherry-pick","text":"<p>Apply the changes introduced by an existing commit</p> Synopsis <p>Apply the changes from the given commit to the tip of the branch. The changes will be added as a new commit.</p> <pre><code>lakectl cherry-pick &lt;commit URI&gt; &lt;branch&gt; [flags]\n</code></pre> Examples <pre><code>lakectl cherry-pick lakefs://my-repo/600dc0ffee lakefs://my-repo/my-branch\n</code></pre> Options <pre><code>  -h, --help                help for cherry-pick\n  -m, --parent-number int   the parent number (starting from 1) of the cherry-picked commit. The cherry-pick will apply the change relative to the specified parent.\n</code></pre>"},{"location":"reference/cli/#lakectl-commit","title":"lakectl commit","text":"<p>Commit changes on a given branch</p> <pre><code>lakectl commit &lt;branch URI&gt; [flags]\n</code></pre> Options <pre><code>      --allow-empty-commit    allow a commit with no changes\n      --allow-empty-message   allow an empty commit message\n  -h, --help                  help for commit\n  -m, --message string        commit message\n      --meta strings          key value pair in the form of key=value\n</code></pre>"},{"location":"reference/cli/#lakectl-completion","title":"lakectl completion","text":"<p>Generate completion script</p> Synopsis <p>To load completions:</p> <p>Bash:</p> <pre><code>$ source &lt;(lakectl completion bash)\n</code></pre> <p>To load completions for each session, execute once: Linux:</p> <pre><code>$ lakectl completion bash &gt; /etc/bash_completion.d/lakectl\n</code></pre> <p>MacOS:</p> <pre><code>$ lakectl completion bash &gt; /usr/local/etc/bash_completion.d/lakectl\n</code></pre> <p>Zsh:</p> <p>If shell completion is not already enabled in your environment you will need to enable it.  You can execute the following once:</p> <pre><code>$ echo \"autoload -U compinit; compinit\" &gt;&gt; ~/.zshrc\n</code></pre> <p>To load completions for each session, execute once: <pre><code>$ lakectl completion zsh &gt; \"${fpath[1]}/_lakectl\"\n</code></pre></p> <p>You will need to start a new shell for this setup to take effect.</p> <p>Fish:</p> <pre><code>$ lakectl completion fish | source\n</code></pre> <p>To load completions for each session, execute once:</p> <pre><code>$ lakectl completion fish &gt; ~/.config/fish/completions/lakectl.fish\n</code></pre> <pre><code>lakectl completion &lt;bash|zsh|fish&gt;\n</code></pre> Options <pre><code>  -h, --help   help for completion\n</code></pre>"},{"location":"reference/cli/#lakectl-config","title":"lakectl config","text":"<p>Create/update local lakeFS configuration</p> <pre><code>lakectl config [flags]\n</code></pre> Options <pre><code>  -h, --help   help for config\n</code></pre>"},{"location":"reference/cli/#lakectl-diff","title":"lakectl diff","text":"<p>Show changes between two commits, or the currently uncommitted changes</p> <pre><code>lakectl diff &lt;ref URI&gt; [ref URI] [flags]\n</code></pre> Examples <pre><code>    lakectl diff lakefs://example-repo/example-branch\n    Show uncommitted changes in example-branch.\n\n    lakectl diff lakefs://example-repo/main lakefs://example-repo/dev\n    This shows the differences between master and dev starting at the last common commit.\n    This is similar to the three-dot (...) syntax in git.\n    Uncommitted changes are not shown.\n\n    lakectl diff --two-way lakefs://example-repo/main lakefs://example-repo/dev\n    Show changes between the tips of the main and dev branches.\n    This is similar to the two-dot (..) syntax in git.\n    Uncommitted changes are not shown.\n\n    lakectl diff --two-way lakefs://example-repo/main lakefs://example-repo/dev$\n    Show changes between the tip of the main and the dev branch, including uncommitted changes on dev.\n\n    lakectl diff --prefix some/path lakefs://example-repo/main lakefs://example-repo/dev\n    Show changes of objects prefixed with 'some/path' between the tips of the main and dev branches.\n</code></pre> Options <pre><code>  -h, --help            help for diff\n      --prefix string   Show only changes in the given prefix.\n      --two-way         Use two-way diff: show difference between the given refs, regardless of a common ancestor.\n</code></pre>"},{"location":"reference/cli/#lakectl-doctor","title":"lakectl doctor","text":"<p>Run a basic diagnosis of the LakeFS configuration</p> <pre><code>lakectl doctor [flags]\n</code></pre> Options <pre><code>  -h, --help   help for doctor\n</code></pre>"},{"location":"reference/cli/#lakectl-fs","title":"lakectl fs","text":"<p>View and manipulate objects</p> Options <pre><code>  -h, --help   help for fs\n</code></pre>"},{"location":"reference/cli/#lakectl-fs-cat","title":"lakectl fs cat","text":"<p>Dump content of object to stdout</p> <pre><code>lakectl fs cat &lt;path URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help       help for cat\n      --pre-sign   Use pre-signed URLs when downloading/uploading data (recommended) (default true)\n</code></pre>"},{"location":"reference/cli/#lakectl-fs-download","title":"lakectl fs download","text":"<p>Download object(s) from a given repository path</p> <pre><code>lakectl fs download &lt;path URI&gt; [&lt;destination path&gt;] [flags]\n</code></pre> Options <pre><code>  -h, --help              help for download\n      --no-progress       Disable progress bar animation for IO operations\n  -p, --parallelism int   Max concurrent operations to perform (default 25)\n      --part-size int     part size in bytes for multipart download (default 8388608)\n      --pre-sign          Use pre-signed URLs when downloading/uploading data (recommended) (default true)\n  -r, --recursive         recursively download all objects under path\n</code></pre>"},{"location":"reference/cli/#lakectl-fs-help","title":"lakectl fs help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type fs help [path to command] for full details.</p> <pre><code>lakectl fs help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-fs-ls","title":"lakectl fs ls","text":"<p>List entries under a given tree</p> <pre><code>lakectl fs ls &lt;path URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help        help for ls\n  -r, --recursive   list all objects under the specified path\n</code></pre>"},{"location":"reference/cli/#lakectl-fs-presign","title":"lakectl fs presign","text":"<p>return a pre-signed URL for reading the specified object</p> <pre><code>lakectl fs presign &lt;path URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help   help for presign\n</code></pre>"},{"location":"reference/cli/#lakectl-fs-rm","title":"lakectl fs rm","text":"<p>Delete object</p> <pre><code>lakectl fs rm &lt;path URI&gt; [flags]\n</code></pre> Options <pre><code>  -C, --concurrency int   max concurrent single delete operations to send to the lakeFS server (default 50)\n  -h, --help              help for rm\n  -r, --recursive         recursively delete all objects under the specified path\n</code></pre>"},{"location":"reference/cli/#lakectl-fs-stat","title":"lakectl fs stat","text":"<p>View object metadata</p> <pre><code>lakectl fs stat &lt;path URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help       help for stat\n      --pre-sign   Use pre-signed URLs when downloading/uploading data (recommended) (default true)\n</code></pre>"},{"location":"reference/cli/#lakectl-fs-upload","title":"lakectl fs upload","text":"<p>Upload a local file to the specified URI</p> <pre><code>lakectl fs upload &lt;path URI&gt; [flags]\n</code></pre> Options <pre><code>      --content-type string   MIME type of contents\n  -h, --help                  help for upload\n      --no-progress           Disable progress bar animation for IO operations\n  -p, --parallelism int       Max concurrent operations to perform (default 25)\n      --pre-sign              Use pre-signed URLs when downloading/uploading data (recommended) (default true)\n  -r, --recursive             recursively copy all files under local source\n  -s, --source string         local file to upload, or \"-\" for stdin\n</code></pre>"},{"location":"reference/cli/#lakectl-fs-stage","title":"lakectl fs stage","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Link an external object with a path in a repository</p> Synopsis <p>Link an external object with a path in a repository, creating an uncommitted change. The object location must be outside the repository's storage namespace</p> <pre><code>lakectl fs stage &lt;path URI&gt; [flags]\n</code></pre> Options <pre><code>      --checksum string       Object MD5 checksum as a hexadecimal string\n      --content-type string   MIME type of contents\n  -h, --help                  help for stage\n      --location string       fully qualified storage location (i.e. \"s3://bucket/path/to/object\")\n      --meta strings          key value pairs in the form of key=value\n      --mtime int             Object modified time (Unix Epoch in seconds). Defaults to current time\n      --size int              Object size in bytes\n</code></pre>"},{"location":"reference/cli/#lakectl-fs-update-metadata","title":"lakectl fs update-metadata","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Update user metadata on the specified URI</p> <pre><code>lakectl fs update-metadata &lt;path URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help               help for update-metadata\n      --metadata strings   Metadata to set, in the form key1=value1,key2=value2\n</code></pre>"},{"location":"reference/cli/#lakectl-gc","title":"lakectl gc","text":"<p>Manage the garbage collection policy</p> Options <pre><code>  -h, --help   help for gc\n</code></pre>"},{"location":"reference/cli/#lakectl-gc-delete-config","title":"lakectl gc delete-config","text":"<p>Deletes the garbage collection policy for the repository</p> <pre><code>lakectl gc delete-config &lt;repository URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl gc delete-config lakefs://my-repo\n</code></pre> Options <pre><code>  -h, --help   help for delete-config\n</code></pre>"},{"location":"reference/cli/#lakectl-gc-get-config","title":"lakectl gc get-config","text":"<p>Show the garbage collection policy for this repository</p> <pre><code>lakectl gc get-config &lt;repository URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl gc get-config lakefs://my-repo\n</code></pre> Options <pre><code>  -h, --help   help for get-config\n  -p, --json   get rules as JSON\n</code></pre>"},{"location":"reference/cli/#lakectl-gc-help","title":"lakectl gc help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type gc help [path to command] for full details.</p> <pre><code>lakectl gc help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-gc-set-config","title":"lakectl gc set-config","text":"<p>Set garbage collection policy JSON</p> Synopsis <p>Sets the garbage collection policy JSON. Example configuration file: {   \"default_retention_days\": 21,   \"branches\": [     {       \"branch_id\": \"main\",       \"retention_days\": 28     },     {       \"branch_id\": \"dev\",       \"retention_days\": 14     }   ] }</p> <pre><code>lakectl gc set-config &lt;repository URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl gc set-config lakefs://my-repo -f config.json\n</code></pre> Options <pre><code>  -f, --filename string   file containing the GC policy as JSON\n  -h, --help              help for set-config\n</code></pre>"},{"location":"reference/cli/#lakectl-help","title":"lakectl help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type lakectl help [path to command] for full details.</p> <pre><code>lakectl help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-identity","title":"lakectl identity","text":"<p>Show identity info</p> Synopsis <p>Show the info of the configured user in lakectl</p> <pre><code>lakectl identity [flags]\n</code></pre> Examples <pre><code>lakectl identity\n</code></pre> Options <pre><code>  -h, --help   help for identity\n</code></pre>"},{"location":"reference/cli/#lakectl-import","title":"lakectl import","text":"<p>Import data from external source to a destination branch</p> <pre><code>lakectl import --from &lt;object store URI&gt; --to &lt;lakeFS path URI&gt; [flags]\n</code></pre> Options <pre><code>      --allow-empty-message   allow an empty commit message (default true)\n      --from string           prefix to read from (e.g. \"s3://bucket/sub/path/\"). must not be in a storage namespace\n  -h, --help                  help for import\n  -m, --message string        commit message\n      --meta strings          key value pair in the form of key=value\n      --no-progress           switch off the progress output\n      --to string             lakeFS path to load objects into (e.g. \"lakefs://repo/branch/sub/path/\")\n</code></pre>"},{"location":"reference/cli/#lakectl-local","title":"lakectl local","text":"<p>Sync local directories with lakeFS paths</p> Options <pre><code>  -h, --help   help for local\n</code></pre>"},{"location":"reference/cli/#lakectl-local-checkout","title":"lakectl local checkout","text":"<p>Sync local directory with the remote state.</p> <pre><code>lakectl local checkout [directory] [flags]\n</code></pre> Options <pre><code>      --all               Checkout given source branch or reference for all linked directories\n  -h, --help              help for checkout\n      --no-progress       Disable progress bar animation for IO operations\n  -p, --parallelism int   Max concurrent operations to perform (default 25)\n      --pre-sign          Use pre-signed URLs when downloading/uploading data (recommended) (default true)\n  -r, --ref string        Checkout the given reference\n  -y, --yes               Automatically say yes to all confirmations\n</code></pre>"},{"location":"reference/cli/#lakectl-local-clone","title":"lakectl local clone","text":"<p>Clone a path from a lakeFS repository into a new directory.</p> <pre><code>lakectl local clone &lt;path URI&gt; [directory] [flags]\n</code></pre> Options <pre><code>      --gitignore         Update .gitignore file when working in a git repository context (default true)\n  -h, --help              help for clone\n      --no-progress       Disable progress bar animation for IO operations\n  -p, --parallelism int   Max concurrent operations to perform (default 25)\n      --pre-sign          Use pre-signed URLs when downloading/uploading data (recommended) (default true)\n</code></pre>"},{"location":"reference/cli/#lakectl-local-commit","title":"lakectl local commit","text":"<p>Commit changes from local directory to the lakeFS branch it tracks.</p> <pre><code>lakectl local commit [directory] [flags]\n</code></pre> Options <pre><code>      --allow-empty-message   allow an empty commit message\n      --force                 Commit changes even if remote branch includes uncommitted changes external to the synced path\n  -h, --help                  help for commit\n  -m, --message string        commit message\n      --meta strings          key value pair in the form of key=value\n      --no-progress           Disable progress bar animation for IO operations\n  -p, --parallelism int       Max concurrent operations to perform (default 25)\n      --pre-sign              Use pre-signed URLs when downloading/uploading data (recommended) (default true)\n</code></pre>"},{"location":"reference/cli/#lakectl-local-help","title":"lakectl local help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type local help [path to command] for full details.</p> <pre><code>lakectl local help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-local-init","title":"lakectl local init","text":"<p>set a local directory to sync with a lakeFS path.</p> <pre><code>lakectl local init &lt;path URI&gt; [directory] [flags]\n</code></pre> Options <pre><code>      --force       Overwrites if directory already linked to a lakeFS path\n      --gitignore   Update .gitignore file when working in a git repository context (default true)\n  -h, --help        help for init\n</code></pre>"},{"location":"reference/cli/#lakectl-local-list","title":"lakectl local list","text":"<p>find and list directories that are synced with lakeFS.</p> <pre><code>lakectl local list [directory] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-local-pull","title":"lakectl local pull","text":"<p>Fetch latest changes from lakeFS.</p> <pre><code>lakectl local pull [directory] [flags]\n</code></pre> Options <pre><code>      --force             Reset any uncommitted local change\n  -h, --help              help for pull\n      --no-progress       Disable progress bar animation for IO operations\n  -p, --parallelism int   Max concurrent operations to perform (default 25)\n      --pre-sign          Use pre-signed URLs when downloading/uploading data (recommended) (default true)\n</code></pre>"},{"location":"reference/cli/#lakectl-local-status","title":"lakectl local status","text":"<p>show modifications (both remote and local) to the directory and the remote location it tracks</p> <pre><code>lakectl local status [directory] [flags]\n</code></pre> Options <pre><code>  -h, --help    help for status\n  -l, --local   Don't compare against remote changes\n</code></pre>"},{"location":"reference/cli/#lakectl-log","title":"lakectl log","text":"<p>Show log of commits</p> Synopsis <p>Show log of commits for a given branch</p> <pre><code>lakectl log &lt;branch URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl log --dot lakefs://example-repository/main | dot -Tsvg &gt; graph.svg\n</code></pre> Options <pre><code>      --after string         show results after this value (used for pagination)\n      --amount int           number of results to return. By default, all results are returned\n      --dot                  return results in a dotgraph format\n      --first-parent         follow only the first parent commit upon seeing a merge commit\n  -h, --help                 help for log\n      --limit                limit result just to amount. By default, returns whether more items are available.\n      --no-merges            skip merge commits\n      --objects strings      show results that contains changes to at least one path in that list of objects. Use comma separator to pass all objects together\n      --prefixes strings     show results that contains changes to at least one path in that list of prefixes. Use comma separator to pass all prefixes together\n      --show-meta-range-id   also show meta range ID\n      --since string         show results since this date-time (RFC3339 format)\n      --stop-at string       a Ref to stop at (included in results)\n</code></pre>"},{"location":"reference/cli/#lakectl-merge","title":"lakectl merge","text":"<p>Merge &amp; commit changes from source branch into destination branch</p> Synopsis <p>Merge &amp; commit changes from source branch into destination branch</p> <pre><code>lakectl merge &lt;source ref&gt; &lt;destination ref&gt; [flags]\n</code></pre> Options <pre><code>      --allow-empty           Allow merge when the branches have the same content\n      --allow-empty-message   allow an empty commit message (default true)\n      --force                 Allow merge into a read-only branch or into a branch with the same content\n  -h, --help                  help for merge\n  -m, --message string        commit message\n      --meta strings          key value pair in the form of key=value\n      --squash                Squash all changes from source into a single commit on destination\n      --strategy string       In case of a merge conflict, this option will force the merge process to automatically favor changes from the dest branch (\"dest-wins\") or from the source branch(\"source-wins\"). In case no selection is made, the merge process will fail in case of a conflict\n</code></pre>"},{"location":"reference/cli/#lakectl-metastore","title":"lakectl metastore","text":"<p>Manage metastore commands</p> Options <pre><code>  -h, --help   help for metastore\n</code></pre>"},{"location":"reference/cli/#lakectl-metastore-copy","title":"lakectl metastore copy","text":"<p>Copy or merge table</p> Synopsis <p>Copy or merge table. the destination table will point to the selected branch</p> <pre><code>lakectl metastore copy [flags]\n</code></pre> Options <pre><code>      --catalog-id string         Glue catalog ID\n      --dbfs-root dbfs:/          dbfs location root will replace dbfs:/ in the location before transforming\n      --from-client-type string   metastore type [hive, glue]\n      --from-schema string        source schema name\n      --from-table string         source table name\n  -h, --help                      help for copy\n      --metastore-uri string      Hive metastore URI\n  -p, --partition strings         partition to copy\n      --serde string              serde to set copy to  [default is  to-table]\n      --to-branch string          lakeFS branch name\n      --to-client-type string     metastore type [hive, glue]\n      --to-schema string          destination schema name [default is from-branch]\n      --to-table string           destination table name [default is  from-table] \n</code></pre>"},{"location":"reference/cli/#lakectl-metastore-copy-all","title":"lakectl metastore copy-all","text":"<p>Copy from one metastore to another</p> Synopsis <p>copy or merge requested tables between hive metastores. the destination tables will point to the selected branch</p> <pre><code>lakectl metastore copy-all [flags]\n</code></pre> Options <pre><code>      --branch string             lakeFS branch name\n      --continue-on-error         prevent copy-all from failing when a single table fails\n      --dbfs-root dbfs:/          dbfs location root will replace dbfs:/ in the location before transforming\n      --from-address string       source metastore address\n      --from-client-type string   metastore type [hive, glue]\n  -h, --help                      help for copy-all\n      --schema-filter string      filter for schemas to copy in metastore pattern (default \".*\")\n      --table-filter string       filter for tables to copy in metastore pattern (default \".*\")\n      --to-address string         destination metastore address\n      --to-client-type string     metastore type [hive, glue]\n</code></pre>"},{"location":"reference/cli/#lakectl-metastore-copy-schema","title":"lakectl metastore copy-schema","text":"<p>Copy schema</p> Synopsis <p>Copy schema (without tables). the destination schema will point to the selected branch</p> <pre><code>lakectl metastore copy-schema [flags]\n</code></pre> Options <pre><code>      --catalog-id string         Glue catalog ID\n      --dbfs-root dbfs:/          dbfs location root will replace dbfs:/ in the location before transforming\n      --from-client-type string   metastore type [hive, glue]\n      --from-schema string        source schema name\n  -h, --help                      help for copy-schema\n      --metastore-uri string      Hive metastore URI\n      --to-branch string          lakeFS branch name\n      --to-client-type string     metastore type [hive, glue]\n      --to-schema string          destination schema name [default is from-branch]\n</code></pre>"},{"location":"reference/cli/#lakectl-metastore-create-symlink","title":"lakectl metastore create-symlink","text":"<p>Create symlink table and data</p> Synopsis <p>create table with symlinks, and create the symlinks in s3 in order to access from external services that could only access s3 directly (e.g athena)</p> <pre><code>lakectl metastore create-symlink [flags]\n</code></pre> Options <pre><code>      --branch string             lakeFS branch name\n      --catalog-id string         Glue catalog ID\n      --from-client-type string   metastore type [hive, glue]\n      --from-schema string        source schema name\n      --from-table string         source table name\n  -h, --help                      help for create-symlink\n      --path string               path to table on lakeFS\n      --repo string               lakeFS repository name\n      --to-schema string          destination schema name\n      --to-table string           destination table name\n</code></pre>"},{"location":"reference/cli/#lakectl-metastore-diff","title":"lakectl metastore diff","text":"<p>Show column and partition differences between two tables</p> <pre><code>lakectl metastore diff [flags]\n</code></pre> Options <pre><code>      --catalog-id string         Glue catalog ID\n      --from-address string       source metastore address\n      --from-client-type string   metastore type [hive, glue]\n      --from-schema string        source schema name\n      --from-table string         source table name\n  -h, --help                      help for diff\n      --metastore-uri string      Hive metastore URI\n      --to-address string         destination metastore address\n      --to-client-type string     metastore type [hive, glue]\n      --to-schema string          destination schema name \n      --to-table string           destination table name [default is from-table]\n</code></pre>"},{"location":"reference/cli/#lakectl-metastore-help","title":"lakectl metastore help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type metastore help [path to command] for full details.</p> <pre><code>lakectl metastore help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-metastore-import-all","title":"lakectl metastore import-all","text":"<p>Import from one metastore to another</p> Synopsis <p>import requested tables between hive metastores. the destination tables will point to the selected repository and branch table with location s3://my-s3-bucket/path/to/table  will be transformed to location s3://repo-param/bucket-param/path/to/table</p> <pre><code>lakectl metastore import-all [flags]\n</code></pre> Options <pre><code>      --branch string             lakeFS branch name\n      --continue-on-error         prevent import-all from failing when a single table fails\n      --dbfs-root dbfs:/          dbfs location root will replace dbfs:/ in the location before transforming\n      --from-address string       source metastore address\n      --from-client-type string   metastore type [hive, glue]\n  -h, --help                      help for import-all\n      --repo string               lakeFS repo name\n      --schema-filter string      filter for schemas to copy in metastore pattern (default \".*\")\n      --table-filter string       filter for tables to copy in metastore pattern (default \".*\")\n      --to-address string         destination metastore address\n      --to-client-type string     metastore type [hive, glue]\n</code></pre>"},{"location":"reference/cli/#lakectl-repo","title":"lakectl repo","text":"<p>Manage and explore repos</p> Options <pre><code>  -h, --help   help for repo\n</code></pre>"},{"location":"reference/cli/#lakectl-repo-create","title":"lakectl repo create","text":"<p>Create a new repository</p> <pre><code>lakectl repo create &lt;repository URI&gt; &lt;storage namespace&gt; [flags]\n</code></pre> Examples <pre><code>lakectl repo create lakefs://my-repo s3://my-bucket\n</code></pre> Options <pre><code>  -d, --default-branch string   the default branch of this repository (default \"main\")\n  -h, --help                    help for create\n      --sample-data             create sample data in the repository\n</code></pre>"},{"location":"reference/cli/#lakectl-repo-delete","title":"lakectl repo delete","text":"<p>Delete existing repository</p> <pre><code>lakectl repo delete &lt;repository URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl repo delete lakefs://my-repo\n</code></pre> Options <pre><code>  -h, --help   help for delete\n  -y, --yes    Automatically say yes to all confirmations\n</code></pre>"},{"location":"reference/cli/#lakectl-repo-help","title":"lakectl repo help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type repo help [path to command] for full details.</p> <pre><code>lakectl repo help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-repo-list","title":"lakectl repo list","text":"<p>List repositories</p> <pre><code>lakectl repo list [flags]\n</code></pre> Options <pre><code>      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-repo-create-bare","title":"lakectl repo create-bare","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Create a new repository with no initial branch or commit</p> <pre><code>lakectl repo create-bare &lt;repository URI&gt; &lt;storage namespace&gt; [flags]\n</code></pre> Examples <pre><code>lakectl create-bare lakefs://my-repo s3://my-bucket\n</code></pre> Options <pre><code>  -d, --default-branch string   the default branch name of this repository (will not be created) (default \"main\")\n  -h, --help                    help for create-bare\n</code></pre>"},{"location":"reference/cli/#lakectl-show","title":"lakectl show","text":"<p>See detailed information about an entity</p> Options <pre><code>  -h, --help   help for show\n</code></pre>"},{"location":"reference/cli/#lakectl-show-commit","title":"lakectl show commit","text":"<p>See detailed information about a commit</p> <pre><code>lakectl show commit &lt;commit URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help                 help for commit\n      --show-meta-range-id   show meta range ID\n</code></pre>"},{"location":"reference/cli/#lakectl-show-help","title":"lakectl show help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type show help [path to command] for full details.</p> <pre><code>lakectl show help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-tag","title":"lakectl tag","text":"<p>Create and manage tags within a repository</p> Synopsis <p>Create delete and list tags within a lakeFS repository</p> Options <pre><code>  -h, --help   help for tag\n</code></pre>"},{"location":"reference/cli/#lakectl-tag-create","title":"lakectl tag create","text":"<p>Create a new tag in a repository</p> <pre><code>lakectl tag create &lt;tag URI&gt; &lt;commit URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl tag create lakefs://example-repo/example-tag lakefs://example-repo/2397cc9a9d04c20a4e5739b42c1dd3d8ba655c0b3a3b974850895a13d8bf9917\n</code></pre> Options <pre><code>  -f, --force   override the tag if it exists\n  -h, --help    help for create\n</code></pre>"},{"location":"reference/cli/#lakectl-tag-delete","title":"lakectl tag delete","text":"<p>Delete a tag from a repository</p> <pre><code>lakectl tag delete &lt;tag URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help   help for delete\n</code></pre>"},{"location":"reference/cli/#lakectl-tag-help","title":"lakectl tag help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type tag help [path to command] for full details.</p> <pre><code>lakectl tag help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-tag-list","title":"lakectl tag list","text":"<p>List tags in a repository</p> <pre><code>lakectl tag list &lt;repository URI&gt; [flags]\n</code></pre> Examples <pre><code>lakectl tag list lakefs://my-repo\n</code></pre> Options <pre><code>      --amount int      how many results to return (default 100)\n      --after string    show results after this value (used for pagination)\n      --prefix string   filter results by prefix (used for pagination)\n  -h, --help            help for list\n</code></pre>"},{"location":"reference/cli/#lakectl-tag-show","title":"lakectl tag show","text":"<p>Show tag's commit reference</p> <pre><code>lakectl tag show &lt;tag URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help   help for show\n</code></pre>"},{"location":"reference/cli/#undocumented-commands","title":"Undocumented commands","text":"<p>Warning</p> <p>These commands are plumbing commands and for internal use only. Avoid using them unless you're really sure you know what you're doing, or have been in contact with lakeFS support!</p>"},{"location":"reference/cli/#lakectl-abuse","title":"lakectl abuse","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Abuse a running lakeFS instance. See sub commands for more info.</p> Options <pre><code>  -h, --help   help for abuse\n</code></pre>"},{"location":"reference/cli/#lakectl-abuse-commit","title":"lakectl abuse commit","text":"<p>Commits to the source branch repeatedly</p> <pre><code>lakectl abuse commit &lt;branch URI&gt; [flags]\n</code></pre> Options <pre><code>      --amount int     amount of commits to do (default 100)\n      --gap duration   duration to wait between commits (default 2s)\n  -h, --help           help for commit\n</code></pre>"},{"location":"reference/cli/#lakectl-abuse-create-branches","title":"lakectl abuse create-branches","text":"<p>Create a lot of branches very quickly.</p> <pre><code>lakectl abuse create-branches &lt;source ref URI&gt; [flags]\n</code></pre> Options <pre><code>      --amount int             amount of things to do (default 1000000)\n      --branch-prefix string   prefix to create branches under (default \"abuse-\")\n      --clean-only             only clean up past runs\n  -h, --help                   help for create-branches\n      --parallelism int        amount of things to do in parallel (default 100)\n</code></pre>"},{"location":"reference/cli/#lakectl-abuse-help","title":"lakectl abuse help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type abuse help [path to command] for full details.</p> <pre><code>lakectl abuse help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-abuse-link-same-object","title":"lakectl abuse link-same-object","text":"<p>Link the same object in parallel.</p> <pre><code>lakectl abuse link-same-object &lt;branch URI&gt; [flags]\n</code></pre> Options <pre><code>      --amount int        amount of link object to do (default 1000000)\n  -h, --help              help for link-same-object\n      --key string        key used for the test (default \"linked-object\")\n      --parallelism int   amount of link object to do in parallel (default 100)\n</code></pre>"},{"location":"reference/cli/#lakectl-abuse-list","title":"lakectl abuse list","text":"<p>List from the source ref</p> <pre><code>lakectl abuse list &lt;source ref URI&gt; [flags]\n</code></pre> Options <pre><code>      --amount int        amount of lists to do (default 1000000)\n  -h, --help              help for list\n      --parallelism int   amount of lists to do in parallel (default 100)\n      --prefix string     prefix to list under (default \"abuse/\")\n</code></pre>"},{"location":"reference/cli/#lakectl-abuse-merge","title":"lakectl abuse merge","text":"<p>Merge non-conflicting objects to the source branch in parallel</p> <pre><code>lakectl abuse merge &lt;branch URI&gt; [flags]\n</code></pre> Options <pre><code>      --amount int        amount of merges to perform (default 1000)\n  -h, --help              help for merge\n      --parallelism int   number of merges to perform in parallel (default 100)\n</code></pre>"},{"location":"reference/cli/#lakectl-abuse-random-delete","title":"lakectl abuse random-delete","text":"<p>Delete keys from a file and generate random delete from the source ref for those keys.</p> <pre><code>lakectl abuse random-delete &lt;source ref URI&gt; [flags]\n</code></pre> Options <pre><code>      --amount int         amount of reads to do (default 1000000)\n      --from-file string   read keys from this file (\"-\" for stdin)\n  -h, --help               help for random-delete\n      --parallelism int    amount of reads to do in parallel (default 100)\n</code></pre>"},{"location":"reference/cli/#lakectl-abuse-random-read","title":"lakectl abuse random-read","text":"<p>Read keys from a file and generate random reads from the source ref for those keys.</p> <pre><code>lakectl abuse random-read &lt;source ref URI&gt; [flags]\n</code></pre> Options <pre><code>      --amount int         amount of reads to do (default 1000000)\n      --from-file string   read keys from this file (\"-\" for stdin)\n  -h, --help               help for random-read\n      --parallelism int    amount of reads to do in parallel (default 100)\n</code></pre>"},{"location":"reference/cli/#lakectl-abuse-random-write","title":"lakectl abuse random-write","text":"<p>Generate random writes to the source branch</p> <pre><code>lakectl abuse random-write &lt;branch URI&gt; [flags]\n</code></pre> Options <pre><code>      --amount int        amount of writes to do (default 1000000)\n  -h, --help              help for random-write\n      --parallelism int   amount of writes to do in parallel (default 100)\n      --prefix string     prefix to create paths under (default \"abuse/\")\n</code></pre>"},{"location":"reference/cli/#lakectl-bisect","title":"lakectl bisect","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Binary search to find the commit that introduced a bug</p> Options <pre><code>  -h, --help   help for bisect\n</code></pre>"},{"location":"reference/cli/#lakectl-bisect-bad","title":"lakectl bisect bad","text":"<p>Set 'bad' commit that is known to contain the bug</p> <pre><code>lakectl bisect bad [flags]\n</code></pre> Options <pre><code>  -h, --help   help for bad\n</code></pre>"},{"location":"reference/cli/#lakectl-bisect-good","title":"lakectl bisect good","text":"<p>Set current commit as 'good' commit that is known to be before the bug was introduced</p> <pre><code>lakectl bisect good [flags]\n</code></pre> Options <pre><code>  -h, --help   help for good\n</code></pre>"},{"location":"reference/cli/#lakectl-bisect-help","title":"lakectl bisect help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type bisect help [path to command] for full details.</p> <pre><code>lakectl bisect help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-bisect-log","title":"lakectl bisect log","text":"<p>Print out the current bisect state</p> <pre><code>lakectl bisect log [flags]\n</code></pre> Options <pre><code>  -h, --help   help for log\n</code></pre>"},{"location":"reference/cli/#lakectl-bisect-reset","title":"lakectl bisect reset","text":"<p>Clean up the bisection state</p> <pre><code>lakectl bisect reset [flags]\n</code></pre> Options <pre><code>  -h, --help   help for reset\n</code></pre>"},{"location":"reference/cli/#lakectl-bisect-run","title":"lakectl bisect run","text":"<p>Bisecting based on command status code</p> <pre><code>lakectl bisect run &lt;command&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help   help for run\n</code></pre>"},{"location":"reference/cli/#lakectl-bisect-start","title":"lakectl bisect start","text":"<p>Start a bisect session</p> <pre><code>lakectl bisect start &lt;bad ref URI&gt; &lt;good ref URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help   help for start\n</code></pre>"},{"location":"reference/cli/#lakectl-bisect-view","title":"lakectl bisect view","text":"<p>Current bisect commits</p> <pre><code>lakectl bisect view [flags]\n</code></pre> Options <pre><code>  -h, --help   help for view\n</code></pre>"},{"location":"reference/cli/#lakectl-cat-hook-output","title":"lakectl cat-hook-output","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Cat actions hook output</p> <pre><code>lakectl cat-hook-output &lt;repository URI&gt; &lt;run_id&gt; &lt;hook_id&gt; [flags]\n</code></pre> Examples <pre><code>lakectl cat-hook-output lakefs://my-repo 20230719152411arS0z6I my_hook_name\n</code></pre> Options <pre><code>  -h, --help   help for cat-hook-output\n</code></pre>"},{"location":"reference/cli/#lakectl-cat-sst","title":"lakectl cat-sst","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Explore lakeFS .sst files</p> <pre><code>lakectl cat-sst &lt;sst-file&gt; [flags]\n</code></pre> Options <pre><code>      --amount int    how many records to return, or -1 for all records (default -1)\n  -f, --file string   path to an sstable file, or \"-\" for stdin\n  -h, --help          help for cat-sst\n</code></pre>"},{"location":"reference/cli/#lakectl-docs","title":"lakectl docs","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <pre><code>lakectl docs [outfile] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for docs\n</code></pre>"},{"location":"reference/cli/#lakectl-find-merge-base","title":"lakectl find-merge-base","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Find the commits for the merge operation</p> <pre><code>lakectl find-merge-base &lt;source ref URI&gt; &lt;destination ref URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help   help for find-merge-base\n</code></pre>"},{"location":"reference/cli/#lakectl-refs-dump","title":"lakectl refs-dump","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Dumps refs (branches, commits, tags) to the underlying object store</p> <pre><code>lakectl refs-dump &lt;repository URI&gt; [flags]\n</code></pre> Options <pre><code>  -h, --help                     help for refs-dump\n  -o, --output string            output filename (default stdout)\n      --poll-interval duration   poll status check interval (default 3s)\n      --timeout duration         timeout for polling status checks (default 1h0m0s)\n</code></pre>"},{"location":"reference/cli/#lakectl-refs-restore","title":"lakectl refs-restore","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Restores refs (branches, commits, tags) from the underlying object store to a bare repository</p> Synopsis <p>restores refs (branches, commits, tags) from the underlying object store to a bare repository.</p> <p>This command is expected to run on a bare repository (i.e. one created with 'lakectl repo create-bare'). Since a bare repo is expected, in case of transient failure, delete the repository and recreate it as bare and retry.</p> <pre><code>lakectl refs-restore &lt;repository URI&gt; [flags]\n</code></pre> Examples <pre><code>aws s3 cp s3://bucket/_lakefs/refs_manifest.json - | lakectl refs-restore lakefs://my-bare-repository --manifest -\n</code></pre> Options <pre><code>  -h, --help                     help for refs-restore\n      --manifest refs-dump       path to a refs manifest json file (as generated by refs-dump). Alternatively, use \"-\" to read from stdin\n      --poll-interval duration   poll status check interval (default 3s)\n      --timeout duration         timeout for polling status checks (default 1h0m0s)\n</code></pre>"},{"location":"reference/cli/#lakectl-usage","title":"lakectl usage","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Usage reports from lakeFS</p> Options <pre><code>  -h, --help   help for usage\n</code></pre>"},{"location":"reference/cli/#lakectl-usage-help","title":"lakectl usage help","text":"<p>Help about any command</p> Synopsis <p>Help provides help for any command in the application. Simply type usage help [path to command] for full details.</p> <pre><code>lakectl usage help [command] [flags]\n</code></pre> Options <pre><code>  -h, --help   help for help\n</code></pre>"},{"location":"reference/cli/#lakectl-usage-summary","title":"lakectl usage summary","text":"<p>Warning</p> <p>lakeFS plumbing command. Don't use unless you're really sure you know what you're doing.</p> <p>Summary reports from lakeFS</p> <pre><code>lakectl usage summary [flags]\n</code></pre> Options <pre><code>  -h, --help   help for summary\n</code></pre>"},{"location":"reference/configuration/","title":"lakeFS Server Configuration","text":"<p>Configuring lakeFS is done using a YAML configuration file and/or environment variable. The configuration file's location can be set with the '--config' flag. If not specified, the first file found in the following order will be used:</p> <ol> <li><code>./config.yaml</code></li> <li><code>$HOME/lakefs/config.yaml</code></li> <li><code>/etc/lakefs/config.yaml</code></li> <li><code>$HOME/.lakefs.yaml</code></li> </ol> <p>Configuration items can each be controlled by an environment variable. The variable name will have a prefix of <code>LAKEFS_</code>, followed by the name of the configuration, replacing every <code>.</code> with a <code>_</code>.</p> <p>Example</p> <p><code>LAKEFS_LOGGING_LEVEL</code> controls <code>logging.level</code>.</p> <p>This reference uses <code>.</code> to denote the nesting of values.</p>"},{"location":"reference/configuration/#reference","title":"Reference","text":"<ul> <li><code>listen_address</code> <code>(string : \"0.0.0.0:8000\")</code> - A <code>&lt;host&gt;:&lt;port&gt;</code> structured string representing the address to listen on</li> </ul>"},{"location":"reference/configuration/#logging","title":"logging","text":"<ul> <li><code>logging.format</code> <code>(one of [\"json\", \"text\"] : \"text\")</code> - Format to output log message in</li> <li><code>logging.level</code> <code>(one of [\"TRACE\", \"DEBUG\", \"INFO\", \"WARN\", \"ERROR\", \"NONE\"] : \"INFO\")</code> - Logging level to output</li> <li> <p><code>logging.audit_log_level</code> <code>(one of [\"TRACE\", \"DEBUG\", \"INFO\", \"WARN\", \"ERROR\", \"NONE\"] : \"DEBUG\")</code> - Audit logs level to output.</p> <p>Note</p> <p>In case you configure this field to be lower than the main logger level, you won't be able to get the audit logs</p> </li> <li> <p><code>logging.output</code> <code>(string : \"-\")</code> - A path or paths to write logs to. A <code>-</code> means the standard output, <code>=</code> means the standard error.</p> </li> <li><code>logging.file_max_size_mb</code> <code>(int : 100)</code> - Output file maximum size in megabytes.</li> <li><code>logging.files_keep</code> <code>(int : 0)</code> - Number of log files to keep, default is all.</li> </ul>"},{"location":"reference/configuration/#actions","title":"actions","text":"<ul> <li><code>actions.enabled</code> <code>(bool : true)</code> - Setting this to false will block hooks from being executed.</li> <li><code>actions.lua.net_http_enabled</code> <code>(bool : false)</code> - Setting this to true will load the <code>net/http</code> package.</li> <li><code>actions.env.enabled</code> <code>(bool : true)</code> - Environment variables accessible by hooks, disabled values evaluated to empty strings</li> <li><code>actions.env.prefix</code> <code>(string : \"LAKEFSACTION_\")</code> - Access to environment variables is restricted to those with the prefix. When environment access is enabled and no prefix is provided, all variables are accessible.</li> </ul>"},{"location":"reference/configuration/#database","title":"database","text":"<p>Configuration section for the lakeFS key-value store database.</p> <ul> <li><code>database.type</code> <code>(string [\"postgres\"|\"dynamodb\"|\"cosmosdb\"|\"local\"] : )</code> - lakeFS database type</li> </ul> <code>database.postgres</code><code>database.dynamodb</code><code>database.cosmosdb</code><code>database.local</code> <ul> <li><code>database.postgres.connection_string</code> <code>(string : \"postgres://localhost:5432/postgres?sslmode=disable\")</code> - PostgreSQL connection string to use</li> <li><code>database.postgres.max_open_connections</code> <code>(int : 25)</code> - Maximum number of open connections to the database</li> <li><code>database.postgres.max_idle_connections</code> <code>(int : 25)</code> - Maximum number of connections in the idle connection pool</li> <li><code>database.postgres.connection_max_lifetime</code> <code>(duration : 5m)</code> - Sets the maximum amount of time a connection may be reused <code>(valid units: ns|us|ms|s|m|h)</code></li> </ul> <ul> <li><code>database.dynamodb.table_name</code> <code>(string : \"kvstore\")</code> - Table used to store the data</li> <li> <p><code>database.dynamodb.scan_limit</code> <code>(int : 1025)</code> - Maximal number of items per page during scan operation</p> <p>Note</p> <p>Refer to the following AWS documentation for further information</p> </li> <li> <p><code>database.dynamodb.endpoint</code> <code>(string : )</code> - Endpoint URL for database instance</p> </li> <li><code>database.dynamodb.aws_region</code> <code>(string : )</code> - AWS Region of database instance</li> <li><code>database.dynamodb.aws_profile</code> <code>(string : )</code> - AWS named profile to use</li> <li><code>database.dynamodb.aws_access_key_id</code> <code>(string : )</code> - AWS access key ID</li> <li> <p><code>database.dynamodb.aws_secret_access_key</code> <code>(string : )</code> - AWS secret access key</p> <p>Note</p> <p><code>endpoint</code> <code>aws_region</code> <code>aws_access_key_id</code> <code>aws_secret_access_key</code> are not required and used mainly for experimental purposes when working with DynamoDB with different AWS credentials.</p> </li> <li> <p><code>database.dynamodb.health_check_interval</code> <code>(duration : 0s)</code> - Interval to run health check for the DynamoDB instance (won't run if equal to 0).</p> </li> <li><code>database.dynamodb.max_attempts</code> <code>(int : 10)</code> - The maximum number of attempts to perform on a DynamoDB request</li> <li><code>database.dynamodb.max_connections</code> <code>(int : 0)</code> - The maximum number of connections to DynamoDB. 0 means no limit.</li> </ul> <ul> <li><code>database.cosmosdb.key</code> <code>(string : \"\")</code> - If specified, will  be used to authenticate to the CosmosDB account. Otherwise, Azure SDK  default authentication (with env vars) will be used.</li> <li><code>database.cosmosdb.endpoint</code> <code>(string : \"\")</code> - CosmosDB account endpoint, e.g. <code>https://&lt;account&gt;.documents.azure.com/</code>.</li> <li><code>database.cosmosdb.database</code> <code>(string : \"\")</code> - CosmosDB database name.</li> <li><code>database.cosmosdb.container</code> <code>(string : \"\")</code> - CosmosDB container name.</li> <li><code>database.cosmosdb.throughput</code> <code>(int32 : )</code> - CosmosDB container's RU/s. If not set - the default CosmosDB container throughput is used. </li> <li><code>database.cosmosdb.autoscale</code> <code>(bool : false)</code> - If set, CosmosDB container throughput is autoscaled (See CosmosDB docs for minimum throughput requirement). Otherwise, uses \"Manual\" mode (Docs).</li> </ul> <ul> <li><code>database.local.path</code> <code>(string : \"~/lakefs/metadata\")</code> - Local path on the filesystem to store embedded KV metadata, like branches and uncommitted entries</li> <li><code>database.local.sync_writes</code> <code>(bool: true)</code> - Ensure each write is written to the disk. Disable to increase performance</li> <li><code>database.local.prefetch_size</code> <code>(int: 256)</code> - How many items to prefetch when iterating over embedded KV records</li> <li><code>database.local.enable_logging</code> <code>(bool: false)</code> - Enable trace logging for local driver</li> </ul>"},{"location":"reference/configuration/#auth","title":"auth","text":"<ul> <li><code>auth.login_duration</code> <code>(time duration : \"168h\")</code> - The duration the login token is valid for</li> <li><code>auth.login_max_duration</code> <code>(time duration : \"168h\")</code> - The maximum duration user can ask for a login token</li> <li><code>auth.cookie_domain</code> <code>(string : \"\")</code> - Domain attribute to set the access_token cookie on (the default is an empty string which defaults to the same host that sets the cookie)</li> <li> <p><code>auth.ui_config.rbac</code> <code>(string: \"none\")</code> - \"none\", \"simplified\", \"external\" or \"internal\" (enterprise feature). </p> <p>If you have configured an external auth server you can set this to \"external\" to support the policy editor.</p> <p>If you are using the enteprrise version of lakeFS, you can set this to \"internal\" to use the built-in policy editor.</p> </li> <li> <p><code>auth.ui_config.use_login_placeholders</code> <code>(bool: false)</code> - If set to true, the login page will show placeholders for the Access Key ID and Secret Access Key (Username and Password).</p> </li> </ul>"},{"location":"reference/configuration/#authcache","title":"auth.cache","text":"<ul> <li><code>auth.cache.enabled</code> <code>(bool : true)</code> - Whether to cache access credentials and user policies in-memory. Can greatly improve throughput when enabled.</li> <li><code>auth.cache.size</code> <code>(int : 1024)</code> - How many items to store in the auth cache. Systems with a very high user count should use a larger value at the expense of ~1kb of memory per cached user.</li> <li><code>auth.cache.ttl</code> <code>(time duration : \"20s\")</code> - How long to store an item in the auth cache. Using a higher value reduces load on the database, but will cause changes longer to take effect for cached users.</li> <li><code>auth.cache.jitter</code> <code>(time duration : \"3s\")</code> - A random amount of time between 0 and this value is added to each item's TTL. This is done to avoid a large bulk of keys expiring at once and overwhelming the database.</li> <li> <p><code>auth.encrypt.secret_key</code> <code>(string : required)</code> - A random (cryptographically safe) generated string that is used for encryption and HMAC signing</p> <p>Warning</p> <p>It is best to keep this somewhere safe such as KMS or Hashicorp Vault, and provide it to the system at run time</p> </li> </ul>"},{"location":"reference/configuration/#authapi","title":"auth.api","text":"<ul> <li><code>auth.api.endpoint</code> <code>(string: https://external.service/api/v1)</code> - URL to external Authorization Service described at authorization.yml;</li> <li><code>auth.api.token</code> <code>(string: eyJhbGciOiJIUzI1NiIsInR5...)</code> - API token used to authenticate requests to api endpoint</li> <li><code>auth.api.health_check_timeout</code> <code>(time duration : \"20s\")</code> - Timeout duration for external auth API health check</li> <li><code>auth.api.skip_health_check</code> <code>(bool : false)</code> - Skip external auth API health check</li> </ul>"},{"location":"reference/configuration/#authauthentication_api","title":"auth.authentication_api","text":"<ul> <li><code>auth.authentication_api.endpoint</code> <code>(string : \"\")</code> - URL to external Authentication Service described at authentication.yml;</li> <li><code>auth.authentication_api.external_principals_enabled</code> <code>(bool : false)</code> - If true, external principals API will be enabled, e.g auth service and login api's.</li> </ul>"},{"location":"reference/configuration/#authremote_authenticator","title":"auth.remote_authenticator","text":"<ul> <li><code>auth.remote_authenticator.enabled</code> <code>(bool : false)</code> - If specified, also authenticate users via this Remote Authenticator server.</li> <li><code>auth.remote_authenticator.endpoint</code> <code>(string : required)</code> - Endpoint URL of the remote authentication service (e.g. https://my-auth.example.com/auth).</li> <li><code>auth.remote_authenticator.default_user_group</code> <code>(string : Viewers)</code> - Create users in this group (i.e <code>Viewers</code>, <code>Developers</code>, etc).</li> <li><code>auth.remote_authenticator.request_timeout</code> <code>(duration : 10s)</code> - If specified, timeout for remote authentication requests.</li> </ul>"},{"location":"reference/configuration/#authcookie_auth_verification","title":"auth.cookie_auth_verification","text":"<ul> <li><code>auth.cookie_auth_verification.validate_id_token_claims</code> <code>(map[string]string : )</code> - When a user tries to access lakeFS, validate that the ID token contains these claims with the corresponding values.</li> <li><code>auth.cookie_auth_verification.default_initial_groups</code> (string[] : [])` - By default, users will be assigned to these groups</li> <li><code>auth.cookie_auth_verification.initial_groups_claim_name</code> <code>(string[] : [])</code> - Use this claim from the ID token to provide the initial group for new users. This will take priority if <code>auth.cookie_auth_verification.default_initial_groups</code> is also set.</li> <li><code>auth.cookie_auth_verification.friendly_name_claim_name</code> <code>(string[] : )</code> - If specified, the value from the claim with this name will be used as the user's display name.</li> <li><code>auth.cookie_auth_verification.persist_friendly_name</code> <code>(string : false)</code> - If set to <code>true</code>, the friendly name is persisted to the KV store and can be displayed in the user list. This is meant to be used in conjunction with <code>auth.cookie_auth_verification.friendly_name_claim_name</code>.</li> <li><code>auth.cookie_auth_verification.external_user_id_claim_name</code> - <code>(string : )</code> - If specified, the value from the claim with this name will be used as the user's id name.</li> <li><code>auth.cookie_auth_verification.auth_source</code> - <code>(string : )</code> - If specified, user will be labeled with this auth source.</li> </ul>"},{"location":"reference/configuration/#authoidc","title":"auth.oidc","text":"<ul> <li><code>auth.oidc.default_initial_groups</code> <code>(string[] : [])</code> - By default, OIDC users will be assigned to these groups</li> <li><code>auth.oidc.initial_groups_claim_name</code> <code>(string[] : [])</code> - Use this claim from the ID token to provide the initial group for new users. This will take priority if <code>auth.oidc.default_initial_groups</code> is also set.</li> <li><code>auth.oidc.friendly_name_claim_name</code> <code>(string[] : )</code> - If specified, the value from the claim with this name will be used as the user's display name.</li> <li><code>auth.oidc.persist_friendly_name</code> <code>(string : false)</code> - If set to <code>true</code>, the friendly name is persisted to the KV store and can be displayed in the user list. This is meant to be used in conjunction with <code>auth.oidc.friendly_name_claim_name</code>.</li> <li><code>auth.oidc.validate_id_token_claims</code> <code>(map[string]string : )</code> - When a user tries to access lakeFS, validate that the ID token contains these claims with the corresponding values.</li> </ul>"},{"location":"reference/configuration/#blockstore","title":"blockstore","text":"<ul> <li><code>blockstore.type</code> <code>(one of [\"local\", \"s3\", \"gs\", \"azure\", \"mem\"] : required)</code>. Block adapter to use. This controls where the underlying data will be stored</li> <li><code>blockstore.default_namespace_prefix</code> <code>(string : )</code> - Use this to help your users choose a storage namespace for their repositories.    If specified, the storage namespace will be filled with this default value as a prefix when creating a repository from the UI.    The user may still change it to something else.</li> <li><code>blockstore.signing.secret_key</code> <code>(string : required)</code> - A random generated string that is used for HMAC signing when using get/link physical address</li> </ul> <code>blockstore.local</code><code>blockstore.s3</code><code>blockstore.azure</code><code>blockstore.gs</code> <ul> <li><code>blockstore.local.path</code> <code>(string: \"~/lakefs/data/block\")</code> - When using the local Block Adapter, which directory to store files in</li> <li><code>blockstore.local.import_enabled</code> <code>(bool: false)</code> - Enable import for local Block Adapter, relevant only if you are using shared location</li> <li><code>blockstore.local.import_hidden</code> <code>(bool: false)</code> - When enabled import will scan and import any file or folder that starts with a dot character.</li> <li><code>blockstore.local.allowed_external_prefixes</code> <code>([]string: [])</code> - List of absolute path prefixes used to match any access for external location (ex: /var/data/). Empty list mean no access to external location.</li> </ul> <ul> <li><code>blockstore.s3.region</code> <code>(string : \"us-east-1\")</code> - Default region for lakeFS to use when interacting with S3.</li> <li><code>blockstore.s3.profile</code> <code>(string : )</code> - If specified, will be used as a named credentials profile</li> <li><code>blockstore.s3.credentials_file</code> <code>(string : )</code> - If specified, will be used as a credentials file</li> <li><code>blockstore.s3.credentials.access_key_id</code> <code>(string : )</code> - If specified, will be used as a static set of credential</li> <li><code>blockstore.s3.credentials.secret_access_key</code> <code>(string : )</code> - If specified, will be used as a static set of credential</li> <li><code>blockstore.s3.credentials.session_token</code> <code>(string : )</code> - If specified, will be used as a static session token</li> <li><code>blockstore.s3.endpoint</code> <code>(string : )</code> - If specified, custom endpoint for the AWS S3 API (https://s3_compatible_service_endpoint:port)</li> <li><code>blockstore.s3.force_path_style</code> <code>(bool : false)</code> - When true, use path-style S3 URLs (https:/// instead of https://.) <li><code>blockstore.s3.discover_bucket_region</code> <code>(bool : true)</code> - (Can be turned off if the underlying S3 bucket doesn't support the GetBucketRegion API).</li> <li><code>blockstore.s3.skip_verify_certificate_test_only</code> <code>(bool : false)</code> - Skip certificate verification while connecting to the storage endpoint. Should be used only for testing.</li> <li><code>blockstore.s3.server_side_encryption</code> <code>(string : )</code> - Server side encryption format used (Example on AWS using SSE-KMS while passing \"aws:kms\")</li> <li><code>blockstore.s3.server_side_encryption_kms_key_id</code> <code>(string : )</code> - Server side encryption KMS key ID</li> <li><code>blockstore.s3.pre_signed_expiry</code> <code>(time duration : \"15m\")</code> - Expiry of pre-signed URL.</li> <li><code>blockstore.s3.pre_signed_endpoint</code> <code>(string : )</code> - Custom endpoint for pre-signed URLs.</li> <li><code>blockstore.s3.disable_pre_signed</code> <code>(bool : false)</code> - Disable use of pre-signed URL.</li> <li><code>blockstore.s3.disable_pre_signed_ui</code> <code>(bool : true)</code> - Disable use of pre-signed URL in the UI.</li> <li><code>blockstore.s3.disable_pre_signed_multipart</code> <code>(bool : )</code> - Disable use of pre-signed multipart upload experimental, enabled on s3 block adapter with presign support.</li> <li><code>blockstore.s3.client_log_request</code> <code>(bool : false)</code> - Set SDK logging bit to log requests</li> <li><code>blockstore.s3.client_log_retries</code> <code>(bool : false)</code> - Set SDK logging bit to log retries</li> <ul> <li><code>blockstore.azure.storage_account</code> <code>(string : )</code> - If specified, will be used as the Azure storage account</li> <li><code>blockstore.azure.storage_access_key</code> <code>(string : )</code> - If specified, will be used as the Azure storage access key</li> <li><code>blockstore.azure.pre_signed_expiry</code> <code>(time duration : \"15m\")</code> - Expiry of pre-signed URL.</li> <li><code>blockstore.azure.disable_pre_signed</code> <code>(bool : false)</code> - Disable use of pre-signed URL.</li> <li><code>blockstore.azure.disable_pre_signed_ui</code> <code>(bool : true)</code> - Disable use of pre-signed URL in the UI.</li> <li> <p><code>blockstore.azure.china_cloud</code> <code>(bool : false)</code> - Enable for using lakeFS on Azure China Cloud.  </p> <p>Deprecated</p> <p>Please use <code>blockstore.azure.domain</code></p> </li> <li> <p><code>blockstore.azure.domain</code> <code>(string : blob.core.windows.net)</code> - Enables support of different Azure cloud domains. </p> <p>Current supported domains (in Beta stage): [<code>blob.core.chinacloudapi.cn</code>, <code>blob.core.usgovcloudapi.net</code>]</p> </li> </ul> <ul> <li><code>blockstore.gs.credentials_file</code> <code>(string : )</code> - If specified will be used as a file path of the JSON file that contains your Google service account key</li> <li><code>blockstore.gs.credentials_json</code> <code>(string : )</code> - If specified will be used as JSON string that contains your Google service account key (when credentials_file is not set)</li> <li><code>blockstore.gs.pre_signed_expiry</code> <code>(time duration : \"15m\")</code> - Expiry of pre-signed URL.</li> <li><code>blockstore.gs.disable_pre_signed</code> <code>(bool : false)</code> - Disable use of pre-signed URL.</li> <li><code>blockstore.gs.disable_pre_signed_ui</code> <code>(bool : true)</code> - Disable use of pre-signed URL in the UI.</li> <li><code>blockstore.gs.server_side_encryption_customer_supplied</code> <code>(string : )</code> - Server side encryption with AES key in hex format, exclusive with key ID below</li> <li><code>blockstore.gs.server_side_encryption_kms_key_id</code> <code>(string : )</code> - Server side encryption KMS key ID, exclusive with above</li> </ul>"},{"location":"reference/configuration/#graveler","title":"graveler","text":"<ul> <li><code>graveler.ensure_readable_root_namespace</code> <code>(bool: true)</code> - When creating a new repository use this to verify that lakeFS has access to the root of the underlying storage namespace. Set <code>false</code> only if lakeFS should not have access (i.e pre-sign mode only).</li> <li><code>graveler.max_batch_delay</code> <code>(duration : 3ms)</code> - Controls the server batching period for references store operations.</li> <li><code>graveler.background.rate_limit</code> <code>(int : 0)</code> - Requests per seconds limit on background work performed (default: 0 - unlimited), like deleting committed staging tokens.</li> </ul>"},{"location":"reference/configuration/#gravelerrepository_cache","title":"graveler.repository_cache","text":"<ul> <li><code>graveler.reposiory_cache.size</code> <code>(int : 1000)</code> - How many items to store in the repository cache.</li> <li><code>graveler.reposiory_cache.ttl</code> <code>(time duration : \"5s\")</code> - How long to store an item in the repository cache.</li> <li><code>graveler.reposiory_cache.jitter</code> <code>(time duration : \"2s\")</code> - A random amount of time between 0 and this value is added to each item's TTL.</li> </ul>"},{"location":"reference/configuration/#gravelercommit_cache","title":"graveler.commit_cache","text":"<ul> <li><code>graveler.commit_cache.size</code> <code>(int : 50000)</code> - How many items to store in the commit cache.</li> <li><code>graveler.commit_cache.ttl</code> <code>(time duration : \"10m\")</code> - How long to store an item in the commit cache.</li> <li><code>graveler.commit_cache.jitter</code> <code>(time duration : \"2s\")</code> - A random amount of time between 0 and this value is added to each item's TTL.</li> </ul>"},{"location":"reference/configuration/#committed","title":"committed","text":"<ul> <li><code>committed.block_storage_prefix</code> (<code>string</code> : <code>_lakefs</code>) - Prefix for metadata file storage   in each repository's storage namespace</li> <li><code>committed.sstable.memory.cache_size_bytes</code> (<code>int</code> : <code>200_000_000</code>) - maximal size of   in-memory cache used for each SSTable reader.</li> </ul>"},{"location":"reference/configuration/#committedlocal_cache","title":"committed.local_cache","text":"<p>An object describing the local (on-disk) cache of metadata from permanent storage.</p> <ul> <li><code>committed.local_cache.size_bytes</code> (<code>int</code> : <code>1073741824</code>) - bytes for local cache to use on disk.  The cache may use more storage for short periods of time.</li> <li><code>committed.local_cache.dir</code> (<code>string</code>, <code>~/lakefs/data/cache</code>) - directory to store local cache.</li> <li><code>committed.local_cache.range_proportion</code> (<code>float</code> : <code>0.9</code>) - proportion of local cache to   use for storing ranges (leaves of committed metadata storage).</li> <li><code>committed.local_cache.range.open_readers</code> (<code>int</code> : <code>500</code>) - maximal number of unused open   SSTable readers to keep for ranges.</li> <li><code>committed.local_cache.range.num_shards</code> (<code>int</code> : <code>30</code>) - sharding factor for open SSTable   readers for ranges.  Should be at least <code>sqrt(committed.local_cache.range.open_readers)</code>.</li> <li><code>committed.local_cache.metarange_proportion</code> (<code>float</code> : <code>0.1</code>) - proportion of local cache   to use for storing metaranges (roots of committed metadata storage).</li> <li><code>committed.local_cache.metarange.open_readers</code> (<code>int</code> : <code>50</code>) - maximal number of unused open   SSTable readers to keep for metaranges.</li> <li><code>committed.local_cache.metarange.num_shards</code> (<code>int</code> : <code>10</code>) - sharding factor for open   SSTable readers for metaranges.  Should be at least   <code>sqrt(committed.local_cache.metarange.open_readers)</code>.</li> </ul>"},{"location":"reference/configuration/#committedpermanent","title":"committed.permanent","text":"<ul> <li><code>committed.permanent.min_range_size_bytes</code> (<code>int</code> : <code>0</code>) - Smallest allowable range in   metadata.  Increase to somewhat reduce random access time on committed metadata, at the cost   of increased committed metadata storage cost.</li> <li><code>committed.permanent.max_range_size_bytes</code> (<code>int</code> : <code>20971520</code>) - Largest allowable range in   metadata.  Should be close to the size at which fetching from remote storage becomes linear.</li> <li><code>committed.permanent.range_raggedness_entries</code> (<code>int</code> : <code>50_000</code>) - Average number of object   pointers to store in each range (subject to <code>min_range_size_bytes</code> and   <code>max_range_size_bytes</code>).</li> </ul>"},{"location":"reference/configuration/#email","title":"email","text":"<ul> <li><code>email.smtp_host</code> <code>(string)</code> - A string representing the URL of the SMTP host.</li> <li><code>email.smtp_port</code> (<code>int</code>) - An integer representing the port of the SMTP service (465, 587, 993, 25 are some standard ports)</li> <li><code>email.use_ssl</code> (<code>bool : false</code>) - Use SSL connection with SMTP host.</li> <li><code>email.username</code> <code>(string)</code> - A string representing the username of the specific account at the SMTP. It's recommended to provide this value at runtime from a secret vault of some sort.</li> <li><code>email.password</code> <code>(string)</code> - A string representing the password of the account. It's recommended to provide this value at runtime from a secret vault of some sort.</li> <li><code>email.local_name</code> <code>(string)</code> - A string representing the hostname sent to the SMTP server with the HELO command. By default, \"localhost\" is sent.</li> <li><code>email.sender</code> <code>(string)</code> - A string representing the email account which is set as the sender.</li> <li><code>email.limit_every_duration</code> <code>(duration : 1m)</code> - The average time between sending emails. If zero is entered, there is no limit to the amount of emails that can be sent.</li> <li><code>email.burst</code> <code>(int: 10)</code> - Maximal burst of emails before applying <code>limit_every_duration</code>. The zero value means no burst and therefore no emails can be sent.</li> <li><code>email.lakefs_base_url</code> <code>(string : \"http://localhost:8000\")</code> - A string representing the base lakeFS endpoint to be directed to when emails are sent inviting users, reseting passwords etc.</li> </ul>"},{"location":"reference/configuration/#gateways","title":"gateways","text":"<ul> <li><code>gateways.s3.domain_name</code> <code>(string : \"s3.local.lakefs.io\")</code> - a FQDN   representing the S3 endpoint used by S3 clients to call this server   (<code>*.s3.local.lakefs.io</code> always resolves to 127.0.0.1, useful for   local development, if using virtual-host addressing.</li> <li><code>gateways.s3.region</code> <code>(string : \"us-east-1\")</code> - AWS region we're pretending to be in, it should match the region configuration used in AWS SDK clients</li> <li><code>gateways.s3.fallback_url</code> <code>(string)</code> - If specified, requests with a non-existing repository will be forwarded to this URL. This can be useful for using lakeFS side-by-side with S3, with the URL pointing at an S3Proxy instance.</li> <li><code>gateways.s3.verify_unsupported</code> <code>(bool : true)</code> - The S3 gateway errors on unsupported requests, but when disabled, defers to target-based handlers.</li> </ul>"},{"location":"reference/configuration/#tls","title":"tls","text":"<ul> <li><code>tls.enabled</code> <code>(bool :false)</code> - Enable TLS listening. The <code>listen_address</code> will be used to serve HTTPS requests. (mainly for local development)</li> <li><code>tls.cert_file</code> <code>(string : )</code> - Server certificate file path used while serve HTTPS (.cert or .crt file - signed certificates).</li> <li><code>tls.key_file</code> <code>(string : )</code> - Server secret key file path used whie serve HTTPS (.key file - private key).</li> </ul>"},{"location":"reference/configuration/#stats","title":"stats","text":"<ul> <li><code>stats.enabled</code> <code>(bool : true)</code> - Whether to periodically collect anonymous usage statistics</li> <li><code>stats.flush_interval</code> <code>(duration : 30s)</code> - Interval used to post anonymous statistics collected</li> <li><code>stats.flush_size</code> <code>(int : 100)</code> - A size (in records) of anonymous statistics collected in which we post</li> </ul>"},{"location":"reference/configuration/#installation","title":"installation","text":"<ul> <li><code>installation.user_name</code> <code>(string : )</code> - When specified, an initial admin user will be created when the server is first run. Works only when <code>database.type</code> is set to local. Requires <code>installation.access_key_id</code> and <code>installation.secret_access_key</code>. </li> <li><code>installation.access_key_id</code> <code>(string : )</code> - Admin's initial access key id (used once in the initial setup process)</li> <li><code>installation.secret_access_key</code> <code>(string : )</code> - Admin's initial secret access key (used once in the initial setup process)</li> <li><code>installation.allow_inter_region_storage</code> <code>(bool : true)</code> - Allow storage in a different region than the one the server is running in.</li> </ul>"},{"location":"reference/configuration/#usage_report","title":"usage_report","text":"<ul> <li><code>usage_report.enabled</code> <code>(bool : true)</code> - Store API and Gateway usage reports into key-value store.</li> <li><code>usage_report.flush_interval</code> <code>(duration : 5m)</code> - Sets interval for flushing in-memory usage data to key-value store.</li> </ul>"},{"location":"reference/configuration/#ui","title":"ui","text":"<ul> <li><code>ui.enabled</code> <code>(bool: true)</code> - Whether to serve the embedded UI from the binary</li> </ul>"},{"location":"reference/configuration/#security","title":"security","text":"<ul> <li><code>security.audit_check_interval</code> <code>(duration : 24h)</code> - Duration in which we check for security audit.</li> </ul>"},{"location":"reference/configuration/#garbage-collection","title":"garbage collection","text":"<ul> <li><code>ugc.prepare_max_file_size</code> <code>(int: 125829120)</code> - Uncommitted garbage collection prepare request, limit the produced file maximum size</li> <li><code>ugc.prepare_interval</code> <code>(duraction: 1m)</code> - Uncommitted garbage collection prepare request, limit produce time to interval</li> </ul>"},{"location":"reference/configuration/#lakefs-enterprise","title":"lakeFS Enterprise","text":"<p>This section provides configuration references exclusive to lakeFS Enterprise, and serves as a complement to the general configuration reference above.</p>"},{"location":"reference/configuration/#blockstores","title":"blockstores","text":"<p>Info</p> <p>The <code>blockstores</code> configuration is required for multi-storage backend setups and replaces the previous <code>blockstore</code> configuration.</p> <ul> <li><code>blockstores.signing.secret_key</code> <code>(string : required)</code> - A random generated string that is used for HMAC signing when using get/link physical address</li> <li><code>blockstores.stores</code> <code>([{id: string, type: string, ...}] : required)</code> - Defines multiple storage backends used in a multi-storage backend setup. Each storage backend must have a unique id and a valid configuration.</li> </ul>"},{"location":"reference/configuration/#common-fields-for-all-stores","title":"Common Fields for All Stores","text":"<ul> <li><code>blockstores.stores[].id</code> <code>(string : required)</code> - Unique identifier for the storage backend.</li> <li><code>blockstores.stores[].backward_compatible</code> <code>(bool : false)</code> - Optional. Defaults to false. Used to migrate from a single to a multi-storage backend setup.</li> <li><code>blockstores.stores[].description</code> <code>(string : )</code> - A human-readable description of the storage backend.</li> <li><code>blockstores.stores[].type</code> <code>(string : required)</code> - <code>(one of [\"local\", \"s3\", \"gs\", \"azure\", \"mem\"] : required)</code>. Block adapter to use. This controls where the underlying data will be stored.</li> </ul> <code>blockstores.stores.local</code><code>blockstores.stores.s3</code><code>blockstores.azure</code><code>blockstores.gs</code> <ul> <li><code>blockstores.stores[].local.path</code> <code>(string: \"~/lakefs/data\")</code> - When using the local Block Adapter, which directory to store files in</li> <li><code>blockstores.stores[].local.import_enabled</code> <code>(bool: false)</code> - Enable import for local Block Adapter, relevant only if you are using shared location</li> <li><code>blockstores.stores[].local.import_hidden</code> <code>(bool: false)</code> - When enabled import will scan and import any file or folder that starts with a dot character.</li> <li><code>blockstores.stores[].local.allowed_external_prefixes</code> <code>([]string: [])</code> - List of absolute path prefixes used to match any access for external location (ex: /var/data/). Empty list mean no access to external location.</li> </ul> <ul> <li><code>blockstores.stores[].s3.region</code> <code>(string : \"us-east-1\")</code> - Default region for lakeFS to use when interacting with S3.</li> <li><code>blockstores.stores[].s3.profile</code> <code>(string : )</code> - If specified, will be used as a named credentials profile</li> <li><code>blockstores.stores[].credentials_file</code> <code>(string : )</code> - If specified, will be used as a credentials file</li> <li><code>blockstores.stores[].credentials.access_key_id</code> <code>(string : )</code> - If specified, will be used as a static set of credential</li> <li><code>blockstores.stores[].credentials.secret_access_key</code> <code>(string : )</code> - If specified, will be used as a static set of credential</li> <li><code>blockstores.stores[].s3.credentials.session_token</code> <code>(string : )</code> - If specified, will be used as a static session token</li> <li><code>blockstores.stores[].s3.endpoint</code> <code>(string : )</code> - If specified, custom endpoint for the AWS S3 API (https://s3_compatible_service_endpoint:port)</li> <li><code>blockstores.stores[].s3.force_path_style</code> <code>(bool : false)</code> - When true, use path-style S3 URLs (https:/// instead of https://.) <li><code>blockstores.stores[].s3.discover_bucket_region</code> <code>(bool : true)</code> - (Can be turned off if the underlying S3 bucket doesn't support the GetBucketRegion API).</li> <li><code>blockstores.stores[].s3.skip_verify_certificate_test_only</code> <code>(bool : false)</code> - Skip certificate verification while connecting to the storage endpoint. Should be used only for testing.</li> <li><code>blockstores.stores[].s3.server_side_encryption</code> <code>(string : )</code> - Server side encryption format used (Example on AWS using SSE-KMS while passing \"aws:kms\")</li> <li><code>blockstores.stores[].s3.server_side_encryption_kms_key_id</code> <code>(string : )</code> - Server side encryption KMS key ID</li> <li><code>blockstores.stores[].s3.pre_signed_expiry</code> <code>(time duration : \"15m\")</code> - Expiry of pre-signed URL.</li> <li><code>blockstores.stores[].s3.pre_signed_endpoint</code> <code>(string : )</code> - Custom endpoint for pre-signed URLs.</li> <li><code>blockstores.stores[].s3.disable_pre_signed</code> <code>(bool : false)</code> - Disable use of pre-signed URL.</li> <li><code>blockstores.stores[].s3.disable_pre_signed_ui</code> <code>(bool : true)</code> - Disable use of pre-signed URL in the UI.</li> <li><code>blockstores.stores[].s3.disable_pre_signed_multipart</code> <code>(bool : )</code> - Disable use of pre-signed multipart upload experimental, enabled on S3 block adapter with presign support.</li> <li><code>blockstores.stores[].s3.client_log_request</code> <code>(bool : false)</code> - Set SDK logging bit to log requests</li> <li><code>blockstores.stores[].s3.client_log_retries</code> <code>(bool : false)</code> - Set SDK logging bit to log retries</li> <ul> <li><code>blockstores.stores[].azure.storage_account</code> <code>(string : )</code> - If specified, will be used as the Azure storage account</li> <li><code>blockstores.stores[].azure.storage_access_key</code> <code>(string : )</code> - If specified, will be used as the Azure storage access key</li> <li><code>blockstores.stores[].azure.pre_signed_expiry</code> <code>(time duration : \"15m\")</code> - Expiry of pre-signed URL.</li> <li><code>blockstores.stores[].azure.disable_pre_signed</code> <code>(bool : false)</code> - Disable use of pre-signed URL.</li> <li><code>blockstores.stores[].azure.disable_pre_signed_ui</code> <code>(bool : true)</code> - Disable use of pre-signed URL in the UI.</li> <li><code>blockstores.stores[].azure.domain</code> <code>(string : blob.core.windows.net)</code> - Enables support of different Azure cloud domains. Current supported domains (in Beta stage): [<code>blob.core.chinacloudapi.cn</code>, <code>blob.core.usgovcloudapi.net</code>]</li> </ul> <ul> <li><code>blockstores.stores[].gs.credentials_file</code> <code>(string : )</code> - If specified will be used as a file path of the JSON file that contains your Google service account key</li> <li><code>blockstores.stores[].gs.credentials_json</code> <code>(string : )</code> - If specified will be used as JSON string that contains your Google service account key (when credentials_file is not set)</li> <li><code>blockstores.stores[].gs.pre_signed_expiry</code> <code>(time duration : \"15m\")</code> - Expiry of pre-signed URL.</li> <li><code>blockstores.stores[].gs.disable_pre_signed</code> <code>(bool : false)</code> - Disable use of pre-signed URL.</li> <li><code>blockstores.stores[].gs.disable_pre_signed_ui</code> <code>(bool : true)</code> - Disable use of pre-signed URL in the UI.</li> <li><code>blockstores.stores[].gs.server_side_encryption_customer_supplied</code> <code>(string : )</code> - Server side encryption with AES key in hex format, exclusive with key ID below</li> <li><code>blockstores.stores[].gs.server_side_encryption_kms_key_id</code> <code>(string : )</code> - Server side encryption KMS key ID, exclusive with above</li> </ul>"},{"location":"reference/configuration/#using-environment-variables","title":"Using Environment Variables","text":"<p>All the configuration variables can be set or overridden using environment variables.</p> <p>To set an environment variable, prepend <code>LAKEFS_</code> to its name, convert it to upper case, and replace <code>.</code> with <code>_</code>:</p> <p>For example, <code>logging.format</code> becomes <code>LAKEFS_LOGGING_FORMAT</code>, <code>blockstore.s3.region</code> becomes <code>LAKEFS_BLOCKSTORE_S3_REGION</code>, etc.</p> <p>To set a value into a <code>map[string]string</code> type field, use the syntax <code>key1=value1,key2=value2,...</code></p>"},{"location":"reference/configuration/#example-configurations","title":"Example Configurations","text":"<p>Local Development with PostgreSQL database</p> <pre><code>---\nlisten_address: \"0.0.0.0:8000\"\n\ndatabase:\ntype: \"postgres\"\npostgres:\n    connection_string: \"postgres://localhost:5432/postgres?sslmode=disable\"\n\nlogging:\nformat: text\nlevel: DEBUG\noutput: \"-\"\n\nauth:\nencrypt:\n    secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\"\n\nblockstore:\ntype: local\nlocal:\n    path: \"~/lakefs/dev/data\"\n\ngateways:\ns3:\n    region: us-east-1\n</code></pre> <p>AWS Deployment with DynamoDB database</p> <pre><code>---\nlogging:\nformat: json\nlevel: WARN\noutput: \"-\"\n\ndatabase:\ntype: \"dynamodb\"\ndynamodb:\n    table_name: \"kvstore\"\n\nauth:\nencrypt:\n    secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\"\n\nblockstore:\ntype: s3\ns3:\n    region: us-east-1 # optional, fallback in case discover from bucket is not supported\n    credentials_file: /secrets/aws/credentials\n    profile: default\n</code></pre> <p>Google Storage</p> <pre><code>---\nlogging:\nformat: json\nlevel: WARN\noutput: \"-\"\n\ndatabase:\ntype: \"postgres\"\npostgres:\n    connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\"\n\nauth:\nencrypt:\n    secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\"\n\nblockstore:\ntype: gs\ngs:\n    credentials_file: /secrets/lakefs-service-account.json\n</code></pre> <p>MinIO</p> <pre><code>---\nlogging:\nformat: json\nlevel: WARN\noutput: \"-\"\n\ndatabase:\ntype: \"postgres\"\npostgres:\n    connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\"\n\nauth:\nencrypt:\n    secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\"\n\nblockstore:\ntype: s3\ns3:\n    force_path_style: true\n    endpoint: http://localhost:9000\n    discover_bucket_region: false\n    credentials:\n    access_key_id: minioadmin\n    secret_access_key: minioadmin\n</code></pre> <p>Azure blob storage</p> <pre><code>---\nlogging:\nformat: json\nlevel: WARN\noutput: \"-\"\n\ndatabase:\ntype: \"cosmosdb\"\ncosmosdb:\n    key: \"ExampleReadWriteKeyMD7nkPOWgV7d4BUjzLw==\"\n    endpoint: \"https://lakefs-account.documents.azure.com:443/\"\n    database: \"lakefs-db\"\n    container: \"lakefs-container\"\n\nauth:\nencrypt:\n    secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\"\n\nblockstore:\ntype: azure\nazure:\n    storage_account: exampleStorageAcount\n    storage_access_key: ExampleAcessKeyMD7nkPOWgV7d4BUjzLw==\n</code></pre>"},{"location":"reference/monitor/","title":"Monitoring using Prometheus","text":""},{"location":"reference/monitor/#example-prometheusyml","title":"Example prometheus.yml","text":"<p>lakeFS exposes metrics through the same port used by the lakeFS service, using the standard <code>/metrics</code> path.</p> <p>An example could look like this:</p> <p><code>prometheus.yml</code></p> <pre><code>scrape_configs:\n- job_name: lakeFS\n  scrape_interval: 10s\n  metrics_path: /metrics\n  static_configs:\n  - targets:\n    - lakefs.example.com:8000\n</code></pre>"},{"location":"reference/monitor/#metrics-exposed-by-lakefs","title":"Metrics exposed by lakeFS","text":"<p>By default, Prometheus exports metrics with OS process information like memory and CPU. It also includes Go-specific metrics such as details about GC and a number of goroutines. You can learn about these default metrics in this post.</p> <p>In addition, lakeFS exposes the following metrics to help monitor your deployment:</p> Name in Prometheus Description Labels <code>api_requests_total</code> lakeFS API requests (counter) code: http statusmethod: http method <code>api_request_duration_seconds</code> Durations of lakeFS API requests (histogram) operation: name of API operationcode: http status <code>gateway_request_duration_seconds</code> lakeFS S3-compatible endpoint request (histogram) operation: name of gateway operationcode: http status <code>s3_operation_duration_seconds</code> Outgoing S3 operations (histogram) operation: operation nameerror: \"true\" if error, \"false\" otherwise <code>gs_operation_duration_seconds</code> Outgoing Google Storage operations (histogram) operation: operation nameerror: \"true\" if error, \"false\" otherwise <code>azure_operation_duration_seconds</code> Outgoing Azure storage operations (histogram) operation: operation nameerror: \"true\" if error, \"false\" otherwise <code>kv_request_duration_seconds</code> Durations of KV requests(histogram) operation: name of KV operationtype: KV type(dynamodb, postgres, etc) <code>dynamo_request_duration_seconds</code> Time spent doing DynamoDB requests operation: DynamoDB operation name <code>dynamo_consumed_capacity_total</code> The capacity units consumed by operation operation: DynamoDB operation name <code>dynamo_failures_total</code> The total number of errors while working for kv store operation: DynamoDB operation name <code>pgxpool_acquire_count</code> PostgreSQL cumulative count of successful acquires from the pool db_name default to the kv table name (kv) <code>pgxpool_acquire_duration_ns</code> PostgreSQL total duration of all successful acquires from the pool in nanoseconds db_name default to the kv table name (kv) <code>pgxpool_acquired_conns</code> PostgreSQL number of currently acquired connections in the pool db_name default to the kv table name (kv) <code>pgxpool_canceled_acquire_count</code> PostgreSQL cumulative count of acquires from the pool that were canceled by a context db_name default to the kv table name (kv) <code>pgxpool_constructing_conns</code> PostgreSQL number of conns with construction in progress in the pool db_name default to the kv table name (kv) <code>pgxpool_empty_acquire</code> PostgreSQL cumulative count of successful acquires from the pool that waited for a resource to be released or constructed because the pool was empty db_name default to the kv table name (kv) <code>pgxpool_idle_conns</code> PostgreSQL number of currently idle conns in the pool db_name default to the kv table name (kv) <code>pgxpool_max_conns</code> PostgreSQL maximum size of the pool db_name default to the kv table name (kv) <code>pgxpool_total_conns</code> PostgreSQL total number of resources currently in the pool db_name default to the kv table name (kv)"},{"location":"reference/monitor/#example-queries","title":"Example queries","text":"<p>Note</p> <p>when using Prometheus functions like rate or increase, results are extrapolated and may not be exact.</p> <p>99th percentile of API request latencies</p> <p>```</p> <p>sum by (operation)(histogram_quantile(0.99, rate(api_request_duration_seconds_bucket[1m])))     ```</p> <p>50th percentile of S3-compatible API latencies</p> <pre><code>sum by (operation)(histogram_quantile(0.5, rate(gateway_request_duration_seconds_bucket[1m])))\n</code></pre> <p>Number of errors in outgoing S3 requests</p> <pre><code>sum by (operation) (increase(s3_operation_duration_seconds_count{error=\"true\"}[1m]))\n</code></pre> <p>Number of open connections to the database</p> <pre><code>go_sql_stats_connections_open\n</code></pre>"},{"location":"reference/monitor/#example-grafana-dashboard","title":"Example Grafana dashboard","text":""},{"location":"reference/mount-csi-driver/","title":"Mount CSI Driver (Everest on Kubernetes)","text":"<p>Info</p> <p>Available in lakeFS Cloud and lakeFS Enterprise</p> <p>The lakeFS CSI (Container Storage Interface) Driver is an extension for Kubernetes that enables seamless access to data within a lakeFS repository, allowing Pods to interact with lakeFS data as if it were part of the local filesystem. This driver builds on the functionality of Everest, which provides a read-only view of lakeFS data by virtually mounting a repository.</p> <p>Private Preview</p> <p>The CSI Driver is in private preview. Please contact us to get access.</p>"},{"location":"reference/mount-csi-driver/#how-mount-is-executed-on-a-host","title":"How mount is executed on a Host","text":"<ul> <li>While the <code>csi</code> driver is responsible for mounting and unmounting the volume on the host, it does not need permissions to execute the <code>mount</code> and <code>umount</code> commands directly.</li> <li>The <code>everest</code> commands are executed by <code>systemd</code> service on the Host itself (i.e <code>everest mount...</code>). </li> <li>The <code>csi</code> driver communicates with the <code>systemd</code> service via a unix socket to execute the <code>mount</code> and <code>umount</code> commands.</li> </ul>"},{"location":"reference/mount-csi-driver/#status-and-limitations","title":"Status and Limitations","text":"<ul> <li>Tested OS: BottleRocket-OS, Amazon Linux 2 and RHEL 8.</li> <li>Minimal Kubernetes versions <code>&gt;=1.23.0</code>. </li> <li>Tested Cluster providers EKS, Openshift (Partially).</li> <li>Static provisioning only explain below.</li> <li>Setting Pods <code>securityContext</code> UID and GID (i.e <code>runAsUser: 1000</code>, <code>runAsGroup: 2000</code>) is very nuanced in nature and does not have wide coverage currently, not supported but might work.</li> <li>Pod only supports access mode <code>ReadOnlyMany</code></li> </ul> <p>Static Provisioning only (Relevant for pods)</p> <p>When requesting a mount from the CSI driver, the driver will create a <code>PersistentVolume</code> (PV) and <code>PersistentVolumeClaim</code> (PVC) for the Pod. The driver only supports Static Provisioning as of today, and you need an existing lakeFS repository to use.</p> <p>To use Static Provisioning, you should set <code>storageClassName</code> field of your <code>PersistentVolume (PV)</code> and <code>PersistentVolumeClaim (PVC)</code> to <code>\"\"</code> (empty string). Also, in order to make sure no other PVCs can claim your PV, you should define a one-to-one mapping using <code>claimRef</code>.</p>"},{"location":"reference/mount-csi-driver/#requirements","title":"Requirements","text":"<ol> <li>For enterprise installations: lakeFS Version <code>1.25.0</code> or higher.</li> <li>You have a Kubernetes cluster with version <code>&gt;=1.23.0</code> and Helm installed. </li> <li>lakeFS Server that can be access from pods in the cluster.</li> <li>Access to download treeverse/everest-lakefs-csi-driver from Docker Hub. Contact us to gain access to lakeFS Enterprise features.</li> </ol>"},{"location":"reference/mount-csi-driver/#deploy-the-csi-driver","title":"Deploy the CSI Driver","text":"<p>The CSI Driver is deployed to K8S cluster using a dedicated Helm chart everest-lakefs-csi-driver.</p>"},{"location":"reference/mount-csi-driver/#1-update-your-helm-with-the-chart","title":"1. Update your helm with the chart:","text":"<p>Add lakeFS Helm repository if not already added:</p> <pre><code>helm repo add lakefs https://charts.lakefs.io\n</code></pre> <p>Fetch the chart from lakeFS repository:</p> <pre><code>helm repo update lakefs\n</code></pre> <p>Verify the chart is available and updated: </p> <pre><code>helm show chart lakefs/everest-lakefs-csi-driver \n</code></pre> <p>List all available chart versions:</p> <pre><code>helm search repo lakefs/everest-lakefs-csi-driver -l\n</code></pre>"},{"location":"reference/mount-csi-driver/#2-configure-the-values-for-the-csi-driver-in-a-valuesyaml-file","title":"2. Configure the values for the CSI Driver in a <code>values.yaml</code> file","text":"<p>Helm Chart default values:</p> <pre><code>helm show values lakefs/everest-lakefs-csi-driver --version &lt;version&gt;\n</code></pre> <p>CSI driver config:</p> <p>All the driver CLI flags can be configured via environment variables (prefixed <code>CSI_DRIVER_</code>) and can be passed to the driver.</p> <p>Example: Minimal required arguments not commented</p> <pre><code># image:  \n#   repository: treeverse/everest-lakefs-csi-driver\n# # Optional CSI Driver override version (default .Chart.AppVersion)\n#   tag: 1.2.3\n\n# Same as fluffy https://github.com/treeverse/fluffy?tab=readme-ov-file#1-dockerhub-token-for-fluffy\nimagePullSecret:\ntoken: &lt;dockerhub-token&gt;\nusername: &lt;dockerhub-user&gt;\n\n# Credentials that will be used by everest as a default to access lakeFS mount paths\nlakeFSAccessSecret:\nkeyId: &lt;lakefs-key-id&gt;\naccessKey: &lt;lakefs-access-key&gt;\nendpoint: &lt;lakefs-endpoint&gt;\n\nnode:\n# verbosity level of the driver (normal values are 0-4, 5 would be most verbose)\nlogLevel: 4\n# Only set if having issues with running or installing the everest binary \n# Path directory where the everest binary accessed by the underlying K8S Nodes (${everestInstallPath}/everest)\n# The binary will copied from the CSI pod into that location by the init container job in the node.yaml\n# This path will be a host path on the K8S Nodes\n# depending on the underlying OS and the SELinux policy the binary will be executed by systemd on the Host.\n# Known issue when using Bottlerocket OS https://github.com/bottlerocket-os/bottlerocket/pull/3779 \n# everestInstallPath: /opt/everest-mount/bin/  # should end with \"/\"\n\n# Additional environment variables that will be passed to the driver can be used to configure the csi driver\n# extraEnvVars:\n#   - name: CSI_DRIVER_MOUNT_TIMEOUT\n#     value: \"30s\"\n#   - name: CSI_DRIVER_EVEREST_DEFAULT_CACHE_SIZE\n#     value: \"10000000000\"\n#   - name: VALUE_FROM_SECRET\n#     valueFrom:\n#       secretKeyRef:\n#         name: secret_name\n#         key: secret_key\n</code></pre>"},{"location":"reference/mount-csi-driver/#3-install-the-chart-to-k8s-cluster","title":"3. Install the Chart to K8S cluster","text":"<p>Install the chart with the values file:</p> <pre><code>helm install -f values.yaml lakefs lakefs/everest-lakefs-csi-driver --version &lt;version&gt;\n</code></pre>"},{"location":"reference/mount-csi-driver/#use-in-pods","title":"Use in Pods","text":"<p>Once the CSI Driver is installed, we can start using it similarly to how all <code>PersistentVolume</code> (PV) and <code>PersistentVolumeClaim</code> (PVC) are used in Kubernetes.</p> <p>The only required argument to set is <code>lakeFSMountUri</code> in the <code>PV</code> (See examples below).</p>"},{"location":"reference/mount-csi-driver/#mount-options","title":"Mount Options","text":"<p>Most of the options are optional and can be omitted, but each mount request can be configured with everest mount cli options, they are passed as <code>mountOptions</code> in the <code>PVC</code> spec.</p>"},{"location":"reference/mount-csi-driver/#examples","title":"Examples","text":"<p>The examples demonstrates different mount scenarios with the CSI Driver. All of them are essentially running <code>ls &lt;mount-dir&gt;</code> and <code>tail -f /dev/null</code> in a centos container. If the mount succeeded you will see the contents of your mount directory.</p> <ol> <li>Set <code>lakeFSMountUri</code> (i.e <code>lakefs://&lt;repo&gt;/&lt;repo&gt;/[prefix/]</code>) to the lakeFS mount URI you want to mount.</li> <li>Run <code>kubectl apply -f values.yaml</code></li> <li>View the example pod logs to see the mount output <code>kubectl logs -f &lt;pod-name&gt;</code></li> </ol> Single Pod and mountMultiple Pods, one mount (Deployment)Multiple mounts, single PodStatefulSet (Advanced)Mount Options <p>Configure <code>lakeFSMountUri</code> to the target URI. </p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\nname: everest-pv\nspec:\ncapacity:\n    storage: 100Gi # ignored, required\naccessModes:\n    - ReadOnlyMany # supported options: ReadOnlyMany\n# everest mount options goes under mountOptions and forwarded to the everest mount command \n# mountOptions:\n    # set cache size in bytes \n    # - cache-size 1000000000\ncsi:\n    driver: csi.everest.lakefs.io # required\n    volumeHandle: everest-csi-driver-volume\n    volumeAttributes:\n    # mount target, replace with your lakeFS mount URI\n    lakeFSMountUri: &lt;LAKEFS_MOUNT_URI&gt;\n\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: everest-claim\nspec:\naccessModes:\n    - ReadOnlyMany # supported options: ReadOnlyMany\nstorageClassName: \"\" # required for static provisioning\nresources:\n    requests:\n    storage: 5Gi # ignored, required\nvolumeName: everest-pv\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: everest-app\nspec:\ncontainers:\n    - name: app\n    image: centos\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"ls /data/; tail -f /dev/null\"]\n    volumeMounts:\n        - name: persistent-storage-isan\n        mountPath: /data\nvolumes:\n    - name: persistent-storage-isan\n    persistentVolumeClaim:\n        claimName: everest-claim\n</code></pre> <p>Configure <code>lakeFSMountUri</code> to the target URI.</p> <p>In this example a deployment is created with 3 replicas, all sharing a single <code>PersistentVolume</code> and PVC  Behind the scenes each pod get's their own mount, even if on the same k8s node, each pod will get their own mount directory.</p> <p>Unlike in StatefulSet, this can scale-up-down with no additional interference and deleted easily the same way.</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\nname: multiple-pods-one-pv\nspec:\ncapacity:\n    storage: 1200Gi # ignored, required\naccessModes:\n    - ReadOnlyMany # supported options: ReadOnlyMany\n# everest mount options goes under mountOptions and forwarded to the everest mount command \n# mountOptions:\n#   - cache-size 1000000555\ncsi:\n    driver: csi.everest.lakefs.io # required\n    volumeHandle: everest-csi-driver-volume\n    volumeAttributes:\n    # mount target, replace with your lakeFS mount URI\n    lakeFSMountUri: &lt;LAKEFS_MOUNT_URI&gt;\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: multiple-pods-one-claim\nspec:\naccessModes:\n    - ReadOnlyMany # supported options: ReadOnlyMany\nstorageClassName: \"\" # required for static provisioning\nresources:\n    requests:\n    storage: 1200Gi # ignored, required\nvolumeName: multiple-pods-one-pv\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: multiple-pods-one-pv-app\nlabels:\n    app: multiple-pods-one-pv-app\nspec:\nreplicas: 3\nselector:\n    matchLabels:\n    app: multiple-pods-one-pv-app\ntemplate:\n    metadata:\n    labels:\n        app: multiple-pods-one-pv-app\n    spec:\n    containers:\n    - name: multiple-pods-one-pv-app\n        image: centos\n        command: [\"/bin/sh\"]\n        args: [\"-c\", \"ls /data/; tail -f /dev/null\"]\n        volumeMounts:\n        - name: persistent-storage\n        mountPath: /data\n        ports:\n        - containerPort: 80\n    volumes:\n    - name: persistent-storage\n        persistentVolumeClaim:\n        claimName: multiple-pods-one-claim\n</code></pre> <p>Deploy a pod with two mounts to different mount points. Configure <code>lakeFSMountUri</code> for each <code>PersistentVolume</code>. </p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\nname: multiple-mounts-one-pod-pv\nspec:\ncapacity:\n    storage: 1200Gi # ignored, required\naccessModes:\n    - ReadOnlyMany # supported options: ReadOnlyMany\nmountOptions:\n    - cache-size 1000000111\ncsi:\n    driver: csi.everest.lakefs.io # required\n    volumeHandle: everest-csi-driver-volume # must be unique\n    volumeAttributes:\n    # mount target local-lakefs dir, replace with your lakeFS mount URI\n    lakeFSMountUri: &lt;LAKEFS_URI_1&gt;\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: multple-mounts-one-pod-claim\nspec:\naccessModes:\n    - ReadOnlyMany # supported options: ReadOnlyMany\nstorageClassName: \"\" # required for static provisioning\nresources:\n    requests:\n    storage: 1200Gi # ignored, required\nvolumeName: multiple-mounts-one-pod-pv\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\nname: multiple-mounts-one-pod-pv-2\nspec:\ncapacity:\n    storage: 1200Gi # ignored, required\naccessModes:\n    - ReadOnlyMany # ReadOnlyMany\nmountOptions:\n    - cache-size 1000000555\ncsi:\n    driver: csi.everest.lakefs.io # required\n    volumeHandle: everest-csi-driver-volume-2 # must be unique\n    volumeAttributes:\n    # mount target images dir, replace with your lakeFS mount URI\n    lakeFSMountUri: &lt;LAKEFS_URI_2&gt;\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: multple-mounts-one-pod-claim-2\nspec:\naccessModes:\n    - ReadOnlyMany # supported options: ReadOnlyMany\nstorageClassName: \"\" # required for static provisioning\nresources:\n    requests:\n    storage: 1200Gi # ignored, required\nvolumeName: multiple-mounts-one-pod-pv-2\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: everest-multi-mounts-one-pod\nspec:\ncontainers:\n    - name: app\n    image: centos\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"ls /data/; ls /data2/; tail -f /dev/null\"]\n    volumeMounts:\n        - name: persistent-storage\n        mountPath: /data\n        - name: persistent-storage-2\n        mountPath: /data2\nvolumes:\n    - name: persistent-storage\n    persistentVolumeClaim:\n        claimName: multple-mounts-one-pod-claim\n    - name: persistent-storage-2\n    persistentVolumeClaim:\n        claimName: multple-mounts-one-pod-claim-2\n</code></pre> <p>Configure <code>lakeFSMountUri</code> to the target URI. Because of the neuances described below, if not required it is best to avoid using a <code>StatefulSet</code>.</p> <p>Deletion:</p> <p>It's intended behavior for StatefulSet in K8S that the PVC is not deleted automatically when the pod is deleted since the StatefulSet controller does not manage the PVC. To completley delete use k delete with --force flag or first delete the PVC: 'kubectl delete pvc -l app=sts-app-simple-everest'</p> <p>Scale Down:</p> <p>replicas: 0 can be set to scale down the StatefulSet and bring back up with replicas: 1.</p> <p>Replicas &gt; 1:</p> <p>not supported in this example, since the driver only supports static provisioning.  to use Statefulset with replica &gt; 1 we need to add PersistentVolume(s) manually.</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\nname: sts-simple-mount\nlabels:\n    app: sts-app-simple-everest\nspec:\ncapacity:\n    storage: 100Gi # ignored, required\naccessModes:\n    - ReadOnlyMany # supported options: ReadOnlyMany\nmountOptions:\n    # override default cache size for the mount (in bytes)\n    - cache-size 1000000555\n    - log-level debug\ncsi:\n    driver: csi.everest.lakefs.io # required\n    volumeHandle: everest-csi-driver-volume\n    volumeAttributes:\n    # mount target, replace with your lakeFS mount URI\n    lakeFSMountUri: &lt;LAKEFS_MOUNT_URI&gt;\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\nname: sts-app-simple-everest\nspec:\nreplicas: 1\nselector:\n    matchLabels:\n    app: sts-app-simple-everest\ntemplate:\n    metadata:\n    labels:\n        app: sts-app-simple-everest\n    spec:\n    containers:\n        - name: app\n        image: centos\n        command: [\"/bin/sh\"]\n        args: [\"-c\", \"ls /data/; tail -f /dev/null\"]\n        volumeMounts:\n            - name: sts-simple-mount\n            mountPath: /data\nvolumeClaimTemplates:\n- metadata:\n    name: sts-simple-mount\n    spec:\n    selector:\n        matchLabels:\n        app: sts-app-simple-everest\n    storageClassName: \"\" # required for static provisioning\n    accessModes: [ \"ReadOnlyMany\" ]\n    resources:\n        requests:\n        storage: 5Gi # ignored, required\n</code></pre> <p>This demonstrates common flags and uncommon flags that can be used for a mount. In general, the flags are set in <code>mountOptions</code> and are passed to the everest mount command.</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\nname: options-demo-pv\nspec:\ncapacity:\n    storage: 100Gi # ignored, required\naccessModes:\n    - ReadOnlyMany # supported options: ReadOnlyMany\n# everest mount options goes under mountOptions and forwarded to the everest mount command \nmountOptions:\n    # set cache size in bytes \n    - cache-size 10000000\n    # set log level to debug when inspecting mount logs (very noisy!)\n    - log-level trace\n    # WARN: lakeFS credentials / endpoint should be managed securely by the CSI-driver, this is an advanced flag use-case\n    # override default lakeFS credentials (for use-cases where the default csi-driver credentials are not sufficient)\n    - lakectl-access-key-id &lt;LAKEFS_ACCESS_KEY_ID&gt;\n    - lakectl-secret-access-key &lt;LAKEFS_SECRET_ACCESS_KEY&gt;\n    - lakectl-server-url &lt;LAKEFS_ENDPOINT&gt;\n    # WARN: an advanced flag and rarely needed if at all, performs mount directly using fuser relying on it to exist on the host server without using FUSE syscalls\n    # be default fuse-direct-mount is true\n    # - fuse-direct-mount false\n    # - mount-gid 2000\n    # - mount-uid 1000\n    # - presign false\n    # - log-enable-syslog false\n\ncsi:\n    driver: csi.everest.lakefs.io # required\n    volumeHandle: everest-csi-driver-volume\n    volumeAttributes:\n    # mount target, staging org (non default credentials on csi), replace with your lakeFS mount URI\n    lakeFSMountUri: &lt;LAKEFS_MOUNT_URI&gt;\n\n# REST OF THE RESOURCES\n# ... \n</code></pre>"},{"location":"reference/mount-csi-driver/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Use <code>kubectl</code> and check the CSI driver pod and failed Pod for logs and events. </li> <li>If a specific mount request failed, specifically inspect csi-node that the failed mount pod was deployed on. </li> <li>Check the events and status of the <code>PVC</code> and <code>PV</code> of the failing pod <code>kubectl get pv &amp;&amp; kubectl get pvc</code></li> </ul> <p>Advanced: SSH into the underlying K8S node:</p> <p>Find the failed mount service <code>systemctl list-units --type=service</code>: </p> <pre><code>everest-lakefs-mount-0.0.8-everest-123.service loaded active running CSI driver FUSE daemon\n</code></pre> <p>Get systemd service status:</p> <pre><code># service name example: everest-lakefs-mount-0.0.8-everest-123.service\nsystemctl status &lt;service&gt;\n\n# output contains many things including the exec command to run, example:\n# ExecStart=/opt/bin/everest mount lakefs://test-mount/main/local-lakefs/ /var/lib/kubelet/pods/123/volumes/kubernetes.io~csi/everest-pv/mount --log-level=trace --cache-dir=/var/lib/kubelet/pods/123/volumes/kubernetes.io~csi/everest-pv/.everest --lakectl-config=/opt/mountpoint-s3-csi/bin/lakectl.yaml\n</code></pre> <p>See systemd logs of a service:</p> <pre><code>journalctl -f -u &lt;service&gt;\n\n# example:\njournalctl -f -u everest-lakefs-mount-0.0.8-everest-123.service\n</code></pre>"},{"location":"reference/mount-write-mode-semantics/","title":"Mount Write Mode Semantics","text":"<p>Experimental</p>"},{"location":"reference/mount-write-mode-semantics/#consistency-model","title":"Consistency Model","text":""},{"location":"reference/mount-write-mode-semantics/#file-system-consistency","title":"File System Consistency","text":"<p>Everest mount provides a strong read-after-write consistency model within a single mount point. This means that once a write operation is done, the data is guaranteed to be available for subsequent read operations.</p>"},{"location":"reference/mount-write-mode-semantics/#lakefs-consistency","title":"lakeFS Consistency","text":"<p>Local changes are reflected in lakeFS only after the changes are committed. That means that the data is not available for other users until the changes are committed. If, for example, two users mount the same branch, they will not see each other's changes until they are committed.</p>"},{"location":"reference/mount-write-mode-semantics/#sync-local-changes-to-lakefs","title":"Sync local changes to lakeFS","text":"<ul> <li>As part of <code>commit</code> and <code>diff</code> commands a <code>sync</code> operation will upload all the local changes to a temporary write-branch.</li> </ul>"},{"location":"reference/mount-write-mode-semantics/#mount-write-mode-file-system-behavior","title":"Mount Write Mode File System Behavior","text":""},{"location":"reference/mount-write-mode-semantics/#functionality-limitations","title":"Functionality Limitations","text":"<ul> <li>Newly created empty directories will not reflect as directory markers in lakeFS.</li> <li>lakeFS allows having 2 path keys that one is a \"directory\" prefix of the other, for example the following 2 lakeFS keys are valid: <code>animals/cat.png</code> and <code>animals</code> (empty object) but since a file system cannot contain both a file and a directory of the same name it will lead to an undefined behavior depending on the Filesystem type (e.g., dir and dir/file).</li> </ul>"},{"location":"reference/mount-write-mode-semantics/#file-system-behavior","title":"File System Behavior","text":""},{"location":"reference/mount-write-mode-semantics/#not-supported","title":"Not Supported","text":"<ul> <li>Rename is not supported.</li> <li>Temporary files are not supported.</li> <li>Hard/symbolic links are not supported.</li> <li>POSIX file locks (lockf) are not supported.</li> <li>POSIX permissions are not supported - default permissions are given to files and dirs.</li> <li>A deleted file's name cannot be used as a directory type later and the same for opposite types (e.g, Not allowed: touch foo; rm foo; mkdir foo;).</li> <li>Calling remove on a directory type will fail explicitly with an error.</li> </ul>"},{"location":"reference/mount-write-mode-semantics/#behavior-modified","title":"Behavior modified","text":"<ul> <li>Modifying file metadata (chmod, chown, chgrp, time) will result in noop (the file metadata will not be changed). </li> <li>When calling <code>remove</code> we mark a file as a tombstone using Extended Attributes API's.</li> <li>Removal is not an atomic operation, calling remove and open at the same time might result in a race condition where the open might succeed.</li> </ul>"},{"location":"reference/mount/","title":"Mount (Everest)","text":"<p>Info</p> <p>Available in lakeFS Cloud and lakeFS Enterprise</p> <p>Everest is a complementary binary to lakeFS that allows users to virtually mount a remote lakeFS repository onto a local directory. Once mounted, users can access the data as if it resides on their local filesystem, using any tool, library, or framework that reads from a local filesystem.</p> <p>Note</p> <p>No installation is required. Please contact us to get access to the Everest binary.</p> <p>Tip</p> <p>Everest mount supports writing to the file system for both NFS and FUSE protocols starting version 0.2.0!</p> <p>Everest mount write mode semantics \u2192.</p>"},{"location":"reference/mount/#use-cases","title":"Use Cases","text":"<ul> <li>Simplified Data Loading: With lakeFS Mount, there's no need to write custom data loaders or use special SDKs. You can use your existing tools to read and write files directly from the filesystem.</li> <li>Handle Large-scale Data Without changing Work Habits: Seamlessly scale from a few local files to millions without changing your tools or workflow. Use the same code from early experimentation all the way to production.</li> <li>Enhanced Data Loading Efficiency: lakeFS Mount supports billions of files and offers fast data fetching, making it ideal for optimizing GPU utilization and other performance-sensitive tasks.</li> </ul>"},{"location":"reference/mount/#requirements","title":"Requirements","text":"<ul> <li>For enterprise installations: lakeFS Version <code>1.25.0</code> or higher.</li> </ul>"},{"location":"reference/mount/#os-and-protocol-support","title":"OS and Protocol Support","text":"<p>Currently, the implemented protocols are <code>nfs</code> and <code>fuse</code>.</p> <ul> <li>NFS V3 (Network File System) is supported on macOS.</li> </ul>"},{"location":"reference/mount/#authentication-chain-for-lakefs","title":"Authentication Chain for lakeFS","text":"<p>When running an Everest <code>mount</code> command, authentication occurs in the following order:</p> <ol> <li>Session token from the environment variable <code>EVEREST_LAKEFS_CREDENTIALS_SESSION_TOKEN</code> or <code>LAKECTL_CREDENTIALS_SESSION_TOKEN</code>.    If the token is expired, authentication will fail.</li> <li>lakeFS key pair, using lakeFS access key ID and secret key. (picked up from lakectl if Everest not provided)</li> <li>IAM authentication, if configured and no static credentials are set.</li> </ol>"},{"location":"reference/mount/#authenticate-with-lakefs-credentials","title":"Authenticate with lakeFS Credentials","text":"<p>The authentication with the target lakeFS server is equal to lakectl CLI. Searching for lakeFS credentials and server endpoint in the following order:</p> <ul> <li>Command line flags <code>--lakectl-access-key-id</code>, <code>--lakectl-secret-access-key</code> and <code>--lakectl-server-url</code></li> <li><code>LAKECTL_*</code> Environment variables</li> <li><code>~/.lakectl.yaml</code> Configuration file or via <code>--lakectl-config</code> flag</li> </ul>"},{"location":"reference/mount/#authenticating-with-aws-iam-role","title":"Authenticating with AWS IAM Role","text":"<p>Starting from lakeFS \u2265 v1.57.0 and Everest \u2265 v0.4.0, authenticating with IAM roles is supported! When IAM authentication is configured, Everest will use AWS SDK default behavior that will pick your AWS environment to generate a session token used for authenticating against lakeFS (i.e use <code>AWS_PROFILE</code>, <code>AWS_ACCESS_KEY_ID</code>, etc). This token is seamlessly refreshed as long as the AWS session remains valid.  </p>"},{"location":"reference/mount/#prerequisites","title":"Prerequisites","text":"<ol> <li>Make sure your lakeFS server supports AWS IAM Role Login.</li> <li>Make sure your IAM role is attached to lakeFS. See Administration of IAM Roles in lakeFS</li> </ol>"},{"location":"reference/mount/#configure-everest-to-use-iam","title":"Configure everest to use IAM","text":"<p>To use IAM authentication, new configuration fields were introduced:</p> <ul> <li><code>credentials.provider.type</code> <code>(string: '')</code> - Settings this <code>aws_iam</code> will expect <code>aws_iam</code> block and try to use IAM.</li> <li><code>credentials.provider.aws_iam.token_ttl_second</code> <code>(duration: 60m)</code> - Optional: lakeFS token duration.</li> <li><code>credentials.provider.aws_iam.url_presign_ttl_seconds</code> <code>(duration: 15m)</code> - Optional: AWS STS's presigned URL validation duration.  </li> <li><code>credentials.provider.aws_iam.refresh_interval</code> <code>(duration: 15m)</code> - Optional: Amount of time before token expiration that Everest will try to fetch a new session token instead of using the current one.  </li> <li><code>credentials.provider.aws_iam.token_request_headers</code>: Map of required headers and their values to be signed by the AWS STS request as configured in your lakeFS server. If nothing is set the default behavior is adding <code>x-lakefs-server-id:&lt;lakeFS host&gt;</code>. If your lakeFS server doesn't require any headers (less secure) you can set this empty by setting <code>{}</code> empty map in your config. </li> </ul> <p>These configuration fields can be set via <code>.lakectl.yaml</code>: </p> <p>Example</p> <pre><code>credentials:\nprovider:\n    type: aws_iam          # Required\n    aws_iam:\n    token_ttl_seconds: 60m              # Optional, default: 1h\n    url_presign_ttl_seconds: 15m        # Optional, default: 15m\n    refresh_interval: 5m                # Optional, default: 5m\n    token_request_headers:              # Optional, if omitted then will set x-lakefs-server-id: &lt;lakeFS host&gt; by default, to override default set to '{}'\n    # x-lakefs-server-id: &lt;lakeFS host&gt;     Added by default if token_request_headers is not set    \n    custome-key:  custome-val\nserver:\nendpoint_url: &lt;lakeFS endpoint url&gt;\n</code></pre> <p>To set using environment variables - those will start with the prefix <code>EVEREST_LAKEFS_*</code> or <code>LAKECTL_*</code>. For example, setting the provider type using env vars: <code>export EVEREST_LAKEFS_CREDENTIALS_PROVIDER_TYPE=aws_iam</code> or <code>LAKECTL_CREDENTIALS_PROVIDER_TYPE=aws_iam</code>.</p> <p>Tip</p> <p>To troubleshoot presign request issues, you can enable debug logging for presign requests using the environment variable:</p> <pre><code>EVEREST_LAKEFS_CREDENTIALS_PROVIDER_AWS_IAM_CLIENT_LOG_PRE_SIGNING_REQUEST=true\n</code></pre> <p>Warning</p> <p>If you choose to configure IAM provider using the same lakectl file (i.e <code>lakectl.yaml</code>) that you use for the lakectl cli,  you must upgrade lakectl to version (<code>\u2265 v1.57.0</code>) otherwise lakectl will raise errors when using it.</p>"},{"location":"reference/mount/#command-line-interface","title":"Command Line Interface","text":""},{"location":"reference/mount/#mount-command","title":"Mount Command","text":"<p>The <code>mount</code> command is used to mount a lakeFS repository to a local directory, it does it in 2 steps:</p> <ol> <li>Starting a server that listens on a local address and serves the data from the remote lakeFS repository.</li> <li>Running the required mount command on the OS level to connect the server to the local directory.</li> </ol>"},{"location":"reference/mount/#tips","title":"Tips:","text":"<ul> <li>Since the server runs in the background set <code>--log-output /some/file</code> to view the logs in a file.</li> <li>Cache: Everest uses a local cache to store the data and metadata of the lakeFS repository. The optimal cache size is the size of the data you are going to read/write.</li> <li>Reusing Cache: between restarts of the same mount endpoint, set <code>--cache-dir</code> to make sure the cache is reused.</li> <li>Mounted data consistency (read-mode): When providing lakeFS URI mount endpoint <code>lakefs://&lt;repo&gt;/&lt;ref&gt;/&lt;path&gt;</code> the <code>&lt;ref&gt;</code> should be a specific commit ID. If a branch/tag is provided, Everest will use the HEAD commit instead.</li> <li>When running mount in write-mode, the lakeFS URI must be a branch name, not a commit ID or a tag.</li> </ul>"},{"location":"reference/mount/#usage","title":"Usage","text":"<pre><code>everest mount &lt;lakefs_uri&gt; &lt;mount_directory&gt;\n\nFlags\n--presign: Use presign for downloading.\n--cache-dir: Directory to cache read files in.\n--cache-size: Size of the local cache in bytes.\n--cache-create-provided-dir: If cache-dir is explicitly provided and does not exist, create it.\n--listen: Address to listen on.\n--no-spawn: Do not spawn a new server, assume one is already running.\n--protocol: Protocol to use (default: nfs).\n--log-level: Set logging level.\n--log-format: Set logging output format.\n--log-output: Set logging output(s).\n--write-mode: Enable write mode (default: false).\n</code></pre>"},{"location":"reference/mount/#umount-command","title":"Umount Command","text":"<p>The <code>umount</code> command is used to unmount a currently mounted lakeFS repository.</p> <pre><code>everest umount &lt;mount_directory&gt;\n</code></pre>"},{"location":"reference/mount/#diff-command-write-mode-only","title":"Diff Command (write-mode only)","text":"<p>The <code>diff</code> command Show the diff between the source branch and the current mount directory.  If <code>&lt;mount_directory&gt;</code> not specified, the command searches for the mount directory in the current working directory and upwards based on <code>.everest</code> directory existence. Please note that the diffs are from the source branch state at the time of mounting and not the current state of the source branch, i.e., changes to the source branch from other operations will not be reflected in the diff result.</p> <pre><code>everest diff &lt;mount_directory&gt;\n\n#Example output:\n# - removed datasets/pets/cats/persian/cute.jpg\n# ~ modified datasets/pets/dogs/golden_retrievers/cute.jpg\n# + added datasets/pets/birds/parrot/cute.jpg\n</code></pre>"},{"location":"reference/mount/#commit-command-write-mode-only","title":"Commit Command (write-mode only)","text":"<p>The <code>commit</code> command commits the changes made in the mounted directory to the original lakeFS branch. If <code>&lt;mount_directory&gt;</code> not specified, the command searches for the mount directory in the current working directory and upwards based on <code>.everest</code> directory existence. The new commit will be merged to the original branch with the <code>source-wins</code> strategy. After the commit is successful, the mounted directory source commit will be updated to the HEAD of the latest commit at the source branch; that means that changes made to the source branch out of the mount scope will also be reflected in the mounted directory.</p> <p>Warning</p> <p>Writes to a mount directory during commit may be lost.</p> <pre><code>everest commit &lt;mount_directory&gt; -m &lt;optional_commit_message&gt;\n</code></pre>"},{"location":"reference/mount/#mount-server-command-advanced","title":"mount-server Command (Advanced)","text":"<p>Note</p> <p>The <code>mount-server</code> command is for advanced use cases and will only spin the server without calling OS mount command.</p> <p>The mount-server command starts a mount server manually. Generally, users would use the mount command which handles server operations automatically.</p> <pre><code>everest mount-server &lt;remote_mount_uri&gt;\nFlags\n--cache-dir: Directory to cache read files and metadata.\n--cache-create-provided-dir: Create the cache directory if it does not exist.\n--listen: Address to listen on.\n--protocol: Protocol to use (nfs | webdav).\n--callback-addr: Callback address to report back to.\n--log-level: Set logging level.\n--log-format: Set logging output format.\n--log-output: Set logging output(s).\n--cache-size: Size of the local cache in bytes.\n--parallelism: Number of parallel downloads for metadata.\n--presign: Use presign for downloading.\n--write-mode: Enable write mode (default: false).\n</code></pre>"},{"location":"reference/mount/#partial-reads","title":"Partial Reads","text":"<p>Experimental</p> <p>When reading large files, Everest can fetch from lakeFS only the parts actually accessed. This can be useful for streaming workloads or for applications handling file formats such as Parquet, m4a, zip, tar that do not need to read the entire file.</p> <p>To enable partial reads, pass the <code>--partial-reads</code> flag to the <code>mount</code> (or <code>mount-server</code>) command:</p> <pre><code>everest mount --partial-reads \"lakefs://image-repo/main/datasets/pets/\" \"./pets\"\n</code></pre>"},{"location":"reference/mount/#examples","title":"Examples","text":""},{"location":"reference/mount/#read-only-mode-default","title":"Read-Only Mode (default)","text":"<p>Info</p> <p>For simplicity, the examples show <code>main</code> as the ref, Everest will always mount a specific commit ID when using read-only mode, given a ref it will use the HEAD (e.g the most recent commit).</p> <p>Data Exploration</p> <p>Mount the lakeFS repository and explore data as if it's on the local filesystem.</p> <pre><code>everest mount \"lakefs://image-repo/main/datasets/pets/\" \"./pets\"\nls -l \"./pets/dogs/\"\nfind ./pets -name \"*.small.jpg\"\nopen -a Preview \"./pets/dogs/golden_retrievers/cute.jpg\"\neverest umount \"./pets\"\n</code></pre> <p>Working with Data Locally</p> <p>Mount the remote lakeFS server and use all familiar tools without changing the workflow.</p> <pre><code>everest mount lakefs://image-repo/main/datasets/pets/ ./pets\npytorch_train.py --input ./pets\nduckdb \"SELECT * FROM read_parquet('pets/labels.parquet')\"\neverest umount ./pets\n</code></pre>"},{"location":"reference/mount/#write-mode","title":"Write Mode","text":"<p>Changing Data Locally</p> <p>Mount the remote lakeFS server in write mode and change data locally.</p> <pre><code>everest mount lakefs://image-repo/main/datasets/pets/ ./pets --write-mode\n# Add a new file\necho \"new data\" &gt;&gt; ./pets/birds/parrot/cute.jpg\n# Update an existing file\necho \"new data\" &gt;&gt; ./pets/dogs/golden_retrievers/cute.jpg\n# Delete a file\nrm ./pets/cats/persian/cute.jpg\n\n# Check the changes\neverest diff ./pets\n# - removed datasets/pets/cats/persian/cute.jpg\n# ~ modified datasets/pets/dogs/golden_retrievers/cute.jpg\n# + added datasets/pets/birds/parrot/cute.jpg\n\n# Commit the changes to the original lakeFS branch\neverest commit ./pets\n\neverest diff ./pets\n# No changes\n\neverest umount ./pets\n</code></pre> <p>To learn more, read about Mount Write Mode Semantics.</p>"},{"location":"reference/mount/#faqs","title":"FAQs","text":"How do I get started with lakeFS Mount (Everest)? <p>lakeFS Mount is available for lakeFS Cloud and lakeFS Enterprise customers. Once your setup is complete, contact us to access the lakeFS Mounts binary and follow the provided docs.</p> <ul> <li>Want to try lakeFS Cloud? Signup for a 30-day free trial.</li> <li>Interested in lakeFS Enterprise? Contact sales for a 30-day free license.</li> </ul> What operating systems are supported by lakeFS Mount? <p>lakeFS Mount supports Linux and MacOS. Windows support is on the roadmap.</p> How can I control access to my data when using lakeFS Mount? <p>You can use lakeFS's existing Role-Based Access Control mechanism, which includes repository and path-level policies. lakeFS Mount translates filesystem operations into lakeFS API operations and authorizes them based on these policies.</p> <p>The minimal RBAC permissions required for mounting a prefix from a lakeFS repository in read-only mode:</p> <pre><code>{\n  \"id\": \"MountPolicy\",\n  \"statement\": [\n    {\n      \"action\": [\n        \"fs:ReadObject\"\n      ],\n      \"effect\": \"allow\",\n      \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;/object/&lt;prefix&gt;/*\"\n    },\n    {\n      \"action\": [\n        \"fs:ListObjects\",\n        \"fs:ReadCommit\",\n        \"fs:ReadBranch\",\n        \"fs:ReadTag\",\n        \"fs:ReadRepository\"\n      ],\n      \"effect\": \"allow\",\n      \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;\"\n    },\n    {\n      \"action\": [\"fs:ReadConfig\"],\n      \"effect\": \"allow\",\n      \"resource\": \"*\"\n    }\n  ]\n}\n</code></pre> <p>The minimal RBAC permissions required for mounting a prefix from a lakeFS repository in write mode:</p> <pre><code>{\n  \"id\": \"MountPolicy\",\n  \"statement\": [\n    {\n      \"action\": [\n        \"fs:ReadObject\",\n        \"fs:WriteObject\",\n        \"fs:DeleteObject\"\n      ],\n      \"effect\": \"allow\",\n      \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;/object/&lt;prefix&gt;/*\"\n    },\n    {\n      \"action\": [\n        \"fs:ListObjects\",\n        \"fs:ReadCommit\",\n        \"fs:ReadBranch\",\n        \"fs:ReadRepository\",\n        \"fs:CreateCommit\",\n        \"fs:CreateBranch\",\n        \"fs:DeleteBranch\",\n        \"fs:RevertBranch\"\n      ],\n      \"effect\": \"allow\",\n      \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;\"\n    },\n    {\n      \"action\": [\"fs:ReadConfig\"],\n      \"effect\": \"allow\",\n      \"resource\": \"*\"\n    }\n  ]\n}\n</code></pre> Does data pass through the lakeFS server when using lakeFS Mount? <p>lakeFS Mount leverages pre-signed URLs to read data directly from the underlying object store, meaning data doesn't  pass through the lakeFS server. By default, presign is enabled. To disable it, use:</p> <pre><code>everest mount &lt;lakefs_uri&gt; &lt;mount_directory&gt; --presign=false\n</code></pre> What happens if a lakeFS branch is updated after I mount it? <p>lakeFS Mount points to the commit that was the HEAD commit of the branch at the time of mounting. This means the local directory reflects the branch state at the time of mounting and does not update with subsequent branch changes.</p> When are files downloaded to my local environment? <p>lakeFS Mount uses a lazy prefetch strategy. Files are not downloaded at mount time or during operations that only inspect file metadata (e.g., <code>ls</code>). Files are downloaded only when commands that require file access (e.g., <code>cat</code>) are used.</p> What are the scale limitations of lakeFS Mount, and what are the recommended configurations for dealing with large datasets? <p>When using lakeFS Mount, the volume of data accessed by the local machine influences the scale limitations more than the total size of the dataset under the mounted prefix. This is because lakeFS Mount uses a lazy downloading approach, meaning it only downloads the accessed files. lakeFS Mount listing capability is limited to performing efficiently for prefixes containing fewer than 8000 objects, but we are working to increase this limit.</p> Recommended Configuration <p>Ensure your cache size is large enough to accommodate the volume of files being accessed.</p> How does lakeFS Mount integrate with a Git repository? <p>It is perfectly safe to mount a lakeFS path within a Git repository. lakeFS Mount prevents git from adding mounted objects to the git repository (i.e when running <code>git add -A</code>) by adding a virtual <code>.gitignore</code> file to the mounted directory.</p> <p>The <code>.gitignore</code> file will also instruct Git to ignore all files except <code>.everest/source</code> and in its absence, it will try to find a <code>.everest/source</code> file in the destination folder, and read the lakeFS URI from there. Since <code>.everest/source</code> is in source control, it will mount the same lakeFS commit every time!</p> I'm already using lakectl local for working with lakeFS data locally, why should I use lakeFS Mount? <p>While both lakectl local and lakeFS Mount enable working with lakeFS data locally, they serve different purposes:</p> Use lakectl local <ul> <li>For enabling lakeFS writes with lakectl local commit.</li> <li>To integrate seamlessly with Git.</li> </ul> Use lakeFS Mount <p>For local data access, lakeFS Mount offers several benefits over lakectl local:</p> <ul> <li>Optimized selective data access: The lazy prefetch strategy saves storage space and reduces latency by only fetching the required data.</li> <li>Reduced initial latency: Start working on your data immediately without waiting for downloads.</li> </ul>"},{"location":"reference/s3/","title":"S3-Supported API","text":"<p>The S3 Gateway emulates a subset of the API exposed by S3. This subset includes all API endpoints relevant to data systems.</p> <p>Tip</p> <p>For more information, see architecture.</p> <p>lakeFS supports the following API operations:</p> <ol> <li>Identity and authorization<ol> <li>SIGv2</li> <li>SIGv4</li> </ol> </li> <li>Bucket operations:<ol> <li>HEAD bucket</li> </ol> </li> <li>Object operations:<ol> <li>DeleteObject</li> <li>DeleteObjects</li> <li>GetObject<ol> <li>Support for caching headers, ETag</li> <li>Support for range requests</li> <li>Support for reading user metadata.</li> <li>No support for SSE</li> <li>No support for SelectObject operations</li> </ol> </li> <li>HeadObject</li> <li>PutObject<ol> <li>Support multi-part uploads</li> <li>Support for writing user metadata.</li> <li>No support for storage classes</li> <li>No object level tagging</li> </ol> </li> <li>CopyObject</li> </ol> </li> <li>Object Listing:<ol> <li>ListObjects</li> <li>ListObjectsV2</li> <li>Delimiter support (for <code>\"/\"</code> only)</li> </ol> </li> <li>Multipart Uploads:<ol> <li>AbortMultipartUpload</li> <li>CompleteMultipartUpload</li> <li>CreateMultipartUpload</li> <li>ListParts Currently supported only on AWS S3. Link to tracked issue</li> <li>ListMultipartUploads Currently supported only on AWS S3. Link to tracked issue</li> <li>Upload Part</li> <li>UploadPartCopy</li> </ol> </li> </ol>"},{"location":"reference/spark-client/","title":"lakeFS Spark Metadata Client","text":"<p>Utilize the power of Spark to interact with the metadata on lakeFS. Possible use cases include:</p> <ul> <li>Creating a DataFrame for listing the objects in a specific commit or branch.</li> <li>Computing changes between two commits.</li> <li>Exporting your data for consumption outside lakeFS.</li> <li>Bulk operations on the underlying storage.</li> </ul>"},{"location":"reference/spark-client/#getting-started","title":"Getting Started","text":"<p>Note</p> <p>Please note that Spark 2.x is no longer supported with the lakeFS metadata client.</p> <p>The Spark metadata client is compiled for Spark 3.1.2 with Hadoop 3.2.1, but can work for other Spark versions and higher Hadoop versions.</p> PySpark, spark-shell, spark-submit, spark-sqlDatabricks <p>Start Spark Shell / PySpark with the <code>--packages</code> flag, for instance:</p> <pre><code>spark-shell --packages io.lakefs:lakefs-spark-client_2.12:0.14.3\n</code></pre> <p>Alternatively use the assembled jar (an \"\u00dcberjar\") on S3, from <code>s3://treeverse-clients-us-east/lakefs-spark-client/0.14.3/lakefs-spark-client-assembly-0.14.3.jar</code> by passing its path to <code>--jars</code>.</p> <p>The assembled jar is larger but shades several common libraries.  Use it if Spark complains about bad classes or missing methods.</p> <p>Include this assembled jar (an \"\u00dcberjar\") from S3, from <code>s3://treeverse-clients-us-east/lakefs-spark-client/0.14.3/lakefs-spark-client-assembly-0.14.3.jar</code>.</p>"},{"location":"reference/spark-client/#configuration","title":"Configuration","text":"<ol> <li>To read metadata from lakeFS, the client should be configured with your lakeFS endpoint and credentials, using the following Hadoop configurations:</li> </ol> Configuration Description <code>spark.hadoop.lakefs.api.url</code> lakeFS API endpoint, e.g: <code>http://lakefs.example.com/api/v1</code> <code>spark.hadoop.lakefs.api.access_key</code> The access key to use for fetching metadata from lakeFS <code>spark.hadoop.lakefs.api.secret_key</code> Corresponding lakeFS secret key"},{"location":"reference/spark-client/#examples","title":"Examples","text":"<p>Get a DataFrame for listing all objects in a commit</p> <pre><code>import io.treeverse.clients.LakeFSContext\n\nval commitID = \"a1b2c3d4\"\nval df = LakeFSContext.newDF(spark, \"example-repo\", commitID)\ndf.show\n/* output example:\n+------------+--------------------+--------------------+-------------------+----+\n|        key |             address|                etag|      last_modified|size|\n+------------+--------------------+--------------------+-------------------+----+\n|     file_1 |791457df80a0465a8...|7b90878a7c9be5a27...|2021-03-05 11:23:30|  36|\n|     file_2 |e15be8f6e2a74c329...|95bee987e9504e2c3...|2021-03-05 11:45:25|  36|\n|     file_3 |f6089c25029240578...|32e2f296cb3867d57...|2021-03-07 13:43:19|  36|\n|     file_4 |bef38ef97883445c8...|e920efe2bc220ffbb...|2021-03-07 13:43:11|  13|\n+------------+--------------------+--------------------+-------------------+----+\n*/\n</code></pre> <p>Run SQL queries on your metadata</p> <pre><code>df.createOrReplaceTempView(\"files\")\nspark.sql(\"SELECT DATE(last_modified), COUNT(*) FROM files GROUP BY 1 ORDER BY 1\")\n/* output example:\n+----------+--------+\n|        dt|count(1)|\n+----------+--------+\n|2021-03-05|       2|\n|2021-03-07|       2|\n+----------+--------+\n*/\n</code></pre> <p>Search by user metadata</p> <pre><code>import io.treeverse.clients.LakeFSContext\n\nval namespace = \"s3://bucket/repo/path/\"\nval df = LakeFSContext.newDF(spark, namespace)\n\nval key = \"SomeKey\"\nval searchedValue = \"val3\"\ndf.select(\"key\", \"user_metadata\")\n.filter(_.getMap[String, String](1).toMap.get(s\"X-Amz-Meta-${key}\").getOrElse(\"\") == searchedValue)\n.show()\n/* output example:\n+---------+-----------------------------------------------------+\n|key      |user_metadata                                        |\n+---------+-----------------------------------------------------+\n|file1.txt|{X-Amz-Meta-SomeKey -&gt; val3, X-Amz-Meta-Tag -&gt; blue} |\n|file8.txt|{X-Amz-Meta-SomeKey -&gt; val3, X-Amz-Meta-Tag -&gt; green}|\n+---------+-----------------------------------------------------+\n*/\n</code></pre>"},{"location":"security/","title":"lakeFS Security Reference","text":""},{"location":"security/#understanding-your-data-security","title":"Understanding Your Data Security","text":"<p>At lakeFS, we understand the critical nature of data security. Thousands of organizations worldwide rely on lakeFS to manage their data with confidence. Here's a few concepts we follow to ensure your data remains secure:</p> <p>Data Stays in Place: The data you version control remains within your existing object storage. lakeFS creates metadata for your data without moving it. New data is stored in the bucket you designate within your object storage.</p> <p>lakeFS Servers Stores only Metadata: The lakeFS Server (also in the case of lakeFS Cloud) only stores the metadata used for version control operations (i.e. diff, merge). It does not store any of your actual data.</p> <p>Minimal Permissions: lakeFS requires minimal permissions to manage your data. We can even version data we cannot directly access by utilizing presigned URLs.</p> <p>Learn more about some of the featured that help keep lakeFS Secure:</p> <ul> <li>Authentication - An overview of the authentication and authorization mechanisms available in lakeFS, including built-in and external services, API and S3 Gateway authentication, and user permissions management.</li> <li>Remote Authenticator - This feature allows organizations to leverage their existing identity infrastructure while using lakeFS, providing a flexible and secure authentication mechanism.</li> <li>Role-Based Access Control (RBAC) - RBAC with lakeFS provides a flexible and granular approach to managing access and permissions, similar to other cloud-based systems like AWS IAM.</li> <li>Presigned URL - This feature allows for more flexible and direct data access in lakeFS, particularly useful for scenarios where bypassing the lakeFS server for data retrieval or storage is beneficial.</li> <li>Single Sign On (SSO) - lakeFS provides administrators with the necessary information to set up SSO for both lakeFS Cloud and Enterprise editions, covering various authentication protocols and identity providers.</li> <li>Short Lived Token (STS like) - This feature allows lakeFS to leverage temporary credentials for secure and flexible authentication, integrating seamlessly with existing identity providers.</li> <li>Login to lakeFS with AWS IAM - This feature enhances the integration between lakeFS and AWS by supporting authenticating users programmatically using AWS IAM roles instead of using static lakeFS access and secret keys.</li> </ul>"},{"location":"security/#soc2-compliance","title":"SOC2 Compliance","text":"<p>lakeFS Cloud is SOC2 compliant, demonstrating our commitment to stringent security standards.</p>"},{"location":"security/#more-questions-contact-us","title":"More questions? Contact us","text":"<p>For details on supported versions, security updates, and vulnerability reporting, please refer to our security policy on GitHub.</p> <p>If you have additional questions regarding lakeFS Security, talk to an expert.</p>"},{"location":"security/ACL-server-implementation/","title":"ACL Server Implementation","text":""},{"location":"security/ACL-server-implementation/#overview","title":"Overview","text":"<p>This guide explains how to implement an ACL (Access Control List) server and configure lakeFS OSS to work with it. This is intended for contributors who want to understand or extend the ACL authentication mechanism in lakeFS.</p> <p>Contents:</p> <ol> <li>Required APIs for implementing an ACL server.</li> <li>How to configure lakeFS OSS to connect to your ACL server.</li> <li>How to run lakeFS OSS with your ACL server.</li> </ol>"},{"location":"security/ACL-server-implementation/#what-is-acl","title":"What is ACL?","text":"<p>Access Control List (ACL) in lakeFS manages permissions by associating a set of permissions directly with a specific object or a group of objects. In the context of the lakeFS authorization API, ACLs are represented within policies. These policies can then be attached to users or groups to grant them the specified permissions.</p>"},{"location":"security/ACL-server-implementation/#implementation-and-setup","title":"Implementation and Setup","text":"<p>Follow these steps to implement an ACL server compatible with lakeFS.</p>"},{"location":"security/ACL-server-implementation/#1-implementation","title":"1. Implementation","text":"<p>To implement the ACL server, you need to implement a subset of the APIs defined in the authorization.yaml specification. Not all APIs in the specification are required \u2014 only those listed below, grouped into the following categories:</p> <ul> <li>Credentials</li> <li>Users</li> <li>Groups</li> <li>Policies</li> </ul> <p>Implement all APIs under these categories.</p> <p>Info</p> <p>For detailed descriptions of the different schemas and each API, including their input and output parameters, refer to each API in the authorization.yaml specification.</p>"},{"location":"security/ACL-server-implementation/#credentials-apis","title":"Credentials APIs","text":"<p>These APIs are used to manage credentials (access key ID and secret access key) for users.</p> <p>Implement the following endpoints under the <code>credentials</code> tag in the authorization.yaml specification:</p> <ul> <li> <p><code>GET /auth/users/{userId}/credentials</code>:</p> <ul> <li>Description: Returns a list of all access_key_ids and their creation dates for a specific user.</li> <li>Input: <code>userId</code> (path parameter), pagination parameters (<code>prefix</code>, <code>after</code>, <code>amount</code>).</li> <li>Output: A <code>CredentialsList</code> object containing a list of <code>Credentials</code> objects and pagination information.</li> <li>Implementation Notes: The results should be sorted by <code>access_key_id</code>.</li> <li> <p>Output Schema (<code>CredentialsList</code>):</p> <pre><code>type: object\nproperties:\n    pagination:\n        $ref: \"#/components/schemas/Pagination\"\n    results:\n        type: array\n        items:\n        $ref: \"#/components/schemas/Credentials\"\n</code></pre> </li> <li> <p>Output Schema (<code>Credentials</code>):</p> <pre><code>type: object\nproperties:\n    access_key_id:\n        type: string\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds\n</code></pre> </li> </ul> </li> <li> <p><code>POST /auth/users/{userId}/credentials</code>:</p> <ul> <li>Description: Creates new credentials for a specific user.</li> <li>Input: <code>userId</code> (path parameter), optional query parameters (<code>access_key</code>, <code>secret_key</code>).</li> <li>Output: A <code>CredentialsWithSecret</code> object containing the generated or provided access key ID, secret access key, creation date, and username.</li> <li>Implementation Notes: If <code>access_key</code> or <code>secret_key</code> are empty, the server should generate random values. The <code>username</code> field in the response is required.</li> <li> <p>Output Schema (<code>CredentialsWithSecret</code>):</p> <pre><code>type: object\nproperties:\n    access_key_id:\n        type: string\n    secret_access_key:\n        type: string\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    user_name:\n        type: string\n        description: A unique identifier for the user.\n</code></pre> </li> </ul> </li> <li> <p><code>DELETE /auth/users/{userId}/credentials/{accessKeyId}</code>:</p> <ul> <li>Description: Deletes credentials for a specific user.</li> <li>Input: <code>userId</code> (path parameter), <code>accessKeyId</code> (path parameter).</li> <li>Output: No output on success (HTTP 204).</li> <li>Implementation Notes: Ensure the user and credentials exist before deleting.</li> </ul> </li> <li> <p><code>GET /auth/users/{userId}/credentials/{accessKeyId}</code>:</p> <ul> <li>Description: Returns a specific user's credentials details (excluding the secret key).</li> <li>Input: <code>userId</code> (path parameter), <code>accessKeyId</code> (path parameter).</li> <li>Output: A <code>Credentials</code> object containing the access key ID and creation date.</li> <li>Implementation Notes: Ensure the user and credentials exist. The secret access key should not be returned by this endpoint.</li> <li> <p>Output Schema (<code>Credentials</code>):</p> <pre><code>type: object\nproperties:\n    access_key_id:\n        type: string\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds\n</code></pre> </li> </ul> </li> <li> <p><code>GET /auth/credentials/{accessKeyId}</code>:</p> <ul> <li>Description: Returns the credentials details associated with a specific accessKeyId (including the secret key).</li> <li>Input: <code>accessKeyId</code> (path parameter).</li> <li>Output: A <code>CredentialsWithSecret</code> object containing all credential details.</li> <li>Implementation Notes: This endpoint is used by lakeFS to authenticate requests using access key IDs and secret access keys. The <code>username</code> field in the response is required.</li> <li> <p>Output Schema (<code>CredentialsWithSecret</code>):</p> <pre><code>type: object\nproperties:\n    access_key_id:\n        type: string\n    secret_access_key:\n        type: string\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    user_name:\n        type: string\n        description: A unique identifier for the user.\n</code></pre> </li> </ul> </li> </ul>"},{"location":"security/ACL-server-implementation/#users-apis","title":"Users APIs","text":"<p>These APIs are used to manage users.</p> <p>Implement the following endpoints under the <code>users</code> tag in the authorization.yaml specification:</p> <ul> <li> <p><code>GET /auth/users</code>:</p> <ul> <li>Description: Returns a list of all users.</li> <li>Input: Pagination parameters (<code>prefix</code>, <code>after</code>, <code>amount</code>).</li> <li>Output: A <code>UserList</code> object containing a list of <code>User</code> objects and pagination information.</li> <li>Implementation Notes: The results must be sorted by the username. The <code>external_id</code> and <code>encryptedPassword</code> fields in the <code>User</code> object are not used internally by lakeFS in the ACL implementation.</li> <li> <p>Output Schema (<code>UserList</code>):</p> <pre><code>type: object\nproperties:\n    pagination:\n        $ref: \"#/components/schemas/Pagination\"\n    results:\n        type: array\n        items:\n        $ref: \"#/components/schemas/User\"\n</code></pre> </li> <li> <p>Output Schema (<code>User</code>):</p> <pre><code>type: object\nproperties:\n    username:\n        type: string\n        description: A unique identifier for the user.\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    friendly_name:\n        type: string\n        description: A name for the user that is not necessarily unique.\n    email:\n        type: string\n    source:\n        type: string\n        description: User source. Based on implementation.\n</code></pre> </li> </ul> </li> <li> <p><code>POST /auth/users</code>:</p> <ul> <li>Description: Creates a new user.</li> <li>Input: Request body containing <code>UserCreation</code> object (<code>username</code>, optional <code>email</code>, <code>friendlyName</code>, <code>source</code>, <code>external_id</code>, <code>invite</code>).</li> <li>Output: A <code>User</code> object representing the created user.</li> <li>Implementation Notes: The <code>username</code> must be unique. The <code>external_id</code> and <code>encryptedPassword</code> fields in the input and output are not used internally by lakeFS in the ACL implementation. If <code>invite</code> is true, an invitation email should be sent (if supported by the implementation).</li> <li> <p>Input Schema (<code>UserCreation</code>):</p> <pre><code>type: object\nproperties:\n    username:\n        type: string\n        minLength: 1\n        description: A unique identifier for the user.\n    email:\n        type: string\n        description: If provided, the email is set to the same value as the username.\n    friendlyName:\n        type: string\n    source:\n        type: string\n        description: User source. Based on implementation.\n    invite:\n        type: boolean\n        description: A boolean that determines whether an invitation email should be sent.\n</code></pre> </li> <li> <p>Output Schema (<code>User</code>):</p> <pre><code>type: object\nproperties:\n    username:\n        type: string\n        description: A unique identifier for the user.\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    friendly_name:\n        type: string\n        description: A name for the user that is not necessarily unique.\n    email:\n        type: string\n    source:\n        type: string\n        description: User source. Based on implementation.\n</code></pre> </li> </ul> </li> <li> <p><code>GET /auth/users/{userId}</code>:</p> <ul> <li>Description: Returns the details of a specific user.</li> <li>Input: <code>userId</code> (path parameter).</li> <li>Output: A <code>User</code> object representing the user.</li> <li>Implementation Notes: Ensure the user exists. The <code>external_id</code> and <code>encryptedPassword</code> fields in the <code>User</code> object are not used internally by lakeFS in the ACL implementation.</li> <li> <p>Output Schema (<code>User</code>):</p> <pre><code>type: object\nproperties:\n    username:\n        type: string\n        description: A unique identifier for the user.\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    friendly_name:\n        type: string\n        description: A name for the user that is not necessarily unique.\n    email:\n        type: string\n    source:\n        type: string\n        description: User source. Based on implementation.\n</code></pre> </li> </ul> </li> <li> <p><code>DELETE /auth/users/{userId}</code>:</p> <ul> <li>Description: Deletes a user.</li> <li>Input: <code>userId</code> (path parameter).</li> <li>Output: No output on success (HTTP 204).</li> <li>Implementation Notes: Ensure the user exists. When a user is deleted, their associated credentials, group memberships, and policy attachments should also be removed.</li> </ul> </li> <li> <p><code>GET /auth/users/{userId}/groups</code>:</p> <ul> <li>Description: Returns the list of groups that a specific user is associated with.</li> <li>Input: <code>userId</code> (path parameter), pagination parameters (<code>prefix</code>, <code>after</code>, <code>amount</code>).</li> <li>Output: A <code>GroupList</code> object containing a list of <code>Group</code> objects and pagination information.</li> <li>Implementation Notes: The results must be sorted by the group name.</li> <li> <p>Output Schema (<code>GroupList</code>):</p> <pre><code>type: object\nproperties:\n    pagination:\n        $ref: \"#/components/schemas/Pagination\"\n    results:\n        type: array\n        items:\n        $ref: \"#/components/schemas/Group\"\n</code></pre> </li> <li> <p>Output Schema (<code>Group</code>):</p> <pre><code>type: object\nproperties:\n    id:\n        type: string\n        description: A unique identifier of the group.\n    name:\n        type: string\n        description: A unique identifier for the group, represented by a human-readable name.\n    description:\n        type: string\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n</code></pre> </li> </ul> </li> <li> <p><code>GET /auth/users/{userId}/policies</code>:</p> <ul> <li>Description: Returns the list of policies associated with a specific user.</li> <li>Input: <code>userId</code> (path parameter), pagination parameters (<code>prefix</code>, <code>after</code>, <code>amount</code>), optional query parameter <code>effective</code> (boolean).</li> <li>Output: A <code>PolicyList</code> object containing a list of <code>Policy</code> objects and pagination information.</li> <li>Implementation Notes: If <code>effective</code> is true, return all distinct policies attached to the user directly or through their groups. If <code>effective</code> is false (default), return only policies directly attached to the user.</li> <li> <p>Output Schema (<code>PolicyList</code>):</p> <pre><code>type: object\nproperties:\n    pagination:\n        $ref: \"#/components/schemas/Pagination\"\n    results:\n        type: array\n        items:\n        $ref: \"#/components/schemas/Policy\"\n</code></pre> </li> <li> <p>Output Schema (<code>Policy</code>):</p> <pre><code>type: object\nproperties:\n    name:\n        type: string\n        description: A unique, human-readable name for the policy.\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    acl:\n        type: string\n        description: Represents the access control list assigned to this policy.\n</code></pre> </li> </ul> </li> <li> <p><code>PUT /auth/users/{userId}/policies/{policyId}</code>:</p> <ul> <li>Description: Attaches a policy to a specific user.</li> <li>Input: <code>userId</code> (path parameter), <code>policyId</code> (path parameter).</li> <li>Output: No output on success (HTTP 201).</li> <li>Implementation Notes: Ensure the user and policy exist.</li> </ul> </li> <li> <p><code>DELETE /auth/users/{userId}/policies/{policyId}</code>:</p> <ul> <li>Description: Detaches a policy from a specific user.</li> <li>Input: <code>userId</code> (path parameter), <code>policyId</code> (path parameter).</li> <li>Output: No output on success (HTTP 204).</li> <li>Implementation Notes: Ensure the user and policy attachment exist.</li> </ul> </li> </ul>"},{"location":"security/ACL-server-implementation/#groups-apis","title":"Groups APIs","text":"<p>These APIs are used to manage groups.</p> <p>Implement the following endpoints under the <code>groups</code> tag:</p> <ul> <li> <p><code>GET /auth/groups</code>:</p> <ul> <li>Description: Returns a list of groups.</li> <li>Input: Pagination parameters (<code>prefix</code>, <code>after</code>, <code>amount</code>).</li> <li>Output: A <code>GroupList</code> object containing a list of <code>Group</code> objects and pagination information.</li> <li>Implementation Notes: The results must be sorted by the group name.</li> <li>Output Schema (<code>GroupList</code>):</li> </ul> <pre><code>type: object\nproperties:\n    pagination:\n        $ref: \"#/components/schemas/Pagination\"\n    results:\n        type: array\n        items:\n        $ref: \"#/components/schemas/Group\"\n</code></pre> <ul> <li> <p>Output Schema (<code>Group</code>):</p> <pre><code>type: object\nproperties:\n    id:\n        type: string\n        description: A unique identifier of the group.\n    name:\n        type: string\n        description: A unique identifier for the group, represented by a human-readable name.\n    description:\n        type: string\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n</code></pre> </li> </ul> </li> <li> <p><code>POST /auth/groups</code>:</p> <ul> <li>Description: Creates a new group.</li> <li>Input: Request body containing <code>GroupCreation</code> object (<code>id</code>, optional <code>description</code>).</li> <li>Output: A <code>Group</code> object representing the created group.</li> <li>Implementation Notes: The <code>id</code> must be a unique, human-readable name for the group. This endpoint is called during setup to create initial groups.</li> <li> <p>Input Schema (<code>GroupCreation</code>):</p> <pre><code>type: object\nrequired:\n    - id\nproperties:\n    id:\n        type: string\n        description: A unique identifier for the group, represented by a human-readable name.\n    description:\n        type: string\n</code></pre> </li> <li> <p>Output Schema (<code>Group</code>):</p> <pre><code>type: object\nproperties:\n    id:\n        type: string\n        description: A unique identifier of the group.\n    name:\n        type: string\n        description: A unique identifier for the group, represented by a human-readable name.\n    description:\n        type: string\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n</code></pre> </li> </ul> </li> <li> <p><code>GET /auth/groups/{groupId}</code>:</p> <ul> <li>Description: Returns the details of a specific group.</li> <li>Input: <code>groupId</code> (path parameter).</li> <li>Output: A <code>Group</code> object representing the group.</li> <li>Implementation Notes: Ensure the group exists.</li> <li> <p>Output Schema (<code>Group</code>):</p> <pre><code>type: object\nproperties:\n    id:\n        type: string\n        description: A unique identifier of the group.\n    name:\n        type: string\n        description: A unique identifier for the group, represented by a human-readable name.\n    description:\n        type: string\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n</code></pre> </li> </ul> </li> <li> <p><code>DELETE /auth/groups/{groupId}</code>:</p> <ul> <li>Description: Deletes a group.</li> <li>Input: <code>groupId</code> (path parameter).</li> <li>Output: No output on success (HTTP 204).</li> <li>Implementation Notes: Ensure the group exists. When a group is deleted, its associated user memberships and policy attachments should also be removed.</li> </ul> </li> <li> <p><code>GET /auth/groups/{groupId}/members</code>:</p> <ul> <li>Description: Returns the list of users associated with a specific group.</li> <li>Input: <code>groupId</code> (path parameter), pagination parameters (<code>prefix</code>, <code>after</code>, <code>amount</code>).</li> <li>Output: A <code>UserList</code> object containing a list of <code>User</code> objects and pagination information.</li> <li>Implementation Notes: The results must be sorted by the username. The <code>external_id</code> and <code>encryptedPassword</code> fields in the <code>User</code> object are not used internally by lakeFS in the ACL implementation.</li> <li> <p>Output Schema (<code>UserList</code>):</p> <p><pre><code>type: object\nproperties:\n    pagination:\n        $ref: \"#/components/schemas/Pagination\"\n    results:\n        type: array\n        items:\n        $ref: \"#/components/schemas/User\"\n</code></pre>     - Output Schema (<code>User</code>):</p> <pre><code>type: object\nproperties:\n    username:\n        type: string\n        description: A unique identifier for the user.\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    friendly_name:\n        type: string\n        description: A name for the user that is not necessarily unique.\n    email:\n        type: string\n    source:\n        type: string\n        description: User source. Based on implementation.\n</code></pre> </li> </ul> </li> <li> <p><code>PUT /auth/groups/{groupId}/members/{userId}</code>:</p> <ul> <li>Description: Adds a specific user to a specific group.</li> <li>Input: <code>groupId</code> (path parameter), <code>userId</code> (path parameter).</li> <li>Output: No output on success (HTTP 201).</li> <li>Implementation Notes: Ensure the group and user exist.</li> </ul> </li> <li> <p><code>DELETE /auth/groups/{groupId}/members/{userId}</code>:</p> <ul> <li>Description: Removes a specific user from a specific group.</li> <li>Input: <code>groupId</code> (path parameter), <code>userId</code> (path parameter).</li> <li>Output: No output on success (HTTP 204).</li> <li>Implementation Notes: Ensure the group and user membership exist.</li> </ul> </li> <li> <p><code>GET /auth/groups/{groupId}/policies</code>:</p> <ul> <li>Description: Returns the list of policies attached to a specific group.</li> <li>Input: <code>groupId</code> (path parameter), pagination parameters (<code>prefix</code>, <code>after</code>, <code>amount</code>).</li> <li>Output: A <code>PolicyList</code> object containing a list of <code>Policy</code> objects and pagination information.</li> <li>Implementation Notes: The results must be sorted by the policy name.</li> <li> <p>Output Schema (<code>PolicyList</code>):</p> <pre><code>type: object\nproperties:\n    pagination:\n        $ref: \"#/components/schemas/Pagination\"\n    results:\n        type: array\n        items:\n        $ref: \"#/components/schemas/Policy\"\n</code></pre> </li> <li> <p>Output Schema (<code>Policy</code>):</p> <pre><code>type: object\nproperties:\n    name:\n        type: string\n        description: A unique, human-readable name for the policy.\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    acl:\n        type: string\n        description: Represents the access control list assigned to this policy.\n</code></pre> </li> </ul> </li> <li> <p><code>PUT /auth/groups/{groupId}/policies/{policyId}</code>:</p> <ul> <li>Description: Attaches a policy to a specific group.</li> <li>Input: <code>groupId</code> (path parameter), <code>policyId</code> (path parameter).</li> <li>Output: No output on success (HTTP 201).</li> <li>Implementation Notes: Ensure the group and policy exist.</li> </ul> </li> <li> <p><code>DELETE /auth/groups/{groupId}/policies/{policyId}</code>:</p> <ul> <li>Description: Detaches a policy from a specific group.</li> <li>Input: <code>groupId</code> (path parameter), <code>policyId</code> (path parameter).</li> <li>Output: No output on success (HTTP 204).</li> <li>Implementation Notes: Ensure the group and policy attachment exist.</li> </ul> </li> </ul>"},{"location":"security/ACL-server-implementation/#policies-apis","title":"Policies APIs","text":"<p>These APIs are used to manage policies, which contain the ACL information.</p> <p>Implement the following endpoints under the <code>policies</code> tag:</p> <ul> <li> <p><code>GET /auth/policies</code>:</p> <ul> <li>Description: Returns a list of policies.</li> <li>Input: Pagination parameters (<code>prefix</code>, <code>after</code>, <code>amount</code>).</li> <li>Output: A <code>PolicyList</code> object containing a list of <code>Policy</code> objects and pagination information.</li> <li>Implementation Notes: The results must be sorted by the policy name.</li> <li> <p>Output Schema (<code>PolicyList</code>):</p> <pre><code>type: object\nproperties:\n    pagination:\n        $ref: \"#/components/schemas/Pagination\"\n    results:\n        type: array\n        items:\n        $ref: \"#/components/schemas/Policy\"\n</code></pre> </li> <li> <p>Output Schema (<code>Policy</code> - relevant fields for ACL):</p> <pre><code>type: object\nproperties:\n    name:\n        type: string\n        description: A unique, human-readable name for the policy.\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    acl:\n        type: string\n        description: Represents the access control list assigned to this policy.\n</code></pre> </li> </ul> </li> <li> <p><code>POST /auth/policies</code>:</p> <ul> <li>Description: Creates a new policy.</li> <li>Input: Request body containing a <code>Policy</code> object (<code>name</code>, <code>statement</code> and <code>acl</code>).</li> <li>Output: A <code>Policy</code> object representing the created policy.</li> <li>Implementation Notes: The <code>name</code> must be a unique, human-readable name for the policy. The <code>acl</code> property in the <code>Policy</code> object is used to define the permissions. The <code>statement</code> property is not used in the ACL implementation. This endpoint is called during setup to create default policies.</li> <li> <p>Input Schema (<code>Policy</code> - relevant fields for ACL):</p> <pre><code>type: object\nproperties:\n    name:\n        type: string\n        description: A unique, human-readable name for the policy.\n    acl:\n        type: string\n        description: Represents the access control list assigned to this policy.\n</code></pre> </li> <li> <p>Output Schema (<code>Policy</code>):</p> <pre><code>type: object\nproperties:\n    name:\n        type: string\n        description: A unique, human-readable name for the policy.\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    acl:\n        type: string\n        description: Represents the access control list assigned to this policy.\n</code></pre> </li> </ul> </li> <li> <p><code>GET /auth/policies/{policyId}</code>:</p> <ul> <li>Description: Returns the details of a specific policy.</li> <li>Input: <code>policyId</code> (path parameter).</li> <li>Output: A <code>Policy</code> object representing the policy.</li> <li>Implementation Notes: Ensure the policy exists. The <code>statement</code> property is not used in the ACL implementation.</li> <li> <p>Output Schema (<code>Policy</code>):</p> <pre><code>type: object\nproperties:\n    name:\n        type: string\n        description: A unique, human-readable name for the policy.\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    acl:\n        type: string\n        description: Represents the access control list assigned to this policy.\n</code></pre> </li> </ul> </li> <li> <p><code>PUT /auth/policies/{policyId}</code>:</p> <ul> <li>Description: Updates an existing policy.</li> <li>Input: <code>policyId</code> (path parameter), request body containing the updated <code>Policy</code> object.</li> <li>Output: A <code>Policy</code> object representing the updated policy.</li> <li>Implementation Notes: Ensure the policy exists and the provided <code>policyId</code> matches the <code>name</code> in the request body. The request is to update the <code>acl</code> property. The <code>statement</code> property is not used in the ACL implementation.</li> <li> <p>Input Schema (<code>Policy</code>):</p> <pre><code>type: object\nproperties:\n    name:\n        type: string\n        description: A unique, human-readable name for the policy.\n    acl:\n        type: string\n        description: Represents the access control list assigned to this policy.\n</code></pre> </li> <li> <p>Output Schema (<code>Policy</code>):</p> <pre><code>type: object\nproperties:\n    name:\n        type: string\n        description: A unique, human-readable name for the policy.\n    creation_date:\n        type: integer\n        format: int64\n        description: Unix Epoch in seconds.\n    acl:\n        type: string\n        description: Represents the access control list assigned to this policy.\n</code></pre> </li> </ul> </li> <li> <p><code>DELETE /auth/policies/{policyId}</code>:</p> <ul> <li>Description: Deletes a policy.</li> <li>Input: <code>policyId</code> (path parameter).</li> <li>Output: No output on success (HTTP 204).</li> <li>Implementation Notes: Ensure the policy exists. When a policy is deleted, its attachments to users and groups should also be removed.</li> </ul> </li> </ul>"},{"location":"security/ACL-server-implementation/#2-setup","title":"2. Setup","text":""},{"location":"security/ACL-server-implementation/#key-steps-in-the-initial-setup","title":"Key Steps in the Initial Setup","text":"<p>When deploying an ACL server for the first time, it is essential to establish a set of standard user groups and assign each group a default set of permissions (policies). This process ensures that the system starts with a clear structure for access control, making it easy to manage users and their roles securely and consistently.</p> <p>Define Standard Groups</p> <p>Define Standard Groups</p> <p>Establish a set of base groups that represent the typical roles in your system, such as Admins (full privileges), Writers (read/write access), and Readers (read-only access). Each group should be mapped to a specific policy that defines the permissions for its members.</p> <p>You can reference the lakeFS contrib ACL implementation to see practical examples of how standard groups are defined and structured, along with their associated permission policies.</p>"},{"location":"security/ACL-server-implementation/#lakefs-configuration","title":"lakeFS Configuration","text":"<p>Update your lakeFS configuration file (<code>config.yaml</code>) to include:</p> <pre><code>auth:\n  encrypt:\n    secret_key: \"some_string\"\n  ui_config:\n    rbac: \"simplified\"\n  api:\n    endpoint: {ENDPOINT_TO_YOUR_ACL_SERVER} # e.g., http://localhost:9006/api/v1\n    token: {ACL_SERVER_TOKEN} # Used as authentication bearer calling the ACL server\n</code></pre> <p>Note</p> <p>The <code>auth.api.token</code> parameter is optional. If unspecified, lakeFS uses the <code>auth.encrypt.secret_key</code> as the secret to generate JWT. If specified, provide a JWT token or via the environment variable <code>LAKEFS_AUTH_API_TOKEN</code>.</p>"},{"location":"security/access-control-lists/","title":"Access Control Lists (ACLs)","text":"<p>Warning</p> <p>ACLs were removed from core lakeFS.</p> <p>For a more robust authorization solution, please see Role-Based Access Control, available in lakeFS Cloud and lakeFS Enterprise. The following documentation is aimed for users with existing installations who wish to continue working with ACLs. </p>"},{"location":"security/access-control-lists/#basic-auth-functionality","title":"Basic Auth Functionality","text":"<p>New lakeFS versions will provide basic auth functionality featuring a single Admin user with a single set of credentials. Existing lakeFS installations that have a single user and a single set of credentials will migrate seamlessly to the new version. Installations that have more than one user / credentials will require to run a command and choose which set of user + credentials to migrate (more details here)</p>"},{"location":"security/access-control-lists/#credentials-replacement","title":"Credentials Replacement","text":"<p>In a single user setup, replacing credentials can be done as follows:</p> <ol> <li> <p>Delete the existing user:</p> <pre><code>lakectl auth users delete --id &lt;user-id&gt;\n</code></pre> </li> <li> <p>Shut down the lakeFS server - Required for invalidating the old credentials on the server</p> </li> <li> <p>Create a new user, with the same name and new credentials:</p> <pre><code>lakefs superuser --user-name &lt;user-id&gt;\n</code></pre> <p>This will generate a new set of credentials, and will print it out to the screen:</p> <pre><code>credentials:\n  access_key_id: *** (omitted)\n  secret_access_key: *** (omitted)\n</code></pre> </li> <li> <p>Re-run lakeFS server</p> </li> </ol> <p>Warning</p> <p>Calling the <code>superuser</code> command with pre-defined <code>--access-key-id</code> and <code>--secret-access-key</code> is possible, but should be done with caution. Make sure that <code>--secret-access-key</code> is not empty, as providing an access key without a secret key will trigger an ACL import flow (see Migration of existing user).</p> <p>In case you already deleted the user by following step (1), this import operation will fail and result in an  unrecoverable state, from which a clean installation is the only way out.</p>"},{"location":"security/access-control-lists/#acls","title":"ACLs","text":"<p>ACL server was moved out of core lakeFS and into a new package under <code>contrib/auth/acl</code>. Though we decided to move ACLs out, we are committed to making sure existing users who still need the use of ACLs can continue using this feature. In order to do that, users will need to run the separate ACL server as part of their lakeFS deployment environment and configure lakeFS to work with it.</p>"},{"location":"security/access-control-lists/#acl-server-configuration","title":"ACL server Configuration","text":"<p>Under the <code>contrib/auth/acl</code> you will be able to find an ACL server reference.</p> <p>Warning</p> <p>This implementation is a reference and is not fit for production use.</p> <p>For a more robust authorization solution, please see Role-Based Access Control, available in lakeFS Cloud and lakeFS Enterprise. </p> <p>The configuration of the ACL server is similar to lakeFS configuration, here's an example of an <code>.aclserver.yaml</code> config file:</p> <pre><code>---\nlisten_address: \"[ACL_SERVER_LISTEN_ADDRESS]\"\ndatabase:\n    type: \"postgres\"\n    postgres:\n    connection_string: \"[DATABASE_CONNECTION_STRING]\"\n\nencrypt:\n    # This should be the same encryption key as in lakeFS\n    secret_key: \"[ENCRYPTION_SECRET_KEY]\"\n</code></pre> <p>It is possible to use environment variables to configure the server as in lakeFS. Use the <code>ACLSERVER_</code> prefix to do so.  </p> <p>Info</p> <p>For full configuration reference see: this</p>"},{"location":"security/access-control-lists/#lakefs-configuration","title":"lakeFS Configuration","text":"<p>For the ACL server to work, configure the following values in lakeFS:  </p> <ul> <li><code>auth.ui_config.rbac</code>: <code>simplified</code> </li> <li><code>auth.api.endpoint</code>: <code>[ACL_SERVER_LISTEN_ADDRESS]</code></li> </ul>"},{"location":"security/access-control-lists/#migration-of-existing-user","title":"Migration of existing user","text":"<p>For installation with multiple users / credentials, upgrading to the new lakeFS version requires choosing which user + credentials will be used for the single user mode. This is done via the <code>lakefs superuser</code> command. For example, if you have a user with username <code>&lt;my-username&gt;</code> and credential key <code>&lt;my-access-key-id&gt;</code> use the following command to migrate that user:</p> <pre><code>lakefs superuser --user-name &lt;my-username&gt; --access-key-id &lt;my-access-key-id&gt;\n</code></pre> <p>After running the command you will be able to access the installation using the user's access key id and its respective secret access key.</p>"},{"location":"security/authentication/","title":"Authentication","text":""},{"location":"security/authentication/#user-authentication","title":"User Authentication","text":"<p>lakeFS authenticates users from a built-in authentication database.</p>"},{"location":"security/authentication/#built-in-database","title":"Built-in database","text":"<p>The built-in authentication database is always present and active. You can use the Web UI at Administration / Users to create users.</p> <p>Users have an access key <code>AKIA...</code> and an associated secret access key. These credentials are valid for logging into the Web UI or authenticating programmatic requests to the API Server or the S3 Gateway.</p>"},{"location":"security/authentication/#remote-authenticator-service","title":"Remote Authenticator Service","text":"<p>lakeFS server supports external authentication, the feature can be configured by providing an HTTP endpoint to an external authentication service. This integration can be especially useful if you already have an existing authentication system in place, as it allows you to reuse that system instead of maintaining a new one.</p> <p>Info</p> <p>To configure a Remote Authenticator see the configuration fields.</p>"},{"location":"security/authentication/#api-server-authentication","title":"API Server Authentication","text":"<p>Authenticating against the API server is done using a key-pair, passed via Basic Access Authentication.</p> <p>All HTTP requests must carry an <code>Authorization</code> header with the following structure:</p> <pre><code>Authorization: Basic &lt;base64 encoded access_key_id:secret_access_key&gt;\n</code></pre> <p>For example, assuming my access_key_id is <code>my_access_key_id</code> and my secret_access_key is <code>my_secret_access_key</code>, we'd send the following header with every request:</p> <pre><code>Authorization: Basic bXlfYWNjZXNzX2tleV9pZDpteV9zZWNyZXRfYWNjZXNzX2tleQ==\n</code></pre>"},{"location":"security/authentication/#s3-gateway-authentication","title":"S3 Gateway Authentication","text":"<p>To provide API compatibility with Amazon S3, authentication with the S3 Gateway supports both SIGv2 and SIGv4. Clients such as the AWS SDK that implement these authentication methods should work without modification.</p> <p>See this example for authenticating with the AWS CLI.</p>"},{"location":"security/authentication/#oidc-support","title":"OIDC support","text":"<p>Deprecated</p> <p>This feature is deprecated. For single sign-on with lakeFS, try lakeFS Cloud</p> <p>OpenID Connect (OIDC) is a simple identity layer on top of the OAuth 2.0 protocol. You can configure lakeFS to enable OIDC to manage your lakeFS users externally. Essentially, once configured, this enables you the benefit of OpenID connect, such as a single sign-on (SSO), etc.</p>"},{"location":"security/authentication/#configuring-lakefs-server-for-oidc","title":"Configuring lakeFS server for OIDC","text":"<p>To support OIDC, add the following to your lakeFS configuration:</p> <pre><code>auth:\n  oidc:\n    enabled: true\n    client_id: example-client-id\n    client_secret: exampleSecretValue\n    callback_base_url: https://lakefs.example.com       # The scheme, domain (and port) of your lakeFS installation\n    url: https://my-account.oidc-provider-example.com\n    default_initial_groups: [\"Developers\"]\n    friendly_name_claim_name: name                      #  Optional: use the value from this claim as the user's display name\n    persist_friendly_name: true                         #  Optional: persist friendly name to KV store so it can be displayed in the user list\n</code></pre> <p>Your login page will now include a link to sign in using the OIDC provider. When a user first logs in through the provider, a corresponding user is created in lakeFS.</p>"},{"location":"security/authentication/#friendly-name-persistence","title":"Friendly Name Persistence","text":"<p>When the <code>persist_friendly_name</code> configuration property is set to <code>true</code> and <code>friendly_name_claim_name</code> is set to a valid claim name, which exists in the incoming <code>id_token</code>, the friendly name will be persisted to the KV store. This will allow users with access to the lakeFS administration section to see friendly names in the users list, when listing group members, and when adding/removing group members. The friendly name stored in KV is updated with each successful login, if the incoming value is different than the stored value. This means it will be kept up-to-date with changes to the user's profile or if <code>friendly_name_claim_name</code> is re-configured.</p> <p>Notes</p> <ol> <li>As always, you may choose to provide these configurations using environment variables.</li> <li>You may already have other configuration values under the auth key, so make sure you combine them correctly.</li> </ol>"},{"location":"security/authentication/#user-permissions","title":"User permissions","text":"<p>Authorization is managed via lakeFS groups and policies.</p> <p>By default, an externally managed user is assigned to the lakeFS groups configured in the default_initial_groups property above. For a user to be assigned to other groups, add the initial_groups claim to their ID token claims. The claim should contain a comma-separated list of group names.</p> <p>Once the user has been created, you can manage their permissions from the Administration pages in the lakeFS UI or using lakectl.</p>"},{"location":"security/authentication/#using-a-different-claim-name","title":"Using a different claim name","text":"<p>To supply the initial groups using another claim from your ID token, you can use the <code>auth.oidc.initial_groups_claim_name</code> lakeFS configuration. For example, to take the initial groups from the roles claim, add:</p> <pre><code>auth:\n  oidc:\n    # ... Other OIDC configurations\n    initial_groups_claim_name: roles\n</code></pre>"},{"location":"security/authorization-yaml/","title":"Authorization API","text":""},{"location":"security/external-principals-aws/","title":"Authenticate to lakeFS with AWS IAM Roles","text":"<p>Info</p> <p>Available in lakeFS Cloud and lakeFS Enterprise If you're using the open-source version you can check the pluggable APIs.</p>","boost":2},{"location":"security/external-principals-aws/#overview","title":"Overview","text":"<p>lakeFS supports authenticating users programmatically using AWS IAM roles instead of using static lakeFS access and secret keys. The method enables you to bound IAM principal ARNs to lakeFS users. A single lakeFS user may have many AWS's principle ARNs attached to it. When a client is authenticating to a lakeFS server with an AWS's session, the actions performed by the client are on behalf of the user attached to the ARN.</p>","boost":2},{"location":"security/external-principals-aws/#using-session-names","title":"Using Session Names","text":"<p>The bound ARN can be attached to a single lakeFS user with or without SessionName, serving different users. For example, consider the following mapping: </p> Principal ARN lakeFS User <code>arn:aws:sts::123456:assumed-role/Dev</code> <code>foo</code> <code>arn:aws:sts::123456:assumed-role/Dev/john@acme.com</code> <code>john</code> <p>if the bound ARN were <code>arn:aws:sts::123456:assumed-role/Dev/&lt;SessionName&gt;</code> it would allow any principal assuming <code>Dev</code> role in AWS account <code>123456</code> to login to it. If the <code>SessionName</code> is <code>john@acme.com</code> then lakeFS would return token for <code>john</code> user</p>","boost":2},{"location":"security/external-principals-aws/#how-aws-authentication-works","title":"How AWS authentication works","text":"<p>The AWS STS API includes a method, <code>sts:GetCallerIdentity</code>, which allows you to validate the identity of a client. The client signs a GetCallerIdentity query using the AWS Signature v4 algorithm and sends it to the lakeFS server. </p> <p>The <code>GetCallerIdentity</code> query consists of four pieces of information: the request URL, the request body, the request headers and the request method. The AWS signature is computed over those fields. The lakeFS server reconstructs the query using this information and forwards it on to the AWS STS service. Depending on the response from the STS service, the server authenticates the client.</p> <p>Notably, clients don't need network-level access themselves to talk to the AWS STS API endpoint; they merely need access to the credentials to sign the request. However, it means that the lakeFS server does need network-level access to send requests to the STS endpoint.</p> <p>Each signed AWS request includes the current timestamp to mitigate the risk of replay attacks. In addition, lakeFS allows you to require an additional header, <code>X-LakeFS-Server-ID</code> (added by default), to be present to mitigate against different types of replay attacks (such as a signed <code>GetCallerIdentity</code> request stolen from a dev lakeFS instance and used to authenticate to a prod lakeFS instance). </p> <p>It's also important to note that Amazon does NOT appear to include any sort of authorization around calls to GetCallerIdentity. For example, if you have an IAM policy on your credential that requires all access to be MFA authenticated, non-MFA authenticated credentials will still be able to authenticate to lakeFS using this method.</p>","boost":2},{"location":"security/external-principals-aws/#server-configuration","title":"Server Configuration","text":"<p>Info</p> <p>lakeFS Helm chart supports the configuration since version <code>1.2.11</code> - see usage values.yaml example.</p> <ul> <li>in lakeFS <code>auth.authentication_api.external_principals_enabled</code> must be set to <code>true</code> in the configuration file, other configuration (<code>auth.authentication_api.*</code>) can be found at configuration reference</li> </ul> <p>For the full list of the Fluffy server configuration, see Fluffy Configuration under <code>auth.external.aws_auth</code></p> <p>Note</p> <p>By default, lakeFS clients will add the parameter <code>X-LakeFS-Server-ID: &lt;lakefs.ingress.domain&gt;</code> to the initial login request for STS.</p> <p>Example configuration with required headers:</p> <p>Configuration for <code>lakefs.yaml</code>: </p> <pre><code>auth:\n  authentication_api:\n    endpoint: http://&lt;fluffy-sso&gt;/api/v1\n    external_principals_enabled: true\n  api:\n    endpoint: http://&lt;fluffy-rbac&gt;/api/v1\n</code></pre> <p>Configuration for <code>fluffy.yaml</code>:</p> <pre><code># fluffy address for lakefs auth.authentication_api.endpoint\n# used by lakeFS to log in and get the token\nlisten_address: &lt;fluffy-sso&gt;\nauth:\n  # fluffy address for lakeFS auth.api.endpoint \n  # used by lakeFS to manage the lifecycle attach/detach of the external principals\n  serve_listen_address: &lt;fluffy-rbac&gt;\n  external:\n    aws_auth:\n      enabled: true\n      # headers that must be present by the client when doing login request\n      required_headers:\n        # same host as the lakeFS server ingress\n        X-LakeFS-Server-ID: &lt;lakefs.ingress.domain&gt;\n</code></pre>","boost":2},{"location":"security/external-principals-aws/#administration-of-iam-roles-in-lakefs","title":"Administration of IAM Roles in lakeFS","text":"<p>Administration refers to the management of the IAM roles that are allowed to authenticate to lakeFS. Operations such as attaching and detaching IAM roles to a user, listing the roles attached to a user, and listing the users attached to a role.  Currently, this is done through the lakeFS External Principals API and generated clients.</p> <p>Example of attaching an IAM roles to a user:</p> <pre><code>import lakefs_sdk as lakefs  \n\nconfiguration = lakefs.Configuration(host = \"...\",username=\"...\",password=\"...\")\nusername = \"&lt;lakefs-user&gt;\"\napi = lakefs.ApiClient(configuration)\nauth_api = lakefs.AuthApi(api)\n\n# attach the role(s)to a lakeFS user\nauth_api.create_user_external_principal(\n    user_id=username, principal_id='arn:aws:sts::&lt;id&gt;:assumed-role/&lt;role A&gt;/&lt;optional session name&gt;')\nauth_api.create_user_external_principal(\n    user_id=username, principal_id='arn:aws:sts::&lt;id&gt;:assumed-role/&lt;role B&gt;')\n\n# list the roles attached to the user\nresp = auth_api.list_user_external_principals(user_id=username)\nfor p in resp.results:\n    # do something\n</code></pre>","boost":2},{"location":"security/external-principals-aws/#get-lakefs-api-token","title":"Get lakeFS API Token","text":"<p>The login to lakeFS is done by calling the login API with the <code>GetCallerIdentity</code> request signed by the client. Currently, the login operation is supported out of the box in:</p> <ul> <li>lakeFS Hadoop FileSystem version 0.2.4, see Spark usage</li> <li>python</li> <li>Everest mount</li> </ul> <p>For other use cases authenticate to lakeFS via login endpoint, this will require building the request input.</p>","boost":2},{"location":"security/external-principals-aws/#login-with-python","title":"Login with python","text":"","boost":2},{"location":"security/external-principals-aws/#prerequisites","title":"prerequisites","text":"<ol> <li>lakeFS should be configured to allow external principals to authenticate and the used IAM role should be attached to the relevant lakeFS user</li> <li>The Python SDK requires additional packages to be installed in order to generate a lakeFS client with the assumed role. To install the required packages, run the following command:</li> </ol> <pre><code>  pip install \"lakefs[aws-iam]\"\n</code></pre> <p>There are two ways in which external principals can be used to authenticate to lakeFS:</p> <ol> <li> <p>If no other authentication flow is provided, and the <code>credentials.provider.type</code> configuration is set to <code>aws_iam</code> in <code>.lakectl.yaml</code>, the client will use the machine's AWS role to authenticate with lakeFS:</p> <p><pre><code>credentials:\n    provider:\n    type: aws_iam\n    aws_iam:\n        token_ttl_seconds: 3600      # TTL for the temporary token (default: 3600)\n        url_presign_ttl_seconds: 60  # TTL for presigned URLs (default: 60)\n        token_request_headers:       # Optional headers for token requests\n        HeaderName: HeaderValue\n</code></pre> Or using environment variables: <pre><code>export LAKECTL_CREDENTIALS_PROVIDER_TYPE=\"aws_iam\"\nexport LAKECTL_CREDENTIALS_PROVIDER_AWS_IAM_TOKEN_TTL_SECONDS=\"3600\"\nexport LAKECTL_CREDENTIALS_PROVIDER_AWS_IAM_PRESIGNED_URL_TTL_SECONDS=\"60\"\nexport LAKECTL_CREDENTIALS_PROVIDER_AWS_IAM_TOKEN_REQUEST_HEADERS='{\"HeaderName\":\"HeaderValue\"}'\n</code></pre> To use the client, merely <code>import lakefs</code> and use it as you would normally do: <pre><code>import lakefs\n\nfor branch in lakefs.repository(\"example-repo\").branches():\nprint(branch)\n</code></pre></p> <p>Warning</p> <p>Please note, using the IAM provider configurations will not work with the lakectl command line tool, and will stop you from running it.</p> </li> <li> <p>Generate a lakeFS client with the assumed role by initiating a boto3 session with the desired role and call <code>lakefs.client.frow_aws_role</code>:</p> <pre><code>import lakefs\nimport boto3    \n\nsession = boto3.Session()\nmy_client = lakefs.client.from_aws_role(session=session, ttl_seconds=7200, host=\"&lt;lakefs-host&gt;\")\n\n# list repositories\nrepos = lakefs.repositories(client=my_client)\nfor r in repos:\n    print(r)\n</code></pre> </li> </ol>","boost":2},{"location":"security/presigned-url/","title":"Configuring lakeFS to use presigned URLs","text":"<p>With lakeFS, you can access data directly from the storage and not through lakeFS using a presigned URL. Based on the user's access to an object in the object store, the presigned URL will get read or write access.</p> <p>The presign support is enabled for block adapter that supports it (AWS, GCP, Azure), and can be disabled by the configuration (<code>blockstore.&lt;blockstore_type&gt;.disable_pre_signed</code>). Note that the UI support is disabled by default.</p> <p>It is possible to override the default pre-signed URL endpoint in AWS by setting the configuration (<code>blockstore.s3.pre_signed_endpoint</code>). This is useful, for example, when you wish to define a VPC endpoint access for the pre-signed URL.</p>"},{"location":"security/presigned-url/#using-presigned-urls-in-the-ui","title":"Using presigned URLs in the UI","text":"<p>For using presigned URLs in the UI: 1. Enable the presigned URL support UI in the lakeFS configuration (<code>blockstore.&lt;blockstore_type&gt;.disable_pre_signed_ui</code>   ). 2. Add CORS (Cross-Origin Resource Sharing) permissions to the bucket for the UI to fetch objects using a presigned URL (instead of through lakeFS). 3. The <code>blockstore.&lt;blockstore_type&gt;.disable_pre_signed</code> must be false to enable it in the UI.</p> <p>Warning</p> <p>Currently DuckDB fetching data from lakeFS does not support fetching data using presigned URL.</p>"},{"location":"security/presigned-url/#examples","title":"Examples","text":"<p>AWS S3</p> <pre><code>  [\n    {\n        \"AllowedHeaders\": [\n            \"*\"\n        ],\n        \"AllowedMethods\": [\n            \"GET\",\n            \"PUT\",\n            \"HEAD\"\n        ],\n        \"AllowedOrigins\": [\n            \"lakefs.endpoint\"\n        ],\n        \"ExposeHeaders\": [\n            \"ETag\"\n        ]\n    }\n  ]\n</code></pre> <p>Google Storage</p> <pre><code>  [\n    {\n        \"origin\": [\"lakefs.endpoint\"],\n        \"responseHeader\": [\"ETag\"],\n        \"method\": [\"PUT\", \"GET\", \"HEAD\"],\n        \"maxAgeSeconds\": 3600\n    }\n  ]\n</code></pre> <p>Azure blob storage</p> <pre><code>  &lt;Cors&gt;\n      &lt;CorsRule&gt;  \n          &lt;AllowedOrigins&gt;lakefs.endpoint&lt;/AllowedOrigins&gt;  \n          &lt;AllowedMethods&gt;PUT,GET,HEAD&lt;/AllowedMethods&gt;  \n          &lt;AllowedHeaders&gt;*&lt;/AllowedHeaders&gt;  \n          &lt;ExposedHeaders&gt;ETag,x-ms-*&lt;/ExposedHeaders&gt;  \n          &lt;MaxAgeInSeconds&gt;3600&lt;/MaxAgeInSeconds&gt;  \n      &lt;/CorsRule&gt;  \n  &lt;/Cors&gt;\n</code></pre>"},{"location":"security/rbac/","title":"Role-Based Access Control (RBAC)","text":"<p>Info</p> <p>Available in lakeFS Cloud and lakeFS Enterprise</p>","boost":2},{"location":"security/rbac/#rbac-model","title":"RBAC Model","text":"<p>Access to resources is managed very much like AWS IAM.</p> <p>There are five basic components to the system:</p> <ol> <li>Users - Representing entities that access and use the system. A user is given one or more Access Credentials for authentication.</li> <li>Actions - Representing a logical action within the system - reading a file, creating a repository, etc.</li> <li>Resources - A unique identifier representing a specific resource in the system - a repository, an object, a user, etc.</li> <li>Policies - Representing a set of Actions, a Resource and an effect: whether or not these actions are <code>allowed</code> or <code>denied</code> for the given resource(s).</li> <li>Groups - A named collection of users. Users can belong to multiple groups.</li> </ol> <p>Controlling access is done by attaching Policies, either directly to Users, or to Groups they belong to.</p>","boost":2},{"location":"security/rbac/#authorization-process","title":"Authorization process","text":"<p>Every action in the system - be it an API request, UI interaction, S3 Gateway call, or CLI command - requires a set of actions to be allowed for one or more resources.</p> <p>When a user makes a request to perform that action, the following process takes place:</p> <ol> <li>Authentication - the credentials passed in the request are evaluated and the user's identity is extracted.</li> <li>Action permission resolution - lakeFS then calculates the set of allowed actions and resources that this request requires.</li> <li>Effective policy resolution - the user's policies (either attached directly or through group memberships) are calculated.</li> <li>Policy/Permission evaluation - lakeFS will compare the given user policies with the request actions and determine whether or not the request is allowed to continue.</li> </ol>","boost":2},{"location":"security/rbac/#policy-precedence","title":"Policy Precedence","text":"<p>Each policy attached to a user or a group has an <code>Effect</code> - either <code>allow</code> or <code>deny</code>. During evaluation of a request, <code>deny</code> would take precedence over any other <code>allow</code> policy.</p> <p>This helps us compose policies together. For example, we could attach a very permissive policy to a user and use <code>deny</code> rules to then selectively restrict what that user can do.</p>","boost":2},{"location":"security/rbac/#resource-naming-arns","title":"Resource naming - ARNs","text":"<p>lakeFS uses ARN identifier - very similar in structure to those used by AWS.  The resource segment of the ARN supports wildcards: use <code>*</code> to match 0 or more characters, or <code>?</code> to match exactly one character.  </p> <p>Here are a some examples of valid ARNs within lakeFS and their meaning:</p> ARN Meaning <code>arn:lakefs:auth:::user/jane.doe</code> A specific user <code>arn:lakefs:auth:::user/*</code> All users <code>arn:lakefs:fs:::repository/myrepo/*</code> All resources under <code>myrepo</code> <code>arn:lakefs:fs:::repository/myrepo/object/foo/bar/baz</code> A single object ARN <code>arn:lakefs:fs:::repository/myrepo/object/*</code> All objects in <code>myrepo</code> <code>arn:lakefs:fs:::repository/*</code> All repositories <code>arn:lakefs:fs:::*</code> All resources under the fs ARN prefix <p>Additionally, the current user's ID is interpolated in runtime into the ARN using the <code>${user}</code> placeholder.</p> <p>This allows us to create fine-grained policies affecting only a specific subset of resources.</p> <p>See below for a full reference of ARNs and actions.</p>","boost":2},{"location":"security/rbac/#actions-and-permissions","title":"Actions and Permissions","text":"<p>For the full list of actions and their required permissions see the following table:</p> Action name required action Resource API endpoint S3 gateway operation List Repositories <code>fs:ListRepositories</code> <code>*</code> GET <code>/repositories</code> ListBuckets Get Repository <code>fs:ReadRepository</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repositoryId}</code> HeadBucket Get Commit <code>fs:ReadCommit</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repositoryId}/commits/{commitId}</code> - Create Commit <code>fs:CreateCommit</code> <code>arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId}</code> POST <code>/repositories/{repositoryId}/branches/{branchId}/commits</code> - Get Commit log <code>fs:ReadBranch</code> <code>arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId}</code> GET <code>/repositories/{repositoryId}/branches/{branchId}/commits</code> - Create Repository <code>fs:CreateRepository</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> POST <code>/repositories</code> - Namespace Attach to Repository <code>fs:AttachStorageNamespace</code> <code>arn:lakefs:fs:::namespace/{storageNamespace}</code> POST <code>/repositories</code> - Import From Source <code>fs:ImportFromStorage</code> <code>arn:lakefs:fs:::namespace/{storageNamespace}</code> POST <code>/repositories/{repositoryId}/branches/{branchId}/import</code> - Cancel Import <code>fs:ImportCancel</code> <code>arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId}</code> DELETE <code>/repositories/{repositoryId}/branches/{branchId}/import</code> - Delete Repository <code>fs:DeleteRepository</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> DELETE <code>/repositories/{repositoryId}</code> - List Branches <code>fs:ListBranches</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repositoryId}/branches</code> ListObjects/ListObjectsV2 (with delimiter = <code>/</code> and empty` prefix) Get Branch <code>fs:ReadBranch</code> <code>arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId}</code> GET <code>/repositories/{repositoryId}/branches/{branchId}</code> - Create Branch <code>fs:CreateBranch</code> <code>arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId}</code> POST <code>/repositories/{repositoryId}/branches</code> - Delete Branch <code>fs:DeleteBranch</code> <code>arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId}</code> DELETE <code>/repositories/{repositoryId}/branches/{branchId}</code> - Merge branches <code>fs:CreateCommit</code> <code>arn:lakefs:fs:::repository/{repositoryId}/branch/{destinationBranchId}</code> POST <code>/repositories/{repositoryId}/refs/{sourceBranchId}/merge/{destinationBranchId}</code> - Diff branch uncommitted changes <code>fs:ListObjects</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repositoryId}/branches/{branchId}/diff</code> - Diff refs <code>fs:ListObjects</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repositoryId}/refs/{leftRef}/diff/{rightRef}</code> - Stat object <code>fs:ReadObject</code> <code>arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey}</code> GET <code>/repositories/{repositoryId}/refs/{ref}/objects/stat</code> HeadObject Get Object <code>fs:ReadObject</code> <code>arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey}</code> GET <code>/repositories/{repositoryId}/refs/{ref}/objects</code> GetObject List Objects <code>fs:ListObjects</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repositoryId}/refs/{ref}/objects/ls</code> ListObjects, ListObjectsV2 (no delimiter, or \"/\" + non-empty` prefix) Upload Object <code>fs:WriteObject</code> <code>arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey}</code> POST <code>/repositories/{repositoryId}/branches/{branchId}/objects</code> PutObject, CreateMultipartUpload, UploadPart`, CompleteMultipartUpload Delete Object <code>fs:DeleteObject</code> <code>arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey}</code> DELETE <code>/repositories/{repositoryId}/branches/{branchId}/objects</code> DeleteObject, DeleteObjects`, AbortMultipartUpload Revert Branch <code>fs:RevertBranch</code> <code>arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId}</code> PUT <code>/repositories/{repositoryId}/branches/{branchId}</code> - Get Branch Protection Rules <code>branches:GetBranchProtectionRules</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repository}/branch_protection</code> - Set Branch Protection Rules <code>branches:SetBranchProtectionRules</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> POST <code>/repositories/{repository}/branch_protection</code> - Delete Branch Protection Rules <code>branches:SetBranchProtectionRules</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> DELETE <code>/repositories/{repository}/branch_protection</code> - Create User <code>auth:CreateUser</code> <code>arn:lakefs:auth:::user/{userId}</code> POST <code>/auth/users</code> - List Users <code>auth:ListUsers</code> <code>*</code> GET <code>/auth/users</code> - Get User <code>auth:ReadUser</code> <code>arn:lakefs:auth:::user/{userId}</code> GET <code>/auth/users/{userId}</code> - Delete User <code>auth:DeleteUser</code> <code>arn:lakefs:auth:::user/{userId}</code> DELETE <code>/auth/users/{userId}</code> - Get Group <code>auth:ReadGroup</code> <code>arn:lakefs:auth:::group/{groupId}</code> GET <code>/auth/groups/{groupId}</code> - List Groups <code>auth:ListGroups</code> <code>*</code> GET <code>/auth/groups</code> - Create Group <code>auth:CreateGroup</code> <code>arn:lakefs:auth:::group/{groupId}</code> POST <code>/auth/groups</code> - Delete Group <code>auth:DeleteGroup</code> <code>arn:lakefs:auth:::group/{groupId}</code> DELETE <code>/auth/groups/{groupId}</code> - List Policies <code>auth:ListPolicies</code> <code>*</code> GET <code>/auth/policies</code> - Create Policy <code>auth:CreatePolicy</code> <code>arn:lakefs:auth:::policy/{policyId}</code> POST <code>/auth/policies</code> - Update Policy <code>auth:UpdatePolicy</code> <code>arn:lakefs:auth:::policy/{policyId}</code> POST <code>/auth/policies</code> - Delete Policy <code>auth:DeletePolicy</code> <code>arn:lakefs:auth:::policy/{policyId}</code> DELETE <code>/auth/policies/{policyId}</code> - Get Policy <code>auth:ReadPolicy</code> <code>arn:lakefs:auth:::policy/{policyId}</code> GET <code>/auth/policies/{policyId}</code> - List Group Members <code>auth:ReadGroup</code> <code>arn:lakefs:auth:::group/{groupId}</code> GET <code>/auth/groups/{groupId}/members</code> - Add Group Member <code>auth:AddGroupMember</code> <code>arn:lakefs:auth:::group/{groupId}</code> PUT <code>/auth/groups/{groupId}/members/{userId}</code> - Remove Group Member <code>auth:RemoveGroupMember</code> <code>arn:lakefs:auth:::group/{groupId}</code> DELETE <code>/auth/groups/{groupId}/members/{userId}</code> - List User Credentials <code>auth:ListCredentials</code> <code>arn:lakefs:auth:::user/{userId}</code> GET <code>/auth/users/{userId}/credentials</code> - Create User Credentials <code>auth:CreateCredentials</code> <code>arn:lakefs:auth:::user/{userId}</code> POST <code>/auth/users/{userId}/credentials</code> - Delete User Credentials <code>auth:DeleteCredentials</code> <code>arn:lakefs:auth:::user/{userId}</code> DELETE <code>/auth/users/{userId}/credentials/{accessKeyId}</code> - Get User Credentials <code>auth:ReadCredentials</code> <code>arn:lakefs:auth:::user/{userId}</code> GET <code>/auth/users/{userId}/credentials/{accessKeyId}</code> - List User Groups <code>auth:ReadUser</code> <code>arn:lakefs:auth:::user/{userId}</code> GET <code>/auth/users/{userId}/groups</code> - List User Policies <code>auth:ReadUser</code> <code>arn:lakefs:auth:::user/{userId}</code> GET <code>/auth/users/{userId}/policies</code> - Attach Policy To User <code>auth:AttachPolicy</code> <code>arn:lakefs:auth:::user/{userId}</code> PUT <code>/auth/users/{userId}/policies/{policyId}</code> - Detach Policy From User <code>auth:DetachPolicy</code> <code>arn:lakefs:auth:::user/{userId}</code> DELETE <code>/auth/users/{userId}/policies/{policyId}</code> - List Group Policies <code>auth:ReadGroup</code> <code>arn:lakefs:auth:::group/{groupId}</code> GET <code>/auth/groups/{groupId}/policies</code> - Attach Policy To Group <code>auth:AttachPolicy</code> <code>arn:lakefs:auth:::group/{groupId}</code> PUT <code>/auth/groups/{groupId}/policies/{policyId}</code> - Detach Policy From Group <code>auth:DetachPolicy</code> <code>arn:lakefs:auth:::group/{groupId}</code> DELETE <code>/auth/groups/{groupId}/policies/{policyId}</code> - Attach External Principal to a User <code>auth:CreateUserExternalPrincipal</code> <code>arn:lakefs:auth:::user/{userId}</code> POST <code>/auth/users/{userId}/external/principals</code> - Delete External Principal Attachment from a User <code>auth:DeleteUserExternalPrincipal</code> <code>arn:lakefs:auth:::user/{userId}</code> DELETE <code>/auth/users/{userId}/external/principals</code> - Get the User attached to an External Principal <code>auth:ReadExternalPrincipal</code> <code>arn:lakefs:auth:::externalPrincipal/{principalId}</code> GET <code>/auth/external/principals</code> - Read Storage Config <code>fs:ReadConfig</code> <code>*</code> GET <code>/config/storage</code> - Get Garbage Collection Rules <code>retention:GetGarbageCollectionRules</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repositoryId}/gc/rules</code> - Set Garbage Collection Rules <code>retention:SetGarbageCollectionRules</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> POST <code>/repositories/{repositoryId}/gc/rules</code> - Prepare Garbage Collection Commits <code>retention:PrepareGarbageCollectionCommits</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> POST <code>/repositories/{repositoryId}/gc/prepare_commits</code> - List Repository Action Runs <code>ci:ReadAction</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repository}/actions/runs</code> - Get Action Run <code>ci:ReadAction</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repository}/actions/runs/{run_id}</code> - List Action Run Hooks <code>ci:ReadAction</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repository}/actions/runs/{run_id}/hooks</code> - Get Action Run Hook Output <code>ci:ReadAction</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repository}/actions/runs/{run_id}/hooks/{hook_run_id}/output</code> - Get Pull Request <code>pr:ReadPullRequest</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repository}/pulls/{pull_request}</code> - Create Pull Request <code>pr:WritePullRequest</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> POST <code>/repositories/{repository}/pulls</code> - Update Pull Request <code>pr:WritePullRequest</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> PATCH <code>/repositories/{repository}/pulls/{pull_request}</code> - Merge Pull Request <code>pr:WritePullRequest</code> + Merge Branches <code>arn:lakefs:fs:::repository/{repositoryId}</code> PUT <code>/repositories/{repository}/pulls/{pull_request}/merge</code> - List Pull Requests <code>pr:ListPullRequests</code> <code>arn:lakefs:fs:::repository/{repositoryId}</code> GET <code>/repositories/{repository}/pulls</code> - <p>Some APIs may require more than one action.For instance, in order to create a repository (<code>POST /repositories</code>), you need permission to <code>fs:CreateRepository</code> for the name of the repository and also <code>fs:AttachStorageNamespace</code> for the storage namespace used.</p>","boost":2},{"location":"security/rbac/#preconfigured-policies","title":"Preconfigured Policies","text":"<p>The following Policies are created during initial setup:</p>","boost":2},{"location":"security/rbac/#fsfullaccess","title":"FSFullAccess","text":"<pre><code>{\n  \"statement\": [\n    {\n      \"action\": [\n        \"fs:*\"\n      ],\n      \"effect\": \"allow\",\n      \"resource\": \"*\"\n    }\n  ]\n}\n</code></pre>","boost":2},{"location":"security/rbac/#fsreadall","title":"FSReadAll","text":"<pre><code>{\n  \"statement\": [\n    {\n      \"action\": [\n        \"fs:List*\",\n        \"fs:Read*\"\n      ],\n      \"effect\": \"allow\",\n      \"resource\": \"*\"\n    }\n  ]\n}\n</code></pre>","boost":2},{"location":"security/rbac/#fsreadwriteall","title":"FSReadWriteAll","text":"<pre><code>{\n    \"statement\": [\n        {\n            \"action\": [\n                \"fs:Read*\",\n                \"fs:List*\",\n                \"fs:WriteObject\",\n                \"fs:DeleteObject\",\n                \"fs:RevertBranch\",\n                \"fs:CreateBranch\",\n                \"fs:CreateTag\",\n                \"fs:DeleteBranch\",\n                \"fs:DeleteTag\",\n                \"fs:CreateCommit\"\n            ],\n            \"effect\": \"allow\",\n            \"resource\": \"*\"\n        }\n    ]\n}\n</code></pre>","boost":2},{"location":"security/rbac/#authfullaccess","title":"AuthFullAccess","text":"<pre><code>{\n  \"statement\": [\n    {\n      \"action\": [\n        \"auth:*\"\n      ],\n      \"effect\": \"allow\",\n      \"resource\": \"*\"\n    }\n  ]\n}\n</code></pre>","boost":2},{"location":"security/rbac/#authmanageowncredentials","title":"AuthManageOwnCredentials","text":"<pre><code>{\n  \"statement\": [\n    {\n      \"action\": [\n        \"auth:CreateCredentials\",\n        \"auth:DeleteCredentials\",\n        \"auth:ListCredentials\",\n        \"auth:ReadCredentials\"\n      ],\n      \"effect\": \"allow\",\n      \"resource\": \"arn:lakefs:auth:::user/${user}\"\n    }\n  ]\n}\n</code></pre>","boost":2},{"location":"security/rbac/#repomanagementfullaccess","title":"RepoManagementFullAccess","text":"<pre><code>{\n    \"statement\": [\n        {\n            \"action\": [\n                \"ci:*\"\n            ],\n            \"effect\": \"allow\",\n            \"resource\": \"*\"\n        },\n        {\n            \"action\": [\n                \"retention:*\"\n            ],\n            \"effect\": \"allow\",\n            \"resource\": \"*\"\n        }\n    ]\n}\n</code></pre>","boost":2},{"location":"security/rbac/#repomanagementreadall","title":"RepoManagementReadAll","text":"<pre><code>{\n    \"statement\": [\n        {\n            \"action\": [\n                \"ci:Read*\"\n            ],\n            \"effect\": \"allow\",\n            \"resource\": \"*\"\n        },\n        {\n            \"action\": [\n                \"retention:Get*\"\n            ],\n            \"effect\": \"allow\",\n            \"resource\": \"*\"\n        }\n    ]\n}\n</code></pre>","boost":2},{"location":"security/rbac/#additional-policies","title":"Additional Policies","text":"<p>You can create additional policies to further limit user access. Use the web UI or the lakectl auth command to create policies. Here is an example to define read/write access for a specific repository:</p> <pre><code>{\n    \"statement\": [\n        {\n            \"action\": [\n                \"fs:ReadRepository\",\n                \"fs:ReadCommit\",\n                \"fs:ListBranches\",\n                \"fs:ListTags\",\n                \"fs:ListObjects\"\n            ],\n            \"effect\": \"allow\",\n            \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;\"\n        },\n        {\n            \"action\": [\n                \"fs:RevertBranch\",\n                \"fs:ReadBranch\",\n                \"fs:CreateBranch\",\n                \"fs:DeleteBranch\",\n                \"fs:CreateCommit\"\n            ],\n            \"effect\": \"allow\",\n            \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;/branch/*\"\n        },\n                {\n            \"action\": [\n                \"fs:ListObjects\",\n                \"fs:ReadObject\",\n                \"fs:WriteObject\",\n                \"fs:DeleteObject\"\n            ],\n            \"effect\": \"allow\",\n            \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;/object/*\"\n        },\n                {\n            \"action\": [\n                \"fs:ReadTag\",\n                \"fs:CreateTag\",\n                \"fs:DeleteTag\"\n            ],\n            \"effect\": \"allow\",\n            \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;/tag/*\"\n        },\n        {\n            \"action\": [\"fs:ReadConfig\"],\n            \"effect\": \"allow\",\n            \"resource\": \"*\"\n        }\n    ]\n}\n</code></pre>","boost":2},{"location":"security/rbac/#multiple-resources-statements","title":"Multiple Resources Statements","text":"<p>lakeFS supports specifying multiple resources in a single RBAC statement. This is available on lakeFS Cloud, or on lakeFS Enterprise if starting at version lakeFS v1.54.0 and Fluffy v0.12.0 In addition to a single resource, the resource field can contain a string representing a JSON-encoded list of resources.</p> <p><pre><code>{\n    \"statement\": [\n        {\n            \"action\": [\n                \"fs:Read*\"\n            ],\n            \"effect\": \"allow\",\n            \"resource\": \"[\\\"arn:lakefs:fs:::repository/repo1\\\",\\\"arn:lakefs:fs:::repository/repo2\\\"]\"\n        }\n    ]\n}\n</code></pre> The list must be properly encoded as a JSON string: quote each resource, and escape those quotes as shown. Otherwise, the policy cannot be parsed.</p>","boost":2},{"location":"security/rbac/#multi-resource-policy-creation-using-python-sdk","title":"Multi-Resource Policy Creation Using Python SDK","text":"<p>Here is how you can leverage Python SDK to create a multiple resource policy:</p> <pre><code>import lakefs_sdk\nfrom lakefs_sdk.client import LakeFSClient\nfrom lakefs_sdk import models\n\nconfiguration = lakefs_sdk.Configuration(\n        host=lakefsEndPoint,\n        username=lakefsAccessKey,\n        password=lakefsSecretKey,\n)\nclt = LakeFSClient(configuration)\n\nclt.auth_api.create_policy(\n    policy=models.Policy(\n        id='FSReadTwoRepos',\n        statement=[models.Statement(\n            effect=\"deny\",\n            resource=json.dumps([\"arn:lakefs:fs:::repository/repo1\",\"arn:lakefs:fs:::repository/repo2\"]),\n            action=[\"fs:ReadRepository\"],\n        ),\n        ]\n    )\n)\n</code></pre>","boost":2},{"location":"security/rbac/#preconfigured-groups","title":"Preconfigured Groups","text":"<p>lakeFS has four preconfigured groups:</p> <ul> <li>Admins</li> <li>SuperUsers</li> <li>Developers</li> <li>Viewers</li> </ul> <p>They have the following policies granted to them:</p> Policy Admins SuperUsers Developers Viewers <code>FSFullAccess</code> \u2705 \u2705 <code>AuthFullAccess</code> \u2705 <code>RepoManagementFullAccess</code> \u2705 <code>AuthManageOwnCredentials</code> \u2705 \u2705 \u2705 <code>RepoManagementReadAll</code> \u2705 \u2705 <code>FSReadWriteAll</code> \u2705 <code>FSReadAll</code> \u2705","boost":2},{"location":"security/rbac/#pluggable-authentication-and-authorization","title":"Pluggable Authentication and Authorization","text":"<p>Authorization and authentication is pluggable in lakeFS. </p> <p>If lakeFS is attached to a remote authentication server (or you are using lakeFS Cloud) then the role-based access control user interface can be used. </p> <p>If you are using RBAC with your self-managed lakeFS then the lakeFS configuration element <code>auth.ui_config.rbac</code> should be set to <code>external</code>. </p> <p>An enterprise (paid) solution of lakeFS should set <code>auth.ui_config.rbac</code> as <code>internal</code>.</p>","boost":2},{"location":"security/remote-authenticator/","title":"Remote Authenticator","text":"<p>Remote Authenticator is a pluggable architecture for lakeFS which allows you to use existing organizational identity policies and infrastructure with the authentication mechanism of lakeFS. The Remote Authenticator's job is to abstract away the complexities of existing infrastructure and implement a standard interface, which lakeFS can use to resolve user identity and manage access to lakeFS. This loose coupling allows you to implement federated identity without providing lakeFS with direct access to your identity infrastructure.</p>"},{"location":"security/remote-authenticator/#architecture","title":"Architecture","text":"<p>Here's the authentication flow that lakeFS uses when configured with a remote authenticator:</p> <pre><code>sequenceDiagram\n    participant A as lakeFS Client\n    participant B as lakeFS Server\n    participant C as Remote Authenticator\n    participant D as IdP\n    A-&gt;&gt;B: Submit login form\n    B-&gt;&gt;C: POST user credentials\n    C-&gt;&gt;D: IdP request\n    D-&gt;&gt;C: IdP response\n    C-&gt;&gt;B: Auth response\n    B-&gt;&gt;A: auth JWT</code></pre>"},{"location":"security/remote-authenticator/#the-interface","title":"The Interface","text":"<p>To configure lakeFS to work with a Remote Authenticator add the following YAML to your lakeFS configuration:</p> <pre><code>auth:\n    remote_authenticator:\n        enabled: true\n        endpoint: &lt;url-to-remote-authenticator-endpoint&gt;\n        default_user_group: \"Developers\"\n    ui_config:\n        logout_url: /logout\n        login_cookie_names:\n            - internal_auth_session\n</code></pre> <ul> <li><code>auth.remote_authenticator.enabled</code> - set lakeFS to use the remote authenticator</li> <li><code>auth.remote_authenticator.endpoint</code> - an endpoint where the remote authenticator is able to receive a POST request from lakeFS</li> <li><code>auth.remote_authenticator.default_user_group</code> - the group assigned by default to new users</li> <li><code>auth.ui_config.logout_url</code> - the URL to redirect the browser when clicking the logout link in the user menu</li> <li><code>auth.ui_config.login_cookie_names</code> - the name of the cookie(s) lakeFS will set following a successful authentication. The value is the authenticated user's JWT</li> </ul> <p>A Remote Authenticator implementation should expose a single endpoint, which expects the following JSON request:</p> <pre><code>{\n    \"username\": \"testy.mctestface@example.com\",\n    \"password\": \"Password1\"\n}\n</code></pre> <p>and returns a JSON response like this:</p> <pre><code>{\n    \"external_user_identifier\": \"TestyMcTestface\"\n}\n</code></pre>"},{"location":"security/remote-authenticator/#example-request-responses","title":"Example Request &amp; Responses","text":""},{"location":"security/remote-authenticator/#request","title":"Request","text":"<pre><code>POST https://remote-authenticator.example.com/auth\nContent-Type: application/json\n\n{\n  \"username\": \"testy.mctestface@example.com\",\n  \"password\": \"Password1\"\n}\n</code></pre>"},{"location":"security/remote-authenticator/#successful-response","title":"Successful Response","text":"<pre><code>HTTP/1.1 200 OK\nContent-Type: application/json\n\n\n{\n  \"external_user_identifier\": \"TestyMcTestface\"\n}\n</code></pre>"},{"location":"security/remote-authenticator/#unauthorized-response","title":"Unauthorized Response","text":"<pre><code>HTTP/1.1 401 Unauthorized\nContent-Type: application/json\n\n{\n  \"external_user_identifier\": \"\"\n}\n</code></pre> <p>If the Remote Authenticator returns any HTTP status in the 2xx range, lakeFS considers this a successful authentication. Any HTTP status &lt; 200 or &gt; 300 is considered a failed authentication. If the Remote Authenticator returns a non-empty value for the <code>external_user_identifier</code> property along with a success HTTP status, lakeFS will show this identifier instead of an internal lakeFS user identifier in the UI.</p>"},{"location":"security/remote-authenticator/#sample-implementation","title":"Sample Implementation","text":"<p>Here is a sample Remote Authenticator implemented using node and express and written in TypeScript. This example implementation doesn't integrate with any real IdP but illustrates the expected request/response patterns that you need to implement.</p> <pre><code>import dotenv from \"dotenv\";\nimport express, { Express, Request, Response } from \"express\";\nimport { StatusCodes } from \"http-status-codes\";\n\ntype AuthRequestBody = {\n  username: string;\n  password: string;\n};\n\ntype AuthResponseBody = {\n  external_user_identifier: string;\n};\n\nconst DEFAULT_PORT = 80;\n\ndotenv.config();\n\nconst port = process.env.PORT || DEFAULT_PORT;\nconst app: Express = express();\n\napp.post(\n  \"/auth\",\n  (req: Request&lt;AuthResponseBody, {}, AuthRequestBody&gt;, res: Response) =&gt; {\n    const { username, password } = req.body;\n    if (!username?.length || !password?.length) {\n      return res.status(StatusCodes.BAD_REQUEST).json({\n        external_user_identifier: \"\",\n      });\n    }\n\n    // \ud83d\udc47\ud83c\udffb This is where you would implement your own authentication logic\n    if (\n      username === \"testy.mctestface@example.com\" &amp;&amp;\n      password === \"Password1\"\n    ) {\n      return res.status(StatusCodes.OK).json({\n        external_user_identifier: \"TestyMcTestface\",\n      });\n    } else {\n      return res.status(StatusCodes.UNAUTHORIZED).json({\n        external_user_identifier: \"\",\n      });\n    }\n  }\n);\n\napp.listen(port, () =&gt; {\n  console.log(`Remote Authenticator listening on port ${port}`);\n});\n</code></pre> <p>To run this service on the sub-domain <code>idp.example.com</code>, use a lakeFS configuration that looks like this:</p> <pre><code>auth:\n    remote_authenticator:\n        enabled: true\n        endpoint: https://idp.example.com/auth\n        default_user_group: \"Developers\"\n    ui_config:\n        logout_url: /logout\n        login_cookie_names:\n            - internal_auth_session\n</code></pre>"},{"location":"security/sso/","title":"Single Sign On (SSO)","text":"<p>Info</p> <p>Available in lakeFS Cloud and lakeFS Enterprise If you're using the open-source version of lakeFS you can read more about the authentication options available.</p>","boost":2},{"location":"security/sso/#sso-for-lakefs-cloud","title":"SSO for lakeFS Cloud","text":"<p>lakeFS Cloud uses Auth0 for authentication and thus support the same identity providers as Auth0 including Active Directory/LDAP, ADFS, Azure Active Directory Native, Google Workspace, OpenID Connect, Okta, PingFederate, SAML, and Azure Active Directory.</p> OktaActive Directory Federation Services (AD FS)Azure Active Directory (AD) <p>Note</p> <p>This guide is based on Okta's Create OIDC app integrations guide.</p> <p>Steps:</p> <ol> <li>Login to your Okta account</li> <li>Select Applications &gt; Applications, then Create App Integration.</li> <li>Select Create New App and enter the following:<ol> <li>For Sign-in method, choose OIDC.</li> <li>Under Application type, choose Web app.</li> <li>Select Next.</li> </ol> </li> <li>Under General Settings:<ol> <li>App integration name, enter a name for your application. (i.e lakeFS Cloud)</li> </ol> </li> <li>In the Sign-in redirect URIs field, enter https://lakefs-cloud.us.auth0.com/login (United States) or https://lakefs-cloud.eu.auth0.com/login (Europe).</li> <li>Under Sign-in redirect URIs, click Add URI, enter https://lakefs-cloud.us.auth0.com/login/callback (United States) or https://lakefs-cloud.eu.auth0.com/login/callback (Europe).</li> <li>Under Assignments, choose the wanted Controlled access. (i.e Allow everyone in your organization to access)</li> <li>Uncheck Enable immediate access with Federation Broker Mode.</li> <li>Select Save.</li> </ol> <p>Once you finish registering your application with Okta, save the Client ID, Client Secret and your Okta Domain, send this to Treeverse's team to finish the integration.</p> <p>Prerequisites:</p> <ul> <li>Client's AD FS server should be exposed publicly or to Auth0's IP ranges (either directly or using Web Application Proxy)</li> </ul> <p>Steps:</p> <ol> <li>Connect to the AD FS server</li> <li>Open AD FS' PowerShell CLI as Administrator through the server manager</li> <li> <p>Execute the following:     <pre><code>(new-object Net.WebClient -property @{Encoding = [Text.Encoding]::UTF8}).DownloadString(\"https://raw.github.com/auth0/adfs-auth0/master/adfs.ps1\") | iex\n\nAddRelyingParty \"urn:auth0:lakefs-cloud\" \"https://lakefs-cloud.us.auth0.com/login/callback\"\n</code></pre></p> <p>Note</p> <p>If your organization data is located in Europe, use <code>lakefs-cloud.eu.auth0.com</code> instead of <code>lakefs-cloud.us.auth0.com</code>.</p> </li> </ol> <p>Once you finish registering lakeFS Cloud with AD FS, save the AD FS URL and send this to Treeverse's team to finish the integration.</p> <p>Prerequisites:</p> <ul> <li>Azure account with permissions to manage applications in Azure Active Directory</li> </ul> <p>Note</p> <p>If you've already set up lakeFS Cloud with your Azure account, you can skip the Register lakeFS Cloud with Azure and Add client secret and go directly to Add a redirect URI.</p> <p>Register lakeFS Cloud with Azure</p> <p>Steps:</p> <ol> <li>Sign in to the Azure portal.</li> <li>If you have access to multiple tenants, use the Directories + subscriptions filter in the top menu to switch to the tenant in which you want to register the application.</li> <li>Search for and select Azure Active Directory.</li> <li>Under Manage, select App registrations &gt; New registration.</li> <li>Enter a display Name for your application. Users of your application might see the display name when they use the app, for example during sign-in. You can change the display name at any time and multiple app registrations can share the same name. The app registration's automatically generated Application (client) ID, not its display name, uniquely identifies your app within the identity platform.</li> <li> <p>Specify who can use the application, sometimes called its sign-in audience.</p> <p>Note</p> <p>don't enter anything for Redirect URI (optional). You'll configure a redirect URI in the next section.</p> </li> <li> <p>Select Register to complete the initial app registration.</p> </li> </ol> <p>When registration finishes, the Azure portal displays the app registration's Overview pane. You see the Application (client) ID. Also called the client ID, this value uniquely identifies your application in the Microsoft identity platform.</p> <p>Important: new app registrations are hidden to users by default. When you are ready for users to see the app on their My Apps page you can enable it. To enable the app, in the Azure portal navigate to Azure Active Directory &gt; Enterprise applications and select the app. Then on the Properties page toggle Visible to users? to Yes.</p> <p>Add a secret</p> <p>Sometimes called an application password, a client secret is a string value your app can use in place of a certificate to identity itself.</p> <p>Steps:</p> <ol> <li>In the Azure portal, in App registrations, select your application.</li> <li>Select Certificates &amp; secrets &gt; Client secrets &gt; New client secret.</li> <li>Add a description for your client secret.</li> <li>Select an expiration for the secret or specify a custom lifetime.<ol> <li>Client secret lifetime is limited to two years (24 months) or less. You can't specify a custom lifetime longer than 24 months.</li> <li>Microsoft recommends that you set an expiration value of less than 12 months.</li> </ol> </li> <li>Select Add.</li> <li>Record the secret's value for use in your client application code. This secret value is never displayed again after you leave this page.</li> </ol> <p>Add a redirect URI A redirect URI is the location where the Microsoft identity platform redirects a user's client and sends security tokens after authentication.</p> <p>You add and modify redirect URIs for your registered applications by configuring their platform settings.</p> <p>Enter https://lakefs-cloud.us.auth0.com/login/callback as your redirect URI.</p> <p>Settings for each application type, including redirect URIs, are configured in Platform configurations in the Azure portal. Some platforms, like Web and Single-page applications, require you to manually specify a redirect URI. For other platforms, like mobile and desktop, you can select from redirect URIs generated for you when you configure their other settings.</p> <p>Steps:</p> <ol> <li>In the Azure portal, in App registrations, select your application.</li> <li>Under Manage, select Authentication.</li> <li>Under Platform configurations, select Add a platform.</li> <li>Under Configure platforms, select the web option.</li> <li>Select Configure to complete the platform configuration.</li> </ol> <p>Once you finish registering lakeFS Cloud with Azure AD send the following items to the Treeverse's team:</p> <ol> <li>Client ID</li> <li>Client Secret</li> <li>Azure AD Domain</li> <li>Identity API Version (v1 for Azure AD or v2 for Microsoft Identity Platform/Entra) </li> </ol>","boost":2},{"location":"security/sso/#sso-for-lakefs-enterprise","title":"SSO for lakeFS Enterprise","text":"<p>Authentication in lakeFS Enterprise is handled by a secondary service which runs side-by-side with lakeFS. With a nod to Hogwarts and their security system, we've named this service Fluffy. Details for configuring the supported identity providers with Fluffy are shown below. In addition, please review the necessary Helm configuration to configure Fluffy.</p> <ul> <li>Active Directory Federation Services (AD FS) (using SAML)</li> <li>OpenID Connect</li> <li>LDAP</li> </ul> <p>If you're using an authentication provider that is not listed please contact us for further assistance.</p> Active Directory Federation Services (AD FS) (using SAML)OpenID ConnectLDAP <p>Note</p> <p>AD FS integration uses certificates to sign &amp; encrypt requests going out from Fluffy and decrypt incoming requests from AD FS server.</p> <p>In order for Fluffy to work, the following values must be configured. Update (or override) the following attributes in the chart's <code>values.yaml</code> file.</p> <ol> <li>Replace <code>fluffy.saml_rsa_public_cert</code> and <code>fluffy.saml_rsa_private_key</code> with real certificate values</li> <li>Replace <code>fluffyConfig.auth.saml.idp_metadata_url</code> with the metadata URL of the AD FS provider (e.g <code>adfs-auth.company.com</code>)</li> <li>Replace <code>fluffyConfig.auth.saml.external_user_id_claim_name</code> with the claim name representing user id name in AD FS</li> <li>Replace <code>lakefs.company.com</code> with your lakeFS server URL.</li> </ol> <p>If you'd like to generate the certificates using OpenSSL, you can take a look at the following example:</p> <pre><code>openssl req -x509 -newkey rsa:2048 -keyout myservice.key -out myservice.cert -days 365 -nodes -subj \"/CN=lakefs.company.com\" -\n</code></pre> <p>lakeFS Server Configuration (Update in helm's <code>values.yaml</code> file):</p> <pre><code>auth:\ncookie_auth_verification:\n    auth_source: saml\n    friendly_name_claim_name: displayName\n    persist_friendly_name: true\n    external_user_id_claim_name: samName\n    default_initial_groups:\n    - \"Developers\"\nlogout_redirect_url: \"https://lakefs.company.com/logout-saml\"\nencrypt:\n    secret_key: shared-secrey-key\nui_config:\n    login_url: \"https://lakefs.company.com/sso/login-saml\"\n    logout_url: \"https://lakefs.company.com/sso/logout-saml\"\n    login_cookie_names:\n    - internal_auth_session\n    - saml_auth_session\n</code></pre> <p>Fluffy Configuration (Update in helm's <code>values.yaml</code> file):</p> <pre><code>logging:\nformat: \"json\"\nlevel: \"INFO\"\naudit_log_level: \"INFO\"\noutput: \"=\"\nauth:  \nencrypt:\n    secret_key: shared-secrey-key    \nlogout_redirect_url: https://lakefs.company.com\npost_login_redirect_url: https://lakefs.company.com\nsaml:\n    enabled: true \n    sp_root_url: https://lakefs.company.com\n    sp_x509_key_path: '/etc/saml_certs/rsa_saml_private.cert'\n    sp_x509_cert_path: '/etc/saml_certs/rsa_saml_public.pem'\n    sp_sign_request: true\n    sp_signature_method: \"http://www.w3.org/2001/04/xmldsig-more#rsa-sha256\"\n    idp_metadata_url: \"https://adfs-auth.company.com/federationmetadata/2007-06/federationmetadata.xml\"\n    # idp_authn_name_id_format: \"urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified\"\n    external_user_id_claim_name: samName\n    # idp_metadata_file_path: \n    # idp_skip_verify_tls_cert: true\n</code></pre> <p>In order for Fluffy to work, the following values must be configured. Update (or override) the following attributes in the chart's <code>values.yaml</code> file.</p> <ol> <li>Replace <code>lakefsConfig.friendly_name_claim_name</code> with the right claim name.</li> <li>Replace <code>lakefsConfig.default_initial_groups</code> with desired claim name (See [pre-configured][rbac-preconfigured] groups for enterprise)</li> <li>Replace <code>fluffyConfig.auth.logout_redirect_url</code> with your full OIDC logout URL (e.g <code>https://oidc-provider-url.com/logout/path</code>)</li> <li>Replace <code>fluffyConfig.auth.oidc.url</code> with your OIDC provider URL (e.g <code>https://oidc-provider-url.com</code>)</li> <li>Replace <code>fluffyConfig.auth.oidc.logout_endpoint_query_parameters</code> with parameters you'd like to pass to the OIDC provider for logout.</li> <li>Replace <code>fluffyConfig.auth.oidc.client_id</code> and <code>fluffyConfig.auth.oidc.client_secret</code> with the client ID &amp; secret for OIDC.</li> <li>Replace <code>fluffyConfig.auth.oidc.logout_client_id_query_parameter</code> with the query parameter that represent the client_id, note that it should match the the key/query param that represents the client id and required by the specific OIDC provider.</li> <li>Replace <code>lakefs.company.com</code> with the lakeFS server URL.</li> </ol> <p>lakeFS Server Configuration (Update in helm's <code>values.yaml</code> file):</p> <pre><code># Important: make sure to include the rest of your lakeFS Configuration here!\nauth:\nencrypt:\n    secret_key: shared-secrey-key\noidc:\n    friendly_name_claim_name: \"name\"\n    persist_friendly_name: true\n    default_initial_groups: [\"Developers\"]\nui_config:\n    login_url: /oidc/login\n    logout_url: /oidc/logout\n    login_cookie_names:\n    - internal_auth_session\n    - oidc_auth_session\n</code></pre> <p>Fluffy Configuration (Update in helm's <code>values.yaml</code> file):</p> <pre><code>logging:\nformat: \"json\"\nlevel: \"INFO\"\naudit_log_level: \"INFO\"\noutput: \"=\"\ninstallation:\nfixed_id: fluffy-authenticator\nauth:\npost_login_redirect_url: /\nlogout_redirect_url: https://oidc-provider-url.com/logout/url\noidc:\n    enabled: true\n    url: https://oidc-provider-url.com/\n    client_id: &lt;oidc-client-id&gt;\n    client_secret: &lt;oidc-client-secret&gt;\n    callback_base_url: https://lakefs.company.com\n    is_default_login: true\n    logout_client_id_query_parameter: client_id\n    logout_endpoint_query_parameters:\n    - returnTo \n    - https://lakefs.company.com/oidc/login\nencrypt:\n    secret_key: shared-secrey-key\n</code></pre> <p>Fluffy is incharge of providing LDAP authentication for lakeFS Enterprise.  The authentication works by querying the LDAP server for user information and authenticating the user based on the provided credentials.</p> <p>Important: An administrative bind user must be configured. It should have search permissions for the LDAP server that will be used to query the LDAP server for user information.</p> <p>For Helm: set the following attributes in the Helm chart values, for lakeFS <code>lakefsConfig.*</code> and <code>fluffyConfig.*</code> for fluffy. </p> <p>No Helm: If not using Helm use the YAML below to directly update the configuration file for each service.</p> <p>lakeFS Configuration:</p> <ol> <li>Replace <code>auth.remote_authenticator.enabled</code> with <code>true</code></li> <li>Replace <code>auth.remote_authenticator.endpoint</code> with the fluffy authentication server URL combined with the <code>api/v1/ldap/login</code> suffix (e.g <code>http://lakefs.company.com/api/v1/ldap/login</code>)</li> </ol> <p>fluffy Configuration:</p> <p>See [Fluffy configuration][fluffy-configuration] reference.</p> <ol> <li>Replace <code>auth.ldap.remote_authenticator.server_endpoint</code> with your LDAP server endpoint  (e.g <code>ldaps://ldap.ldap-address.com:636</code>)</li> <li>Replace <code>auth.ldap.remote_authenticator.bind_dn</code> with the LDAP bind user/permissions to query your LDAP server.</li> <li>Replace <code>auth.ldap.remote_authenticator.user_base_dn</code> with the user base to search users in.</li> </ol> <p>lakeFS Server Configuration file:</p> <p><code>$lakefs run -c ./lakefs.yaml</code></p> <pre><code># Important: make sure to include the rest of your lakeFS Configuration here!\n\nauth:\nremote_authenticator:\n    enabled: true\n    endpoint: http://&lt;Fluffy URL&gt;:&lt;Fluffy http port&gt;/api/v1/ldap/login\n    default_user_group: \"Developers\" # Value needs to correspond with an existing group in lakeFS\nui_config:\n    logout_url: /logout\n    login_cookie_names:\n    - internal_auth_session\n</code></pre> <p>Fluffy Configuration file:</p> <p><code>$fluffy run -c ./fluffy.yaml</code></p> <pre><code>logging:\nformat: \"json\"\nlevel: \"INFO\"\naudit_log_level: \"INFO\"\noutput: \"=\"\ninstallation:\nfixed_id: fluffy-authenticator\nauth:\npost_login_redirect_url: /\nldap: \n    server_endpoint: 'ldaps://ldap.company.com:636'\n    bind_dn: uid=&lt;bind-user-name&gt;,ou=&lt;some-ou&gt;,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com\n    bind_password: '&lt;ldap pwd&gt;'\n    username_attribute: uid\n    user_base_dn: ou=&lt;some-ou&gt;,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com\n    user_filter: (objectClass=inetOrgPerson)\n    connection_timeout_seconds: 15\n    request_timeout_seconds: 7\n</code></pre> <p>Troubleshooting LDAP issues <p>Inspecting Logs</p> <p>If you encounter LDAP connection errors, you should inspect the fluffy container logs to get more information.</p> <p>Authentication issues</p> <p>Auth issues (e.g. user not found, invalid credentials) can be debugged with the <code>ldapwhoami</code> CLI tool. </p> <p>The Examples are based on the fluffy config above:</p> <p>To verify that the main bind user can connect:</p> <pre><code>ldapwhoami -H ldap://ldap.company.com:636 -D \"uid=&lt;bind-user-name&gt;,ou=&lt;some-ou&gt;,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com\" -x -W\n</code></pre> <p>To verify that a specific lakeFS user <code>dev-user</code> can connect:</p> <pre><code>ldapwhoami -H ldap://ldap.company.com:636 -D \"uid=dev-user,ou=&lt;some-ou&gt;,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com\" -x -W\n</code></pre> <p>User not found issue</p> <p>Upon a login request in fluffy, the bind user will search for the user in the LDAP server. If the user is not found it will be presented in the logs.</p> <p>We can search the user using ldapsearch CLI tool. </p> <p>Search ALL users in the base DN (no filters):</p> <p>Note</p> <p><code>-b</code> is the <code>user_base_dn</code>, <code>-D</code> is <code>bind_dn</code> and <code>-w</code> is <code>bind_password</code> from the fluffy configuration.</p> <pre><code>ldapsearch -H ldap://ldap.company.com:636 -x -b \"ou=&lt;some-ou&gt;,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com\" -D \"uid=&lt;bind-user-name&gt;,ou=&lt;some-ou&gt;,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com\" -w '&lt;bind_user_pwd&gt;'\n</code></pre> <p>If the user is found, we should now use filters for the specific user the same way fluffy does it and expect to see the user. </p> <p>For example, to repdocue the same search as fluffy does: - user <code>dev-user</code> set from <code>uid</code> attribute in LDAP  - Fluffy configuration values: <code>user_filter: (objectClass=inetOrgPerson)</code> and <code>username_attribute: uid</code></p> <pre><code>ldapsearch -H ldap://ldap.company.com:636 -x -b \"ou=&lt;some-ou&gt;,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com\" -D \"uid=&lt;bind-user-name&gt;,ou=&lt;some-ou&gt;,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com\" -w '&lt;bind_user_pwd&gt;' \"(&amp;(uid=dev-user)(objectClass=inetOrgPerson))\"\n</code></pre>","boost":2},{"location":"security/sso/#helm","title":"Helm <p>In order to use lakeFS Enterprise and Fluffy, we provided out of the box setup, see lakeFS Helm chart configuration.</p> <p>Notes:</p> <ul> <li>Check the examples on GitHub we provide for each authentication method (oidc/adfs/ldap + rbac).</li> <li>The examples are provisioned with a Postgres pod for quick-start, make sure to replace that to a stable database once ready.</li> <li>The encrypt secret key <code>secrets.authEncryptSecretKey</code> is shared between fluffy and lakeFS for authentication.</li> <li>The lakeFS <code>image.tag</code> must be &gt;= 1.0.0</li> <li>The fluffy <code>image.tag</code> must be &gt;= 0.2.7</li> <li>Change the <code>ingress.hosts[0]</code> from <code>lakefs.company.com</code> to a real host (usually same as lakeFS), also update additional references in the file (note: URL path after host if provided should stay unchanged).</li> <li>Update the <code>ingress</code> configuration with other optional fields if used</li> <li>Fluffy docker image: replace the <code>fluffy.image.privateRegistry.secretToken</code> with real token to dockerhub for the fluffy docker image.</li> </ul>","text":"","boost":2},{"location":"security/sts-login/","title":"Short-lived token (STS like)","text":"<p>Info</p> <p>Available in lakeFS Cloud and lakeFS Enterprise</p> <p>Secure Token Service (STS) authentication in lakeFS enables users to authenticate to lakeFS using temporary credentials obtained from an Identity Provider (IdP) via the OpenID Connect (OIDC) Authentication workflow. This document outlines the process of setting up the STS authentication flow and using the temporary credentials to interact with lakeFS through the high-level Python SDK.</p>","boost":2},{"location":"security/sts-login/#login","title":"Login","text":"<p>Initiate a client session with temporary credentials using the high-level Python SDK:</p> <pre><code>import lakefs\n\nmy_client = lakefs.client.from_web_identity(code = '&lt;CODE_FROM_IDP&gt;', state = '&lt;STATE_FROM_IDP&gt;' , redirect_uri = '&lt;URI_USED_FOR_REDIRECT_FROM_IDP&gt;', ttl_seconds = 7200)\n</code></pre>","boost":2},{"location":"security/sts-login/#setup","title":"Setup","text":"","boost":2},{"location":"security/sts-login/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have a way to generate the code, redirect_uri, and state values that are required to initiate a new client session using the STS login feature. For a reference implementation, see the sample implementation section.</p>","boost":2},{"location":"security/sts-login/#configuration","title":"Configuration","text":"<p>To enable STS authentication, configure your lakeFS instance with the endpoint of the external Authentication Service.</p> <pre><code>auth:\n    authentication_api:\n        endpoint: &lt;url-to-remote-authenticator-endpoint&gt;\n</code></pre> <p>The endpoint value should point to the external Authentication Service described at authentication.yml.   Make sure to replace  with the actual URL of your Authentication Service.","boost":2},{"location":"security/sts-login/#sample-implementation-to-generate-the-code-redirect_uri-and-state","title":"Sample implementation to generate the code, redirect_uri, and state","text":"<p>The following code snippet demonstrates how to generate the values that are required to initiate a new client session using the STS login feature.</p> <p>Info</p> <p>Replace <code>&lt;your-authorize-endpoint&gt;</code> with the path to your IdP authorize endpoint.  </p> <p>Examples</p> <ul> <li>Auth0: The authorize endpoint will be <code>https://&lt;your-auth0-domain&gt;/authorize</code> </li> <li>Entra ID: The authorize endpoint will be <code>https://&lt;your-entra-domain&gt;/oauth2/v2.0/authorize</code></li> </ul> <pre><code>import crypto from 'crypto';\n\nimport express from 'express';\nimport axios from 'axios';\nimport url from 'url';\nimport jsonwebtoken from 'jsonwebtoken';\n\nconst app = express();\n// the local script will will spin up the server and the IdP provider will return to this endpoint the response.\nconst callback = \"http://localhost:8080/oidc/callback\"\nconst authorizeEndpoint = \"&lt;your-authorize-endpoint&gt;\"\n\n// step 1 \n// Create a code_verifier, which is a cryptographically-random, Base64-encoded key that will eventually be sent to Auth0 to request tokens.\nfunction base64URLEncode(str) {\n    return str.toString('base64')\n        .replace(/\\+/g, '-')\n        .replace(/\\//g, '_')\n        .replace(/=/g, '');\n}\nvar verifier = base64URLEncode(crypto.randomBytes(32));\nconsole.log(`verifier: ${verifier}`);\n\n// step 2 \n// Generate a code_challenge from the code_verifier that will be sent to Auth0 to request an authorization_code.\nfunction sha256(buffer) {\n    return crypto.createHash('sha256').update(buffer).digest();\n}\n\nvar challenge = base64URLEncode(sha256(verifier));\nconsole.log(`challenge: ${challenge}`);\n\n\nconst authorizeURL = `${authorizeEndpoint}?response_type=code&amp;code_challenge=${challenge}&amp;code_challenge_method=S256&amp;client_id=${auth0ClientId}&amp;redirect_uri=${callback}&amp;scope=openid&amp;state=${verifier}`\n\nconsole.log(`authorizeURL: ${authorizeURL}`)\n\n// Endpoint for OIDC callback\napp.get('/oidc/callback', async (req, res) =&gt; {\n    try {\n        const code = req.query.code;\n        const state = req.query.state;\n        console.log(`code: ${code}`);\n        console.log(`state: ${state}`);\n        // Return a success response\n        res.status(200).json({ code, state, redirect_uri: callback, python-cmd: `lakefs.client.from_web_identity(code = ${code} redirect_uri = ${callback} state = ${state}, ttl_seconds = 7200) ` });\n        return\n    } catch (err) {\n        console.error(err);\n        res.status(500).json({ message: 'Internal server error' });\n    }\n});\n\n// Start the server\nconst PORT = 8080;\napp.listen(PORT, () =&gt; {\n    console.log(`Server is running on port ${PORT}`);\n});\n</code></pre>","boost":2},{"location":"security/sts-login/#architecture","title":"Architecture","text":"<p>The STS authentication flow involves several components that facilitate secure communication between the lakeFS client, lakeFS server, the remote authenticator, and the IdP.</p> <pre><code>sequenceDiagram\n    participant A as lakeFS Client\n    participant B as lakeFS Server\n    participant C as Remote Authenticator\n    participant D as IdP\n    A-&gt;&gt;B: Call STS login endpoint\n    B-&gt;&gt;C: POST idp code state and redirect uri\n    C-&gt;&gt;D: IdP request\n    D-&gt;&gt;C: IdP response\n    C-&gt;&gt;B: Auth response\n    B-&gt;&gt;A: auth JWT</code></pre> <ul> <li>lakeFS Client: Initiates the authentication process by providing IdP credentials.</li> <li>lakeFS Server: Facilitates the authentication request between the client and the remote authenticator.</li> <li>Remote Authenticator: Acts as a bridge between lakeFS and the IdP, handling credential validation.</li> <li>IdP (Identity Provider): Validates the provided credentials and returns the authentication status.</li> </ul>","boost":2},{"location":"understand/","title":"Understanding lakeFS","text":""},{"location":"understand/#architecture-and-internals","title":"Architecture and Internals","text":"<p>The Architecture page includes a logical overview of lakeFS and its components.</p> <p>For deep-dive content about lakeFS see:</p> <ul> <li>Internal database structure</li> <li>Merges in lakeFS</li> <li>Versioning Internals</li> </ul>"},{"location":"understand/#lakefs-use-cases","title":"lakeFS Use Cases","text":"<p>lakeFS has many uses in the data world, including</p> <ul> <li>CI/CD for Data Lakes</li> <li>ETL Testing Environment</li> <li>Reproducibility</li> <li>Rollback</li> </ul> <p>One of the important things that lakeFS provides is full support for Data Lifecycle Management through all stages:</p> <ul> <li>In Test</li> <li>During Deployment</li> <li>In Production</li> </ul>"},{"location":"understand/#lakefs-concepts-and-model","title":"lakeFS Concepts and Model","text":"<p>lakeFS adopts many of the terms and concepts from git. This page goes into details on the similarities and differences, and provides a good background to the concepts used in lakeFS.</p>"},{"location":"understand/#performance","title":"Performance","text":"<p>Check out the Performance best practices guide for useful hints and tips on ensuring high performance from lakeFS.</p>"},{"location":"understand/#faq-and-glossary","title":"FAQ and Glossary","text":"<p>The FAQ covers many common questions around lakeFS, and the glossary provides a useful reference for the terms used in lakeFS.</p>"},{"location":"understand/architecture/","title":"lakeFS Architecture","text":"<p>lakeFS is distributed as a single binary encapsulating several logical services.</p> <p>The server itself is stateless, meaning you can easily add more instances to handle a bigger load.</p> <p></p>"},{"location":"understand/architecture/#object-storage","title":"Object Storage","text":"<p>lakeFS manages data stored on various object storage platforms, including:</p> <ul> <li>AWS S3</li> <li>Google Cloud Storage</li> <li>Azure Blob Storage</li> <li>MinIO</li> <li>NetApp StorageGRID</li> <li>Ceph</li> <li>Any other S3-compatible storage</li> </ul> <p>With lakeFS Enterprise, you can leverage multiple storage backend support to manage data across multiple storage locations, including on-prem, hybrid, and multi-cloud environments.</p>"},{"location":"understand/architecture/#metadata-storage","title":"Metadata Storage","text":"<p>In additional a Key Value storage is used for storing metadata, with supported databases including PostgreSQL, DynamoDB, and CosmosDB Instructions of how to deploy such database on AWS can be found here.</p> <p>Additional information on the data format can be found in Versioning internals and Internal database structure</p>"},{"location":"understand/architecture/#load-balancing","title":"Load Balancing","text":"<p>Accessing lakeFS is done using HTTP. lakeFS exposes a frontend UI, an OpenAPI server, as well as an S3-compatible service (see S3 Gateway below). lakeFS uses a single port that serves all three endpoints, so for most use cases a single load balancer pointing to lakeFS server(s) would do.</p>"},{"location":"understand/architecture/#lakefs-components","title":"lakeFS Components","text":""},{"location":"understand/architecture/#s3-gateway","title":"S3 Gateway","text":"<p>The S3 Gateway is the layer in lakeFS responsible for the compatibility with S3. It implements a compatible subset of the S3 API to ensure most data systems can use lakeFS as a drop-in replacement for S3.</p> <p>See the S3 API Reference section for information on supported API operations.</p>"},{"location":"understand/architecture/#openapi-server","title":"OpenAPI Server","text":"<p>The Swagger (OpenAPI) server exposes the full set of lakeFS operations (see Reference). This includes basic CRUD operations against repositories and objects, as well as versioning related operations such as branching, merging, committing, and reverting changes to data.</p>"},{"location":"understand/architecture/#storage-adapter","title":"Storage Adapter","text":"<p>The Storage Adapter is an abstraction layer for communicating with any underlying object store. Its implementations allow compatibility with many types of underlying storage such as S3, GCS, Azure Blob Storage, or non-production usages such as the local storage adapter.</p> <p>See the roadmap for information on the future plans for storage compatibility.</p>"},{"location":"understand/architecture/#graveler","title":"Graveler","text":"<p>The Graveler handles lakeFS versioning by translating lakeFS addresses to the actual stored objects. To learn about the data model used to store lakeFS metadata, see the versioning internals page.</p>"},{"location":"understand/architecture/#authentication-authorization-service","title":"Authentication &amp; Authorization Service","text":"<p>The Auth service handles the creation, management, and validation of user credentials and RBAC policies.</p> <p>The credential scheme, along with the request signing logic, are compatible with AWS IAM (both SIGv2 and SIGv4).</p> <p>Currently, the Auth service manages its own database of users and credentials and doesn't use IAM in any way.</p>"},{"location":"understand/architecture/#hooks-engine","title":"Hooks Engine","text":"<p>The Hooks Engine enables CI/CD for data by triggering user defined Actions that will run during commit/merge.</p>"},{"location":"understand/architecture/#ui","title":"UI","text":"<p>The UI layer is a simple browser-based client that uses the OpenAPI server. It allows management, exploration, and data access to repositories, branches, commits and objects in the system.</p>"},{"location":"understand/architecture/#applications","title":"Applications","text":"<p>As a rule of thumb, lakeFS supports any S3-compatible application. This means that many common data applications work with lakeFS out-of-the-box. Check out our integrations to learn more.</p>"},{"location":"understand/architecture/#lakefs-clients","title":"lakeFS Clients","text":"<p>Some data applications benefit from deeper integrations with lakeFS to support different use cases or enhanced functionality provided by lakeFS clients.</p>"},{"location":"understand/architecture/#openapi-generated-sdks","title":"OpenAPI Generated SDKs","text":"<p>OpenAPI specification can be used to generate lakeFS clients for many programming languages. For example, the Python lakefs-sdk or the Java client are published with every new lakeFS release.</p>"},{"location":"understand/architecture/#lakectl","title":"lakectl","text":"<p>lakectl is a CLI tool that enables lakeFS operations using the lakeFS API from your preferred terminal.</p>"},{"location":"understand/architecture/#spark-metadata-client","title":"Spark Metadata Client","text":"<p>The lakeFS Spark Metadata Client makes it easy to perform operations related to lakeFS metadata, at scale. Examples include garbage collection or exporting data from lakeFS.</p>"},{"location":"understand/architecture/#lakefs-hadoop-filesystem","title":"lakeFS Hadoop FileSystem","text":"<p>Thanks to the S3 Gateway, it's possible to interact with lakeFS using Hadoop's S3AFIleSystem, but due to limitations of the S3 API, doing so requires reading and writing data objects through the lakeFS server. Using lakeFSFileSystem increases Spark ETL jobs performance by executing the metadata operations on the lakeFS server, and all data operations directly through the same underlying object store that lakeFS uses.</p>"},{"location":"understand/architecture/#how-lakefs-clients-and-gateway-handle-metadata-and-data-access","title":"How lakeFS Clients and Gateway Handle Metadata and Data Access","text":"<p>When using the Python client, lakeCTL, or the lakeFS Spark client, these clients communicate with the lakeFS server to retrieve metadata information. For example, they may query lakeFS to understand which version of a file is needed or to track changes in branches and commits. This communication does not include the actual data transfer, but instead involves passing only metadata about data locations and versions. Once the client knows the exact data location from the lakeFS metadata, it directly accesses the data in the underlying object storage (potentially using presigned URLs) without routing through lakeFS. For instance, if data is stored in S3, the Spark client will retrieve the S3 paths from lakeFS, then directly read and write to those paths in S3 without involving lakeFS in the data transfer.</p> <p></p>"},{"location":"understand/data-structure/","title":"How Does lakeFS Store Your Data","text":"<p>lakeFS being a data versioning engine, requires the ability to save multiple versions of the same object. As a result, lakeFS stores objects in the object store in way that allows it to version the data in an efficient way. This might cause confusion when trying to understand where our data is actually being stored. This page will try to shed a light on this subject.</p>"},{"location":"understand/data-structure/#lakefs-repository-namespace-structure","title":"lakeFS Repository Namespace Structure","text":"<p>lakeFS stores repository data and metadata under the repository's namespace. The lakeFS repository namespace is a dedicated path under the object store used by lakeFS to manage a repository. Listing a repository storage namespace in the object store will provide the following output:</p> <pre><code>aws s3 ls s3://&lt;storage_namespace&gt;/\n                        PRE _lakefs/\n                        PRE data/\n</code></pre> <p>lakeFS stores the actual user data under the <code>data/</code> prefix. The <code>_lakefs/</code> prefix is used to store commit metadata which includes range and meta-range files and internal lakeFS data. Since lakeFS manages immutable data, objects are not saved using their logical name - these might get overwritten, violating the immutability guarantee. This means that when you upload a csv file called <code>allstar_games_stats.csv</code> to branch main, lakeFS will generate a random physical address under the <code>data/</code> prefix and upload it to there. Mapping from a path to an object changes as you upload, commit, and merge on lakeFS. When updating an object, lakeFS will create a new physical address for that version preserving other versions of that object. lakeFS will link between the object's logical address and its physical address - and store that relation under the given commit metadata (range and meta-range)</p> <p>lakeFS uses its object store immutably i.e. anything uploaded is never changed or overridden (Refer to GC for explanation on how and when lakeFS actually deletes data from the storage). To find data, lakeFS uses the logical address e.g. <code>lakefs://my-repo/main/allstar_games_stats.csv</code>, indicating a repository and branch. Using the KV metadata store, lakeFS will first try to find any uncommitted version of the object in the given branch. If no uncommitted version exist, it will take the latest committed version from the branch head (which is the top commit of the branch)</p> <ol> <li>In the KV metadata store under the current staging token of branch main. This will return any uncommitted changes for the given object</li> <li>Read it from the branch's head meta-range and range (which are saved under the <code>_lakefs</code> prefix in the object store. This will return the metadata for the object as it was stored in the latest commit for branch main. The physical path returned will be in the form of <code>s3://&lt;storage_namespace&gt;/data/gp0n1l7d77pn0cke6jjg/cg6p50nd77pn0cke6jk0</code>. The same object in lakeFS might have several physical addresses, one for each version where it exists.</li> </ol>"},{"location":"understand/data-structure/#finding-an-objects-location-on-your-object-store","title":"Finding an object's location on your object store","text":"<p>One way to determine the physical location of an object is using the <code>lakectl fs stat</code> command:</p> <pre><code>lakectl fs stat --pre-sign=false lakefs://my-repo/main/allstar_games_stats.csv\nPath: allstar_games_stats.csv\nModified Time: 2024-08-02 10:13:33 -0400 EDT\nSize: 0 bytes\nHuman Size: 0 B\nPhysical Address: s3://niro-test/repos/docs/data/data/geh1jurck6tfom0s1t8g/cqmej33ck6tfom0s1tvg\nChecksum: d41d8cd98f00b204e9800998ecf8427e\nContent-Type: application/octet-stream\n</code></pre> <p>lakeFS can show any version of an object. For example: to see an object's physical location on branch <code>dev</code> from 3 versions ago, use reference dev~3:</p> <pre><code>lakectl fs stat lakefs://my-repo/dev~3/allstar_games_stats.csv\nPath: allstar_games_stats.csv\nModified Time: 2024-08-02 10:11:49 -0400 EDT\nSize: 916393 bytes\nHuman Size: 916.4 kB\nPhysical Address: s3://&lt;storage_namespace&gt;/data/data/geh1jurck6tfom0s1t8g/cqmei9bck6tfom0s1tt0\nChecksum: 48e04a4c072acdcf932ee6c43f46ef14\nContent-Type: application/octet-stream\n</code></pre> <p>This can be done using any lakeFS reference type.</p> <p>To learn more about the internals of lakeFS and how it stores your data, follow this blog post</p>"},{"location":"understand/faq/","title":"lakeFS Frequently Asked Questions (FAQ)","text":""},{"location":"understand/faq/#1-is-lakefs-open-source","title":"1. Is lakeFS open-source?","text":"<p>lakeFS is free, open-source, and licensed under the Apache 2.0 License. Code and issues are managed publicly on GitHub and a Slack channel for open discussions.</p>"},{"location":"understand/faq/#2-how-does-lakefs-data-versioning-work","title":"2. How does lakeFS data versioning work?","text":"<p>lakeFS uses zero-copy branching to avoid data duplication. That is, creating a new branch is a metadata-only operation: no objects are actually copied. Only when an object changes does lakeFS create another version of the data in the storage. </p> <p>Info</p> <p>For more information, see Versioning internals.</p>"},{"location":"understand/faq/#3-how-do-i-get-support-for-my-lakefs-installation","title":"3. How do I get support for my lakeFS installation?","text":"<p>We are extremely responsive on our Slack channel, and we make sure to prioritize the most pressing issues for the community. For SLA-based support, please contact us at support@treeverse.io.</p>"},{"location":"understand/faq/#4-do-you-collect-data-from-your-active-installations","title":"4. Do you collect data from your active installations?","text":"<p>We collect anonymous usage statistics to understand the patterns of use and to detect product gaps we may have so we can fix them. This is optional and may be turned off by setting <code>stats.enabled</code> to <code>false</code>. See the configuration reference for more details.</p> <p>The data we gather is limited to the following:</p> <ol> <li>A <code>UUID</code> which is generated when setting up lakeFS for the first time and contains no personal or otherwise identifiable information,</li> <li>The lakeFS version currently running,</li> <li>The OS and architecture lakeFS is running on,</li> <li>Metadata regarding the database used (version, installed extensions and parameters such as DB Timezone and work memory),</li> <li>Periodic aggregated action counters (e.g. how many \"get_object\" operations occurred).</li> </ol>"},{"location":"understand/faq/#5-how-is-lakefs-different-from-delta-lake-hudi-iceberg","title":"5. How is lakeFS different from Delta Lake / Hudi / Iceberg?","text":"<p>Delta Lake, Apache Hudi, and Apache Iceberg all define dedicated, structured data formats that allow deletes and upserts. lakeFS is format-agnostic and enables consistent cross-collection versioning of your data using Git-like operations. Read our comparison for a more detailed comparison. </p>"},{"location":"understand/faq/#6-what-inspired-the-lakefs-logo","title":"6. What inspired the lakeFS logo?","text":"<p>The Axolotl \u2013 a species of salamander, also known as the Mexican Lake Monster or the Peter Pan of the animal kingdom. It's a magical creature, living in a lake - just like us! :)</p> <p></p> <p> copyright </p>"},{"location":"understand/glossary/","title":"Glossary","text":"<p>This page has definition and explanations of all terms related to lakeFS technical internals and the architecture.</p>"},{"location":"understand/glossary/#auditing","title":"Auditing","text":"<p>Data auditing is data assessment to ensure its accuracy, security, and efficacy for specific usage. It also involves assessing data quality through its lifecycle and understanding the impact of poor quality data on the organization's performance and revenue. Ensuring data reproducibility, auditability, and governance is one of the key concerns of data engineers today. lakeFS commit history helps the data teams to keep track of all changes to the data, supporting data auditing.</p>"},{"location":"understand/glossary/#branch","title":"Branch","text":"<p>Branches in lakeFS allow users to create their own \"isolated\" view of the repository. Read more.</p>"},{"location":"understand/glossary/#collection","title":"Collection","text":"<p>A collection, roughly speaking, is a set of data. Collections may be structured or unstructured; a structured collection is often referred to as a table.</p>"},{"location":"understand/glossary/#commit","title":"Commit","text":"<p>Using commits, you can view a repository at a certain point in its history and you're guaranteed that the data you see is exactly as it was at the point of committing it. Read More.</p>"},{"location":"understand/glossary/#cross-collection-consistency","title":"Cross-Collection Consistency","text":"<p>It is unfortunate that the word 'consistency' has multiple meanings, at least four of them according to Martin Kleppmann. Consistency in the context of lakeFS and data versioning is, the guarantee that operations in a transaction are performed accurately, correctly and most important, atomically. </p> <p>A repository (and thus a branch) in lakeFS, can span multiple tables or collections. By providing branch, commit, merge and revert operations atomically on a branch, lakeFS achieves consistency guarantees across different logical collections. That is, data versioning is consistent across multiple collections within a repository.</p> <p>It is sometimes referred as multi-table transactions. That is, lakeFS offers transactional guarantees across multiple tables.</p>"},{"location":"understand/glossary/#data-lake-governance","title":"Data Lake Governance","text":"<p>The goal of data lake governance is to apply policies, standards and processes on the data. This allows creating high-quality data and ensuring that it\u2019s used appropriately across the organization. Data lake governance improves the data quality and increases data usage for business decision-making, leading to operational improvements, better-informed business strategies, and stronger financial performance. lakeFS Cloud offers advanced data lake management features such as: Role-Based Access Control, Branch Aware Managed Garbage Collection, Data Lineage and Audit log.</p>"},{"location":"understand/glossary/#data-lifecycle-management","title":"Data Lifecycle Management","text":"<p>In data-intensive applications, data should be managed through its entire lifecycle similar to how teams manage code. By doing so, we could leverage the best practices and tools from application lifecycle management (like CI/CD operations) and apply them to data. lakeFS offers data lifecycle management via isolated data development environments instead of shared buckets.</p>"},{"location":"understand/glossary/#data-pipeline-reproducibility","title":"Data Pipeline Reproducibility","text":"<p>Reproducibility in data pipelines is the ability to repeat a process. An example of this is recreating an issue that occurred in the production pipeline. Reproducibility allows for the controlled manufacture of an error to debug and troubleshoot it at a later point in time. Reproducing a data pipeline issue is a challenge that most data engineers face on a daily basis. Learn more about how lakeFS supports data pipeline reproducibility. Other use cases include running ad-hoc queries (useful for data science), review, and backfill.</p>"},{"location":"understand/glossary/#data-quality-testing","title":"Data Quality Testing","text":"<p>This term describes ways to test data for its accuracy, completeness, consistency, timeliness, validity, and integrity. lakeFS hooks can be used to implement and run data quality tests before promoting staging data into production. </p>"},{"location":"understand/glossary/#data-versioning","title":"Data Versioning","text":"<p>To version data means creating a unique point-in-time reference for data that can be accessed later. This reference can take the form of a query, an ID, or also commonly, a DateTime identifier. Data versioning may also include saving an entire copy of the data under a new name or file path every time you want to create a version of it. More advanced versioning solutions like lakeFS perform versioning through zero-copy data operations. lakeFS also optimizes storage usage between versions and exposes special operations to manage them.</p>"},{"location":"understand/glossary/#git-like-operations","title":"Git-like Operations","text":"<p>lakeFS allows teams to treat their data lake as a Git repository.   Git is used for code versioning, whereas lakeFS is used for data versioning.  lakeFS provides Git-like operations such as branch, commit, merge and revert.</p>"},{"location":"understand/glossary/#graveler","title":"Graveler","text":"<p>Graveler is the core versioning engine of lakeFS. It handles versioning by translating lakeFS addresses to the actual stored objects. See the versioning internals section to learn how lakeFS stores metadata.</p>"},{"location":"understand/glossary/#hooks","title":"Hooks","text":"<p>lakeFS hooks allow you to automate and ensure that a given set of checks and validations happens before important lifecycle events. They are similar conceptually to Git Hooks, but in contrast, they run remotely on a server. Currently, lakeFS allows executing hooks when two types of events occur: pre-commit events that run before a commit is acknowledged and pre-merge events that trigger right before a merge operation. </p>"},{"location":"understand/glossary/#isolated-data-snapshot","title":"Isolated Data Snapshot","text":"<p>Creating a branch in lakeFS provides an isolated environment containing a snapshot of your repository. While working on your branch in isolation, all other data users will be looking at the repository's main branch. So they won't see your changes, and you also won't see the changes applied to the main branch. All of this happens without any data duplication but metadata management.</p>"},{"location":"understand/glossary/#main-branch","title":"Main Branch","text":"<p>Every Git repository has the main branch (unless you take explicit steps to remove it) and it plays a key role in the software development process. In most projects, it represents the source of truth - all the code that works has been tested and is ready to be pushed to production. Similarly, main branch in lakeFS could be used as the single source of truth. For example, the live production data can be on the main branch. </p>"},{"location":"understand/glossary/#metadata-management","title":"Metadata Management","text":"<p>Where there is data, there is also metadata. lakeFS uses metadata to define schema, data types, data versions, relations to other datasets, etc. This helps to improve discoverability and manageability. lakeFS performs data versioning through metadata operations. </p>"},{"location":"understand/glossary/#merge","title":"Merge","text":"<p>lakeFS merge command, similar to the Git merge functionality, allows you to merge data branches. Once you commit data, you can review it and then merge the committed data into the target branch. A merge generates a commit on the target branch with all your changes. lakeFS guarantees atomic merges that are fast, given they don\u2019t involve copying data. Read More.</p>"},{"location":"understand/glossary/#repository","title":"Repository","text":"<p>In lakeFS, a repository is a set of related objects (or collections of objects). Read More.</p>"},{"location":"understand/glossary/#rollback","title":"Rollback","text":"<p>A rollback is an atomic operation reversing the effects of a previous commit. If a developer introduces a new code version to production and discovers that it has a critical bug, they can simply roll back to the previous version. In lakeFS, a rollback is an atomic action that prevents the data consumers from receiving low-quality data until the issue is resolved. Learn more about how lakeFS supports the rollback operation.</p>"},{"location":"understand/glossary/#storage-namespace","title":"Storage Namespace","text":"<p>The storage namespace is a location in the underlying storage dedicated to a specific repository. lakeFS uses it to store the repository's objects and some of its metadata.</p>"},{"location":"understand/glossary/#underlying-storage","title":"Underlying Storage","text":"<p>The underlying storage is a location in some object store where lakeFS keeps your objects and some metadata.</p>"},{"location":"understand/glossary/#tag","title":"Tag","text":"<p>Tags are a way to give a meaningful name to a specific commit. Read More.</p>"},{"location":"understand/glossary/#fluffy","title":"Fluffy","text":"<p>lakeFS Enterprise Single-Sign-On service, it's delegated with lakeFS authentication requests and replies back to lakeFS with the authentication response.</p>"},{"location":"understand/model/","title":"lakeFS Concepts and Model","text":"<p>lakeFS blends concepts from object stores such as S3 with concepts from Git. This reference defines the common concepts of lakeFS.</p>"},{"location":"understand/model/#objects","title":"Objects","text":"<p>lakeFS is an interface to manage objects in an object store.</p> <p>Tip</p> <p>The actual data itself is not stored inside lakeFS directly but in an underlying object store. lakeFS manages pointers and additional metadata about these objects.</p>"},{"location":"understand/model/#version-control","title":"Version Control","text":"<p>lakeFS is spearheading version control semantics for data. Most of these concepts will be familiar to Git users:</p>"},{"location":"understand/model/#repository","title":"Repository","text":"<p>In lakeFS, a repository is a set of related objects (or collections of objects). In many cases, these represent tables of various formats for tabular data, semi-structured data such as JSON or log files - or a set of unstructured objects such as images, videos, sensor data, etc.</p> <p>lakeFS represents repositories as a logical namespace used to group together objects, branches, and commits - analogous to a repository in Git.</p> <p>lakeFS repository naming requirements are as follows:</p> <ul> <li>Start with a lower case letter or number</li> <li>Contain only lower case letters, numbers and hyphens</li> <li>Be between 3 and 63 characters long</li> </ul>"},{"location":"understand/model/#commits","title":"Commits","text":"<p>Using commits, you can view a repository at a certain point in its history and you're guaranteed that the data you see is exactly as it was at the point of committing it.</p> <p>These commits are immutable \"checkpoints\" containing all contents of a repository at a given point in the repository's history.</p> <p>Each commit contains metadata - the committer, timestamp, a commit message, as well as arbitrary key/value pairs you can choose to add.</p> <p>Identifying Commits</p> <p>A commit is identified by its commit ID, a digest of all contents of the commit.  Commit IDs are by nature long, so you may use a unique prefix to abbreviate them. A commit may also be identified by using a textual definition, called a ref.  Examples of refs include tags, branch names, and expressions.</p>"},{"location":"understand/model/#branches","title":"Branches","text":"<p>Branches in lakeFS allow users to create their own \"isolated\" view of the repository.</p> <p>Changes on one branch do not appear on other branches. Users can take changes from one branch and apply it to another by merging them.</p>"},{"location":"understand/model/#zero-copy-branching","title":"Zero-copy branching","text":"<p>Under the hood, branches are simply a pointer to a commit along with a set of uncommitted changes. Creating a branch is a zero-copy operation; instead of duplicating data, it involves creating a pointer to the source commit for the branch.</p>"},{"location":"understand/model/#tags","title":"Tags","text":"<p>Tags are a way to give a meaningful name to a specific commit. Using tags allow users to reference specific releases, experiments, or versions by using a human friendly name.</p> <p>Example tags:</p> <ul> <li><code>v2.3</code> to mark a release.</li> <li><code>dev-jane-before-v2.3-merge</code> to mark Jane's private temporary point.</li> </ul> <p>Tag names adhere to the same rules as git ref names.</p>"},{"location":"understand/model/#history","title":"History","text":"<p>The history of the branch is the list of commits from the branch tip through the first parent of each commit. Histories go back in time.</p>"},{"location":"understand/model/#merge","title":"Merge","text":"<p>Merging is the way to integrate changes from a branch into another branch. The result of a merge is a new commit, with the destination as the first parent and the source as the second.</p> <p>Info</p> <p>To learn more about how merging works in lakeFS, see the merge reference</p>"},{"location":"understand/model/#ref-expressions","title":"Ref expressions","text":"<p>lakeFS also supports expressions for creating a ref. These are similar to revisions in Git; indeed all <code>~</code> and <code>^</code> examples at the end of that section will work unchanged in lakeFS.</p> <ul> <li>A branch or a tag are ref expressions.</li> <li>If <code>&lt;ref&gt;</code> is a ref expression, then:</li> <li><code>&lt;ref&gt;^</code> is a ref expression referring to its first parent.</li> <li><code>&lt;ref&gt;^N</code> is a ref expression referring to its N'th parent; in particular <code>&lt;ref&gt;^1</code> is the     same as <code>&lt;ref&gt;^</code>.</li> <li><code>&lt;ref&gt;~</code> is a ref expression referring to its first parent; in particular <code>&lt;ref&gt;~</code> is the     same as <code>&lt;ref&gt;^</code> and <code>&lt;ref&gt;~</code>.</li> <li><code>&lt;ref&gt;~N</code> is a ref expression referring to its N'th parent, always traversing to the first     parent.  So <code>&lt;ref&gt;~N</code> is the same as <code>&lt;ref&gt;^^...^</code> with N consecutive carets <code>^</code>.</li> </ul>"},{"location":"understand/model/#concepts-unique-to-lakefs","title":"Concepts unique to lakeFS","text":"<p>The underlying storage is a location in an object store where lakeFS keeps your objects and some immutable metadata.</p> <p>When creating a lakeFS repository, you assign it with a storage namespace. The repository's storage namespace is a location in the underlying storage where data for this repository will be stored.</p> <p>We sometimes refer to underlying storage as physical. The path used to store the contents of an object is then termed a physical path. Once lakeFS saves an object in the underlying storage it is never modified, except to remove it entirely during some cleanups.</p> <p>A lot of what lakeFS does is to manage how lakeFS paths translate to physical paths on the object store. This mapping is generally not straightforward. Importantly (and contrary to many object stores), lakeFS may map multiple paths to the same object on backing storage, and always does this for objects that are unchanged across versions.</p>"},{"location":"understand/model/#lakefs-protocol-uris","title":"<code>lakefs</code> protocol URIs","text":"<p>lakeFS uses a specific format for path URIs. The URI <code>lakefs://&lt;REPO&gt;/&lt;REF&gt;/&lt;KEY&gt;</code> is a path to objects in the given repo and ref expression under key. </p> <p>This is used both for path prefixes and for full paths.  In similar fashion, <code>lakefs://&lt;REPO&gt;/&lt;REF&gt;</code> identifies the repository at a ref expression, and <code>lakefs://&lt;REPO&gt;</code> identifies a repo.</p>"},{"location":"understand/performance-best-practices/","title":"Performance Best Practices","text":""},{"location":"understand/performance-best-practices/#overview","title":"Overview","text":"<p>Use this guide to achieve the best performance with lakeFS.</p>"},{"location":"understand/performance-best-practices/#avoid-concurrent-commitsmerges","title":"Avoid concurrent commits/merges","text":"<p>Just like in Git, branch history is composed by commits and is linear by nature. Concurrent commits/merges on the same branch result in a race. The first operation will finish successfully while the rest will retry.</p>"},{"location":"understand/performance-best-practices/#perform-meaningful-commits","title":"Perform meaningful commits","text":"<p>It's a good idea to perform commits that are meaningful in the senese that they represent a logical point in your data's lifecycle. While lakeFS supports arbirartily large commits, avoiding commits with a huge number of objects will result in a more comprehensible commit history.</p>"},{"location":"understand/performance-best-practices/#use-zero-copy-import","title":"Use zero-copy import","text":"<p>To import object into lakeFS, either a single time or regularly, lakeFS offers a [zero-copy import][use-zero-copy-import] feature. Use this feature to import a large number of objects to lakeFS, instead of simply copying them into your repository. This feature will create a reference to the existing objects on your bucket and avoids the copy.</p>"},{"location":"understand/performance-best-practices/#read-data-using-the-commit-id","title":"Read data using the commit ID","text":"<p>In cases where you are only interested in reading committed data:</p> <ul> <li>Use a commit ID (or a tag ID) in your path (e.g: <code>lakefs://repo/a1b2c3</code>).</li> <li>Add <code>@</code> before the path  <code>lakefs://repo/main@/path</code>.</li> </ul> <p>When accessing data using the branch name (e.g. <code>lakefs://repo/main/path</code>) lakeFS will also try to fetch uncommitted data, which may result in reduced performance. For more information, see how uncommitted data is managed in lakeFS.</p>"},{"location":"understand/performance-best-practices/#operate-directly-on-the-storage","title":"Operate directly on the storage","text":"<p>Sometimes, storage operations can become a bottleneck. For example, when your data pipelines upload many big objects. In such cases, it can be beneficial to perform only versioning operations on lakeFS, while performing storage reads/writes directly on the object store. lakeFS offers multiple ways to do that:</p> <ul> <li>The <code>lakectl fs upload --pre-sign</code> command (or download).</li> <li>The lakeFS Hadoop Filesystem.</li> <li>The staging API which can be used to add lakeFS references to objects after having written them to the storage.</li> </ul> <p>Accessing the object store directly is a faster way to interact with your data.</p>"},{"location":"understand/performance-best-practices/#zero-copy","title":"Zero-copy","text":"<p>lakeFS provides a zero-copy mechanism to data. Instead of copying the data, we can check out to a new branch. Creating a new branch will take constant time as the new branch points to the same data as its parent. It will also lower the storage cost.</p>"},{"location":"understand/data_lifecycle_management/","title":"Data Lifecycle Management in lakeFS","text":"<p>lakeFS provides full support for Data Lifecycle Management through all stages:</p> <ul> <li>In Test</li> <li>During Deployment</li> <li>In Production</li> </ul>"},{"location":"understand/data_lifecycle_management/ci/","title":"During Deployment","text":""},{"location":"understand/data_lifecycle_management/ci/#during-deployment","title":"During Deployment","text":"<p>Every day we introduce new data to the lake. And even if the code and infra doesn't change, the data might, and those changes introduce potential quality issues. This is one of the complexities of a data product; the data we consume changes over the course of a month, a week, day, hour, or even minute-to-minute.</p> <p>Examples of changes to data that may occur:  - A client-side bug in the data collection of website events  - A new Android version that interferes with the collecting events from your App  - COVID-19 abrupt impact on consumers' behavior, and its effect on the accuracy of ML models.  - During a change to Salesforce interface, the validation requirement from a certain field had been lost</p> <p>lakeFS enable CI/CD-inspired workflows to help validate expectations and assumptions about the data before it goes live in production or lands in the data environment.</p>"},{"location":"understand/data_lifecycle_management/ci/#example-1-data-update-safety","title":"Example 1: Data update safety","text":"<p>Continuous deployment of existing data we expect to consume, flowing from ingest-pipelines into the lake. We merge data from an ingest branch (\u201cevents-data\u201d), which allows us to create tests using data analysis tools or data quality services (e.g. Great Expectations, Monte Carlo) to ensure reliability of the data we merge to the main branch. Since merge is atomic, no performance issue will be introduced by using lakeFS, but your main branch will only include quality data. </p> <p></p> <p>Each merge to the main branch creates a new commit on the main branch, which serves as a new version of the data. This allows us to easily revert to previous states of the data if a newer change introduces data issues.</p>"},{"location":"understand/data_lifecycle_management/ci/#example-2-test-validate-new-data","title":"Example 2: Test - Validate new data","text":"<p>Examples of common validation checks enforced in organizations:  </p> <ul> <li>No user_* columns except under /private/...</li> <li>Only <code>(*.parquet | *.orc | _delta_log/*.json)</code> files allowed</li> <li>Under /production, only backward-compatible schema changes are allowed</li> <li>New tables on main must be registered in our metadata repository first, with owner and SLA</li> </ul> <p>lakeFS will assist in enforcing best practices by giving you a designated branch to ingest new data (\u201cnew-data-1\u201d in the drawing). . You may run automated tests to validate predefined best practices as pre-merge hooks. If the validation passes, the new data will be automatically and atomically merged to the main branch. However, if the validation fails, you will be alerted and the new data will not be exposed to consumers.</p> <p>By using this branching model and implementing best practices as pre merge hooks, you ensure the main lake is never compromised.</p> <p></p>"},{"location":"understand/data_lifecycle_management/data-devenv/","title":"In Test","text":""},{"location":"understand/data_lifecycle_management/data-devenv/#in-test","title":"In Test","text":"<p>As part of our routine work with data we develop new code, improve and upgrade old code, upgrade infrastructures, and test new technologies. lakeFS enables a safe test environment on your data lake without the need to copy or mock data, work on the pipelines or involve DevOps.</p> <p>Creating a branch provides you an isolated environment with a snapshot of your repository (any part of your data lake you chose to manage on lakeFS). While working on your own branch in isolation, all other data users will be looking at the repository\u2019s main branch. They can't see your changes, and you don\u2019t see changes to main done after you created the branch. </p> <p>No worries, no data duplication is done, it\u2019s all metadata management behind the scenes. Let\u2019s look at 2 examples of a test environment and their branching models.</p>"},{"location":"understand/data_lifecycle_management/data-devenv/#example-1-upgrading-spark-and-using-reset-action","title":"Example 1: Upgrading Spark and using Reset action","text":"<p>You installed the latest version of Apache Spark. As a first step you\u2019ll test your Spark jobs to see that the upgrade doesn't have any undesired side effects.</p> <p>For this purpose, you may create a branch (testing-spark-3.0) which will only be used to test the Spark upgrade, and discarded later. Jobs may run smoothly (the theoretical possibility exists!), or they may fail halfway through, leaving you with some intermediate partitions, data and metadata. In this case, you can simply reset the branch to its original state, without worrying about the intermediate results of your last experiment, and perform another (hopefully successful) test in an isolated branch. Reset actions are atomic and immediate, so no manual cleanup is required.</p> <p>Once testing is completed, and you have achieved the desired result, you can delete this experimental branch, and all data not used on any other branch will be deleted with it.</p> <p></p> <p>Creating a testing branch:</p> <pre><code>lakectl branch create \\\n   lakefs://example-repo/testing-spark-3 \\\n   --source lakefs://example-repo/main\n# output:\n# created branch 'testing-spark-3'\n</code></pre> <p>Resetting changes to a branch:</p> <pre><code>lakectl branch reset lakefs://example-repo/testing-spark-3\n# are you sure you want to reset all uncommitted changes?: y\u2588\n</code></pre> <p>Note lakeFS version &lt;= v0.33.1 uses '@' (instead of '/') as separator between repository and branch.</p>"},{"location":"understand/data_lifecycle_management/data-devenv/#example-2-collaborate-compare-which-option-is-better","title":"Example 2: Collaborate &amp; Compare - Which option is better?","text":"<p>Easily compare by testing which one performs better on your data set.  Examples may be: * Different computation tools, e.g Spark vs. Presto * Different compression algorithms * Different Spark configurations * Different code versions of an ETL</p> <p>Run each experiment on its own independent branch, while the main remains untouched. Once both experiments are done, create a comparison query (using Hive or Presto or any other tool of your choice) to compare data characteristics, performance or any other metric you see fit.</p> <p>With lakeFS you don't need to worry about creating data paths for the experiments, copying data, and remembering to delete it. It\u2019s substantially easier to avoid errors and maintain a clean lake after.</p> <p></p> <p>Reading from and comparing branches using Spark:</p> <pre><code>val dfExperiment1 = sc.read.parquet(\"s3a://example-repo/experiment-1/events/by-date\")\nval dfExperiment2 = sc.read.parquet(\"s3a://example-repo/experiment-2/events/by-date\")\n\ndfExperiment1.groupBy(\"...\").count()\ndfExperiment2.groupBy(\"...\").count() // now we can compare the properties of the data itself\n</code></pre>"},{"location":"understand/data_lifecycle_management/production/","title":"In Production","text":""},{"location":"understand/data_lifecycle_management/production/#in-production","title":"In Production","text":"<p>Errors with data in production inevitably occur. When they do, they best thing we can do is remove the erroneous data, understand why the issue happened, and deploy changes that prevent it from occurring again.</p>"},{"location":"understand/data_lifecycle_management/production/#example-1-rollback-data-ingested-from-a-kafka-stream","title":"Example 1: RollBack! - Data ingested from a Kafka stream","text":"<p>If you introduce a new code version to production and discover it has a critical bug, you can simply roll back to the previous version. But you also need to roll back the results of running it.  Similar to Git, lakeFS allows you to revert your commits in case they introduced low-quality data. Revert in lakeFS is an atomic action that prevents the data consumers from receiving low quality data until the issue is resolved.</p> <p>As previously mentioned, with lakeFS the recommended branching schema is to ingest data to a dedicated branch. When streaming data, we can decide to merge the incoming data to main at a given time interval or checkpoint, depending on how we chose to write it from Kafka. </p> <p>You can run quality tests for each merge (as discussed in the During Deployment section). Alas, tests are not perfect and we might still introduce low quality data to our main branch at some point. In such a case, we can revert the bad commits from main to the last known high quality commit. This will record new commits reversing the effect of the bad commits.</p> <p></p> <p>Reverting commits using the CLI</p> <pre><code>lakectl branch revert lakefs://example-repo/main 20c30c96 ababea32\n</code></pre> <p>Note lakeFS version &lt;= v0.33.1 uses '@' (instead of '/') as separator between repository and branch.</p>"},{"location":"understand/data_lifecycle_management/production/#example-2-troubleshoot-reproduce-a-bug-in-production","title":"Example 2: Troubleshoot - Reproduce a bug in production","text":"<p>You upgraded spark and deployed changes in production. A few days or weeks later, you identify a data quality issue, a performance degradation, or an increase to your infra costs. Something that requires investigation and fixing (aka, a bug).</p> <p>lakeFS allows you to open a branch of your lake from the specific merge/commit that introduced the changes to production. Using the metadata saved on the merge/commit  you can reproduce all aspects of the environment, then reproduce the issue on the branch and debug it. Meanwhile,  you can revert the main to a previous point in time, or keep it as is, depending on the use case</p> <p></p> <p>Reading from a historic version (a previous commit) using Spark</p> <pre><code>// represents the data as existed at commit \"11eef40b\":\nspark.read.parquet(\"s3://example-repo/11eef40b/events/by-date\")\n</code></pre>"},{"location":"understand/data_lifecycle_management/production/#example-3-cross-collection-consistency","title":"Example 3: Cross collection consistency","text":"<p>We often need consistency between different data collections. A few examples may be:  - To join different collections in order to create a unified view of an account, a user or another entity we measure.  - To introduce the same data in different formats  - To introduce the same data with a different leading index or sorting due to performance considerations</p> <p>lakeFS will help ensure you introduce only consistent data to your consumers by exposing the new collections and their join in one atomic action to main. Once you consumed the collections on a different branch, and only when both are synchronized, we calculated the join and merged to main. </p> <p>In this example you can see two data sets (Sales data and Marketing data) consumed each to its own independent branch, and after the write of both data sets is completed, they are merged to a different branch (leads branch) where the join ETL runs and creates a joined collection by account. The joined table is then merged to main. The same logic can apply if the data is ingested in streaming, using standard formats, or formats that allow upsert/delete such as Apache Hudi, Delta Lake or Iceberg.</p> <p></p>"},{"location":"understand/data_lifecycle_management/production/#case-study-windward","title":"Case Study: Windward","text":"<p>See how Windward is using lakeFS\u2019 isolation and atomic commits to achieve consistency on top of S3.</p>"},{"location":"understand/how/","title":"How lakeFS Works","text":"<p>The Architecture page includes a logical overview of lakeFS and its components. </p> <p>For deep-dive content about lakeFS see: </p> <ul> <li>Internal database structure</li> <li>Merges in lakeFS</li> <li>Versioning Internals</li> </ul>"},{"location":"understand/how/kv/","title":"Internal database structure","text":"<p>Starting at version 0.80.2, lakeFS abandoned the tight coupling to PostgreSQL and moved all database operations to work over Key-Value Store</p> <p>While SQL databases, and Postgres among them, have their obvious advantages, we felt that the tight coupling to Postgres is limiting our users and so, lakeFS with Key Value Store is introduced.</p> <p>Our KV Store implements a generic interface, with methods for Get, Set, Compare-and-Set, Delete and Scan. Each entry is represented by a [partition, key, value] triplet. All these fields are generic byte-array, and the using module has maximal flexibility on the format to use for each field</p> <p>Under the hood, our KV implementation relies on a backing DB, which persists the data. Theoretically, it could be any type of database and out of the box, we already implemented drivers for DynamoDB, for AWS users, and PostgreSQL, using its relational nature to store a KV Store. </p> <p>More databases will be supported in the future, and lakeFS users and contributors can develop their own driver to use their own favorite database. For experimenting purposes, an in-memory KV store can be used, though it obviously lack the persistency aspect</p> <p>In order to store its metadata objects (that is Repositories, Branches, Commits, Tags, and Uncommitted Objects), lakeFS implements another layer over the generic KV Store, which supports serialization and deserialization of these objects as protobuf. </p> <p>As this layer relies on the generic interface of the KV Store layer, it is totally agnostic to whichever store implementation is in use, gaining our users the maximal flexibility</p> <p>Further Reading</p> <p>For a deeper explanation, see to our KV Design</p>"},{"location":"understand/how/kv/#optimistic-locking-with-kv","title":"Optimistic Locking with KV","text":"<p>One important key difference between SQL databases and Key Value Store is the ability to lock resources. </p> <p>While this is a common practice with relational databases, Key Value stores not always support this ability.  When designing our KV Store, we tried to support the most simplistic straight-forward interface, with flexibility in backing DB selection, and so, we decided not to support locking. </p> <p>This decision brought some concurrency challenges we had to overcome. Let us take a look at a common lakeFS flow, Commit, during which several database operations are performed:</p> <ul> <li>All relevant (Branch correlated) uncommitted objects are collected and marked as committed </li> <li>A new Commit object is created</li> <li>The relevant Branch is updated to point to the new commit</li> </ul> <p>The Commit flow includes multiple database accesses and modifications, and is very sensitive to concurrent executions: If 2 Commit flows run in parallel, we must guarantee correctness of the data.</p> <p>lakeFS with PostgreSQL simply locks the Branch for the entire Commit operation, preventing concurrent execution of such flows.</p> <p>Now, with KV Store replacing the SQL database, this easy solution is no longer available. Instead, we implemented an Optimistic Locking algorithm, which leverages the KV Store Compare-And-Set (CAS) functionality to remember the Branch state at the beginning of the Commit flow, and updating the branch at the end, only if it remains unchanged, using CAS, with the former Branch state, used as a comparison criteria. </p> <p>If the sampled Branch state and the current state differ, it could only mean that another, later, Commit is in progress, causing the first Commit to fail, and give the later Commit a chance to complete.</p> <p>Here's a running example:</p> <ul> <li>Commit A sets the StagingToken to tokenA and samples the Branch,</li> <li>Commit B sets the StagingToken to tokenB and samples the Branch,</li> <li>Commit A finishes, tries to update the Branch and fails due to the recent modification by Commit B - the StagingToken is set to tokenB and not tokenA as expected by Commit A,</li> <li>Commit B finishes and updates the branch, as tokenB is set as StagingToken and it matches the flow expectation</li> </ul> <p>An important detail to note, is that as a Commit starts, and the StagingToken is set a new value, the former value is added to a list of 'still valid' StagingToken_s - _SealedToken - on the Branch, which makes sure no StagingToken and no object are lost due to a failed Commit</p> <p>Further Reading</p> <p>You can read more on the Commit Flow in the dedicated section in the KV Design</p>"},{"location":"understand/how/kv/#db-transactions-and-atomic-updates","title":"DB Transactions and Atomic Updates","text":"<p>Another notable difference is the existence of DB transactions with PostgreSQL, ability that our KV Store lacks.</p> <p>This ability was leveraged by lakeFS to construct several DB updates, into one \"atomic\" operation - each failure, in each step, rolled back the entire operation, keeping the DB consistent and clean. With KV Store, this ability is gone, and we had to come up with various solutions. As a starting point, the DB consistency is, obviously, not anything we can risk. On the other hand, maintaining the DB clean, and as a result smaller, is something that can be sacrificed, at least as a first step. Let us take a look at a relatively simple flow of a new Repository creation:</p> <p>A brand new Repository has 3 objects: The Repository object itself, an initial Branch object and an initial Commit, which the Branch points to. </p> <p>With SQL DB, it was as simple as creating all 3 objects in the DB under one transaction (at this order). Any failure resulted in a rollback and no redundant leftovers in our DB. With no transaction in KV Store, if for example the Branch creation fails, it will leave the Repository without an initial Branch (or a Branch at all), yet the Repository will be accessible.  Trying to delete the Repository as a response to Branch creation failure is ony a partial solution as this operation can fail as well.</p> <p>To mitigate this we introduced a per-Repository-partition, which holds all repository related objects (the Branch and Commit in this scenario).  The partition key can only be derived from the specific Repository instance itself. In addition we first create the Repository objects, the Commit and the Branch, under the Repository's partition key, and then the Repository is created.  The Repository and its objects will be accessible only after a successful creation of all 3 entities. </p> <p>A failure in this flow might leave some dangling objects, but consistency is maintained.</p> <p>The number of such dangling objects is not expected to be significant, and we plan to implement a cleaning algorithm to keep our KV Store neat and clean</p>"},{"location":"understand/how/kv/#so-which-approach-is-better","title":"So, Which Approach is Better?","text":"<p>This documents provides a peek into our new database approach - Key Value Store instead of a Relational SQL. It discusses the challenges we faced, and the solutions we provided to overcome these challenges. Considering the fact that lakeFS over with relational database did work, you might ask yourself why did we bother to develop another solution. The simple answer, is that while PostgreSQL was not a bad option, it was the only option, and any drawback of PostgreSQL, reflected on our users:</p> <ul> <li>PostgreSQL can only scale vertically and that is a limitation. At some point this might not hold.</li> <li>PostgreSQL is not a managed solution, meaning that users had to take care of all maintenance tasks, including the above mentioned scale (when needed)</li> <li>As an unmanaged database, scaling means downtime - is that acceptable?</li> <li>It might even get to the point that your organization is not willing to work with PostgreSQL due to various business considerations</li> </ul> <p>If none of the above apply, and you have no seemingly reason to switch from PostgreSQL, it can definitely still be used as an excellent option for the backing database for the lakeFS KV Store. If you do need another solution, you have DynamoDB support, out of the box. DynamoDB, as a fully managed solution, with horizontal scalability support and optimized partitions support, answers all the pain-points specified above. It is definitely an option to consider, if you need to overcome these And, of course, you can always decide to implement your own KV Store driver to use your database of choice - we would love to add your contribution to lakeFS</p>"},{"location":"understand/how/merge/","title":"Merges in lakeFS","text":"<p>The merge operation in lakeFS is similar to Git. It incorporates changes from a merge source (a commit/reference) into a merge destination (a branch). </p>"},{"location":"understand/how/merge/#how-does-it-work","title":"How does it work?","text":"<p>lakeFS first finds the merge base: the nearest common ancestor of the two commits. It can now perform a three-way merge, by examining the presence and identity of files in each commit. In the table below, \"A\", \"B\" and \"C\" are possible file contents, \"X\" is a missing file, and \"conflict\" (which only appears as a result) is a merge failure.</p> In base In source In destination Result Comment A A A A Unchanged file A B B B Files changed on both sides in same way A B C conflict Files changed on both sides differently A A B B File changed only on one branch A B A B File changed only on one branch A X X X Files deleted on both sides A B X conflict File changed on one side, deleted on the other A X B conflict File changed on one side, deleted on the other A A X X File deleted on one side A X A X File deleted on one side"},{"location":"understand/how/merge/#merge-strategies","title":"Merge Strategies","text":"<p>The API and <code>lakectl</code> allow passing an optional <code>strategy</code> flag with the following values:</p>"},{"location":"understand/how/merge/#source-wins","title":"<code>source-wins</code>","text":"<p>In case of a conflict, merge will pick the source objects.</p> <p>Example</p> <pre><code>lakectl merge lakefs://example-repo/validated-data lakefs://example-repo/production --strategy source-wins\n</code></pre> <p>When a merge conflict arises, the conflicting objects in the <code>validated-data</code> branch will be chosen to end up in <code>production</code>.</p>"},{"location":"understand/how/merge/#dest-wins","title":"<code>dest-wins</code>","text":"<p>In case of a conflict, merge will pick the destination objects.</p> <p>Example</p> <pre><code>lakectl merge lakefs://example-repo/validated-data lakefs://example-repo/production --strategy dest-wins\n</code></pre> <p>When a merge conflict arises, the conflicting objects in the <code>production</code> branch will be chosen to end up in <code>validated-data</code>. The <code>production</code> branch will not be affected by object changes from <code>validated-data</code> conflicting objects.</p> <p>The strategy will affect all conflicting objects in the merge if it is set. Currently it is not possible to treat conflicts individually.</p> <p>As a format-agnostic system, lakeFS currently merges by complete files. Format-specific and other user-defined merge strategies for handling conflicts are on the roadmap.</p>"},{"location":"understand/how/versioning-internals/","title":"Versioning Internals","text":""},{"location":"understand/how/versioning-internals/#overview","title":"Overview","text":"<p>Since commits in lakeFS are immutable, they are easy to store on an immutable object store.</p> <p>Older commits are rarely accessed, while newer commits are accessed very frequently, a tiered storage approach can work very well - the object store is the source of truth, while local disk and even RAM can be used to cache the more frequently accessed ones.</p> <p>Since they are immutable - once cached, you only need to evict them when space is running out. There\u2019s no complex invalidation that needs to happen.</p> <p>In terms of storage format, commits are be stored as SSTables, compatible with RocksDB.</p> <p>SSTables were chosen as a storage format for 3 major reasons:</p> <ol> <li>Extremely high read throughput on modern hardware: using commits representing a 200m object repository (modeled after the S3 inventory of one of our design partners), we were able to achieve close to 500k random GetObject calls / second. This provides a very high throughput/cost ratio, probably as high as can be achieved on public clouds.</li> <li>Being a known storage format means it\u2019s relatively easy to generate and consume. Storing it in the object store makes it accessible to data engineering tools for analysis and distributed computation, effectively reducing the silo effect of storing it in an operational database.</li> <li>The SSTable format supports delta encoding for keys which makes them very space efficient for data lakes where many keys share the same common prefixes.</li> </ol> <p>Each lakeFS commit is represented as a set of contiguous, non-overlapping SSTables that make up the entire keyspace of a repository at that commit.</p>"},{"location":"understand/how/versioning-internals/#sstable-file-format-graveler-file","title":"SSTable File Format (\"Graveler File\")","text":"<p>lakeFS metadata is encoded into a format called \"Graveler\" - a standardized way to encode content-addressable key value pairs. This is what a Graveler file looks like:</p> <p></p> <p>Each Key/Value pair (\"ValueRecord\") is constructed of a <code>key</code>, <code>identity</code>, and <code>value</code>.</p> <p>A simple identity could be, for example, a sha256 hash of the value\u2019s bytes. It could be any sequence of bytes that uniquely identifies the value. As far as the Graveler is concerned, two <code>ValueRecord</code>s are considered identical if their key and identity fields are equal.</p> <p>A Graveler file itself is content-addressable, i.e., similarly to Git, the name of the file is its identity. File identity is calculated based on the identity of the ValueRecords the file contains:</p> <p>valueRecordID = h(h(valueRecord.key) || h(valueRecord.Identity)) fileID = h(valueRecordID<sub>1</sub> + \u2026 + valueRecordID<sub>N</sub>)</p>"},{"location":"understand/how/versioning-internals/#constructing-a-consistent-view-of-the-keyspace-ie-a-commit","title":"Constructing a consistent view of the keyspace (i.e., a commit)","text":"<p>We have two additional requirements for the storage format:</p> <ol> <li>Be space and time efficient when creating a commit - assuming a commit changes a single object out of a billion, we don\u2019t want to write a full snapshot of the entire repository. Ideally, we\u2019ll be able to reuse some data files that haven\u2019t changed to make the commit operations (in both space and time) proportional to the size of the difference as opposed to the total size of the repository.</li> <li>Allow an efficient diff between commits which runs in time proportional to the size of their difference and not their absolute sizes.</li> </ol> <p>To support these requirements, we decided to essentially build a 2-layer Merkle tree composed of a set of leaf nodes (\"Range\") addressed by their content address, and a \"Meta Range\", which is a special range containing all ranges, thus representing an entire consistent view of the keyspace:</p> <p></p> <p>Assuming commit B is derived from commit A, and only changed files in range <code>e-f</code>, it can reuse all ranges except for SSTable #N (the one containing the modified range of keys), which will be recreated with a new hash representing the state as exists after applying commit B\u2019s changes. This will, in turn, also create a new Metarange since its hash is now changed as well (as it is derived from the hash of all contained ranges).</p> <p>Assuming most commits usually change related objects (i.e., that are likely to share some common prefix), the reuse ratio could be very high. We tested this assumption using S3 inventory from 2 design partners - we partitioned the keyspace to an arbitrary number of simulated blocks and measured their change over time. We saw a daily change rate of about 5-20%.</p> <p>Given the size of the repositories, it's safe to assume that a single day would translate into multiple commits. At a modest 20 commits per day, a commit is expected to reuse &gt;= 99% of the previous commit blocks, so acceptable in terms of write amplification generated on commit.</p> <p>On the object store, ranges are stored in the following hierarchy:</p> <pre><code>&lt;lakefs root&gt;\n    _lakefs/\n        &lt;range hash1&gt;\n        &lt;range hash2&gt;\n        &lt;range hashN&gt;\n        ...\n        &lt;metarange hash1&gt;\n        &lt;metarange hash2&gt;\n        &lt;metarange hashN&gt;\n        ...\n    &lt;data object hash1&gt;\n    &lt;data object hash2&gt;\n    &lt;data object hashN&gt;\n    ...\n</code></pre> <p>Note: This relatively flat structure could be modified in the future. Looking at the diagram above, it imposes no real limitations on the depth of the tree. A tree could easily be made recursive by having Meta Ranges point to other Meta Ranges - and still provide all the same characteristics. For simplicity, we decided to start with a fixed 2-level hierarchy.</p>"},{"location":"understand/how/versioning-internals/#representing-references-and-uncommitted-metadata","title":"Representing references and uncommitted metadata","text":"<p>lakeFS always stores the object data in the storage namespace in the user's object store, committed and uncommitted data alike.</p> <p>However, the lakeFS object metadata might be stored in either the object store or a key-value store.</p> <p>Unlike committed metadata which is immutable, uncommitted (or \"staged\") metadata experiences frequent random writes and is very mutable in nature. This is also true for \"refs\" - in particular, branches, which are simply pointers to an underlying commit, are modified frequently: on every commit or merge operation.</p> <p>Both these types of metadata are not only mutable, but also require strong consistency guarantees while also being fault tolerant. If we can\u2019t access the current pointer of the main branch, a big portion of the system is essentially down. </p> <p>Luckily, this is also much smaller set of metadata compared to the committed metadata.</p> <p>References and uncommitted metadata are currently stored on a key-value store (See supported databases) for consistency guarantees.</p>"},{"location":"understand/use_cases/","title":"lakeFS Use Cases","text":"<p>lakeFS has many uses in the data world, including</p> <ul> <li>CI/CD for Data Lakes</li> <li>ETL Testing Environment</li> <li>Reproducibility</li> <li>Rollback</li> </ul> <p>One of the important things that lakeFS provides is full support for Data Lifecycle Management through all stages:</p> <ul> <li>In Test</li> <li>During Deployment</li> <li>In Production</li> </ul>"},{"location":"understand/use_cases/cicd_for_data/","title":"CI/CD for Data","text":""},{"location":"understand/use_cases/cicd_for_data/#why-do-i-need-cicd","title":"Why do I need CI/CD?","text":"<p>Data pipelines feed processed data from data lakes to downstream consumers like business dashboards and machine learning models. As more and more organizations rely on data to enable business critical decisions, data reliability and trust are of paramount concern. Thus, it's important to ensure that production data adheres to the data governance policies of businesses. These data governance requirements can be as simple as a file format validation, schema check, or an exhaustive PII(Personally Identifiable Information) data removal from all of organization's data. </p> <p>Thus, to ensure the quality and reliability at each stage of the data lifecycle, data quality gates need to be implemented. That is, we need to run Continuous Integration(CI) tests on the data, and only if data governance requirements are met can the data can be promoted to production for business use. </p> <p>Everytime there is an update to production data, the best practice would be to run CI tests and then promote(deploy) the data to production. </p>"},{"location":"understand/use_cases/cicd_for_data/#how-do-i-implement-cicd-for-data-with-lakefs","title":"How do I implement CI/CD for data with lakeFS?","text":"<p>lakeFS makes implementing CI/CD pipelines for data simpler. lakeFS provides a feature called hooks that allow automation of checks and validations of data on lakeFS branches. These checks can be triggered by certain data operations like committing, merging, etc. </p> <p>Functionally, lakeFS hooks are similar to Git Hooks. lakeFS hooks are run remotely on a server, and they are guaranteed to run when the appropriate event is triggered.</p> <p>Here are some examples of the hooks lakeFS supports: * pre-merge * pre-commit * post-merge * post-commit * pre-create-branch * post-create-branch</p> <p>and so on.</p> <p>By leveraging the pre-commit and pre-merge hooks with lakeFS, you can implement CI/CD pipelines on your data lakes.</p> <p>Specific trigger rules, quality checks and the branch on which the rules are to be applied are declared in <code>actions.yaml</code> file. When a specific event (say, pre-merge) occurs, lakeFS runs all the validations declared in <code>actions.yaml</code> file. If validations error out, the merge event is blocked.</p> <p>Here is a sample <code>actions.yaml</code> file that has pre-merge hook configured to allow only parquet and delta lake file formats on main branch.</p> <pre><code>name: ParquetOnlyInProduction\ndescription: This webhook ensures that only parquet files are written under production/\non:\n  pre-merge:\n    branches:\n      - main\nhooks:\n  - id: production_format_validator\n    type: webhook\n    description: Validate file formats\n    properties:\n      url: \"http://lakefs-hooks:5001/webhooks/format\"\n      query_params:\n        allow: [\"parquet\", \"delta_lake\"]\n        prefix: analytics/\n</code></pre>"},{"location":"understand/use_cases/cicd_for_data/#using-hooks-as-data-quality-gates","title":"Using hooks as data quality gates","text":"<p>Hooks are run on a remote server that can serve http requests from lakeFS server. lakeFS supports two types of hooks. 1. webhooks (run remotely on a web server. e.g.: flask server in python)  2. airflow hooks (a dag of complex data quality checks/tasks that can be run on airflow server) </p> <p>In this tutorial, we will show how to use webhooks (python flask webserver) to implement quality gates on your data branches. Specifically, how to configure hooks to allow only parquet and delta lake format files in the main branch.</p> <p>The tutorial provides a lakeFS environment, python flask server, a Jupyter notebook and sample data sets to demonstrate the integration of lakeFS hooks with Apache Spark and Python. It runs on Docker Compose.</p> <p>To understand how hooks work and how to configure hooks in your production system, refer to the documentation: Hooks. </p> <p></p> <p>Follow the steps below to try out CI/CD for data lakes.</p>"},{"location":"understand/use_cases/cicd_for_data/#implementing-cicd-pipeline-with-lakefs-demo","title":"Implementing CI/CD pipeline with lakeFS - Demo","text":"<p>The sample below provides a lakeFS environment, a Jupyter notebook, and a server on which for the lakeFS webhooks to run. </p>"},{"location":"understand/use_cases/cicd_for_data/#prerequisites-setup","title":"Prerequisites &amp; Setup","text":"<p>Before we get started, make sure Docker is installed on your machine.</p> <ul> <li>Start by cloning the lakeFS samples Git repository:</li> </ul> <pre><code>git clone https://github.com/treeverse/lakeFS-samples.git\ncd lakeFS-samples\n</code></pre> <ul> <li>Run following commands to start the components: </li> </ul> <pre><code>git submodule init\ngit submodule update\ndocker compose up\n</code></pre> <p>Open the local Jupyter Notebook and go to the <code>hooks-demo.ipynb</code> notebook.</p>"},{"location":"understand/use_cases/cicd_for_data/#resources","title":"Resources","text":"<p>To explore different checks and validations on your data, refer to pre-built hooks config by the lakeFS team. </p> <p>To understand the comprehensive list of hooks supported by lakeFS, refer to the documentation.</p>"},{"location":"understand/use_cases/etl_testing/","title":"ETL Testing with Isolated Dev/Test Environments","text":""},{"location":"understand/use_cases/etl_testing/#why-are-multiple-environments-so-important","title":"Why are multiple environments so important?","text":"<p>When working with a data lake, it's useful to have replicas of your production environment. These replicas allow you to test these ETLs and understand changes to your data without impacting the consumers of the production data.</p> <p>Running ETL and transformation jobs directly in production without proper ETL testing presents a huge risk of having data issues flow into dashboards, ML models, and other consumers sooner or later.</p> <p>The most common approach to avoid making changes directly in production is to create and maintain multiple data environments and perform ETL testing on them. Dev environments give you a space in which to develop the data pipelines and test environment where pipeline changes are tested before pushing it to production.</p> <p>Without lakeFS, the challenge with this approach is that it can be time-consuming and costly to maintain these separate dev/test environments to enable thorough effective ETL testing. And for larger teams it forces multiple people to share these environments, requiring significant coordination. Depending on the size of the data involved there can also be high costs due to the duplication of data.</p>"},{"location":"understand/use_cases/etl_testing/#how-does-lakefs-help-with-devtest-environments","title":"How does lakeFS help with Dev/Test environments?","text":"<p>lakeFS makes creating isolated dev/test environments for ETL testing quick and cheap. lakeFS uses zero-copy branching which means that there is no duplication of data when you create a new environment. This frees you from spending time on environment maintenance and makes it possible to create as many environments as needed.</p> <p>In a lakeFS repository, data is always located on a <code>branch</code>. You can think of each <code>branch</code> in lakeFS as its own environment. This is because branches are isolated, meaning changes on one branch have no effect other branches.</p> <p>Info</p> <p>Objects that remain unchanged between two branches are not copied, but rather shared to both branches via metadata pointers that lakeFS manages.</p> <p>If you make a change on one branch and want it reflected on another, you can perform a <code>merge</code> operation to update one branch with the changes from another.</p>"},{"location":"understand/use_cases/etl_testing/#using-branches-as-development-and-testing-environments","title":"Using branches as development and testing environments","text":"<p>The key difference when using lakeFS for isolated data environments is that you can create them immediately before testing a change. And once new data is merged into production, you can delete the branch - effectively deleting the old environment.</p> <p>This is different from creating a long-living test environment used as a staging area to test all the updates. With lakeFS, we create a new branch for each change to production that we want to make. One benefit of this is the ability to test multiple changes at one time.</p> <p></p>"},{"location":"understand/use_cases/etl_testing/#try-it-out-creating-devtest-environments-with-lakefs-for-etl-testing","title":"Try it out! Creating Dev/Test Environments with lakeFS for ETL Testing","text":"<p>lakeFS supports UI, CLI (<code>lakectl</code> command-line utility) and several clients for the API to run the Git-like operations. Let us explore how to create dev/test environments using each of these options below.</p> <p>There are two ways that you can try out lakeFS:</p> <ul> <li>The lakeFS Playground on lakeFS Cloud - fully managed lakeFS with a 30-day free trial</li> <li>Local Docker-based quickstart and samples</li> </ul> <p>You can also deploy lakeFS locally or self-managed on your cloud of choice.</p>"},{"location":"understand/use_cases/etl_testing/#using-lakefs-playground-on-lakefs-cloud","title":"Using lakeFS Playground on lakeFS Cloud","text":"<p>In this tutorial, we will use a lakeFS playground environment to create dev/test data environments for ETL testing. This allows you to spin up a lakeFS instance in a click, create different data environments by simply branching out of your data repository and develop &amp; test data pipelines in these isolated branches.</p> <p>First, let us spin up a playground instance. Once you have a live environment, login to your instance with access and secret keys. Then, you can work with the sample data repository <code>my-repo</code> that is created for you.</p> <p></p> <p>Click on <code>my-repo</code> and notice that by default, the repository has a <code>main</code> branch created and <code>sample_data</code> preloaded to work with.</p> <p></p> <p>You can create a new branch (say, <code>test-env</code>) by going to the Branches tab and clicking Create Branch. Once it is successful, you will see two branches under the repository: <code>main</code> and <code>test-env</code>.</p> <p></p> <p>Now you can add, modify or delete objects under the <code>test-env</code> branch without affecting the data in the main branch.</p>"},{"location":"understand/use_cases/etl_testing/#trying-out-lakefs-with-docker-and-jupyter-notebooks","title":"Trying out lakeFS with Docker and Jupyter Notebooks","text":"<p>This use case shows how to create dev/test data environments for ETL testing using lakeFS branches. The following tutorial provides a lakeFS environment, a Jupyter notebook, and Python SDK API to demonstrate integration of lakeFS with Spark. You can run this tutorial on your local machine.</p> <p>Follow the tutorial video below to get started with the playground and Jupyter notebook, or follow the instructions on this page.</p>"},{"location":"understand/use_cases/etl_testing/#prerequisites","title":"Prerequisites","text":"<p>Before getting started, you will need Docker installed on your machine.</p>"},{"location":"understand/use_cases/etl_testing/#running-lakefs-and-jupyter-notebooks","title":"Running lakeFS and Jupyter Notebooks","text":"<p>Follow along the steps below to create dev/test environment with lakeFS.</p> <ul> <li> <p>Start by cloning the lakeFS samples Git repository:</p> <pre><code>git clone https://github.com/treeverse/lakeFS-samples.git\ncd lakeFS-samples\n</code></pre> </li> <li> <p>Run following commands to download and run Docker container which includes Python, Spark, Jupyter Notebook, JDK, Hadoop binaries, lakeFS Python SDK and Airflow (Docker image size is around 4.5GB):</p> <pre><code>git submodule init &amp;&amp; git submodule update\ndocker compose up\n</code></pre> </li> <li> <p>Open the local Jupyter Notebook and go to the <code>spark-demo.ipynb</code> notebook.</p> </li> </ul>"},{"location":"understand/use_cases/etl_testing/#configuring-lakefs-python-client","title":"Configuring lakeFS Python Client","text":"<p>Setup lakeFS access credentials for the lakeFS instance running. The defaults for these that the samples repository Docker Compose uses are shown here:</p> <pre><code>lakefs_access_key = 'AKIAIOSFODNN7EXAMPLE'\nlakefs_secret_key = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'\nlakefs_endpoint = 'http://lakefs:8000'\n</code></pre> <p>Next, setup the storage namespace to a location in the bucket you have configured. The storage namespace is a location in the underlying storage where data for this repository will be stored.</p> <pre><code>storageNamespace = 's3://example/' \n</code></pre> <p>You can use lakeFS through the UI, API or <code>lakectl</code> command-line. For this use-case, we use python <code>lakefs</code> to run lakeFS core operations.</p> <pre><code>import lakefs\nfrom lakefs import Client\n\n# lakeFS credentials and endpoint\nclient = Client(\n    host=lakefs_endpoint,\n    username=lakefs_access_key,\n    password=lakefs_secret_key\n)\n</code></pre> <p>lakeFS can be configured to work with Spark in two ways:</p> <ul> <li>Access lakeFS using the S3-compatible API</li> <li>Access lakeFS using the lakeFS-specific Hadoop FileSystem</li> </ul>"},{"location":"understand/use_cases/etl_testing/#upload-the-sample-data-to-main-branch","title":"Upload the Sample Data to Main Branch","text":"<p>To upload an object to the <code>my-repo</code>, use the following command.</p> <pre><code>import os\nimport lakefs\n\nwith open('/data/lakefs_test.csv', 'rb') as f:\n    lakefs.repository(\"my-repo\", client=client).branch(\"main\").object(filenName).upload(data=f.read())\n</code></pre> <p>Once uploaded, commit the changes to the <code>main</code> branch and attach some metadata to the commit as well.</p> <pre><code>lakefs.repository(\"my-repo\", client=client).branch(\"main\").commit(message=\"Added my first object!\", metadata={'using': 'python'})\n</code></pre> <p>In this example, we use lakeFS S3A gateway to read data from the storage bucket.</p> <pre><code>dataPath = f\"s3a://my-repo/main/lakefs_test.csv\"\ndf = spark.read.csv(dataPath)\ndf.show()\n</code></pre>"},{"location":"understand/use_cases/etl_testing/#create-a-test-branch","title":"Create a Test Branch","text":"<p>Let us start by creating a new branch <code>test-env</code> on the example repository <code>my-repo</code>.</p> <pre><code>lakefs.repository(\"my-repo\", client=client).branch(\"test-env\").create(source_reference=\"main\")\n</code></pre> <p>Now we can use Spark to write the csv file from <code>main</code> branch as a Parquet file to the <code>test-env</code> of our lakeFS repository. Suppose we accidentally write the dataframe back to \"test-env\" branch again, this time in append mode.</p> <pre><code>df.write.mode('overwrite').parquet('s3a://my-repo/test-env/')\ndf.write.mode('append').parquet('s3a://my-repo/test-env/')\n</code></pre> <p>What happens if we re-read in the data on both branches and perform a count on the resulting DataFrames? There will be twice as many rows in <code>test-env</code> branch. That is, we accidentally duplicated our data! Oh no!</p> <p>Data duplication introduce errors into our data analytics, BI and machine learning efforts; hence we would like to avoid duplicating our data.</p> <p>On the <code>main</code> branch however, there is still just the original data - untouched by our Spark code. This shows the utility of branch-based isolated environments with lakeFS.</p> <p>You can safely continue working with the data from main which is unharmed due to lakeFS isolation capabilities.</p>"},{"location":"understand/use_cases/etl_testing/#further-reading","title":"Further Reading","text":"<ul> <li>Case Study: How Enigma use lakeFS for isolated development and staging environments</li> <li>Tutorial: ETL Testing Tutorial with lakeFS: Step-by-Step Guide</li> <li>ETL Testing: A Practical Guide</li> <li>Top 5 ETL Testing Challenges - Solved!</li> </ul>"},{"location":"understand/use_cases/reproducibility/","title":"Reproducibility","text":""},{"location":"understand/use_cases/reproducibility/#the-benefits-of-reproducible-data","title":"The Benefits of Reproducible Data","text":"<p>Data changes frequently. This makes the task of keeping track of its exact state over time difficult. Oftentimes, people maintain only one state of their data - its current state.</p> <p>This has a negative impact on the work, as it becomes hard to:</p> <ul> <li>Debug a data issue.</li> <li>Validate machine learning training accuracy (re-running a model over different data gives different results).</li> <li>Comply with data audits.</li> </ul> <p>In comparison, lakeFS exposes a Git-like interface to data that allows keeping track of more than just the current state of data. This makes reproducing its state at any point in time straightforward.</p>"},{"location":"understand/use_cases/reproducibility/#achieving-reproducibility-with-lakefs","title":"Achieving Reproducibility with lakeFS","text":"<p>To make data reproducible, we recommend taking a new commit of your lakeFS repository every time the data in it changes. As long as there\u2019s a commit taken, the process to reproduce a given state is as simple as reading the data from a path that includes the unique <code>commit_id</code> generated for each commit.</p> <p>To read data at it\u2019s current state, we can use a static path containing the repository and branch names. To give an example, if you have a repository named <code>example</code> with a branch named <code>main</code>, reading the latest state of this data into a Spark Dataframe is always:</p> <p>Example</p> <pre><code>df = spark.read.parquet(\"s3://example/main/\")\n</code></pre> <p>The code above assumes that all objects in the repository under this path are stored in parquet format. If a different format is used, the applicable Spark read method should be used.</p> <p>In a lakeFS repository, we are capable of taking many commits over the data, making many points in time reproducible. </p> <p></p> <p>In the repository above, a new commit is taken each time a model training script is run, and the commit message includes the specific run number. </p> <p>If we wanted to re-run the model training script and reproduce the exact same results for a historical run, say run #435, we could copy the commit ID associated with the run and read the data into a dataframe like so:</p> <pre><code>df = spark.read.parquet(\"s3://example/296e54fbee5e176f3f4f4aeb7e087f9d57515750e8c3d033b8b841778613cb23/training_dataset/\")\n</code></pre> <p>The ability to reference a specific <code>commit_id</code> in code simplifies reproducing the specific state a data collection or even multiple collections. This has many applications that are common in data development, such as historical debugging, identifying deltas in a data collection, audit compliance, and more.</p>"},{"location":"understand/use_cases/rollback/","title":"Rollbacks","text":""},{"location":"understand/use_cases/rollback/#what-is-a-rollback","title":"What Is a Rollback?","text":"<p>A rollback operation is used to to fix critical data errors immediately.</p> <p>What is a critical data error? Think of a situation where erroneous or misformatted data causes a significant issue with an important service or function. In such situations, the first thing to do is stop the bleeding.</p> <p>Rolling back returns data to a state in the past, before the error was present. You might not be showing all the latest data after a rollback, but at least you aren\u2019t showing incorrect data or raising errors.</p>"},{"location":"understand/use_cases/rollback/#why-rollbacks-are-useful","title":"Why Rollbacks Are Useful","text":"<p>A Rollback is used as a stopgap measure to \u201cput out the fire\u201d as quickly as possible while RCA (root cause analysis) is performed to understand 1) exactly how the error happened, and 2) what can be done to prevent it from happening again.</p> <p>It can be a pressured, stressful situation to deal with a critical data error. Having the ability to employ a rollback relieves some of the pressure and makes it more likely you can figure out what happened without creating additional issues.</p> <p>Example</p> <p>As a real world example, the 14-day outage some Atlassian users experienced in May 2022 could have been an uninteresting minor incident had rolling back the deleted customer data been an option.</p>"},{"location":"understand/use_cases/rollback/#performing-rollbacks-with-lakefs","title":"Performing Rollbacks with lakeFS","text":"<p>lakeFS lets you develop in your data lake in such a way that rollbacks are simple to perform. This starts by taking a commit over your lakeFS repository whenever a change to its state occurs.</p> <p>Using the lakeFS UI or CLI, you can set the current state, or HEAD, of a branch to any historical commit in seconds, effectively performing a rollback.</p> <p>To demonstrate how this works, let's take the example of a lakeFS repo with the following commit history:</p> <p></p> <p>As can be inferred from the history, this repo is updated every minute with a data sync from some data source. An example data sync is a typical ETL job that replicates data from an internal database or any other data source. After each sync, a commit is taken in lakeFS to save a snapshot of data at that point in time.</p>"},{"location":"understand/use_cases/rollback/#how-to-rollback-from-a-bad-data-sync","title":"How to Rollback From a Bad Data Sync?","text":"<p>Say a situation occurs where one of the syncs had bad data and is causing downstream dashboards to fail to load. Since we took a commit of the repo right after the sync ran, we can use a <code>revert</code> operation to undo the changes introduced in that sync.</p> <p></p> <p>Step 1: Copy the <code>commit_id</code> associated with the commit we want to revert. As the screenshot above shows, you can use the Copy ID to Clipboard button to do this.</p> <p>Step 2: Run the revert command using lakectl, the lakeFS CLI. In this example, the command will be as follows:</p> <pre><code>lakectl branch revert \"lakefs://example/main\" 9666d7c9daf37b3ba6964e733d08596ace2ec2c7bc3a4023ad8e80737a6c3e9d\n</code></pre> <p>This will undo the changes introduced by this commit, completing the rollback! </p> <p></p> <p>The rollback operation is that simple, even if many changes were introduced in a commit, spanning acrossmultiple data collections.</p> <p>In lakeFS, rolling back data is always a one-liner.</p>"}]}