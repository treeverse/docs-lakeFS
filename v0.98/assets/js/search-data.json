{"0": {
    "doc": "Page not found",
    "title": "Page not found",
    "content": " ",
    "url": "/v0.98/404.html",
    
    "relUrl": "/404.html"
  },"1": {
    "doc": "Page not found",
    "title": "We can’t seem to find the page you’re looking for.",
    "content": "Error code: 404 . Back to Docs . ",
    "url": "/v0.98/404.html#we-cant-seem-to-find-the-page-youre-looking-for",
    
    "relUrl": "/404.html#we-cant-seem-to-find-the-page-youre-looking-for"
  },"2": {
    "doc": "Access Control Lists (ACLs)",
    "title": "Access Control Lists (ACLs)",
    "content": "ACLs were introduced in their current form in v0.97 of lakeFS as part of changes to the security model in lakeFS. They are an alternative to the more granular control that role-based access control provides. ",
    "url": "/v0.98/reference/access-control-lists.html",
    
    "relUrl": "/reference/access-control-lists.html"
  },"3": {
    "doc": "Access Control Lists (ACLs)",
    "title": "Table of contents",
    "content": ". | ACLs | Scopes | Pluggable Authentication and Authorization | Previous versions of ACL in lakeFS . | Migrating from the previous version of ACLs | . | . ",
    "url": "/v0.98/reference/access-control-lists.html#table-of-contents",
    
    "relUrl": "/reference/access-control-lists.html#table-of-contents"
  },"4": {
    "doc": "Access Control Lists (ACLs)",
    "title": "ACLs",
    "content": "You can attach Permissions and scope them to groups in the Groups page. There are 4 default groups, named after the 4 permissions. Each group is global (applies for all repositories). | Group ID | Allows | Repositories | . | Read | Read operations, creating access keys | All | . | Write | Allows all data read and write operations. | All | . | Super | Allows all operations except auth. | All | . | Admin | Allows all operations. | All | . ",
    "url": "/v0.98/reference/access-control-lists.html#acls",
    
    "relUrl": "/reference/access-control-lists.html#acls"
  },"5": {
    "doc": "Access Control Lists (ACLs)",
    "title": "Scopes",
    "content": "When granted to a group, permissions Read, Write, and Super may be scoped to a set of repositories. Admin includes global abilities that apply across repos and cannot be scoped to a set of repositories . ",
    "url": "/v0.98/reference/access-control-lists.html#scopes",
    
    "relUrl": "/reference/access-control-lists.html#scopes"
  },"6": {
    "doc": "Access Control Lists (ACLs)",
    "title": "Pluggable Authentication and Authorization",
    "content": "Authorization and authentication is pluggable in lakeFS. If lakeFS is attached to a remote authentication server (or you are using lakeFS Cloud) then the role-based access control user interface can be used. If you are using ACL then the lakeFS configuration element auth.ui_config.RBAC should be set to simplified. ",
    "url": "/v0.98/reference/access-control-lists.html#pluggable-authentication-and-authorization",
    
    "relUrl": "/reference/access-control-lists.html#pluggable-authentication-and-authorization"
  },"7": {
    "doc": "Access Control Lists (ACLs)",
    "title": "Previous versions of ACL in lakeFS",
    "content": "Here’s how the current ACL model compares to to that prior to the changes introduced in v0.97. | Permission | Allows | Previous Group Name | Previous Policy Names and Actions | . | Read | Read operations, creating access keys. | Viewers | FSReadAll [fs:List, fs:Read] | . | Write | Allows all data read and write operations. | Developers | FSReadWriteAll [fs:ListRepositories, fs:ReadRepository, fs:ReadCommit, fs:ListBranches, fs:ListTags, fs:ListObjects, fs:ReadObject, fs:WriteObject, fs:DeleteObject, fs:RevertBranch, fs:ReadBranch, fs:ReadTag, fs:CreateBranch, fs:CreateTag, fs:DeleteBranch, fs:DeleteTag, fs:CreateCommit] RepoManagementReadAll [ci:Read, retention:Get, branches:Get*, fs:ReadConfig] | . | Super | Allows all operations except auth. | SuperUsers (with changes) | FSFullAccess [fs:] RepoManagementReadAll [ci:Read, retention:Get, branches:Get, fs:ReadConfig] | . | Admin | Allows all operations. | Admins | AuthFullAccess [auth:] FSFullAccess [fs:] RepoManagementFullAccess [ci:, retention:, branches:*, fs:ReadConfig] | . Migrating from the previous version of ACLs . Upgrading the lakeFS version will require migrating to the new ACL authorization model. In order to run the migration run: . lakefs migrate auth-acl . The command defaults to dry-run. We recommend running the script only after running the dry-run and going over the warnings. To apply the migration, re-run with the --yes flag . The upgrade will ensure that the 4 default groups exist, and modify existing groups to fit into the new ACLs model: . | When creating the 4 default global groups: if another group exists and has the desired name, upgrading will rename it by appending “.orig”. So after upgrading the 4 default global groups exist, with these known names. | For any group, upgrading configured policies follows these rules, possibly increasing access: . | Any “Deny” rules are stripped, and a warning printed. | “Manage own credentials” is added. | If any actions outside of “fs:” and manage own credentials are allowed, the group becomes an Admin group, a warning is printed, and no further changes apply. | The upgrade script unifies repositories: If a resource applies to a set of repositories with a wildcard, permissions are unified to all repositories. Otherwise they apply to the list of all repositories, in all the policies. | The upgrade script unifies actions: it selects the least permission of Read, Write, Super that contains all of the allowed actions. | . | . Once you have completed the migration you should update the lakeFS server configuration for auth.ui_config.RBAC to simplified. Note that moving to simplified from external may only be performed once and will lose some configuration. The upgrade script will detail the changes made by the transition. For any question or concern during the upgrade, don’t hesitate to get in touch with us through Slack or email. ",
    "url": "/v0.98/reference/access-control-lists.html#previous-versions-of-acl-in-lakefs",
    
    "relUrl": "/reference/access-control-lists.html#previous-versions-of-acl-in-lakefs"
  },"8": {
    "doc": "Airbyte",
    "title": "Table of contents",
    "content": ". | Using lakeFS with Airbyte | Use cases | S3 Connector . | Configuring lakeFS using the connector | . | . ",
    "url": "/v0.98/integrations/airbyte.html#table-of-contents",
    
    "relUrl": "/integrations/airbyte.html#table-of-contents"
  },"9": {
    "doc": "Airbyte",
    "title": "Using lakeFS with Airbyte",
    "content": "The integration between the two open-source projects brings resilience and manageability when you use Airbyte connectors to sync data to your S3 buckets by leveraging lakeFS branches and atomic commits and merges. ",
    "url": "/v0.98/integrations/airbyte.html#using-lakefs-with-airbyte",
    
    "relUrl": "/integrations/airbyte.html#using-lakefs-with-airbyte"
  },"10": {
    "doc": "Airbyte",
    "title": "Use cases",
    "content": "You can take advantage of lakeFS consistency guarantees and Data Lifecycle Management when ingesting data to S3 using lakeFS: . | Consolidate many data sources to a single branch and expose them to consumers simultaneously when merging to the main branch. | Test incoming data for breaking schema changes using lakeFS hooks. | Prevent consumers from reading partial data from connectors which failed half-way through sync. | Experiment with ingested data on a branch before exposing it. | . ",
    "url": "/v0.98/integrations/airbyte.html#use-cases",
    
    "relUrl": "/integrations/airbyte.html#use-cases"
  },"11": {
    "doc": "Airbyte",
    "title": "S3 Connector",
    "content": "lakeFS exposes an S3 Gateway that enables applications to communicate with lakeFS the same way they would with Amazon S3. You can use Airbyte’s S3 Destination to upload data to lakeFS. Configuring lakeFS using the connector . Set the following parameters when creating a new Destination of type S3: . | Name | Value | Example | . | Endpoint | The lakeFS S3 gateway URL | https://cute-axolotol.lakefs-demo.io | . | S3 Bucket Name | The lakeFS repository where the data will be written | example-repo | . | S3 Bucket Path | The branch and the path where the data will be written | main/data/from/airbyte Where main is the branch name, and data/from/airbyte is the path under the branch. | . | S3 Bucket Region | Not applicable to lakeFS, use us-east-1 | us-east-1 | . | S3 Key ID | The lakeFS access key id used to authenticate to lakeFS. | AKIAlakefs12345EXAMPLE | . | S3 Access Key | The lakeFS secret access key used to authenticate to lakeFS. | abc/lakefs/1234567bPxRfiCYEXAMPLEKEY | . The UI configuration will look as follows: . ",
    "url": "/v0.98/integrations/airbyte.html#s3-connector",
    
    "relUrl": "/integrations/airbyte.html#s3-connector"
  },"12": {
    "doc": "Airbyte",
    "title": "Airbyte",
    "content": ". Note: If using Airbyte OSS, please ensure you are using S3 destination connector version 0.3.17 or higher. Previous connector versions are not supported. Airbyte is an open-source platform for syncing data from applications, APIs, and databases to warehouses, lakes, and other destinations. You can use Airbyte’s connectors to get your data pipelines to consolidate many input sources. ",
    "url": "/v0.98/integrations/airbyte.html",
    
    "relUrl": "/integrations/airbyte.html"
  },"13": {
    "doc": "Airflow",
    "title": "Using lakeFS with Airflow",
    "content": "Apache Airflow is a platform that allows users to programmatically author, schedule, and monitor workflows. To run Airflow with lakeFS, you need to follow a few steps. ",
    "url": "/v0.98/integrations/airflow.html#using-lakefs-with-airflow",
    
    "relUrl": "/integrations/airflow.html#using-lakefs-with-airflow"
  },"14": {
    "doc": "Airflow",
    "title": "Create a lakeFS connection on Airflow",
    "content": "To access the lakeFS server and authenticate with it, create a new Airflow Connection of type HTTP and add it to your DAG. You can do that using the Airflow UI or the CLI. Here’s an example Airflow command that does just that: . airflow connections add conn_lakefs --conn-type=HTTP --conn-host=http://&lt;LAKEFS_ENDPOINT&gt; \\ --conn-extra='{\"access_key_id\":\"&lt;LAKEFS_ACCESS_KEY_ID&gt;\",\"secret_access_key\":\"&lt;LAKEFS_SECRET_ACCESS_KEY&gt;\"}' . ",
    "url": "/v0.98/integrations/airflow.html#create-a-lakefs-connection-on-airflow",
    
    "relUrl": "/integrations/airflow.html#create-a-lakefs-connection-on-airflow"
  },"15": {
    "doc": "Airflow",
    "title": "Install the lakeFS Airflow package",
    "content": "You can use pip to install the package . pip install airflow-provider-lakefs . ",
    "url": "/v0.98/integrations/airflow.html#install-the-lakefs-airflow-package",
    
    "relUrl": "/integrations/airflow.html#install-the-lakefs-airflow-package"
  },"16": {
    "doc": "Airflow",
    "title": "Use the package",
    "content": "Operators . The package exposes several operations to interact with a lakeFS server: . | CreateBranchOperator creates a new lakeFS branch from the source branch (main by default). task_create_branch = CreateBranchOperator( task_id='create_branch', repo='example-repo', branch='example-branch', source_branch='main' ) . | CommitOperator commits uncommitted changes to a branch. task_commit = CommitOperator( task_id='commit', repo='example-repo', branch='example-branch', msg='committing to lakeFS using airflow!', metadata={'committed_from\": \"airflow-operator'} ) . | MergeOperator merges 2 lakeFS branches. task_merge = MergeOperator( task_id='merge_branches', source_ref='example-branch', destination_branch='main', msg='merging job outputs', metadata={'committer': 'airflow-operator'} ) . | . Sensors . Sensors are also available that allow synchronizing a running DAG with external operations: . | CommitSensor waits until a commit has been applied to the branch . task_sense_commit = CommitSensor( repo='example-repo', branch='example-branch', task_id='sense_commit' ) . | FileSensor waits until a given file is present on a branch. task_sense_file = FileSensor( task_id='sense_file', repo='example-repo', branch='example-branch', path=\"file/to/sense\" ) . | . Example . This example DAG in the airflow-provider-lakeFS repository shows how to use all of these. Performing other operations . Sometimes an operator might not be supported by airflow-provider-lakeFS yet. You can access lakeFS directly by using: . | SimpleHttpOperator to send API requests to lakeFS. | BashOperator with lakectl commands. For example, deleting a branch using BashOperator: commit_extract = BashOperator( task_id='delete_branch', bash_command='lakectl branch delete lakefs://example-repo/example-branch', dag=dag, ) . | . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. ",
    "url": "/v0.98/integrations/airflow.html#use-the-package",
    
    "relUrl": "/integrations/airflow.html#use-the-package"
  },"17": {
    "doc": "Airflow",
    "title": "Airflow",
    "content": " ",
    "url": "/v0.98/integrations/airflow.html",
    
    "relUrl": "/integrations/airflow.html"
  },"18": {
    "doc": "Airflow",
    "title": "Airflow Hooks",
    "content": " ",
    "url": "/v0.98/hooks/airflow.html#airflow-hooks",
    
    "relUrl": "/hooks/airflow.html#airflow-hooks"
  },"19": {
    "doc": "Airflow",
    "title": "Table of contents",
    "content": ". | Action file Airflow hook properties | Hook Record in configuration field | . Airflow Hook triggers a DAG run in an Airflow installation using Airflow’s REST API. The hook run succeeds if the DAG was triggered, and fails otherwise. Action file Airflow hook properties . | Property | Description | Data Type | Example | Required | Env Vars Support | . | url | The URL of the Airflow instance | String | http://localhost:8080 | true | no | . | dag_id | The DAG to trigger | String | example_dag | true | no | . | username | The name of the Airflow user performing the request | String | admin | true | no | . | password | The password of the Airflow user performing the request | String | admin | true | yes | . | dag_conf | DAG run configuration that will be passed as is | JSON |   | false | no | . | wait_for_dag | Wait for DAG run to complete and reflect state (default: false) | Boolean |   | false | no | . | timeout | Time to wait for the DAG run to complete (default: 1m) | String (golang’s Duration representation) |   | false | no | . Example: ... hooks: - id: trigger_my_dag type: airflow description: Trigger an example_dag properties: url: \"http://localhost:8000\" dag_id: \"example_dag\" username: \"admin\" password: \"{{ ENV.AIRFLOW_SECRET }}\" dag_conf: some: \"additional_conf\" ... Hook Record in configuration field . lakeFS will add an entry to the Airflow request configuration property (conf) with the event that triggered the action. The key of the record will be lakeFS_event and the value will match the one described here . ",
    "url": "/v0.98/hooks/airflow.html#table-of-contents",
    
    "relUrl": "/hooks/airflow.html#table-of-contents"
  },"20": {
    "doc": "Airflow",
    "title": "Airflow",
    "content": " ",
    "url": "/v0.98/hooks/airflow.html",
    
    "relUrl": "/hooks/airflow.html"
  },"21": {
    "doc": "API Reference",
    "title": "API Reference",
    "content": "| ",
    "url": "/v0.98/reference/api.html",
    
    "relUrl": "/reference/api.html"
  },"22": {
    "doc": "Architecture",
    "title": "Architecture Overview",
    "content": " ",
    "url": "/v0.98/understand/architecture.html#architecture-overview",
    
    "relUrl": "/understand/architecture.html#architecture-overview"
  },"23": {
    "doc": "Architecture",
    "title": "Table of contents",
    "content": ". | Overview | Ways to deploy lakeFS . | Load Balancing | . | lakeFS Components . | S3 Gateway | OpenAPI Server | Storage Adapter | Graveler | Authentication &amp; Authorization Service | Hooks Engine | UI | . | Applications | lakeFS Clients . | OpenAPI Generated SDKs | lakectl | Spark Metadata Client | lakeFS Hadoop FileSystem | . | . ",
    "url": "/v0.98/understand/architecture.html#table-of-contents",
    
    "relUrl": "/understand/architecture.html#table-of-contents"
  },"24": {
    "doc": "Architecture",
    "title": "Overview",
    "content": "lakeFS is distributed as a single binary encapsulating several logical services: . The server itself is stateless, meaning you can easily add more instances to handle a bigger load. The following underlying object stores (or any S3-compatible store) can be used by lakeFS to store data: . | Google Cloud Storage | Azure Blob Storage | AWS S3 | MinIO | Ceph | . In additional a Key Value storage is used for storing metadata: . | PostgreSQL | DynamoDB | . Instructions of how to deploy such database on AWS can be found here. Additional information on the data format can be found in Versioning internals. ",
    "url": "/v0.98/understand/architecture.html#overview",
    
    "relUrl": "/understand/architecture.html#overview"
  },"25": {
    "doc": "Architecture",
    "title": "Ways to deploy lakeFS",
    "content": "lakeFS releases include binaries for common operating systems, a containerized option or a Helm chart. Check out our guides for running lakeFS on AWS, GCP and more. Load Balancing . Accessing lakeFS is done using HTTP. lakeFS exposes a frontend UI, an OpenAPI server, as well as an S3-compatible service (see S3 Gateway below). lakeFS uses a single port that serves all three endpoints, so for most use cases a single load balancer pointing to lakeFS server(s) would do. ",
    "url": "/v0.98/understand/architecture.html#ways-to-deploy-lakefs",
    
    "relUrl": "/understand/architecture.html#ways-to-deploy-lakefs"
  },"26": {
    "doc": "Architecture",
    "title": "lakeFS Components",
    "content": "S3 Gateway . The S3 Gateway implements lakeFS’s compatibility with S3. It implements a compatible subset of the S3 API to ensure most data systems can use lakeFS as a drop-in replacement for S3. See the S3 API Reference section for information on supported API operations. OpenAPI Server . The Swagger (OpenAPI) server exposes the full set of lakeFS operations (see Reference). This includes basic CRUD operations against repositories and objects, as well as versioning related operations such as branching, merging, committing, and reverting changes to data. Storage Adapter . The Storage Adapter is an abstraction layer for communicating with any underlying object store. Its implementations allow compatibility with many types of underlying storage such as S3, GCS, Azure Blob Storage, or non-production usages such as the local storage adapter. See the roadmap for information on the future plans for storage compatibility. Graveler . The Graveler handles lakeFS versioning by translating lakeFS addresses to the actual stored objects. To learn about the data model used to store lakeFS metadata, see the data model section. Authentication &amp; Authorization Service . The Auth service handles the creation, management, and validation of user credentials and RBAC policies. The credential scheme, along with the request signing logic, are compatible with AWS IAM (both SIGv2 and SIGv4). Currently, the Auth service manages its own database of users and credentials and doesn’t use IAM in any way. Hooks Engine . The Hooks Engine enables CI/CD for data by triggering user defined Actions that will run during commit/merge. UI . The UI layer is a simple browser-based client that uses the OpenAPI server. It allows management, exploration, and data access to repositories, branches, commits and objects in the system. ",
    "url": "/v0.98/understand/architecture.html#lakefs-components",
    
    "relUrl": "/understand/architecture.html#lakefs-components"
  },"27": {
    "doc": "Architecture",
    "title": "Applications",
    "content": "As a rule of thumb, lakeFS supports any S3-compatible application. This means that many common data applications work with lakeFS out-of-the-box. Check out our integrations to learn more. ",
    "url": "/v0.98/understand/architecture.html#applications",
    
    "relUrl": "/understand/architecture.html#applications"
  },"28": {
    "doc": "Architecture",
    "title": "lakeFS Clients",
    "content": "Some data applications benefit from deeper integrations with lakeFS to support different use cases or enhanced functionality provided by lakeFS clients. OpenAPI Generated SDKs . OpenAPI specification can be used to generate lakeFS clients for many programming languages. For example, the Python lakefs-client or the Java client are published with every new lakeFS release. lakectl . lakectl is a CLI tool that enables lakeFS operations using the lakeFS API from your preferred terminal. Spark Metadata Client . The lakeFS Spark Metadata Client makes it easy to perform operations related to lakeFS metadata, at scale. Examples include garbage collection or exporting data from lakeFS. lakeFS Hadoop FileSystem . Thanks to the S3 Gateway, it’s possible to interact with lakeFS using Hadoop’s S3AFIleSystem, but due to limitations of the S3 API, doing so requires reading and writing data objects through the lakeFS server. Using lakeFSFileSystem increases Spark ETL jobs performance by executing the metadata operations on the lakeFS server, and all data operations directly through the same underlying object store that lakeFS uses. ",
    "url": "/v0.98/understand/architecture.html#lakefs-clients",
    
    "relUrl": "/understand/architecture.html#lakefs-clients"
  },"29": {
    "doc": "Architecture",
    "title": "Architecture",
    "content": " ",
    "url": "/v0.98/understand/architecture.html",
    
    "relUrl": "/understand/architecture.html"
  },"30": {
    "doc": "Amazon Athena",
    "title": "Using lakeFS with Amazon Athena",
    "content": "Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Amazon Athena works directly above S3 and can’t access lakeFS. Tables created using Athena aren’t readable by lakeFS. However, tables stored in lakeFS (that were created with glue/hive) can be queried by Athena. To support querying data from lakeFS with Amazon Athena, we will use create-symlink, one of the metastore commands in lakectl. create-symlink receives a source table, destination table, and the table location. It performs two actions: . | It creates partitioned directories with symlink files in the underlying S3 bucket. | It creates a table in Glue catalog with symlink format type and location pointing to the created symlinks. | . Note .lakectl.yaml file should be configured with the proper hive/glue credentials. For more information . create-symlink receives a table in glue or hive pointing to lakeFS and creates a copy of the table in glue. The table data will use the SymlinkTextInputFormat, which will point to the lakeFS repository storage namespace. You will be able to query your data with Athena without copying any data. However, the symlinks table will only show the data that existed during the copy. If the table changed in lakeFS, you need to run create-symlink again for your changed to be reflected in Athena. Example: . Let’s assume that some time ago, we created a hive table my_table that is stored in lakeFS repo example under branch main, using the command: . CREATE EXTERNAL TABLE `my_table`( `id` bigint, `key` string ) PARTITIONED BY (YEAR INT, MONTH INT) LOCATION 's3://example/main/my_table'; WITH (format = 'PARQUET', external_location 's3a://example/main/my_table' ); . The repository example has the S3 storage space s3://my-bucket/my-repo-prefix/. After inserting some data into it, the object structure under lakefs://example/main/my_table looks as follows: . To query that table with Athena, you need to use the create-symlink command as follows: . lakectl metastore create-symlink \\ --repo example \\ --branch main \\ --path my_table \\ --from-client-type hive \\ --from-schema default \\ --from-table my_table \\ --to-schema default \\ --to-table my_table . The command will generate two notable outputs: . | For each partition, the command will create a symlink file: | . ➜ aws s3 ls s3://my-bucket/my-repo-prefix/my_table/ --recursive 2021-11-23 17:46:29 0 my-repo-prefix/my_table/symlinks/example/main/my_table/year=2021/month=11/symlink.txt 2021-11-23 17:46:29 60 my-repo-prefix/my_table/symlinks/example/main/my_table/year=2021/month=12/symlink.txt 2021-11-23 17:46:30 60 my-repo-prefix/my_table/symlinks/example/main/my_table/year=2022/month=1/symlink.txt . An example content of a symlink file, where each line represents a single object of the specific partition: . s3://my-bucket/my-repo-prefix/5bdc62da516944b49889770d98274227 s3://my-bucket/my-repo-prefix/64262fbf3d6347a79ead641d2b2baee6 s3://my-bucket/my-repo-prefix/64486c8de6484de69f12d7d26804c93e s3://my-bucket/my-repo-prefix/b0165d5c5b13473d8a0f460eece9eb26 . | A glue table pointing to the symlink directories structure: | . aws glue get-table --name my_table --database-name default { \"Table\": { \"Name\": \"my_table\", \"DatabaseName\": \"default\", \"Owner\": \"anonymous\", \"CreateTime\": \"2021-11-23T17:46:30+02:00\", \"UpdateTime\": \"2021-11-23T17:46:30+02:00\", \"LastAccessTime\": \"1970-01-01T02:00:00+02:00\", \"Retention\": 0, \"StorageDescriptor\": { \"Columns\": [ { \"Name\": \"id\", \"Type\": \"bigint\", \"Comment\": \"\" }, { \"Name\": \"key\", \"Type\": \"string\", \"Comment\": \"\" } ], \"Location\": \"s3://my-bucket/my-repo-prefix/symlinks/example/main/my_table\", \"InputFormat\": \"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat\", \"OutputFormat\": \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\", \"Compressed\": false, \"NumberOfBuckets\": -1, \"SerdeInfo\": { \"Name\": \"default\", \"SerializationLibrary\": \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\", \"Parameters\": { \"serialization.format\": \"1\" } }, \"StoredAsSubDirectories\": false }, \"PartitionKeys\": [ { \"Name\": \"year\", \"Type\": \"int\", \"Comment\": \"\" }, { \"Name\": \"month\", \"Type\": \"int\", \"Comment\": \"\" } ], \"ViewOriginalText\": \"\", \"ViewExpandedText\": \"\", \"TableType\": \"EXTERNAL_TABLE\", \"Parameters\": { \"EXTERNAL\": \"TRUE\", \"bucketing_version\": \"2\", \"transient_lastDdlTime\": \"1637681750\" }, \"CreatedBy\": \"arn:aws:iam::************:user/********\", \"IsRegisteredWithLakeFormation\": false, \"CatalogId\": \"*********\" } } . You can now safely use Athena to query my_table. ",
    "url": "/v0.98/integrations/athena.html#using-lakefs-with-amazon-athena",
    
    "relUrl": "/integrations/athena.html#using-lakefs-with-amazon-athena"
  },"31": {
    "doc": "Amazon Athena",
    "title": "Amazon Athena",
    "content": " ",
    "url": "/v0.98/integrations/athena.html",
    
    "relUrl": "/integrations/athena.html"
  },"32": {
    "doc": "Auditing",
    "title": "Auditing",
    "content": "lakeFS Cloud . Auditing is only available for lakeFS Cloud. The lakeFS audit log allows you to view all relevant user action information in a clear and organized table, including when the action was performed, by whom, and what it was they did. This can be useful for several purposes, including: . | Compliance - Audit logs can be used to show what data users accessed, as well as any changes they made to user management. | Troubleshooting - If something changes on your underlying object store that you weren’t expecting, such as a big file suddenly breaking into thousands of smaller files, you can use the audit log to find out what action led to this change. | . ",
    "url": "/v0.98/cloud/auditing.html",
    
    "relUrl": "/cloud/auditing.html"
  },"33": {
    "doc": "Auditing",
    "title": "Audit Fields",
    "content": "The audit log includes the following fields: . | Time - Time of action | User - Name of the user who performed the action | Region - Region of the lakeFS installation where the action was performed | Status - Status code returned for the action . | 2xx - Successful | 3xx - Redirection | 4xx - Client error | 5xx - Server error | . | Action - Specific lakeFS action (such as Login, Commit, ListRepositories, CreateUser, etc…) | Resource - Full URL of the command (e.g for a commit on branch main we would see the Action Commit and the resource /api/v1/repositories/e2e-monitoring/branches/main/commits) | . ",
    "url": "/v0.98/cloud/auditing.html#audit-fields",
    
    "relUrl": "/cloud/auditing.html#audit-fields"
  },"34": {
    "doc": "Auditing",
    "title": "Filtering",
    "content": "Filtering is available using the filter bar. The filter bar works with a simple query language. The table fields can be filtered by the following operators . | User - =,!= | Region - =,!= | Status(Number) - &lt;,&gt;,&lt;=,&gt;=,=,!= | Action - =,!= | Resource - =,!= | . ",
    "url": "/v0.98/cloud/auditing.html#filtering",
    
    "relUrl": "/cloud/auditing.html#filtering"
  },"35": {
    "doc": "Authentication",
    "title": "Authentication",
    "content": " ",
    "url": "/v0.98/reference/authentication.html",
    
    "relUrl": "/reference/authentication.html"
  },"36": {
    "doc": "Authentication",
    "title": "Table of contents",
    "content": ". | Authentication . | User Authentication . | Built-in database | Remote Authenticator Service | LDAP server | . | API Server Authentication | S3 Gateway Authentication | . | OIDC support . | Configuring lakeFS server for OIDC | . | User permissions . | Using a different claim name | . | . ",
    "url": "/v0.98/reference/authentication.html#table-of-contents",
    
    "relUrl": "/reference/authentication.html#table-of-contents"
  },"37": {
    "doc": "Authentication",
    "title": "Authentication",
    "content": "User Authentication . lakeFS authenticates users from a built-in authentication database. Built-in database . The built-in authentication database is always present and active. You can use the Web UI at Administration / Users to create users. Users have an access key AKIA... and an associated secret access key. These credentials are valid for logging into the Web UI or authenticating programmatic requests to the API Server or the S3 Gateway. Remote Authenticator Service . lakeFS server supports external authentication, the feature can be configured by providing an HTTP endpoint to an external authentication service. This integration can be especially useful if you already have an existing authentication system in place, as it allows you to reuse that system instead of maintaining a new one. To configure a Remote Authenticator see the configuration fields. LDAP server . Note This feature is deprecated (learn more). For single sign-on with lakeFS, try lakeFS cloud . Configure lakeFS to authenticate users on an LDAP server. Once configured, users can additionally log into lakeFS using their credentials LDAP. These users may then generate an access key and a secret access key on the Web UI at Administration / My Credentials. lakeFS generates an internal user once logged in via the LDAP server. Adding this internal user to a group allows assigning them a different policy. Configure the LDAP server using the configuration fields: . | server_endpoint: the ldaps: (or ldap:) URL of the LDAP server. | bind_dn, bind_password: Credentials for lakeFS to use to query the LDAP server for users. They must identify a user with Basic Authentication, and are used to convert a user ID attribute to a full user DN. | default_user_group: A group to add users the first time they log in using LDAP. Typically “Viewers” or “Developers”. Once logged in, LDAP users may be added as normal to any other group. | username_attribute: Attribute on LDAP user to identify user when logging in. Typically “uid” or “cn”. | user_base_dn: DN of root of DAP tree containing users, e.g. ou=Users,dc=treeverse,dc=io. | user_filter: An additional filter for users allowed to login, e.g. (objectClass=person). | . LDAP users log in using the following flow: . | Bind the lakeFS control connection to server_endpoint using bind_dn, bind_password. | Receive an LDAP user-ID (e.g. “joebloggs”) and a password entered on the Web UI login page. | Attempt to log in as internally-defined users; fail. | Search the LDAP server using the control connection for the user: out of all users under user_base_dn that satisfy user_filter, there must be a single user whose username_attribute was specified by the user. Get their DN. In our example, this may be uid=joebloggs,ou=Users,dc=treeverse,dc=io (this entry must have objectClass: person because of user_filter). | Attempt to bind the received DN on the LDAP server using the password. | On success, the user is authenticated. | Create a new internal user with that DN if needed. When creating a user, add them to the internal group named default_user_group. | . API Server Authentication . Authenticating against the API server is done using a key-pair, passed via Basic Access Authentication. All HTTP requests must carry an Authorization header with the following structure: . Authorization: Basic &lt;base64 encoded access_key_id:secret_access_key&gt; . For example, assuming my access_key_id is my_access_key_id and my secret_access_key is my_secret_access_key, we’d send the following header with every request: . Authorization: Basic bXlfYWNjZXNzX2tleV9pZDpteV9hY2Nlc3Nfc2VjcmV0X2tleQ== . S3 Gateway Authentication . To provide API compatibility with Amazon S3, authentication with the S3 Gateway supports both SIGv2 and SIGv4. Clients such as the AWS SDK that implement these authentication methods should work without modification. See this example for authenticating with the AWS CLI. ",
    "url": "/v0.98/reference/authentication.html",
    
    "relUrl": "/reference/authentication.html"
  },"38": {
    "doc": "Authentication",
    "title": "OIDC support",
    "content": "Note This feature is deprecated. For single sign-on with lakeFS, try lakeFS cloud . OpenID Connect (OIDC) is a simple identity layer on top of the OAuth 2.0 protocol. You can configure lakeFS to enable OIDC to manage your lakeFS users externally. Essentially, once configured, this enables you the benefit of OpenID connect, such as a single sign-on (SSO), etc. Configuring lakeFS server for OIDC . To support OIDC, add the following to your lakeFS configuration: . auth: oidc: enabled: true client_id: example-client-id client_secret: exampleSecretValue callback_base_url: https://lakefs.example.com # The scheme, domain (and port) of your lakeFS installation url: https://my-account.oidc-provider-example.com default_initial_groups: [\"Developers\"] friendly_name_claim_name: name # Optional: use the value from this claim as the user's display name . Your login page will now include a link to sign in using the OIDC provider. When a user first logs in through the provider, a corresponding user is created in lakeFS. Notes . | As always, you may choose to provide these configurations using environment variables. | You may already have other configuration values under the auth key, so make sure you combine them correctly. | . ",
    "url": "/v0.98/reference/authentication.html#oidc-support",
    
    "relUrl": "/reference/authentication.html#oidc-support"
  },"39": {
    "doc": "Authentication",
    "title": "User permissions",
    "content": "Authorization is managed via lakeFS groups and policies. By default, an externally managed user is assigned to the lakeFS groups configured in the default_initial_groups property above. For a user to be assigned to other groups, add the initial_groups claim to their ID token claims. The claim should contain a comma-separated list of group names. Once the user has been created, you can manage their permissions from the Administration pages in the lakeFS UI or using lakectl. Using a different claim name . To supply the initial groups using another claim from your ID token, you can use the auth.oidc.initial_groups_claim_name lakeFS configuration. For example, to take the initial groups from the roles claim, add: . auth: oidc: # ... Other OIDC configurations initial_groups_claim_name: roles . ",
    "url": "/v0.98/reference/authentication.html#user-permissions",
    
    "relUrl": "/reference/authentication.html#user-permissions"
  },"40": {
    "doc": "AWS",
    "title": "Deploy lakeFS on AWS",
    "content": "⏰ Expected deployment time: 25 min . ",
    "url": "/v0.98/deploy/aws.html#deploy-lakefs-on-aws",
    
    "relUrl": "/deploy/aws.html#deploy-lakefs-on-aws"
  },"41": {
    "doc": "AWS",
    "title": "Table of contents",
    "content": ". | Grant lakeFS permissions to DynamoDB | Run the lakeFS server | Prepare your S3 bucket . | Alternative: use an AWS user | . | Create the admin user | Create your first repository | . ",
    "url": "/v0.98/deploy/aws.html#table-of-contents",
    
    "relUrl": "/deploy/aws.html#table-of-contents"
  },"42": {
    "doc": "AWS",
    "title": "Grant lakeFS permissions to DynamoDB",
    "content": "By default, lakeFS will create the required DynamoDB table if it does not already exist. You’ll have to give the IAM role used by lakeFS the following permissions: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"ListAndDescribe\", \"Effect\": \"Allow\", \"Action\": [ \"dynamodb:List*\", \"dynamodb:DescribeReservedCapacity*\", \"dynamodb:DescribeLimits\", \"dynamodb:DescribeTimeToLive\" ], \"Resource\": \"*\" }, { \"Sid\": \"kvstore\", \"Effect\": \"Allow\", \"Action\": [ \"dynamodb:BatchGet*\", \"dynamodb:DescribeTable\", \"dynamodb:Get*\", \"dynamodb:Query\", \"dynamodb:Scan\", \"dynamodb:BatchWrite*\", \"dynamodb:CreateTable\", \"dynamodb:Delete*\", \"dynamodb:Update*\", \"dynamodb:PutItem\" ], \"Resource\": \"arn:aws:dynamodb:*:*:table/kvstore\" } ] } . 💡 You can also use lakeFS with PostgreSQL instead of DynamoDB! See the configuration reference for more information. ",
    "url": "/v0.98/deploy/aws.html#grant-lakefs-permissions-to-dynamodb",
    
    "relUrl": "/deploy/aws.html#grant-lakefs-permissions-to-dynamodb"
  },"43": {
    "doc": "AWS",
    "title": "Run the lakeFS server",
    "content": ". | EC2 | EKS | . Connect to your EC2 instance using SSH: . | Create a config.yaml on your EC2 instance, with the following parameters: . --- database: type: \"dynamodb\" auth: encrypt: # replace this with a randomly-generated string. Make sure to keep it safe! secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: s3 . | Download the binary to the EC2 instance. | Run the lakefs binary on the EC2 instance: . lakefs --config config.yaml run . | . Note: It’s preferable to run the binary as a service using systemd or your operating system’s facilities. Advanced: Deploying lakeFS behind an AWS Application Load Balancer . | Your security groups should allow the load balancer to access the lakeFS server. | Create a target group with a listener for port 8000. | Setup TLS termination using the domain names you wish to use (e.g., lakefs.example.com and potentially s3.lakefs.example.com, *.s3.lakefs.example.com if using virtual-host addressing). | Configure the health-check to use the exposed /_health URL | . You can install lakeFS on Kubernetes using a Helm chart. To install lakeFS with Helm: . | Copy the Helm values file relevant for S3: . secrets: # replace this with a randomly-generated string authEncryptSecretKey: [ENCRYPTION_SECRET_KEY] lakefsConfig: | database: type: dynamodb blockstore: type: s3 . | Fill in the missing values and save the file as conf-values.yaml. For more configuration options, see our Helm chart README. The lakefsConfig parameter is the lakeFS configuration documented here but without sensitive information. Sensitive information like databaseConnectionString is given through separate parameters, and the chart will inject it into Kubernetes secrets. | In the directory where you created conf-values.yaml, run the following commands: . # Add the lakeFS repository helm repo add lakefs https://charts.lakefs.io # Deploy lakeFS helm install my-lakefs lakefs/lakefs -f conf-values.yaml . my-lakefs is the Helm Release name. | . ⚠️ Make sure the Kubernetes nodes have access to all buckets/containers with which you intend to use with lakeFS. If you can’t provide such access, configure lakeFS with an AWS key-pair. Load balancing . To configure a load balancer to direct requests to the lakeFS servers you can use the LoadBalancer Service type or a Kubernetes Ingress. By default, lakeFS operates on port 8000 and exposes a /_health endpoint that you can use for health checks. 💡 The NGINX Ingress Controller by default limits the client body size to 1 MiB. Some clients use bigger chunks to upload objects - for example, multipart upload to lakeFS using the S3 Gateway or a simple PUT request using the OpenAPI Server. Checkout Nginx documentation for increasing the limit, or an example of Nginx configuration with MinIO. ",
    "url": "/v0.98/deploy/aws.html#run-the-lakefs-server",
    
    "relUrl": "/deploy/aws.html#run-the-lakefs-server"
  },"44": {
    "doc": "AWS",
    "title": "Prepare your S3 bucket",
    "content": ". | From the S3 Administration console, choose Create Bucket. | Use the following as your bucket policy, filling in the placeholders: . | Standard Permissions | Minimal Permissions (Advanced) | . { \"Id\": \"lakeFSPolicy\", \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"lakeFSObjects\", \"Action\": [ \"s3:GetObject\", \"s3:PutObject\", \"s3:AbortMultipartUpload\", \"s3:ListMultipartUploadParts\" ], \"Effect\": \"Allow\", \"Resource\": [\"arn:aws:s3:::[BUCKET_NAME_AND_PREFIX]/*\"], \"Principal\": { \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"] } }, { \"Sid\": \"lakeFSBucket\", \"Action\": [ \"s3:ListBucket\", \"s3:GetBucketLocation\", \"s3:ListBucketMultipartUploads\" ], \"Effect\": \"Allow\", \"Resource\": [\"arn:aws:s3:::[BUCKET]\"], \"Principal\": { \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"] } } ] } . | Replace [BUCKET_NAME], [ACCOUNT_ID] and [IAM_ROLE] with values relevant to your environment. | [BUCKET_NAME_AND_PREFIX] can be the bucket name. If you want to minimize the bucket policy permissions, use the bucket name together with a prefix (e.g. example-bucket/a/b/c). This way, lakeFS will be able to create repositories only under this specific path (see: Storage Namespace). | lakeFS will try to assume the role [IAM_ROLE]. | . This permission is useful if you are using the lakeFS Hadoop FileSystem Spark integration. Since this FileSystem performs many operations directly on the storage, lakeFS requires less permissive permissions, resulting in increased security. lakeFS always requires permissions to access the _lakefs prefix under your storage namespace, in which metadata is stored (learn more). By setting this policy you’ll be able to perform only metadata operations through lakeFS, meaning that you’ll not be able to use lakeFS to upload or download objects. Specifically you won’t be able to: . | Upload objects using the lakeFS GUI | Upload objects through Spark using the S3 gateway | Run lakectl fs commands (unless using the --direct flag) | . { \"Id\": \"[POLICY_ID]\", \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"lakeFSObjects\", \"Action\": [ \"s3:GetObject\", \"s3:PutObject\" ], \"Effect\": \"Allow\", \"Resource\": [ \"arn:aws:s3:::[STORAGE_NAMESPACE]/_lakefs/*\" ], \"Principal\": { \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"] } }, { \"Sid\": \"lakeFSBucket\", \"Action\": [ \"s3:ListBucket\", \"s3:GetBucketLocation\" ], \"Effect\": \"Allow\", \"Resource\": [\"arn:aws:s3:::[BUCKET]\"], \"Principal\": { \"AWS\": [\"arn:aws:iam::[ACCOUNT_ID]:role/[IAM_ROLE]\"] } } ] } . | . Alternative: use an AWS user . lakeFS can authenticate with your AWS account using an AWS user, using an access key and secret. To allow this, change the policy’s Principal accordingly: . \"Principal\": { \"AWS\": [\"arn:aws:iam::&lt;ACCOUNT_ID&gt;:user/&lt;IAM_USER&gt;\"] } . ",
    "url": "/v0.98/deploy/aws.html#prepare-your-s3-bucket",
    
    "relUrl": "/deploy/aws.html#prepare-your-s3-bucket"
  },"45": {
    "doc": "AWS",
    "title": "Create the admin user",
    "content": "When you first open the lakeFS UI, you will be asked to create an initial admin user. | open http://&lt;lakefs-host&gt;/ in your browser. If you haven’t set up a load balancer, this will likely be http://&lt;instance ip address&gt;:8000/ | On first use, you’ll be redirected to the setup page: . | Follow the steps to create an initial administrator user. Save the credentials you’ve received somewhere safe, you won’t be able to see them again! . | Follow the link and go to the login screen. Use the credentials from the previous step to log in. | . ",
    "url": "/v0.98/deploy/aws.html#create-the-admin-user",
    
    "relUrl": "/deploy/aws.html#create-the-admin-user"
  },"46": {
    "doc": "AWS",
    "title": "Create your first repository",
    "content": ". | Use the credentials from the previous step to log in | Click Create Repository and choose Blank Repository. | Under Storage Namespace, enter a path to your desired location on the object store. This is where data written to this repository will be stored. | Click Create Repository | You should now have a configured repository, ready to use! . | . Congratulations! Your environment is now ready 🤩 . ",
    "url": "/v0.98/deploy/aws.html#create-your-first-repository",
    
    "relUrl": "/deploy/aws.html#create-your-first-repository"
  },"47": {
    "doc": "AWS",
    "title": "AWS",
    "content": " ",
    "url": "/v0.98/deploy/aws.html",
    
    "relUrl": "/deploy/aws.html"
  },"48": {
    "doc": "AWS CLI",
    "title": "Using lakeFS with AWS CLI",
    "content": "The AWS Command Line Interface (CLI) is a unified tool for managing your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts. You can use the file commands for S3 to access lakeFS. ",
    "url": "/v0.98/integrations/aws_cli.html#using-lakefs-with-aws-cli",
    
    "relUrl": "/integrations/aws_cli.html#using-lakefs-with-aws-cli"
  },"49": {
    "doc": "AWS CLI",
    "title": "Table of contents",
    "content": ". | Configuration | Path convention | Usage | Examples . | List directory | Copy from lakeFS to lakeFS | Copy from lakeFS to a local path | Copy from a local path to lakeFS | Delete file | Delete directory | . | Adding an alias | . ",
    "url": "/v0.98/integrations/aws_cli.html#table-of-contents",
    
    "relUrl": "/integrations/aws_cli.html#table-of-contents"
  },"50": {
    "doc": "AWS CLI",
    "title": "Configuration",
    "content": "You would like to configure an AWS profile for lakeFS. To configure the lakeFS credentials, run: . aws configure --profile lakefs . You will be prompted to enter AWS Access Key ID and AWS Secret Access Key. It should look like this: . aws configure --profile lakefs # output: # AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE # AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # Default region name [None]: # Default output format [None]: . ",
    "url": "/v0.98/integrations/aws_cli.html#configuration",
    
    "relUrl": "/integrations/aws_cli.html#configuration"
  },"51": {
    "doc": "AWS CLI",
    "title": "Path convention",
    "content": "When accessing objects in S3, you will need to use the lakeFS path convention: s3://[REPOSITORY]/[BRANCH]/PATH/TO/OBJECT . ",
    "url": "/v0.98/integrations/aws_cli.html#path-convention",
    
    "relUrl": "/integrations/aws_cli.html#path-convention"
  },"52": {
    "doc": "AWS CLI",
    "title": "Usage",
    "content": "After configuring the credentials, this is what a command should look: . aws s3 --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ ls s3://example-repo/main/example-directory . You can use an alias to make it shorter and more convenient. ",
    "url": "/v0.98/integrations/aws_cli.html#usage",
    
    "relUrl": "/integrations/aws_cli.html#usage"
  },"53": {
    "doc": "AWS CLI",
    "title": "Examples",
    "content": "List directory . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 ls s3://example-repo/main/example-directory . Copy from lakeFS to lakeFS . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 cp s3://example-repo/main/example-file-1 s3://example-repo/main/example-file-2 . Copy from lakeFS to a local path . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 cp s3://example-repo/main/example-file-1 /path/to/local/file . Copy from a local path to lakeFS . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 cp /path/to/local/file s3://example-repo/main/example-file-1 . Delete file . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 rm s3://example-repo/main/example-directory/example-file . Delete directory . aws --profile lakefs \\ --endpoint-url https://lakefs.example.com \\ s3 rm s3://example-repo/main/example-directory/ --recursive . ",
    "url": "/v0.98/integrations/aws_cli.html#examples",
    
    "relUrl": "/integrations/aws_cli.html#examples"
  },"54": {
    "doc": "AWS CLI",
    "title": "Adding an alias",
    "content": "To make the command shorter and more convenient, you can create an alias: . alias awslfs='aws --endpoint https://lakefs.example.com --profile lakefs' . Now, the ls command using the alias will be as follows: . awslfs s3 ls s3://example-repo/main/example-directory . ",
    "url": "/v0.98/integrations/aws_cli.html#adding-an-alias",
    
    "relUrl": "/integrations/aws_cli.html#adding-an-alias"
  },"55": {
    "doc": "AWS CLI",
    "title": "AWS CLI",
    "content": " ",
    "url": "/v0.98/integrations/aws_cli.html",
    
    "relUrl": "/integrations/aws_cli.html"
  },"56": {
    "doc": "Azure",
    "title": "Deploy lakeFS on Azure",
    "content": "⏰ Expected deployment time: 25 min . ",
    "url": "/v0.98/deploy/azure.html#deploy-lakefs-on-azure",
    
    "relUrl": "/deploy/azure.html#deploy-lakefs-on-azure"
  },"57": {
    "doc": "Azure",
    "title": "Table of contents",
    "content": ". | Supported storage types | Authentication methods . | Storage Account Credentials | Identity Based Credentials | How to Create Service Principal for Resource Group | . | Create a database | Run the lakeFS server . | Storage account access | . | Create the admin user | Create your first repository | . ",
    "url": "/v0.98/deploy/azure.html#table-of-contents",
    
    "relUrl": "/deploy/azure.html#table-of-contents"
  },"58": {
    "doc": "Azure",
    "title": "Supported storage types",
    "content": "lakeFS supports the following storage account types: . | Azure Blob Storage | Azure Data Lake Storage Gen2 (HNS) | . Other Azure storage types have not been tested, and specifically Data Lake Storage Gen1 is not supported. For more information see: Introduction to Azure Storage . ",
    "url": "/v0.98/deploy/azure.html#supported-storage-types",
    
    "relUrl": "/deploy/azure.html#supported-storage-types"
  },"59": {
    "doc": "Azure",
    "title": "Authentication methods",
    "content": "lakeFS supports two ways to authenticate with Azure: . | Storage account based authentication | Identity based authentication | . Storage Account Credentials . Storage account credentials can be set directly in the lakeFS configuration using the following parameters: . | blockstore.azure.storage_account | blockstore.azure.storage_access_key | . Please note that using this authentication method limits lakeFS to the scope of the given storage account. Specifically, the following operations will not work: . | Import of data from different storage accounts | Copy/Read/Write of data that was imported from a different storage account | Create pre-signed URL for data that was imported from a different storage account | . Identity Based Credentials . lakeFS uses environment variables to determine credentials to use for authentication. the following authentication methods are supported: . | Managed Service Identity (MSI) | Service Principal RBAC | Azure CLI | . For deployments inside the Azure ecosystem it is recommended to use a managed identity. More information on authentication methods and environment variables can be found here . How to Create Service Principal for Resource Group . It is recommended to create a resource group that consists of all the resources lakeFS should have access to. Using a resource group will allow dynamic removal/addition of services from the group, effectively providing/preventing access for lakeFS to these resources without requiring any changes in configuration in lakeFS or providing lakeFS with any additional credentials. The minimal role required for the service principal is “Storage Blob Data Contributor” . The following Azure CLI command creates a service principal for a resource group called “lakeFS” with permission to access (read/write/delete) Blob Storage resources in the resource group and with an expiry of 5 years . az ad sp create-for-rbac \\ --role \"Storage Blob Data Contributor\" \\ --scopes /subscriptions/XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX/resourceGroups/lakeFS --years 5 Creating 'Storage Blob Data Contributor' role assignment under scope '/subscriptions/947382ea-681a-4541-99ab-b718960c6289/resourceGroups/lakeFS' The output includes credentials that you must protect. Be sure that you do not include these credentials in your code or check the credentials into your source control. For more information, see https://aka.ms/azadsp-cli { \"appId\": \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\", \"displayName\": \"azure-cli-2023-01-30-06-18-30\", \"password\": \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\", \"tenant\": \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\" } . The command output should be used to populate the following environment variables: . AZURE_CLIENT_ID = $appId AZURE_TENANT_ID = $tenant AZURE_CLIENT_SECRET = $password . Note: Service Principal credentials have an expiry date and lakeFS will lose access to resources unless credentials are renewed on time. Note: It is possible to provide both account based credentials and environment variables to lakeFS. In that case - lakeFS will use the account credentials for any access to data located in the given account, and will try to use the identity credentials for any data located outside the given account. ",
    "url": "/v0.98/deploy/azure.html#authentication-methods",
    
    "relUrl": "/deploy/azure.html#authentication-methods"
  },"60": {
    "doc": "Azure",
    "title": "Create a database",
    "content": "lakeFS stores metadata in a database for its versioning engine. This is done via a Key-Value interface that can be implemented on any DB engine and lakeFS comes with several built-in driver implementations (You can read more about it here). One of these is a PostgreSQL implementation which can be used in the Azure ecosystem. We will show you how to create a database on Azure Database, but you can use any PostgreSQL database as long as it’s accessible by your lakeFS installation. If you already have a database, take note of the connection string and skip to the next step . | Follow the official Azure documentation on how to create a PostgreSQL instance and connect to it. Make sure that you’re using PostgreSQL version &gt;= 11. | Once your Azure Database for PostgreSQL server is set up and the server is in the Available state, take note of the endpoint and username. | Make sure your Access control roles allow you to connect to the database instance. | . ",
    "url": "/v0.98/deploy/azure.html#create-a-database",
    
    "relUrl": "/deploy/azure.html#create-a-database"
  },"61": {
    "doc": "Azure",
    "title": "Run the lakeFS server",
    "content": "Storage account access . | Azure VM | Docker | AKS | . Connect to your VM instance using SSH: . | Create a config.yaml on your VM, with the following parameters: . --- database: type: \"postgres\" postgres: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: # replace this with a randomly-generated string. Make sure to keep it safe! secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: azure azure: . | Download the binary to the VM. | Run the lakefs binary: . lakefs --config config.yaml run . | . Note: It’s preferable to run the binary as a service using systemd or your operating system’s facilities. To support container-based environments, you can configure lakeFS using environment variables. Here is a docker run command to demonstrate starting lakeFS using Docker: . docker run \\ --name lakefs \\ -p 8000:8000 \\ -e LAKEFS_DATABASE_TYPE=\"postgres\" \\ -e LAKEFS_DATABASE_POSTGRES_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\ -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\ -e LAKEFS_BLOCKSTORE_TYPE=\"azure\" \\ -e LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCOUNT=\"[YOUR_STORAGE_ACCOUNT]\" \\ -e LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCESS_KEY=\"[YOUR_ACCESS_KEY]\" \\ treeverse/lakefs:latest run . See the reference for a complete list of environment variables. You can install lakeFS on Kubernetes using a Helm chart. To install lakeFS with Helm: . | Copy the Helm values file relevant for Azure Blob: . secrets: # replace this with the connection string of the database you created in a previous step: databaseConnectionString: [DATABASE_CONNECTION_STRING] # replace this with a randomly-generated string authEncryptSecretKey: [ENCRYPTION_SECRET_KEY] lakefsConfig: | blockstore: type: azure azure: # If you chose to authenticate via access key, unmark the following rows and insert the values from the previous step # storage_account: [your storage account] # storage_access_key: [your access key] . | Fill in the missing values and save the file as conf-values.yaml. For more configuration options, see our Helm chart README. The lakefsConfig parameter is the lakeFS configuration documented here but without sensitive information. Sensitive information like databaseConnectionString is given through separate parameters, and the chart will inject it into Kubernetes secrets. | In the directory where you created conf-values.yaml, run the following commands: . # Add the lakeFS repository helm repo add lakefs https://charts.lakefs.io # Deploy lakeFS helm install my-lakefs lakefs/lakefs -f conf-values.yaml . my-lakefs is the Helm Release name. | . ",
    "url": "/v0.98/deploy/azure.html#run-the-lakefs-server",
    
    "relUrl": "/deploy/azure.html#run-the-lakefs-server"
  },"62": {
    "doc": "Azure",
    "title": "Load balancing",
    "content": "To configure a load balancer to direct requests to the lakeFS servers you can use the LoadBalancer Service type or a Kubernetes Ingress. By default, lakeFS operates on port 8000 and exposes a /_health endpoint that you can use for health checks. 💡 The NGINX Ingress Controller by default limits the client body size to 1 MiB. Some clients use bigger chunks to upload objects - for example, multipart upload to lakeFS using the S3-compatible Gateway or a simple PUT request using the OpenAPI Server. Checkout Nginx documentation for increasing the limit, or an example of Nginx configuration with MinIO. ",
    "url": "/v0.98/deploy/azure.html#load-balancing",
    
    "relUrl": "/deploy/azure.html#load-balancing"
  },"63": {
    "doc": "Azure",
    "title": "Create the admin user",
    "content": "When you first open the lakeFS UI, you will be asked to create an initial admin user. | open http://&lt;lakefs-host&gt;/ in your browser. If you haven’t set up a load balancer, this will likely be http://&lt;instance ip address&gt;:8000/ | On first use, you’ll be redirected to the setup page: . | Follow the steps to create an initial administrator user. Save the credentials you’ve received somewhere safe, you won’t be able to see them again! . | Follow the link and go to the login screen. Use the credentials from the previous step to log in. | . ",
    "url": "/v0.98/deploy/azure.html#create-the-admin-user",
    
    "relUrl": "/deploy/azure.html#create-the-admin-user"
  },"64": {
    "doc": "Azure",
    "title": "Create your first repository",
    "content": ". | Use the credentials from the previous step to log in | Click Create Repository and choose Blank Repository. | Under Storage Namespace, enter a path to your desired location on the object store. This is where data written to this repository will be stored. | Click Create Repository | You should now have a configured repository, ready to use! . | . Congratulations! Your environment is now ready 🤩 . ",
    "url": "/v0.98/deploy/azure.html#create-your-first-repository",
    
    "relUrl": "/deploy/azure.html#create-your-first-repository"
  },"65": {
    "doc": "Azure",
    "title": "Azure",
    "content": " ",
    "url": "/v0.98/deploy/azure.html",
    
    "relUrl": "/deploy/azure.html"
  },"66": {
    "doc": "3️⃣ Create a branch",
    "title": "Create a Branch 🪓",
    "content": "lakeFS uses branches in a similar way to git. It’s a great way to isolate changes until, or if, we are ready to re-integrate them. lakeFS uses a copy-on-write technique which means that it’s very efficient to create branches of your data. Having seen the lakes data in the previous step we’re now going to create a new dataset to hold data only for lakes in Denmark. Why? Well, because :) . The first thing we’ll do is create a branch for us to do this development against. We’ll use the lakectl tool to create the branch. In a new terminal window run the following: . docker exec lakefs \\ lakectl branch create \\ lakefs://quickstart/denmark-lakes \\ --source lakefs://quickstart/main . You should get a confirmation message like this: . Source ref: lakefs://quickstart/main created branch 'denmark-lakes' 3384cd7cdc4a2cd5eb6249b52f0a709b49081668bb1574ce8f1ef2d956646816 . ",
    "url": "/v0.98/quickstart/branch.html#create-a-branch-",
    
    "relUrl": "/quickstart/branch.html#create-a-branch-"
  },"67": {
    "doc": "3️⃣ Create a branch",
    "title": "Transforming the Data",
    "content": "Now we’ll make a change to the data. lakeFS has several native clients, as well as an S3-compatible endpoint. This means that anything that can use S3 will work with lakeFS. Pretty neat. We’re going to use DuckDB, but unlike in the previous step where it was run within the lakeFS web page, we’ve got a standalone container running. Setting up DuckDB . Run the following in a terminal window to launch the DuckDB CLI: . docker exec -it duckdb duckdb . The first thing to do is configure the S3 connection so that DuckDB can access lakeFS, as well as tell DuckDB to report back how many rows are changed by the query we’ll soon be executing. Run this from the DuckDB prompt: . SET s3_endpoint='lakefs:8000'; SET s3_access_key_id='AKIA-EXAMPLE-KEY'; SET s3_secret_access_key='EXAMPLE-SECRET'; SET s3_url_style='path'; SET s3_region='us-east-1'; SET s3_use_ssl=false; .changes on . Now we’ll load the lakes data into a DuckDB table so that we can manipulate it: . CREATE TABLE lakes AS SELECT * FROM READ_PARQUET('s3://quickstart/denmark-lakes/lakes.parquet'); . Just to check that it’s the same we saw before we’re run the same query: . SELECT country, COUNT(*) FROM lakes GROUP BY country ORDER BY COUNT(*) DESC LIMIT 5; . ┌──────────────────────────┬──────────────┐ │ Country │ count_star() │ │ varchar │ int64 │ ├──────────────────────────┼──────────────┤ │ Canada │ 83819 │ │ United States of America │ 6175 │ │ Russia │ 2524 │ │ Denmark │ 1677 │ │ China │ 966 │ └──────────────────────────┴──────────────┘ . Making a Change to the Data . Now we can change our table, which was loaded from the original lakes.parquet, to remove all rows not for Denmark: . DELETE FROM lakes WHERE Country != 'Denmark'; . You’ll see that 98k rows have been deleted: . changes: 98323 total_changes: 198323 . We can verify that it’s worked by reissuing the same query as before: . SELECT country, COUNT(*) FROM lakes GROUP BY country ORDER BY COUNT(*) DESC LIMIT 5; . ┌─────────┬──────────────┐ │ Country │ count_star() │ │ varchar │ int64 │ ├─────────┼──────────────┤ │ Denmark │ 1677 │ └─────────┴──────────────┘ . ",
    "url": "/v0.98/quickstart/branch.html#transforming-the-data",
    
    "relUrl": "/quickstart/branch.html#transforming-the-data"
  },"68": {
    "doc": "3️⃣ Create a branch",
    "title": "Write the Data back to lakeFS",
    "content": "The changes so far have only been to DuckDB’s copy of the data. Let’s now push it back to lakeFS. Note the S3 path is different this time as we’re writing it to the denmark-lakes branch, not main: . COPY lakes TO 's3://quickstart/denmark-lakes/lakes.parquet' (FORMAT 'PARQUET', ALLOW_OVERWRITE TRUE); . ",
    "url": "/v0.98/quickstart/branch.html#write-the-data-back-to-lakefs",
    
    "relUrl": "/quickstart/branch.html#write-the-data-back-to-lakefs"
  },"69": {
    "doc": "3️⃣ Create a branch",
    "title": "Verify that the Data’s Changed on the Branch",
    "content": "Let’s just confirm for ourselves that the parquet file itself has the new data. We’ll drop the lakes table just to be sure, and then query the parquet file directly: . DROP TABLE lakes; SELECT country, COUNT(*) FROM READ_PARQUET('s3://quickstart/denmark-lakes/lakes.parquet') GROUP BY country ORDER BY COUNT(*) DESC LIMIT 5; . ┌─────────┬──────────────┐ │ Country │ count_star() │ │ varchar │ int64 │ ├─────────┼──────────────┤ │ Denmark │ 1677 │ └─────────┴──────────────┘ . ",
    "url": "/v0.98/quickstart/branch.html#verify-that-the-datas-changed-on-the-branch",
    
    "relUrl": "/quickstart/branch.html#verify-that-the-datas-changed-on-the-branch"
  },"70": {
    "doc": "3️⃣ Create a branch",
    "title": "What about the data in main?",
    "content": "So we’ve changed the data in our denmark-lakes branch, deleting swathes of the dataset. What’s this done to our original data in the main branch? Absolutely nothing! See for yourself by returning to the lakeFS object view and re-running the same query: . SELECT country, COUNT(*) FROM READ_PARQUET(LAKEFS_OBJECT('quickstart', 'main', 'lakes.parquet')) GROUP BY country ORDER BY COUNT(*) DESC LIMIT 5; . In the next step we’ll see how to merge our branch back into main. ",
    "url": "/v0.98/quickstart/branch.html#what-about-the-data-in-main",
    
    "relUrl": "/quickstart/branch.html#what-about-the-data-in-main"
  },"71": {
    "doc": "3️⃣ Create a branch",
    "title": "3️⃣ Create a branch",
    "content": " ",
    "url": "/v0.98/quickstart/branch.html",
    
    "relUrl": "/quickstart/branch.html"
  },"72": {
    "doc": "During Deployment",
    "title": "During Deployment",
    "content": "Every day we introduce new data to the lake. And even if the code and infra doesn’t change, the data might, and those changes introduce potential quality issues. This is one of the complexities of a data product; the data we consume changes over the course of a month, a week, day, hour, or even minute-to-minute. Examples of changes to data that may occur: . | A client-side bug in the data collection of website events | A new Android version that interferes with the collecting events from your App | COVID-19 abrupt impact on consumers’ behavior, and its effect on the accuracy of ML models. | During a change to Salesforce interface, the validation requirement from a certain field had been lost | . lakeFS enable CI/CD-inspired workflows to help validate expectations and assumptions about the data before it goes live in production or lands in the data environment. Example 1: Data update safety . Continuous deployment of existing data we expect to consume, flowing from ingest-pipelines into the lake. We merge data from an ingest branch (“events-data”), which allows us to create tests using data analysis tools or data quality services (e.g. Great Expectations, Monte Carlo) to ensure reliability of the data we merge to the main branch. Since merge is atomic, no performance issue will be introduced by using lakeFS, but your main branch will only include quality data. Each merge to the main branch creates a new commit on the main branch, which serves as a new version of the data. This allows us to easily revert to previous states of the data if a newer change introduces data issues. Example 2: Test - Validate new data . Examples of common validation checks enforced in organizations: . | No user_* columns except under /private/… | Only (*.parquet | *.orc | _delta_log/*.json) files allowed | Under /production, only backward-compatible schema changes are allowed | New tables on main must be registered in our metadata repository first, with owner and SLA | . lakeFS will assist in enforcing best practices by giving you a designated branch to ingest new data (“new-data-1” in the drawing). You may run automated tests to validate predefined best practices as pre-merge hooks. If the validation passes, the new data will be automatically and atomically merged to the main branch. However, if the validation fails, you will be alerted and the new data will not be exposed to consumers. By using this branching model and implementing best practices as pre merge hooks, you ensure the main lake is never compromised. ",
    "url": "/v0.98/understand/data_lifecycle_management/ci.html",
    
    "relUrl": "/understand/data_lifecycle_management/ci.html"
  },"73": {
    "doc": "CI/CD for data lakes",
    "title": "CI/CD for Data",
    "content": " ",
    "url": "/v0.98/use_cases/cicd_for_data.html#cicd-for-data",
    
    "relUrl": "/use_cases/cicd_for_data.html#cicd-for-data"
  },"74": {
    "doc": "CI/CD for data lakes",
    "title": "Why do I need CI/CD?",
    "content": "Data pipelines feed processed data from data lakes to downstream consumers like business dashboards and machine learning models. As more and more organizations rely on data to enable business critical decisions, data reliability and trust are of paramount concern. Thus, it’s important to ensure that production data adheres to the data governance policies of businesses. These data governance requirements can be as simple as a file format validation, schema check, or an exhaustive PII(Personally Identifiable Information) data removal from all of organization’s data. Thus, to ensure the quality and reliability at each stage of the data lifecycle, data quality gates need to be implemented. That is, we need to run Continuous Integration(CI) tests on the data, and only if data governance requirements are met can the data can be promoted to production for business use. Everytime there is an update to production data, the best practice would be to run CI tests and then promote(deploy) the data to production. ",
    "url": "/v0.98/use_cases/cicd_for_data.html#why-do-i-need-cicd",
    
    "relUrl": "/use_cases/cicd_for_data.html#why-do-i-need-cicd"
  },"75": {
    "doc": "CI/CD for data lakes",
    "title": "How do I implement CI/CD for data with lakeFS?",
    "content": "lakeFS makes implementing CI/CD pipelines for data simpler. lakeFS provides a feature called hooks that allow automation of checks and validations of data on lakeFS branches. These checks can be triggered by certain data operations like committing, merging, etc. Functionally, lakeFS hooks are similar to Git Hooks. lakeFS hooks are run remotely on a server, and they are guaranteed to run when the appropriate event is triggered. Here are some examples of the hooks lakeFS supports: . | pre-merge | pre-commit | post-merge | post-commit | pre-create-branch | post-create-branch | . and so on. By leveraging the pre-commit and pre-merge hooks with lakeFS, you can implement CI/CD pipelines on your data lakes. Specific trigger rules, quality checks and the branch on which the rules are to be applied are declared in actions.yaml file. When a specific event (say, pre-merge) occurs, lakeFS runs all the validations declared in actions.yaml file. If validations error out, the merge event is blocked. Here is a sample actions.yaml file that has pre-merge hook configured to allow only parquet and delta lake file formats on main branch. name: ParquetOnlyInProduction description: This webhook ensures that only parquet files are written under production/ on: pre-merge: branches: - main hooks: - id: production_format_validator type: webhook description: Validate file formats properties: url: \"http://lakefs-hooks:5001/webhooks/format\" query_params: allow: [\"parquet\", \"delta_lake\"] prefix: analytics/ . ",
    "url": "/v0.98/use_cases/cicd_for_data.html#how-do-i-implement-cicd-for-data-with-lakefs",
    
    "relUrl": "/use_cases/cicd_for_data.html#how-do-i-implement-cicd-for-data-with-lakefs"
  },"76": {
    "doc": "CI/CD for data lakes",
    "title": "Using hooks as data quality gates",
    "content": "Hooks are run on a remote server that can serve http requests from lakeFS server. lakeFS supports two types of hooks. | webhooks (run remotely on a web server. e.g.: flask server in python) | airflow hooks (a dag of complex data quality checks/tasks that can be run on airflow server) | . In this tutorial, we will show how to use webhooks (python flask webserver) to implement quality gates on your data branches. Specifically, how to configure hooks to allow only parquet and delta lake format files in the main branch. The tutorial uses an existing lakeFS environment (lakeFS running on everything bagel docker container), python flask server running on a docker container, a Jupyter notebook and sample data sets to demonstrate the integration of lakeFS hooks with Apache Spark and Python. To understand how hooks work and how to configure hooks in your production system, refer to the documentation: Hooks. To configure lakeFS hooks with custom quality check rules, refer to the lakefs-hooks repository. Follow the steps below to clone the hooks demo repository and to implement CI/CD for data lakes. ",
    "url": "/v0.98/use_cases/cicd_for_data.html#using-hooks-as-data-quality-gates",
    
    "relUrl": "/use_cases/cicd_for_data.html#using-hooks-as-data-quality-gates"
  },"77": {
    "doc": "CI/CD for data lakes",
    "title": "Implementing CI/CD pipeline with lakeFS",
    "content": "Prerequisites . Before we get started, make sure docker is installed on your machine. ",
    "url": "/v0.98/use_cases/cicd_for_data.html#implementing-cicd-pipeline-with-lakefs",
    
    "relUrl": "/use_cases/cicd_for_data.html#implementing-cicd-pipeline-with-lakefs"
  },"78": {
    "doc": "CI/CD for data lakes",
    "title": "Setup Webhooks server",
    "content": "First, as you know, lakeFS webhooks need a remote server to serve the http requests from lakeFS. So let us set up the python flask server for webhooks. Start with cloning the lakeFS-hooks repository. The following command can be run in your terminal to get the hooks image. git clone https://github.com/treeverse/lakeFS-hooks.git &amp;&amp; cd lakeFS-hooks/ docker build -t &lt;lakefs-hooks-image-name&gt; . Once you have the &lt;lakefs-hooks-image-name&gt; image built, you can add this image in the lakeFS everything bagel docker. This ensures that flask server and lakeFS server are running inside the same docker container, and are part of the same docker network. This is to simplify the setup, and is not mandatory to have both the services running in the same container. ",
    "url": "/v0.98/use_cases/cicd_for_data.html#setup-webhooks-server",
    
    "relUrl": "/use_cases/cicd_for_data.html#setup-webhooks-server"
  },"79": {
    "doc": "CI/CD for data lakes",
    "title": "Setup lakeFS server",
    "content": "To get a local lakeFS instance running on docker, you can use everything bagel docker. Let us start with cloning the lakeFS repository. git clone https://github.com/treeverse/lakeFS.git &amp;&amp; cd lakeFS/deployments/compose . The python flask server image we built in the above section needs to be added to everything bagel docker-compose.yaml file. So add the following contents to the yaml file. For lakeFS instance running on everything bagel, the lakeFS endpoint, access key id and secret key are found in docker-compose.yaml file under lakefs section. lakefs-webhooks: image: &lt;lakefs-hooks-image-name&gt; container_name: lakefs-hooks ports: - 5001:5001 environment: - LAKEFS_SERVER_ADDRESS=&lt;lakefs_server_endpoint&gt; - LAKEFS_ACCESS_KEY_ID=&lt;lakefs_access_key_id&gt; - LAKEFS_SECRET_ACCESS_KEY=&lt;lakefs_secret_key&gt; . Start the docker container to run lakeFS server and hooks server: docker compose up -d . Next step is to configure the custom logic for your hooks server and configuring hooks in lakeFS. ",
    "url": "/v0.98/use_cases/cicd_for_data.html#setup-lakefs-server",
    
    "relUrl": "/use_cases/cicd_for_data.html#setup-lakefs-server"
  },"80": {
    "doc": "CI/CD for data lakes",
    "title": "Set up your first lakeFS webhook under 10 minutes",
    "content": "To configure and set up a pre-merge lakeFS hook that validates file format of your data on staging branch before promoting it to production, refer to the sample demo notebook and actions.yaml here. To explore different checks and validations on your data, refer to pre-built hooks config by the lakeFS team. To understand the comprehensive list of hooks supported by lakeFS, refer to the documentation. ",
    "url": "/v0.98/use_cases/cicd_for_data.html#set-up-your-first-lakefs-webhook-under-10-minutes",
    
    "relUrl": "/use_cases/cicd_for_data.html#set-up-your-first-lakefs-webhook-under-10-minutes"
  },"81": {
    "doc": "CI/CD for data lakes",
    "title": "CI/CD for data lakes",
    "content": " ",
    "url": "/v0.98/use_cases/cicd_for_data.html",
    
    "relUrl": "/use_cases/cicd_for_data.html"
  },"82": {
    "doc": "lakectl (lakeFS command-line tool)",
    "title": "lakectl (lakeFS command-line tool)",
    "content": " ",
    "url": "/v0.98/reference/cli.html#lakectl-lakefs-command-line-tool",
    
    "relUrl": "/reference/cli.html#lakectl-lakefs-command-line-tool"
  },"83": {
    "doc": "lakectl (lakeFS command-line tool)",
    "title": "Table of contents",
    "content": ". | Installing lakectl locally . | Configuring credentials and API endpoint | . | Running lakectl from Docker | Command Reference . | lakectl | lakectl abuse | lakectl abuse commit | lakectl abuse create-branches | lakectl abuse help | lakectl abuse link-same-object | lakectl abuse list | lakectl abuse random-read | lakectl abuse random-write | lakectl actions | lakectl actions help | lakectl actions runs | lakectl actions runs describe | lakectl actions runs help | lakectl actions runs list | lakectl actions validate | lakectl annotate | lakectl auth | lakectl auth groups | lakectl auth groups acl | lakectl auth groups acl get | lakectl auth groups acl help | lakectl auth groups acl set | lakectl auth groups create | lakectl auth groups delete | lakectl auth groups help | lakectl auth groups list | lakectl auth groups members | lakectl auth groups members add | lakectl auth groups members help | lakectl auth groups members list | lakectl auth groups members remove | lakectl auth groups policies | lakectl auth groups policies attach | lakectl auth groups policies detach | lakectl auth groups policies help | lakectl auth groups policies list | lakectl auth help | lakectl auth policies | lakectl auth policies create | lakectl auth policies delete | lakectl auth policies help | lakectl auth policies list | lakectl auth policies show | lakectl auth users | lakectl auth users create | lakectl auth users credentials | lakectl auth users credentials create | lakectl auth users credentials delete | lakectl auth users credentials help | lakectl auth users credentials list | lakectl auth users delete | lakectl auth users groups | lakectl auth users groups help | lakectl auth users groups list | lakectl auth users help | lakectl auth users list | lakectl auth users policies | lakectl auth users policies attach | lakectl auth users policies detach | lakectl auth users policies help | lakectl auth users policies list | lakectl branch | lakectl branch create | lakectl branch delete | lakectl branch help | lakectl branch list | lakectl branch reset | lakectl branch revert | lakectl branch show | lakectl branch-protect | lakectl branch-protect add | lakectl branch-protect delete | lakectl branch-protect help | lakectl branch-protect list | lakectl cat-hook-output | lakectl cat-sst | lakectl cherry-pick | lakectl commit | lakectl completion | lakectl config | lakectl dbt | lakectl dbt create-branch-schema | lakectl dbt generate-schema-macro | lakectl dbt help | lakectl diff | lakectl docs | lakectl doctor | lakectl find-merge-base | lakectl fs | lakectl fs cat | lakectl fs download | lakectl fs help | lakectl fs ls | lakectl fs rm | lakectl fs stage | lakectl fs stat | lakectl fs upload | lakectl gc | lakectl gc delete-config | lakectl gc get-config | lakectl gc help | lakectl gc set-config | lakectl help | lakectl import | lakectl ingest | lakectl log | lakectl merge | lakectl metastore | lakectl metastore copy | lakectl metastore copy-all | lakectl metastore copy-schema | lakectl metastore create-symlink | lakectl metastore diff | lakectl metastore help | lakectl metastore import-all | lakectl refs-dump | lakectl refs-restore | lakectl repo | lakectl repo create | lakectl repo create-bare | lakectl repo delete | lakectl repo help | lakectl repo list | lakectl show | lakectl show commit | lakectl show help | lakectl tag | lakectl tag create | lakectl tag delete | lakectl tag help | lakectl tag list | lakectl tag show | . | . ",
    "url": "/v0.98/reference/cli.html#table-of-contents",
    
    "relUrl": "/reference/cli.html#table-of-contents"
  },"84": {
    "doc": "lakectl (lakeFS command-line tool)",
    "title": "Installing lakectl locally",
    "content": "lakectl is available for Linux, macOS, and Windows. You can also run it using Docker. Download lakectl . Configuring credentials and API endpoint . Once you’ve installed the lakectl command, run: . lakectl config # output: # Config file /home/janedoe/.lakectl.yaml will be used # Access key ID: AKIAIOSFODNN7EXAMPLE # Secret access key: **************************************** # Server endpoint URL: http://localhost:8000/api/v1 . This will setup a $HOME/.lakectl.yaml file with the credentials and API endpoint you’ve supplied. When setting up a new installation and creating initial credentials (see Quick start), the UI will provide a link to download a preconfigured configuration file for you. lakectl configuration items can each be controlled by an environment variable. The variable name will have a prefix of LAKECTL_, followed by the name of the configuration, replacing every ‘.’ with a ‘_’. Example: LAKECTL_SERVER_ENDPOINT_URL controls server.endpoint_url. ",
    "url": "/v0.98/reference/cli.html#installing-lakectl-locally",
    
    "relUrl": "/reference/cli.html#installing-lakectl-locally"
  },"85": {
    "doc": "lakectl (lakeFS command-line tool)",
    "title": "Running lakectl from Docker",
    "content": "If you’d rather run lakectl from a Docker container you can do so by passing configuration elements as environment variables. Here is an example: . docker run --rm --pull always \\ -e LAKECTL_CREDENTIALS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE \\ -e LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY=xxxxx -e LAKECTL_SERVER_ENDPOINT_URL=https://host.us-east-2.lakefscloud.io/ \\ --entrypoint lakectl treeverse/lakefs \\ repo list . Bear in mind that if you are running lakeFS itself locally you will need to account for this in your networking configuration of the Docker container. That is to say, localhost to a Docker container is itself, not the host machine on which it is running. ",
    "url": "/v0.98/reference/cli.html#running-lakectl-from-docker",
    
    "relUrl": "/reference/cli.html#running-lakectl-from-docker"
  },"86": {
    "doc": "lakectl (lakeFS command-line tool)",
    "title": "Command Reference",
    "content": "lakectl . A cli tool to explore manage and work with lakeFS . Synopsis . lakectl is a CLI tool allowing exploration and manipulation of a lakeFS environment . lakectl [flags] . Options . --base-uri string base URI used for lakeFS address parse -c, --config string config file (default is $HOME/.lakectl.yaml) -h, --help help for lakectl --log-format string set logging output format --log-level string set logging level (default \"none\") --log-output strings set logging output(s) --no-color don't use fancy output colors (default when not attached to an interactive terminal) --verbose run in verbose mode -v, --version version for lakectl . note: The base-uri option can be controlled with the LAKECTL_BASE_URI environment variable. Example usage . $ export LAKECTL_BASE_URI=\"lakefs://my-repo/my-branch\" # Once set, use relative lakefs uri's: $ lakectl fs ls /path . lakectl abuse . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Abuse a running lakeFS instance. See sub commands for more info. Options . -h, --help help for abuse . lakectl abuse commit . Commits to the source ref repeatedly . lakectl abuse commit &lt;source ref uri&gt; [flags] . Options . --amount int amount of commits to do (default 100) --gap duration duration to wait between commits (default 2s) -h, --help help for commit . lakectl abuse create-branches . Create a lot of branches very quickly. lakectl abuse create-branches &lt;source ref uri&gt; [flags] . Options . --amount int amount of things to do (default 1000000) --branch-prefix string prefix to create branches under (default \"abuse-\") --clean-only only clean up past runs -h, --help help for create-branches --parallelism int amount of things to do in parallel (default 100) . lakectl abuse help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type abuse help [path to command] for full details. lakectl abuse help [command] [flags] . Options . -h, --help help for help . lakectl abuse link-same-object . Link the same object in parallel. lakectl abuse link-same-object &lt;source ref uri&gt; [flags] . Options . --amount int amount of link object to do (default 1000000) -h, --help help for link-same-object --key string key used for the test (default \"linked-object\") --parallelism int amount of link object to do in parallel (default 100) . lakectl abuse list . List from the source ref . lakectl abuse list &lt;source ref uri&gt; [flags] . Options . --amount int amount of lists to do (default 1000000) -h, --help help for list --parallelism int amount of lists to do in parallel (default 100) --prefix string prefix to list under (default \"abuse/\") . lakectl abuse random-read . Read keys from a file and generate random reads from the source ref for those keys. lakectl abuse random-read &lt;source ref uri&gt; [flags] . Options . --amount int amount of reads to do (default 1000000) --from-file string read keys from this file (\"-\" for stdin) -h, --help help for random-read --parallelism int amount of reads to do in parallel (default 100) . lakectl abuse random-write . Generate random writes to the source branch . lakectl abuse random-write &lt;source branch uri&gt; [flags] . Options . --amount int amount of writes to do (default 1000000) -h, --help help for random-write --parallelism int amount of writes to do in parallel (default 100) --prefix string prefix to create paths under (default \"abuse/\") . lakectl actions . Manage Actions commands . Options . -h, --help help for actions . lakectl actions help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type actions help [path to command] for full details. lakectl actions help [command] [flags] . Options . -h, --help help for help . lakectl actions runs . Explore runs information . Options . -h, --help help for runs . lakectl actions runs describe . Describe run results . Synopsis . Show information about the run and all the hooks that were executed as part of the run . lakectl actions runs describe [flags] . Examples . lakectl actions runs describe lakefs://&lt;repository&gt; &lt;run_id&gt; . Options . --after string show results after this value (used for pagination) --amount int number of results to return. By default, all results are returned. -h, --help help for describe . lakectl actions runs help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type runs help [path to command] for full details. lakectl actions runs help [command] [flags] . Options . -h, --help help for help . lakectl actions runs list . List runs . Synopsis . List all runs on a repository optional filter by branch or commit . lakectl actions runs list [flags] . Examples . lakectl actions runs list lakefs://&lt;repository&gt; [--branch &lt;branch&gt;] [--commit &lt;commit_id&gt;] . Options . --after string show results after this value (used for pagination) --amount int number of results to return (default 100) --branch string show results for specific branch --commit string show results for specific commit ID -h, --help help for list . lakectl actions validate . Validate action file . Synopsis . Tries to parse the input action file as lakeFS action file . lakectl actions validate [flags] . Examples . lakectl actions validate &lt;path&gt; . Options . -h, --help help for validate . lakectl annotate . List entries under a given path, annotating each with the latest modifying commit . lakectl annotate &lt;path uri&gt; [flags] . Options . -h, --help help for annotate -r, --recursive recursively annotate all entries under a given path or prefix . lakectl auth . Manage authentication and authorization . Synopsis . manage authentication and authorization including users, groups and ACLs . Options . -h, --help help for auth . lakectl auth groups . Manage groups . Options . -h, --help help for groups . lakectl auth groups acl . Manage ACLs . Synopsis . manage ACLs of groups . Options . -h, --help help for acl . lakectl auth groups acl get . Get ACL of group . lakectl auth groups acl get [flags] . Options . -h, --help help for get --id string Group identifier . lakectl auth groups acl help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type acl help [path to command] for full details. lakectl auth groups acl help [command] [flags] . Options . -h, --help help for help . lakectl auth groups acl set . Set ACL of group . Synopsis . Set ACL of group. permission will be attached to all-repositories or to specified repositories. You must specify exactly one of –all-repositories or –repositories. lakectl auth groups acl set [flags] . Options . --all-repositories If set, allow all repositories (current and future) -h, --help help for set --id string Group identifier --permission string Permission, typically one of \"Reader\", \"Writer\", \"Super\" or \"Admin\" --repositories strings List of specific repositories to allow for permission . lakectl auth groups create . Create a group . lakectl auth groups create [flags] . Options . -h, --help help for create --id string Group identifier . lakectl auth groups delete . Delete a group . lakectl auth groups delete [flags] . Options . -h, --help help for delete --id string Group identifier . lakectl auth groups help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type groups help [path to command] for full details. lakectl auth groups help [command] [flags] . Options . -h, --help help for help . lakectl auth groups list . List groups . lakectl auth groups list [flags] . Options . --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth groups members . Manage group user memberships . Options . -h, --help help for members . lakectl auth groups members add . Add a user to a group . lakectl auth groups members add [flags] . Options . -h, --help help for add --id string Group identifier --user string Username (email for password-based users, default: current user) . lakectl auth groups members help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type members help [path to command] for full details. lakectl auth groups members help [command] [flags] . Options . -h, --help help for help . lakectl auth groups members list . List users in a group . lakectl auth groups members list [flags] . Options . --id string Group identifier --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth groups members remove . Remove a user from a group . lakectl auth groups members remove [flags] . Options . -h, --help help for remove --id string Group identifier --user string Username (email for password-based users, default: current user) . lakectl auth groups policies . Manage group policies . Synopsis . Manage group policies. Requires an external authorization server with matching support. Options . -h, --help help for policies . lakectl auth groups policies attach . Attach a policy to a group . lakectl auth groups policies attach [flags] . Options . -h, --help help for attach --id string User identifier --policy string Policy identifier . lakectl auth groups policies detach . Detach a policy from a group . lakectl auth groups policies detach [flags] . Options . -h, --help help for detach --id string User identifier --policy string Policy identifier . lakectl auth groups policies help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type policies help [path to command] for full details. lakectl auth groups policies help [command] [flags] . Options . -h, --help help for help . lakectl auth groups policies list . List policies for the given group . lakectl auth groups policies list [flags] . Options . --id string Group identifier --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type auth help [path to command] for full details. lakectl auth help [command] [flags] . Options . -h, --help help for help . lakectl auth policies . Manage policies . Options . -h, --help help for policies . lakectl auth policies create . Create a policy . lakectl auth policies create [flags] . Options . -h, --help help for create --id string Policy identifier --statement-document string JSON statement document path (or \"-\" for stdin) . lakectl auth policies delete . Delete a policy . lakectl auth policies delete [flags] . Options . -h, --help help for delete --id string Policy identifier . lakectl auth policies help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type policies help [path to command] for full details. lakectl auth policies help [command] [flags] . Options . -h, --help help for help . lakectl auth policies list . List policies . lakectl auth policies list [flags] . Options . --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth policies show . Show a policy . lakectl auth policies show [flags] . Options . -h, --help help for show --id string Policy identifier . lakectl auth users . Manage users . Options . -h, --help help for users . lakectl auth users create . Create a user . lakectl auth users create [flags] . Options . -h, --help help for create --id string Username . lakectl auth users credentials . Manage user credentials . Options . -h, --help help for credentials . lakectl auth users credentials create . Create user credentials . lakectl auth users credentials create [flags] . Options . -h, --help help for create --id string Username (email for password-based users, default: current user) . lakectl auth users credentials delete . Delete user credentials . lakectl auth users credentials delete [flags] . Options . --access-key-id string Access key ID to delete -h, --help help for delete --id string Username (email for password-based users, default: current user) . lakectl auth users credentials help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type credentials help [path to command] for full details. lakectl auth users credentials help [command] [flags] . Options . -h, --help help for help . lakectl auth users credentials list . List user credentials . lakectl auth users credentials list [flags] . Options . --id string Username (email for password-based users, default: current user) --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth users delete . Delete a user . lakectl auth users delete [flags] . Options . -h, --help help for delete --id string Username (email for password-based users) . lakectl auth users groups . Manage user groups . Options . -h, --help help for groups . lakectl auth users groups help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type groups help [path to command] for full details. lakectl auth users groups help [command] [flags] . Options . -h, --help help for help . lakectl auth users groups list . List groups for the given user . lakectl auth users groups list [flags] . Options . --id string Username (email for password-based users) --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth users help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type users help [path to command] for full details. lakectl auth users help [command] [flags] . Options . -h, --help help for help . lakectl auth users list . List users . lakectl auth users list [flags] . Options . --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl auth users policies . Manage user policies . Synopsis . Manage user policies. Requires an external authorization server with matching support. Options . -h, --help help for policies . lakectl auth users policies attach . Attach a policy to a user . lakectl auth users policies attach [flags] . Options . -h, --help help for attach --id string Username (email for password-based users) --policy string Policy identifier . lakectl auth users policies detach . Detach a policy from a user . lakectl auth users policies detach [flags] . Options . -h, --help help for detach --id string Username (email for password-based users) --policy string Policy identifier . lakectl auth users policies help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type policies help [path to command] for full details. lakectl auth users policies help [command] [flags] . Options . -h, --help help for help . lakectl auth users policies list . List policies for the given user . lakectl auth users policies list [flags] . Options . --effective List all distinct policies attached to the user, including by group memberships --id string Username (email for password-based users) --amount int how many results to return (default 100) --after string show results after this value (used for pagination) -h, --help help for list . lakectl branch . Create and manage branches within a repository . Synopsis . Create delete and list branches within a lakeFS repository . Options . -h, --help help for branch . lakectl branch create . Create a new branch in a repository . lakectl branch create &lt;branch uri&gt; -s &lt;source ref uri&gt; [flags] . Examples . lakectl branch create lakefs://example-repo/new-branch -s lakefs://example-repo/main . Options . -h, --help help for create -s, --source string source branch uri . lakectl branch delete . Delete a branch in a repository, along with its uncommitted changes (CAREFUL) . lakectl branch delete &lt;branch uri&gt; [flags] . Examples . lakectl branch delete lakefs://example-repo/example-branch . Options . -h, --help help for delete -y, --yes Automatically say yes to all confirmations . lakectl branch help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type branch help [path to command] for full details. lakectl branch help [command] [flags] . Options . -h, --help help for help . lakectl branch list . List branches in a repository . lakectl branch list &lt;repository uri&gt; [flags] . Examples . lakectl branch list lakefs://&lt;repository&gt; . Options . --after string show results after this value (used for pagination) --amount int number of results to return (default 100) -h, --help help for list . lakectl branch reset . Reset uncommitted changes - all of them, or by path . Synopsis . reset changes. There are four different ways to reset changes: . | reset all uncommitted changes - reset lakefs://myrepo/main | reset uncommitted changes under specific path - reset lakefs://myrepo/main –prefix path | reset uncommitted changes for specific object - reset lakefs://myrepo/main –object path | . lakectl branch reset &lt;branch uri&gt; [--prefix|--object] [flags] . Examples . lakectl branch reset lakefs://example-repo/example-branch . Options . -h, --help help for reset --object string path to object to be reset --prefix string prefix of the objects to be reset -y, --yes Automatically say yes to all confirmations . lakectl branch revert . Given a commit, record a new commit to reverse the effect of this commit . Synopsis . The commits will be reverted in left-to-right order . lakectl branch revert &lt;branch uri&gt; &lt;commit ref to revert&gt; [&lt;more commits&gt;...] [flags] . Examples . lakectl branch revert lakefs://example-repo/example-branch commitA Revert the changes done by commitA in example-branch branch revert lakefs://example-repo/example-branch HEAD~1 HEAD~2 HEAD~3 Revert the changes done by the second last commit to the fourth last commit in example-branch . Options . -h, --help help for revert -m, --parent-number int the parent number (starting from 1) of the mainline. The revert will reverse the change relative to the specified parent. -y, --yes Automatically say yes to all confirmations . lakectl branch show . Show branch latest commit reference . lakectl branch show &lt;branch uri&gt; [flags] . Examples . lakectl branch show lakefs://example-repo/example-branch . Options . -h, --help help for show . lakectl branch-protect . Create and manage branch protection rules . Synopsis . Define branch protection rules to prevent direct changes. Changes to protected branches can only be done by merging from other branches. Options . -h, --help help for branch-protect . lakectl branch-protect add . Add a branch protection rule . Synopsis . Add a branch protection rule for a given branch name pattern . lakectl branch-protect add &lt;repo uri&gt; &lt;pattern&gt; [flags] . Examples . lakectl branch-protect add lakefs://&lt;repository&gt; 'stable_*' . Options . -h, --help help for add . lakectl branch-protect delete . Delete a branch protection rule . Synopsis . Delete a branch protection rule for a given branch name pattern . lakectl branch-protect delete &lt;repo uri&gt; &lt;pattern&gt; [flags] . Examples . lakectl branch-protect delete lakefs://&lt;repository&gt; stable_* . Options . -h, --help help for delete . lakectl branch-protect help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type branch-protect help [path to command] for full details. lakectl branch-protect help [command] [flags] . Options . -h, --help help for help . lakectl branch-protect list . List all branch protection rules . lakectl branch-protect list &lt;repo uri&gt; [flags] . Examples . lakectl branch-protect list lakefs://&lt;repository&gt; . Options . -h, --help help for list . lakectl cat-hook-output . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Cat actions hook output . lakectl cat-hook-output [flags] . Examples . lakectl cat-hook-output lakefs://&lt;repository&gt; &lt;run_id&gt; &lt;run_hook_id&gt; . Options . -h, --help help for cat-hook-output . lakectl cat-sst . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Explore lakeFS .sst files . lakectl cat-sst &lt;sst-file&gt; [flags] . Options . --amount int how many records to return, or -1 for all records (default -1) -f, --file string path to an sstable file, or \"-\" for stdin -h, --help help for cat-sst . lakectl cherry-pick . Apply the changes introduced by an existing commit . Synopsis . Apply the changes from the given commit to the tip of the branch. The changes will be added as a new commit. lakectl cherry-pick &lt;commit ref&gt; &lt;branch&gt; [flags] . Examples . lakectl cherry-pick lakefs://example-repo/example-ref lakefs://example-repo/main . Options . -h, --help help for cherry-pick -m, --parent-number int the parent number (starting from 1) of the cherry-picked commit. The cherry-pick will apply the change relative to the specified parent. lakectl commit . Commit changes on a given branch . lakectl commit &lt;branch uri&gt; [flags] . Options . --allow-empty-message allow an empty commit message -h, --help help for commit -m, --message string commit message --meta strings key value pair in the form of key=value . lakectl completion . Generate completion script . Synopsis . To load completions: . Bash: . $ source &lt;(lakectl completion bash) . To load completions for each session, execute once: Linux: . $ lakectl completion bash &gt; /etc/bash_completion.d/lakectl . MacOS: . $ lakectl completion bash &gt; /usr/local/etc/bash_completion.d/lakectl . Zsh: . If shell completion is not already enabled in your environment you will need to enable it. You can execute the following once: . $ echo \"autoload -U compinit; compinit\" &gt;&gt; ~/.zshrc . To load completions for each session, execute once: . $ lakectl completion zsh &gt; \"${fpath[1]}/_lakectl\" . You will need to start a new shell for this setup to take effect. Fish: . $ lakectl completion fish | source . To load completions for each session, execute once: . $ lakectl completion fish &gt; ~/.config/fish/completions/lakectl.fish . lakectl completion &lt;bash|zsh|fish&gt; . Options . -h, --help help for completion . lakectl config . Create/update local lakeFS configuration . lakectl config [flags] . Options . -h, --help help for config . lakectl dbt . Integration with dbt commands . Options . -h, --help help for dbt . lakectl dbt create-branch-schema . Creates a new schema dedicated for branch and clones all dbt models to new schema . lakectl dbt create-branch-schema [flags] . Examples . lakectl dbt create-branch-schema --branch &lt;branch-name&gt; . Options . --branch string requested branch --continue-on-error prevent command from failing when a single table fails --continue-on-schema-exists allow running on existing schema --create-branch create a new branch for the schema --dbfs-location string -h, --help help for create-branch-schema --project-root string location of dbt project (default \".\") --skip-views --to-schema string destination schema name [default is branch] . lakectl dbt generate-schema-macro . generates the a macro allowing lakectl to run dbt on dynamic schemas . lakectl dbt generate-schema-macro [flags] . Examples . lakectl dbt generate-schema-macro . Options . -h, --help help for generate-schema-macro --project-root string location of dbt project (default \".\") . lakectl dbt help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type dbt help [path to command] for full details. lakectl dbt help [command] [flags] . Options . -h, --help help for help . lakectl diff . Show changes between two commits, or the currently uncommitted changes . lakectl diff &lt;ref uri&gt; [ref uri] [flags] . Examples . lakectl diff lakefs://example-repo/example-branch Show uncommitted changes in example-branch. lakectl diff lakefs://example-repo/main lakefs://example-repo/dev This shows the differences between master and dev starting at the last common commit. This is similar to the three-dot (...) syntax in git. Uncommitted changes are not shown. lakectl diff --two-way lakefs://example-repo/main lakefs://example-repo/dev Show changes between the tips of the main and dev branches. This is similar to the two-dot (..) syntax in git. Uncommitted changes are not shown. lakectl diff --two-way lakefs://example-repo/main lakefs://example-repo/dev$ Show changes between the tip of the main and the dev branch, including uncommitted changes on dev. Options . -h, --help help for diff --two-way Use two-way diff: show difference between the given refs, regardless of a common ancestor. lakectl docs . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. lakectl docs [outfile] [flags] . Options . -h, --help help for docs . lakectl doctor . Run a basic diagnosis of the LakeFS configuration . lakectl doctor [flags] . Options . -h, --help help for doctor . lakectl find-merge-base . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Find the commits for the merge operation . lakectl find-merge-base &lt;source ref&gt; &lt;destination ref&gt; [flags] . Options . -h, --help help for find-merge-base . lakectl fs . View and manipulate objects . Options . -h, --help help for fs . lakectl fs cat . Dump content of object to stdout . lakectl fs cat &lt;path uri&gt; [flags] . Options . -d, --direct read directly from backing store (faster but requires more credentials) -h, --help help for cat --pre-sign Use pre-sign link to access the data . lakectl fs download . Download object(s) from a given repository path . lakectl fs download &lt;path uri&gt; [&lt;destination path&gt;] [flags] . Options . -d, --direct read directly from backing store (requires credentials) -h, --help help for download -p, --parallel int max concurrent downloads (default 6) --pre-sign Request pre-sign link to access the data -r, --recursive recursively all objects under path . lakectl fs help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type fs help [path to command] for full details. lakectl fs help [command] [flags] . Options . -h, --help help for help . lakectl fs ls . List entries under a given tree . lakectl fs ls &lt;path uri&gt; [flags] . Options . -h, --help help for ls --recursive list all objects under the specified prefix . lakectl fs rm . Delete object . lakectl fs rm &lt;path uri&gt; [flags] . Options . -C, --concurrency int max concurrent single delete operations to send to the lakeFS server (default 50) -h, --help help for rm -r, --recursive recursively delete all objects under the specified path . lakectl fs stage . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Stage a reference to an existing object, to be managed in lakeFS . lakectl fs stage &lt;path uri&gt; [flags] . Options . --checksum string Object MD5 checksum as a hexadecimal string --content-type string MIME type of contents -h, --help help for stage --location string fully qualified storage location (i.e. \"s3://bucket/path/to/object\") --meta strings key value pairs in the form of key=value --mtime int Object modified time (Unix Epoch in seconds). Defaults to current time --size int Object size in bytes . lakectl fs stat . View object metadata . lakectl fs stat &lt;path uri&gt; [flags] . Options . -h, --help help for stat --pre-sign Request pre-sign for physical address . lakectl fs upload . Upload a local file to the specified URI . lakectl fs upload &lt;path uri&gt; [flags] . Options . --content-type string MIME type of contents -d, --direct write directly to backing store (faster but requires more credentials) -h, --help help for upload --pre-sign Use pre-sign link to access the data -r, --recursive recursively copy all files under local source -s, --source string local file to upload, or \"-\" for stdin . lakectl gc . Manage the garbage collection policy . Options . -h, --help help for gc . lakectl gc delete-config . Deletes the garbage collection policy for the repository . lakectl gc delete-config [flags] . Examples . lakectl gc delete-config &lt;repository uri&gt; . Options . -h, --help help for delete-config . lakectl gc get-config . Show the garbage collection policy for this repository . lakectl gc get-config [flags] . Examples . lakectl gc get-config &lt;repository uri&gt; . Options . -h, --help help for get-config -p, --json get rules as JSON . lakectl gc help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type gc help [path to command] for full details. lakectl gc help [command] [flags] . Options . -h, --help help for help . lakectl gc set-config . Set garbage collection policy JSON . Synopsis . Sets the garbage collection policy JSON. Example configuration file: { “default_retention_days”: 21, “branches”: [ { “branch_id”: “main”, “retention_days”: 28 }, { “branch_id”: “dev”, “retention_days”: 14 } ] } . lakectl gc set-config [flags] . Examples . lakectl gc set-config &lt;repository uri&gt; -f config.json . Options . -f, --filename string file containing the GC policy as JSON -h, --help help for set-config . lakectl help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type lakectl help [path to command] for full details. lakectl help [command] [flags] . Options . -h, --help help for help . lakectl import . Import data from external source to an imported branch (with optional merge) . lakectl import --from &lt;object store URI&gt; --to &lt;lakeFS path URI&gt; [--merge] [flags] . Options . --from string prefix to read from (e.g. \"s3://bucket/sub/path/\"). must not be in a storage namespace -h, --help help for import --merge merge imported branch into target branch -m, --message string commit message (default \"Import objects\") --meta strings key value pair in the form of key=value --no-progress switch off the progress output --to string lakeFS path to load objects into (e.g. \"lakefs://repo/branch/sub/path/\") . lakectl ingest . Ingest objects from an external source into a lakeFS branch (without actually copying them) . lakectl ingest --from &lt;object store URI&gt; --to &lt;lakeFS path URI&gt; [--dry-run] [flags] . Options . -C, --concurrency int max concurrent API calls to make to the lakeFS server (default 64) --dry-run only print the paths to be ingested --from string prefix to read from (e.g. \"s3://bucket/sub/path/\"). must not be in a storage namespace -h, --help help for ingest --s3-endpoint-url string URL to access S3 storage API (by default, use regular AWS S3 endpoint --to string lakeFS path to load objects into (e.g. \"lakefs://repo/branch/sub/path/\") -v, --verbose print stats for each individual object staged . lakectl log . Show log of commits . Synopsis . Show log of commits for a given branch . lakectl log &lt;branch uri&gt; [flags] . Examples . lakectl log --dot lakefs://example-repository/main | dot -Tsvg &gt; graph.svg . Options . --after string show results after this value (used for pagination) --amount int number of results to return. By default, all results are returned --dot return results in a dotgraph format -h, --help help for log --limit limit result just to amount. By default, returns whether more items are available. --objects strings show results that contains changes to at least one path in that list of objects. Use comma separator to pass all objects together --prefixes strings show results that contains changes to at least one path in that list of prefixes. Use comma separator to pass all prefixes together --show-meta-range-id also show meta range ID . lakectl merge . Merge &amp; commit changes from source branch into destination branch . Synopsis . Merge &amp; commit changes from source branch into destination branch . lakectl merge &lt;source ref&gt; &lt;destination ref&gt; [flags] . Options . -h, --help help for merge --strategy string In case of a merge conflict, this option will force the merge process to automatically favor changes from the dest branch (\"dest-wins\") or from the source branch(\"source-wins\"). In case no selection is made, the merge process will fail in case of a conflict . lakectl metastore . Manage metastore commands . Options . -h, --help help for metastore . lakectl metastore copy . Copy or merge table . Synopsis . Copy or merge table. the destination table will point to the selected branch . lakectl metastore copy [flags] . Options . --catalog-id string Glue catalog ID --dbfs-root dbfs:/ dbfs location root will replace dbfs:/ in the location before transforming --from-client-type string metastore type [hive, glue] --from-schema string source schema name --from-table string source table name -h, --help help for copy --metastore-uri string Hive metastore URI -p, --partition strings partition to copy --serde string serde to set copy to [default is to-table] --to-branch string lakeFS branch name --to-client-type string metastore type [hive, glue] --to-schema string destination schema name [default is from-branch] --to-table string destination table name [default is from-table] . lakectl metastore copy-all . Copy from one metastore to another . Synopsis . copy or merge requested tables between hive metastores. the destination tables will point to the selected branch . lakectl metastore copy-all [flags] . Options . --branch string lakeFS branch name --continue-on-error prevent copy-all from failing when a single table fails --dbfs-root dbfs:/ dbfs location root will replace dbfs:/ in the location before transforming --from-address string source metastore address --from-client-type string metastore type [hive, glue] -h, --help help for copy-all --schema-filter string filter for schemas to copy in metastore pattern (default \".*\") --table-filter string filter for tables to copy in metastore pattern (default \".*\") --to-address string destination metastore address --to-client-type string metastore type [hive, glue] . lakectl metastore copy-schema . Copy schema . Synopsis . Copy schema (without tables). the destination schema will point to the selected branch . lakectl metastore copy-schema [flags] . Options . --catalog-id string Glue catalog ID --dbfs-root dbfs:/ dbfs location root will replace dbfs:/ in the location before transforming --from-client-type string metastore type [hive, glue] --from-schema string source schema name -h, --help help for copy-schema --metastore-uri string Hive metastore URI --to-branch string lakeFS branch name --to-client-type string metastore type [hive, glue] --to-schema string destination schema name [default is from-branch] . lakectl metastore create-symlink . Create symlink table and data . Synopsis . create table with symlinks, and create the symlinks in s3 in order to access from external services that could only access s3 directly (e.g athena) . lakectl metastore create-symlink [flags] . Options . --branch string lakeFS branch name --catalog-id string Glue catalog ID --from-client-type string metastore type [hive, glue] --from-schema string source schema name --from-table string source table name -h, --help help for create-symlink --path string path to table on lakeFS --repo string lakeFS repository name --to-schema string destination schema name --to-table string destination table name . lakectl metastore diff . Show column and partition differences between two tables . lakectl metastore diff [flags] . Options . --catalog-id string Glue catalog ID --from-address string source metastore address --from-client-type string metastore type [hive, glue] --from-schema string source schema name --from-table string source table name -h, --help help for diff --metastore-uri string Hive metastore URI --to-address string destination metastore address --to-client-type string metastore type [hive, glue] --to-schema string destination schema name --to-table string destination table name [default is from-table] . lakectl metastore help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type metastore help [path to command] for full details. lakectl metastore help [command] [flags] . Options . -h, --help help for help . lakectl metastore import-all . Import from one metastore to another . Synopsis . import requested tables between hive metastores. the destination tables will point to the selected repository and branch table with location s3://my-s3-bucket/path/to/table will be transformed to location s3://repo-param/bucket-param/path/to/table . lakectl metastore import-all [flags] . Options . --branch string lakeFS branch name --continue-on-error prevent import-all from failing when a single table fails --dbfs-root dbfs:/ dbfs location root will replace dbfs:/ in the location before transforming --from-address string source metastore address --from-client-type string metastore type [hive, glue] -h, --help help for import-all --repo string lakeFS repo name --schema-filter string filter for schemas to copy in metastore pattern (default \".*\") --table-filter string filter for tables to copy in metastore pattern (default \".*\") --to-address string destination metastore address --to-client-type string metastore type [hive, glue] . lakectl refs-dump . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Dumps refs (branches, commits, tags) to the underlying object store . lakectl refs-dump &lt;repository uri&gt; [flags] . Options . -h, --help help for refs-dump . lakectl refs-restore . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Restores refs (branches, commits, tags) from the underlying object store to a bare repository . Synopsis . restores refs (branches, commits, tags) from the underlying object store to a bare repository. This command is expected to run on a bare repository (i.e. one created with ‘lakectl repo create-bare’). Since a bare repo is expected, in case of transient failure, delete the repository and recreate it as bare and retry. lakectl refs-restore &lt;repository uri&gt; [flags] . Examples . aws s3 cp s3://bucket/_lakefs/refs_manifest.json - | lakectl refs-restore lakefs://my-bare-repository --manifest - . Options . -h, --help help for refs-restore --manifest refs-dump path to a refs manifest json file (as generated by refs-dump). Alternatively, use \"-\" to read from stdin . lakectl repo . Manage and explore repos . Options . -h, --help help for repo . lakectl repo create . Create a new repository . lakectl repo create &lt;repository uri&gt; &lt;storage namespace&gt; [flags] . Examples . lakectl repo create lakefs://some-repo-name s3://some-bucket-name . Options . -d, --default-branch string the default branch of this repository (default \"main\") -h, --help help for create . lakectl repo create-bare . note: This command is a lakeFS plumbing command. Don’t use it unless you’re really sure you know what you’re doing. Create a new repository with no initial branch or commit . lakectl repo create-bare &lt;repository uri&gt; &lt;storage namespace&gt; [flags] . Examples . lakectl create-bare lakefs://some-repo-name s3://some-bucket-name . Options . -d, --default-branch string the default branch name of this repository (will not be created) (default \"main\") -h, --help help for create-bare . lakectl repo delete . Delete existing repository . lakectl repo delete &lt;repository uri&gt; [flags] . Options . -h, --help help for delete -y, --yes Automatically say yes to all confirmations . lakectl repo help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type repo help [path to command] for full details. lakectl repo help [command] [flags] . Options . -h, --help help for help . lakectl repo list . List repositories . lakectl repo list [flags] . Options . --after string show results after this value (used for pagination) --amount int number of results to return (default 100) -h, --help help for list . lakectl show . See detailed information about an entity . Options . -h, --help help for show . lakectl show commit . See detailed information about a commit . lakectl show commit &lt;ref uri&gt; [flags] . Options . -h, --help help for commit --show-meta-range-id show meta range ID . lakectl show help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type show help [path to command] for full details. lakectl show help [command] [flags] . Options . -h, --help help for help . lakectl tag . Create and manage tags within a repository . Synopsis . Create delete and list tags within a lakeFS repository . Options . -h, --help help for tag . lakectl tag create . Create a new tag in a repository . lakectl tag create &lt;tag uri&gt; &lt;commit uri&gt; [flags] . Examples . lakectl tag create lakefs://example-repo/example-tag lakefs://example-repo/2397cc9a9d04c20a4e5739b42c1dd3d8ba655c0b3a3b974850895a13d8bf9917 . Options . -f, --force override the tag if it exists -h, --help help for create . lakectl tag delete . Delete a tag from a repository . lakectl tag delete &lt;tag uri&gt; [flags] . Options . -h, --help help for delete . lakectl tag help . Help about any command . Synopsis . Help provides help for any command in the application. Simply type tag help [path to command] for full details. lakectl tag help [command] [flags] . Options . -h, --help help for help . lakectl tag list . List tags in a repository . lakectl tag list &lt;repository uri&gt; [flags] . Examples . lakectl tag list lakefs://&lt;repository&gt; . Options . --after string show results after this value (used for pagination) --amount int number of results to return (default 100) -h, --help help for list . lakectl tag show . Show tag’s commit reference . lakectl tag show &lt;tag uri&gt; [flags] . Options . -h, --help help for show . ",
    "url": "/v0.98/reference/cli.html#command-reference",
    
    "relUrl": "/reference/cli.html#command-reference"
  },"87": {
    "doc": "lakectl (lakeFS command-line tool)",
    "title": "lakectl (lakeFS command-line tool)",
    "content": " ",
    "url": "/v0.98/reference/cli.html",
    
    "relUrl": "/reference/cli.html"
  },"88": {
    "doc": "4️⃣ Commit and Merge",
    "title": "Committing Changes in lakeFS 🤝🏻",
    "content": "Having make the change to the datafile in the denmark-lakes branch, we now want to commit it. There are various options for interacting with lakeFS’ API, including the web interface, a Python client, and lakectl which is what we’ll use here. Run the following from a terminal window: . docker exec lakefs \\ lakectl commit lakefs://quickstart/denmark-lakes \\ -m \"Create a dataset of just the lakes in Denmark\" . You will get confirmation of the commit including its hash. Branch: lakefs://quickstart/denmark-lakes Commit for branch \"denmark-lakes\" completed. ID: ba6d71d0965fa5d97f309a17ce08ad006c0dde15f99c5ea0904d3ad3e765bd74 Message: Create a dataset of just the lakes in Denmark Timestamp: 2023-03-15 08:09:36 +0000 UTC Parents: 3384cd7cdc4a2cd5eb6249b52f0a709b49081668bb1574ce8f1ef2d956646816 . With our change committed, it’s now time to merge it to back to the main branch. ",
    "url": "/v0.98/quickstart/commit-and-merge.html#committing-changes-in-lakefs-",
    
    "relUrl": "/quickstart/commit-and-merge.html#committing-changes-in-lakefs-"
  },"89": {
    "doc": "4️⃣ Commit and Merge",
    "title": "Merging Branches in lakeFS 🔀",
    "content": "As above, we’ll use lakectl to do this too. The syntax just requires us to specify the source and target of the merge. Run this from a terminal window. docker exec lakefs \\ lakectl merge \\ lakefs://quickstart/denmark-lakes \\ lakefs://quickstart/main . We can confirm that this has worked by returning to the same object view of lakes.parquet as before and clicking on Execute to rerun the same query. You’ll see that the country row counts have changed, and only Denmark is left in the data: . But…oh no! 😬 A slow chill creeps down your spine, and the bottom drops out of your stomach. What have you done! 😱 You were supposed to create a separate file of Denmark’s lakes - not replace the original one 🤦🏻🤦🏻‍♀️ . Is all lost? Will our hero overcome the obstacles? No, and yes respectively! . Have no fear; lakeFS can revert changes. Tune in for the final part of the quickstart to see how. ",
    "url": "/v0.98/quickstart/commit-and-merge.html#merging-branches-in-lakefs-",
    
    "relUrl": "/quickstart/commit-and-merge.html#merging-branches-in-lakefs-"
  },"90": {
    "doc": "4️⃣ Commit and Merge",
    "title": "4️⃣ Commit and Merge",
    "content": "In the previous step we branched our data from main into a new denmark-lakes branch, and overwrote the lakes.parquet to hold solely information about lakes in Denmark. Now we’re going to commit that change (just like git) and merge it back to main (just like git). ",
    "url": "/v0.98/quickstart/commit-and-merge.html",
    
    "relUrl": "/quickstart/commit-and-merge.html"
  },"91": {
    "doc": "Commitment to OSS",
    "title": "Our commitment to open source",
    "content": "lakeFS is an open-source project under the Apache 2.0 license. The project was created and is supported by Treeverse, a commercial company founded by engineers passionate about providing solutions to the evolving world of data engineering. Why did we choose to open the source of our core capabilities? . We believe in the bottom-up adoption of technologies. We believe collaborative communities have the power to bring the best solutions to the community. We believe that every engineer should be able to use, contribute to, and influence cutting edge technologies, so they can innovate in their domain. What is our commitment to open source? . We created lakeFS, our open-source project, to provide a Git-like interface on top of object stores - so that you can fully take advantage of with any data application at any scale. For that reason, we commit that the following capabilities are and will remain open-source as part of lakeFS: . | All versioning capabilities, | Git-Like interface for the versioning operations, | Support for public object store APIs, | Integrations with publicly available applications accessing an object store, | CLI, API, and GUI interfaces. | . We also commit to keeping lakeFS scalable in throughput and performance. We are deeply committed to our community of engineers who use and contribute to the project. We are and will continue to be highly responsive and shape lakeFS together to provide the data versioning capabilities we are all looking for. What is lakeFS Cloud? . Treeverse offers lakeFS Cloud, which provides all the same benefits of the Git-like interface on top of object stores as a fully-managed service. The vision behind lakeFS Cloud is to provide a managed data versioning and management solution for data practitioners. lakeFS Cloud will leverage the lakeFS open-source technology, integrate capabilities and unique features, and lead its users to implement best practices. As part of our commitment to the open source values of transparency and interoperability, we believe everyone should be able to enjoy these benefits, regardless of whether or not they choose to use the managed offering. Because of that, we will not intentionally make it harder to build these features independently on top of the open source solution. ",
    "url": "/v0.98/commitment.html#our-commitment-to-open-source",
    
    "relUrl": "/commitment.html#our-commitment-to-open-source"
  },"92": {
    "doc": "Commitment to OSS",
    "title": "Commitment to OSS",
    "content": " ",
    "url": "/v0.98/commitment.html",
    
    "relUrl": "/commitment.html"
  },"93": {
    "doc": "Configuration",
    "title": "Configuration Reference",
    "content": " ",
    "url": "/v0.98/reference/configuration.html#configuration-reference",
    
    "relUrl": "/reference/configuration.html#configuration-reference"
  },"94": {
    "doc": "Configuration",
    "title": "Table of contents",
    "content": ". | Reference | Using Environment Variables | Example: Local Development with PostgreSQL database | Example: AWS Deployment with DynamoDB database | Example: Google Storage | Example: MinIO | Example: Azure blob storage | . Configuring lakeFS is done using a YAML configuration file and/or environment variable. The configuration file’s location can be set with the ‘–config’ flag. If not specified, the first file found in the following order will be used: . | ./config.yaml | $HOME/lakefs/config.yaml | /etc/lakefs/config.yaml | $HOME/.lakefs.yaml | . Configuration items can each be controlled by an environment variable. The variable name will have a prefix of LAKEFS_, followed by the name of the configuration, replacing every ‘.’ with a ‘_’. Example: LAKEFS_LOGGING_LEVEL controls logging.level. This reference uses . to denote the nesting of values. ",
    "url": "/v0.98/reference/configuration.html#table-of-contents",
    
    "relUrl": "/reference/configuration.html#table-of-contents"
  },"95": {
    "doc": "Configuration",
    "title": "Reference",
    "content": ". | logging.format (one of [\"json\", \"text\"] : \"text\") - Format to output log message in | logging.level (one of [\"TRACE\", \"DEBUG\", \"INFO\", \"WARN\", \"ERROR\", \"NONE\"] : \"DEBUG\") - Logging level to output | logging.audit_log_level (one of [\"TRACE\", \"DEBUG\", \"INFO\", \"WARN\", \"ERROR\", \"NONE\"] : \"DEBUG\") - Audit logs level to output. Note: In case you configure this field to be lower than the main logger level, you won’t be able to get the audit logs . | logging.output (string : \"-\") - A path or paths to write logs to. A - means the standard output, = means the standard error. | logging.file_max_size_mb (int : 100) - Output file maximum size in megabytes. | logging.files_keep (int : 0) - Number of log files to keep, default is all. | actions.enabled (bool : true) - Setting this to false will block hooks from being executed | database.connection_string (string : \"postgres://localhost:5432/postgres?sslmode=disable\") - PostgreSQL connection string to use | database.max_open_connections (int : 25) - Maximum number of open connections to the database | database.max_idle_connections (int : 25) - Sets the maximum number of connections in the idle connection pool | database.connection_max_lifetime (duration : 5m) - Sets the maximum amount of time a connection may be reused . Note: Deprecated - See database section . | database - Configuration section for the lakeFS key-value store database . | database.type (string [\"postgres\"|\"dynamodb\"|\"local\"] : ) - lakeFS database type | database.postgres - Configuration section when using database.type=\"postgres\" . | database.postgres.connection_string (string : \"postgres://localhost:5432/postgres?sslmode=disable\") - PostgreSQL connection string to use | database.postgres.max_open_connections (int : 25) - Maximum number of open connections to the database | database.postgres.max_idle_connections (int : 25) - Maximum number of connections in the idle connection pool | database.postgres.connection_max_lifetime (duration : 5m) - Sets the maximum amount of time a connection may be reused (valid units: ns|us|ms|s|m|h) | . | database.dynamodb - Configuration section when using database.type=\"dynamodb\" . | database.dynamodb.table_name (string : \"kvstore\") - Table used to store the data | database.dynamodb.scan_limit (int : 1025) - Maximal number of items per page during scan operation . Note: Refer to the following AWS documentation for further information . | database.dynamodb.endpoint (string : ) - Endpoint URL for database instance | database.dynamodb.aws_region (string : ) - AWS Region of database instance | database.dynamodb.aws_profile (string : ) - AWS named profile to use | database.dynamodb.aws_access_key_id (string : ) - AWS access key ID | database.dynamodb.aws_secret_access_key (string : ) - AWS secret access key | Note: endpoint aws_region aws_access_key_id aws_secret_access_key are not required and used mainly for experimental purposes when working with DynamoDB with different AWS credentials. | database.dynamodb.health_check_interval (duration : 0s) - Interval to run health check for the DynamoDB instance (won’t run if equal to 0). | . | database.local - Configuration section when using database.type=\"local\" . | database.local.path (string : \"~/lakefs/metadata\") - Local path on the filesystem to store embedded KV metadata, like branches and uncommitted entries | database.local.sync_writes (bool: true) - Ensure each write is written to the disk. Disable to increase performance | database.local.prefetch_size (int: 256) - How many items to prefetch when iterating over embedded KV records | database.local.enable_logging (bool: false) - Enable trace logging for local driver | . | . | listen_address (string : \"0.0.0.0:8000\") - A &lt;host&gt;:&lt;port&gt; structured string representing the address to listen on | auth.cache.enabled (bool : true) - Whether to cache access credentials and user policies in-memory. Can greatly improve throughput when enabled. | auth.cache.size (int : 1024) - How many items to store in the auth cache. Systems with a very high user count should use a larger value at the expense of ~1kb of memory per cached user. | auth.cache.ttl (time duration : \"20s\") - How long to store an item in the auth cache. Using a higher value reduces load on the database, but will cause changes longer to take effect for cached users. | auth.cache.jitter (time duration : \"3s\") - A random amount of time between 0 and this value is added to each item’s TTL. This is done to avoid a large bulk of keys expiring at once and overwhelming the database. | auth.encrypt.secret_key (string : required) - A random (cryptographically safe) generated string that is used for encryption and HMAC signing | auth.login_duration (time duration : \"168h\") - The duration the login token is valid for | auth.cookie_domain (string : \"\") - Domain attribute to set the access_token cookie on (the default is an empty string which defaults to the same host that sets the cookie) | auth.api.endpoint (string: https://external.service/api/v1) - URL to external Authorization Service described at authorization.yml; | auth.api.token (string: eyJhbGciOiJIUzI1NiIsInR5...) - API token used to authenticate requests to api endpoint . Note: It is best to keep this somewhere safe such as KMS or Hashicorp Vault, and provide it to the system at run time . | auth.remote_authenticator.enabled (bool : false) - If specified, also authenticate users via this Remote Authenticator server. | auth.remote_authenticator.endpoint (string : required) - Endpoint URL of the remote authentication service (e.g. https://my-auth.example.com/auth). | auth.remote_authenticator.default_user_group (string : Viewers) - Create users in this group (i.e Viewers, Developers, etc). | auth.remote_authenticator.request_timeout (duration : 10s) - If specified, timeout for remote authentication requests. | auth.cookie_auth_verification.validate_id_token_claims (map[string]string : ) - When a user tries to access lakeFS, validate that the ID token contains these claims with the corresponding values. | auth.cookie_auth_verification.default_initial_groups (string[] : [])` - By default, users will be assigned to these groups | auth.cookie_auth_verification.initial_groups_claim_name (string[] : []) - Use this claim from the ID token to provide the initial group for new users. This will take priority if auth.cookie_auth_verification.default_initial_groups is also set. | auth.cookie_auth_verification.friendly_name_claim_name (string[] : ) - If specified, the value from the claim with this name will be used as the user’s display name. | auth.cookie_auth_verification.external_user_id_claim_name - (string : ) - If specified, the value from the claim with this name will be used as the user’s id name. | auth.cookie_auth_verification.auth_source - (string : ) - If specified, user will be labeled with this auth source. | auth.oidc.default_initial_groups (string[] : []) - By default, OIDC users will be assigned to these groups | auth.oidc.initial_groups_claim_name (string[] : []) - Use this claim from the ID token to provide the initial group for new users. This will take priority if auth.oidc.default_initial_groups is also set. | auth.oidc.friendly_name_claim_name (string[] : ) - If specified, the value from the claim with this name will be used as the user’s display name. | auth.oidc.validate_id_token_claims (map[string]string : ) - When a user tries to access lakeFS, validate that the ID token contains these claims with the corresponding values. | blockstore.type (one of [\"local\", \"s3\", \"gs\", \"azure\", \"mem\"] : required). Block adapter to use. This controls where the underlying data will be stored | blockstore.default_namespace_prefix (string : ) - Use this to help your users choose a storage namespace for their repositories. If specified, the storage namespace will be filled with this default value as a prefix when creating a repository from the UI. The user may still change it to something else. | blockstore.local.path (string: \"~/lakefs/data\") - When using the local Block Adapter, which directory to store files in | blockstore.local.import_enabled (bool: false) - Enable import for local Block Adapter, relevant only if you are using shared location | blockstore.local.import_hidden (bool: false) - When enabled import will scan and import any file or folder that starts with a dot character. | blockstore.local.allowed_external_prefixes ([]string: []) - List of absolute path prefixes used to match any access for external location (ex: /var/data/). Empty list mean no access to external location. | blockstore.gs.credentials_file (string : ) - If specified will be used as a file path of the JSON file that contains your Google service account key | blockstore.gs.credentials_json (string : ) - If specified will be used as JSON string that contains your Google service account key (when credentials_file is not set) | blockstore.gs.pre_signed_expiry (time duration : \"15m\") - Expiry of pre-signed URL. | blockstore.azure.storage_account (string : ) - If specified, will be used as the Azure storage account | blockstore.azure.storage_access_key (string : ) - If specified, will be used as the Azure storage access key | blockstore.azure.pre_signed_expiry (time duration : \"15m\") - Expiry of pre-signed URL. | blockstore.s3.region (string : \"us-east-1\") - Default region for lakeFS to use when interacting with S3. | blockstore.s3.profile (string : ) - If specified, will be used as a named credentials profile | blockstore.s3.credentials_file (string : ) - If specified, will be used as a credentials file | blockstore.s3.credentials.access_key_id (string : ) - If specified, will be used as a static set of credential | blockstore.s3.credentials.secret_access_key (string : ) - If specified, will be used as a static set of credential | blockstore.s3.credentials.session_token (string : ) - If specified, will be used as a static session token | blockstore.s3.endpoint (string : ) - If specified, custom endpoint for the AWS S3 API (https://s3_compatible_service_endpoint:port) | blockstore.s3.force_path_style (bool : false) - When true, use path-style S3 URLs (https:/// instead of https://.) . | blockstore.s3.streaming_chunk_size (int : 1048576) - Object chunk size to buffer before streaming to blockstore (use a lower value for less reliable networks). Minimum is 8192. | blockstore.s3.streaming_chunk_timeout (time duration : \"60s\") - Per object chunk timeout for blockstore streaming operations (use a larger value for less reliable networks). | blockstore.s3.discover_bucket_region (bool : true) - (Can be turned off if the underlying S3 bucket doesn’t support the GetBucketRegion API). | blockstore.s3.skip_verify_certificate_test_only (boolean : false) - Skip certificate verification while connecting to the storage endpoint. Should be used only for testing. | blockstore.s3.server_side_encryption (string : ) - Server side encryption format used (Example on AWS using SSE-KMS while passing “aws:kms”) | blockstore.s3.server_side_encryption_kms_key_id (string : ) - Server side encryption KMS key ID | blockstore.s3.pre_signed_expiry (time duration : \"15m\") - Expiry of pre-signed URL. | graveler.reposiory_cache.size (int : 1000) - How many items to store in the repository cache. | graveler.reposiory_cache.ttl (time duration : \"5s\") - How long to store an item in the repository cache. | graveler.reposiory_cache.jitter (time duration : \"2s\") - A random amount of time between 0 and this value is added to each item’s TTL. | graveler.commit_cache.size (int : 50000) - How many items to store in the commit cache. | graveler.commit_cache.ttl (time duration : \"10m\") - How long to store an item in the commit cache. | graveler.commit_cache.jitter (time duration : \"2s\") - A random amount of time between 0 and this value is added to each item’s TTL. | graveler.background.rate_limit (int : 0) - Advence configuration to control background work done rate limit in requests per second (default: 0 - unlimited). | committed.local_cache - an object describing the local (on-disk) cache of metadata from permanent storage: . | committed.local_cache.size_bytes (int : 1073741824) - bytes for local cache to use on disk. The cache may use more storage for short periods of time. | committed.local_cache.dir (string, ~/lakefs/local_tier) - directory to store local cache. | committed.local_cache.range_proportion (float : 0.9) - proportion of local cache to use for storing ranges (leaves of committed metadata storage). | committed.local_cache.range.open_readers (int : 500) - maximal number of unused open SSTable readers to keep for ranges. | committed.local_cache.range.num_shards (int : 30) - sharding factor for open SSTable readers for ranges. Should be at least sqrt(committed.local_cache.range.open_readers). | committed.local_cache.metarange_proportion (float : 0.1) - proportion of local cache to use for storing metaranges (roots of committed metadata storage). | committed.local_cache.metarange.open_readers (int : 50) - maximal number of unused open SSTable readers to keep for metaranges. | committed.local_cache.metarange.num_shards (int : 10) - sharding factor for open SSTable readers for metaranges. Should be at least sqrt(committed.local_cache.metarange.open_readers). | . | committed.block_storage_prefix (string : _lakefs) - Prefix for metadata file storage in each repository’s storage namespace | committed.permanent.min_range_size_bytes (int : 0) - Smallest allowable range in metadata. Increase to somewhat reduce random access time on committed metadata, at the cost of increased committed metadata storage cost. | committed.permanent.max_range_size_bytes (int : 20971520) - Largest allowable range in metadata. Should be close to the size at which fetching from remote storage becomes linear. | committed.permanent.range_raggedness_entries (int : 50_000) - Average number of object pointers to store in each range (subject to min_range_size_bytes and max_range_size_bytes). | committed.sstable.memory.cache_size_bytes (int : 200_000_000) - maximal size of in-memory cache used for each SSTable reader. | email.smtp_host (string) - A string representing the URL of the SMTP host. | email.smtp_port (int) - An integer representing the port of the SMTP service (465, 587, 993, 25 are some standard ports) | email.use_ssl (bool : false) - Use SSL connection with SMTP host. | email.username (string) - A string representing the username of the specific account at the SMTP. It’s recommended to provide this value at runtime from a secret vault of some sort. | email.password (string) - A string representing the password of the account. It’s recommended to provide this value at runtime from a secret vault of some sort. | email.local_name (string) - A string representing the hostname sent to the SMTP server with the HELO command. By default, “localhost” is sent. | email.sender (string) - A string representing the email account which is set as the sender. | email.limit_every_duration (duration : 1m) - The average time between sending emails. If zero is entered, there is no limit to the amount of emails that can be sent. | email.burst (int: 10) - Maximal burst of emails before applying limit_every_duration. The zero value means no burst and therefore no emails can be sent. | email.lakefs_base_url (string : \"http://localhost:8000\") - A string representing the base lakefs endpoint to be directed to when emails are sent inviting users, reseting passwords etc. | gateways.s3.domain_name (string : \"s3.local.lakefs.io\") - a FQDN representing the S3 endpoint used by S3 clients to call this server (*.s3.local.lakefs.io always resolves to 127.0.0.1, useful for local development, if using virtual-host addressing. | gateways.s3.region (string : \"us-east-1\") - AWS region we’re pretending to be in, it should match the region configuration used in AWS SDK clients | gateways.s3.fallback_url (string) - If specified, requests with a non-existing repository will be forwarded to this URL. This can be useful for using lakeFS side-by-side with S3, with the URL pointing at an S3Proxy instance. | stats.enabled (bool : true) - Whether to periodically collect anonymous usage statistics | stats.flush_interval (duration : 30s) - Interval used to post anonymous statistics collected | stats.flush_size (int : 100) - A size (in records) of anonymous statistics collected in which we post | security.audit_check_interval (duration : 24h) - Duration in which we check for security audit. | ui.enabled (bool: true) - Whether to server the embedded UI from the binary | . ",
    "url": "/v0.98/reference/configuration.html#reference",
    
    "relUrl": "/reference/configuration.html#reference"
  },"96": {
    "doc": "Configuration",
    "title": "Using Environment Variables",
    "content": "All the configuration variables can be set or overridden using environment variables. To set an environment variable, prepend LAKEFS_ to its name, convert it to upper case, and replace . with _: . For example, logging.format becomes LAKEFS_LOGGING_FORMAT, blockstore.s3.region becomes LAKEFS_BLOCKSTORE_S3_REGION, etc. ",
    "url": "/v0.98/reference/configuration.html#using-environment-variables",
    
    "relUrl": "/reference/configuration.html#using-environment-variables"
  },"97": {
    "doc": "Configuration",
    "title": "Example: Local Development with PostgreSQL database",
    "content": "--- listen_address: \"0.0.0.0:8000\" database: type: \"postgres\" postgres: connection_string: \"postgres://localhost:5432/postgres?sslmode=disable\" logging: format: text level: DEBUG output: \"-\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc09e90b6641\" blockstore: type: local local: path: \"~/lakefs/dev/data\" gateways: s3: region: us-east-1 . ",
    "url": "/v0.98/reference/configuration.html#example-local-development-with-postgresql-database",
    
    "relUrl": "/reference/configuration.html#example-local-development-with-postgresql-database"
  },"98": {
    "doc": "Configuration",
    "title": "Example: AWS Deployment with DynamoDB database",
    "content": "--- logging: format: json level: WARN output: \"-\" database: type: \"dynamodb\" dynamodb: table_name: \"kvstore\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: s3 s3: region: us-east-1 # optional, fallback in case discover from bucket is not supported credentials_file: /secrets/aws/credentials profile: default . ",
    "url": "/v0.98/reference/configuration.html#example-aws-deployment-with-dynamodb-database",
    
    "relUrl": "/reference/configuration.html#example-aws-deployment-with-dynamodb-database"
  },"99": {
    "doc": "Configuration",
    "title": "Example: Google Storage",
    "content": "--- logging: format: json level: WARN output: \"-\" database: type: \"postgres\" postgres: connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: gs gs: credentials_file: /secrets/lakefs-service-account.json . ",
    "url": "/v0.98/reference/configuration.html#example-google-storage",
    
    "relUrl": "/reference/configuration.html#example-google-storage"
  },"100": {
    "doc": "Configuration",
    "title": "Example: MinIO",
    "content": "--- logging: format: json level: WARN output: \"-\" database: type: \"postgres\" postgres: connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: s3 s3: force_path_style: true endpoint: http://localhost:9000 discover_bucket_region: false credentials: access_key_id: minioadmin secret_access_key: minioadmin . ",
    "url": "/v0.98/reference/configuration.html#example-minio",
    
    "relUrl": "/reference/configuration.html#example-minio"
  },"101": {
    "doc": "Configuration",
    "title": "Example: Azure blob storage",
    "content": "--- logging: format: json level: WARN output: \"-\" database: type: \"postgres\" postgres: connection_string: \"postgres://user:pass@lakefs.rds.amazonaws.com:5432/postgres\" auth: encrypt: secret_key: \"10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc\" blockstore: type: azure azure: storage_account: exampleStorageAcount storage_access_key: ExampleAcessKeyMD7nkPOWgV7d4BUjzLw== . ",
    "url": "/v0.98/reference/configuration.html#example-azure-blob-storage",
    
    "relUrl": "/reference/configuration.html#example-azure-blob-storage"
  },"102": {
    "doc": "Configuration",
    "title": "Configuration",
    "content": " ",
    "url": "/v0.98/reference/configuration.html",
    
    "relUrl": "/reference/configuration.html"
  },"103": {
    "doc": "Contributing",
    "title": "Contributing to lakeFS",
    "content": "Thank you for your interest in contributing to our project. Whether it’s a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure that we have all the necessary information to effectively respond to your bug report or contribution. Don’t know where to start? Reach out on the #dev channel on our Slack and we will help you get started. We also recommend this free series about contributing to OSS projects. ",
    "url": "/v0.98/contributing.html#contributing-to-lakefs",
    
    "relUrl": "/contributing.html#contributing-to-lakefs"
  },"104": {
    "doc": "Contributing",
    "title": "Getting Started",
    "content": "Before you get started, we kindly ask that you: . | Check out the code of conduct. | Sign the lakeFS CLA when making your first pull request (individual / corporate) | Submit any security issues directly to security@treeverse.io. | Contributions should have an associated GitHub issue. | Before making major contributions, please reach out to us on the #dev channel on Slack. We will make sure no one else is working on the same feature. | . ",
    "url": "/v0.98/contributing.html#getting-started",
    
    "relUrl": "/contributing.html#getting-started"
  },"105": {
    "doc": "Contributing",
    "title": "Setting up an Environment",
    "content": "This section was tested on macOS and Linux (Fedora 32, Ubuntu 20.04) - Your mileage may vary . Our Go release workflow holds the Go and Node.js versions we currently use under go-version and node-version compatibly. The Java workflows use Maven 3.8.x (but any recent version of Maven should work). | Install the required dependencies for your OS: . | Git | GNU make (probably best to install from your OS package manager such as apt or brew) | Docker | Go | Node.js &amp; npm | Maven to build and test Spark client codes. | Java 8 . | Apple M1 users can install this from Azul Zulu Builds for Java JDK. Builds for Intel-based Macs are available from java.com. | . | Optional - PostgreSQL 11 (useful for running and debugging locally) | Optional - Rust &amp; Cargo (only needed if you want to build the Delta Lake diff plugin) | . | Clone the repository from GitHub. This gives you read-only access to the repository. To contribute, see the next section. | Build the project: . make build . Note: make build won’t work for Windows users. | Make sure tests are passing. The following should not return any errors: . make test . | . ",
    "url": "/v0.98/contributing.html#setting-up-an-environment",
    
    "relUrl": "/contributing.html#setting-up-an-environment"
  },"106": {
    "doc": "Contributing",
    "title": "Before creating a pull request",
    "content": ". | Review this document in full. | Make sure there’s an open issue on GitHub that this pull request addresses, and that it isn’t labeled x/wontfix. | Fork the lakeFS repository. | If you’re adding new functionality, create a new branch named feature/&lt;DESCRIPTIVE NAME&gt;. | If you’re fixing a bug, create a new branch named fix/&lt;DESCRIPTIVE NAME&gt;-&lt;ISSUE NUMBER&gt;. | . ",
    "url": "/v0.98/contributing.html#before-creating-a-pull-request",
    
    "relUrl": "/contributing.html#before-creating-a-pull-request"
  },"107": {
    "doc": "Contributing",
    "title": "Testing your change",
    "content": "Once you’ve made the necessary changes to the code, make sure the tests pass: . Run unit tests: . make test . Check that linting rules are passing. make checks-validator . You will need GNU diff to run this. On the macOS it can be installed with brew install diffutils . lakeFS uses go fmt as a style guide for Go code. Run system-tests: . make system-tests . Want to dive deeper into our system tests infrastructure? Need to debug the tests? Follow this documentation. ",
    "url": "/v0.98/contributing.html#testing-your-change",
    
    "relUrl": "/contributing.html#testing-your-change"
  },"108": {
    "doc": "Contributing",
    "title": "Submitting a pull request",
    "content": "Open a GitHub pull request with your change. The PR description should include a brief explanation of your change. You should also mention the related GitHub issue. If the issue should be automatically closed after the merge, please link it to the PR. After submitting your pull request, GitHub Actions will automatically run tests on your changes and make sure that your updated code builds and runs on Go 1.19.x. Check back shortly after submitting your pull request to make sure that your code passes these checks. If any of the checks come back with a red X, then do your best to address the errors. A developer from our team will review your pull request, and may request some changes to it. After the request is approved, it will be merged to our main branch. ",
    "url": "/v0.98/contributing.html#submitting-a-pull-request",
    
    "relUrl": "/contributing.html#submitting-a-pull-request"
  },"109": {
    "doc": "Contributing",
    "title": "Documentation",
    "content": "Any contribution to the docs, whether it is in conjunction with a code contribution or as a standalone, is appreciated. Documentation of features and changes in behaviour should be included in the pull request. You can create separate pull requests for documentation changes only. 📝 Notice! lakeFS documentation is written using Markdown. make sure to familiarize yourself with the Markdown Guide. Customizing the lakeFS docs site should follow the following guidelines: Just The Docs Customization and style-guide. lakeFS Style Guide: . | Don’t use unnecessary tech jargon or vague/wordy constructions - keep it friendly, not condescending. | Be inclusive and welcoming - use gender-neutral words and pronouns when talking about abstract people like developers). | Replace complex expressions with simpler ones. | Keep it short - 25-30 words max per sentence. Otherwise, your readers might get lost on the way. | Use active voice instead of passive. For example: This feature can be used to do task X. vs. You can use this feature to do task X. The second one reads much better, right? | You can explain things better by including examples. Show, not tell. Use illustrations, images, gifs, code snippets, etc. | Establish a visual hierarchy to help people quickly find the information they need. Use text formatting to create levels of title and subtitle (such as h1 to h6 headings in HTML). | . Test your changes locally . To render the documentation locally and preview changes you can run the Jeykll server under Docker. | Launch the Docker container: . docker run --rm \\ --name lakefs_docs \\ -e TZ=\"Etc/UTC\" \\ --publish 4000:4000 --publish 35729:35729 \\ --volume=\"$PWD/docs:/srv/jekyll:Z\" \\ --volume=\"$PWD/docs/.jekyll-bundle-cache:/usr/local/bundle:Z\" \\ --interactive --tty \\ jekyll/jekyll:3.8 \\ jekyll serve --livereload . | The first time you run the container it will need to download dependencies and will take several minutes to be ready. Once you see the following output, the docs server is ready to open in your web browser: . Server running... press ctrl-c to stop. | When you make a change to a page’s source the server will automatically rebuild the page which will be shown in the server log by this entry: . Regenerating: 1 file(s) changed at 2023-01-26 08:34:47 contributing.md Remote Theme: Using theme pmarsceill/just-the-docs . This can take a short while—you’ll see something like this in the server’s output when it’s done...done in 34.714073460 seconds. Your page will automatically reload to show the changes. | . If you are doing lots of work on the docs you may want to leave the Docker container in place (so that you don’t have to wait for the dependencies to load each time you re-create it). To do this replace the --rm with --detach in the docker run command, and use docker logs -f lakefs_docs to view the server log. Link Checking locally . When making a pull request to lakeFS that involves a docs/* file, a GitHub action will automagically check the links. You can also run this link checker manually on your local machine: . | Build the site: . docker run --rm \\ --name lakefs_docs \\ -e TZ=\"Etc/UTC\" \\ --volume=\"$PWD/docs:/srv/jekyll:Z\" \\ --volume=\"$PWD/docs/.jekyll-bundle-cache:/usr/local/bundle:Z\" \\ --interactive --tty \\ jekyll/jekyll:3.8 \\ jekyll build --config _config.yml -d _site --watch . | Check the links: . docker run --rm \\ --name lakefs_docs_lychee \\ --volume \"$PWD:/data\"\\ --volume \"/tmp:/output\"\\ --tty \\ lycheeverse/lychee:master \\ --exclude-file /data/docs/.lycheeignore \\ --output /output/lychee_report.md \\ --format markdown \\ /data/docs/_site . | Review the lychee_report.md in your local /tmp folder . | . CHANGELOG.md . Any user-facing change should be labeled with include-changelog. The PR title should contain a concise summary of the feature or fix and the description should have the GitHub issue number. When we publish a new version of lakeFS, we will add this to the relevant version section of the changelog. If the change should not be included in the changelog, label it with exclude-changelog. ",
    "url": "/v0.98/contributing.html#documentation",
    
    "relUrl": "/contributing.html#documentation"
  },"110": {
    "doc": "Contributing",
    "title": "Contributing",
    "content": " ",
    "url": "/v0.98/contributing.html",
    
    "relUrl": "/contributing.html"
  },"111": {
    "doc": "Copying data to/from lakeFS",
    "title": "Using DistCp",
    "content": "Apache Hadoop DistCp (distributed copy) is a tool used for large inter/intra-cluster copying. You can easily use it with your lakeFS repositories. ",
    "url": "/v0.98/howto/copying.html#using-distcp",
    
    "relUrl": "/howto/copying.html#using-distcp"
  },"112": {
    "doc": "Copying data to/from lakeFS",
    "title": "Table of contents",
    "content": ". | Using DistCp . | Between lakeFS repositories | Between S3 buckets and lakeFS . | From S3 to lakeFs | From lakeFS to S3 | . | . | Using Rclone . | Creating a remote for lakeFS in Rclone . | Option 1: Add an entry in your Rclone configuration file | Option 2: Use the Rclone interactive config command | . | Syncing S3 and lakeFS | Syncing a local directory and lakeFS | . | . Note . In the following examples, we set AWS credentials on the command line for clarity. In production, you should set these properties using one of Hadoop’s standard ways of Authenticating with S3. Between lakeFS repositories . You can use DistCP to copy between two different lakeFS repositories. Replace the access key pair with your lakeFS access key pair: . hadoop distcp \\ -Dfs.s3a.path.style.access=true \\ -Dfs.s3a.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\ -Dfs.s3a.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\ -Dfs.s3a.endpoint=\"https://lakefs.example.com\" \\ \"s3a://example-repo-1/main/example-file.parquet\" \\ \"s3a://example-repo-2/main/example-file.parquet\" . Between S3 buckets and lakeFS . To copy data from an S3 bucket to a lakeFS repository, use Hadoop’s per-bucket configuration. In the following examples, replace the first access key pair with your lakeFS key pair, and the second one with your AWS IAM key pair: . From S3 to lakeFs . hadoop distcp \\ -Dfs.s3a.path.style.access=true \\ -Dfs.s3a.bucket.example-repo.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\ -Dfs.s3a.bucket.example-repo.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\ -Dfs.s3a.bucket.example-repo.endpoint=\"https://lakefs.example.com\" \\ -Dfs.s3a.bucket.example-bucket.access.key=\"AKIAIOSFODNN3EXAMPLE\" \\ -Dfs.s3a.bucket.example-bucket.secret.key=\"wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY\" \\ \"s3a://example-bucket/example-file.parquet\" \\ \"s3a://example-repo/main/example-file.parquet\" . From lakeFS to S3 . hadoop distcp \\ -Dfs.s3a.path.style.access=true \\ -Dfs.s3a.bucket.example-repo.access.key=\"AKIAIOSFODNN7EXAMPLE\" \\ -Dfs.s3a.bucket.example-repo.secret.key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\ -Dfs.s3a.bucket.example-repo.endpoint=\"https://lakefs.example.com\" \\ -Dfs.s3a.bucket.example-bucket.access.key=\"AKIAIOSFODNN3EXAMPLE\" \\ -Dfs.s3a.bucket.example-bucket.secret.key=\"wJalrXUtnFEMI/K3MDENG/bPxRfiCYEXAMPLEKEY\" \\ \"s3a://example-repo/main/myfile\" \\ \"s3a://example-bucket/myfile\" . ",
    "url": "/v0.98/howto/copying.html#table-of-contents",
    
    "relUrl": "/howto/copying.html#table-of-contents"
  },"113": {
    "doc": "Copying data to/from lakeFS",
    "title": "Using Rclone",
    "content": "Rclone is a command line program to sync files and directories between cloud providers. To use it with lakeFS, create an Rclone remote as describe below and then use it as you would any other Rclone remote. Creating a remote for lakeFS in Rclone . To add the remote to Rclone, choose one of the following options: . Option 1: Add an entry in your Rclone configuration file . | Find the path to your Rclone configuration file and copy it for the next step. rclone config file # output: # Configuration file is stored at: # /home/myuser/.config/rclone/rclone.conf . | If your lakeFS access key is already set in an AWS profile or environment variables, run the following command, replacing the endpoint property with your lakeFS endpoint: . cat &lt;&lt;EOT &gt;&gt; /home/myuser/.config/rclone/rclone.conf [lakefs] type = s3 provider = AWS endpoint = https://lakefs.example.com no_check_bucket = true EOT . | Otherwise, also include your lakeFS access key pair in the Rclone configuration file: . cat &lt;&lt;EOT &gt;&gt; /home/myuser/.config/rclone/rclone.conf [lakefs] type = s3 provider = AWS env_auth = false access_key_id = AKIAIOSFODNN7EXAMPLE secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY endpoint = https://lakefs.example.com no_check_bucket = true EOT . | . Option 2: Use the Rclone interactive config command . Run this command and follow the instructions: . rclone config . Choose AWS S3 as your type of storage, and enter your lakeFS endpoint as your S3 endpoint. You will have to choose whether you use your environment for authentication (recommended), or enter the lakeFS access key pair into the Rclone configuration. Select “Edit advanced config” and accept defaults for all values except no_check_bucket: . If set, don't attempt to check the bucket exists or create it. This can be useful when trying to minimize the number of transactions Rclone carries out, if you know the bucket exists already. This might also be needed if the user you're using doesn't have bucket creation permissions. Before v1.52.0, this would have passed silently due to a bug. Enter a boolean value (true or false). Press Enter for the default (\"false\"). no_check_bucket&gt; yes . Syncing S3 and lakeFS . rclone sync mys3remote://mybucket/path/ lakefs:example-repo/main/path . Syncing a local directory and lakeFS . rclone sync /home/myuser/path/ lakefs:example-repo/main/path . ",
    "url": "/v0.98/howto/copying.html#using-rclone",
    
    "relUrl": "/howto/copying.html#using-rclone"
  },"114": {
    "doc": "Copying data to/from lakeFS",
    "title": "Copying data to/from lakeFS",
    "content": " ",
    "url": "/v0.98/howto/copying.html",
    
    "relUrl": "/howto/copying.html"
  },"115": {
    "doc": "In Test",
    "title": "In Test",
    "content": "As part of our routine work with data we develop new code, improve and upgrade old code, upgrade infrastructures, and test new technologies. lakeFS enables a safe test environment on your data lake without the need to copy or mock data, work on the pipelines or involve DevOps. Creating a branch provides you an isolated environment with a snapshot of your repository (any part of your data lake you chose to manage on lakeFS). While working on your own branch in isolation, all other data users will be looking at the repository’s main branch. They can’t see your changes, and you don’t see changes to main done after you created the branch. No worries, no data duplication is done, it’s all metadata management behind the scenes. Let’s look at 2 examples of a test environment and their branching models. Example 1: Upgrading Spark and using Reset action . You installed the latest version of Apache Spark. As a first step you’ll test your Spark jobs to see that the upgrade doesn’t have any undesired side effects. For this purpose, you may create a branch (testing-spark-3.0) which will only be used to test the Spark upgrade, and discarded later. Jobs may run smoothly (the theoretical possibility exists!), or they may fail halfway through, leaving you with some intermediate partitions, data and metadata. In this case, you can simply reset the branch to its original state, without worrying about the intermediate results of your last experiment, and perform another (hopefully successful) test in an isolated branch. Reset actions are atomic and immediate, so no manual cleanup is required. Once testing is completed, and you have achieved the desired result, you can delete this experimental branch, and all data not used on any other branch will be deleted with it. Creating a testing branch: . lakectl branch create \\ lakefs://example-repo/testing-spark-3 \\ --source lakefs://example-repo/main # output: # created branch 'testing-spark-3' . Resetting changes to a branch: . lakectl branch reset lakefs://example-repo/testing-spark-3 # are you sure you want to reset all uncommitted changes?: y█ . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. Example 2: Collaborate &amp; Compare - Which option is better? . Easily compare by testing which one performs better on your data set. Examples may be: . | Different computation tools, e.g Spark vs. Presto | Different compression algorithms | Different Spark configurations | Different code versions of an ETL | . Run each experiment on its own independent branch, while the main remains untouched. Once both experiments are done, create a comparison query (using Hive or Presto or any other tool of your choice) to compare data characteristics, performance or any other metric you see fit. With lakeFS you don’t need to worry about creating data paths for the experiments, copying data, and remembering to delete it. It’s substantially easier to avoid errors and maintain a clean lake after. Reading from and comparing branches using Spark: . val dfExperiment1 = sc.read.parquet(\"s3a://example-repo/experiment-1/events/by-date\") val dfExperiment2 = sc.read.parquet(\"s3a://example-repo/experiment-2/events/by-date\") dfExperiment1.groupBy(\"...\").count() dfExperiment2.groupBy(\"...\").count() // now we can compare the properties of the data itself . ",
    "url": "/v0.98/understand/data_lifecycle_management/data-devenv.html",
    
    "relUrl": "/understand/data_lifecycle_management/data-devenv.html"
  },"116": {
    "doc": "dbt",
    "title": "Maintaining environments with dbt and lakeFS",
    "content": "dbt can run on lakeFS with a Spark adapter or Presto/Trino adapter. Both Spark and Presto use Hive metastore or Glue to manage tables and views. When creating a branch in lakeFS, you receive a logical copy of the data that can be accessed by s3://my-repo/branch/... To run a dbt project on a newly created branch, you need to have a copy of the metadata as well. The lakectl dbt command generates all the metadata needed in order to work on the newly created branch, continuing from the last state in the source branch. The dbt lakectl command does this using dbt commands and lakectl metastore commands. ",
    "url": "/v0.98/integrations/dbt.html#maintaining-environments-with-dbt-and-lakefs",
    
    "relUrl": "/integrations/dbt.html#maintaining-environments-with-dbt-and-lakefs"
  },"117": {
    "doc": "dbt",
    "title": "Table of contents",
    "content": ". | Configuration . | Hive metastore | Glue | . | Views . | Using lakectl | Manual configuration | . | Create Schema | . ",
    "url": "/v0.98/integrations/dbt.html#table-of-contents",
    
    "relUrl": "/integrations/dbt.html#table-of-contents"
  },"118": {
    "doc": "dbt",
    "title": "Configuration",
    "content": "To run the lakectl-dbt commands you need to configure both dbt and lakectl. Assuming dbt is already configured, using either a Spark or Presto/Trino target you’ll need to add configurations to give lakeFS access to your catalog (metastore). This is done by adding the following configurations to the lakectl configuration file (by default ~/.lakectl.yaml) . Hive metastore . metastore: type: hive hive: uri: hive-metastore:9083 . Glue . metastore: type: glue glue: catalog-id: 123456789012 region: us-east-1 profile: default # optional, implies using a credentials file credentials: access_key_id: AKIAIOSFODNN7EXAMPLE secret_access_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY . ",
    "url": "/v0.98/integrations/dbt.html#configuration",
    
    "relUrl": "/integrations/dbt.html#configuration"
  },"119": {
    "doc": "dbt",
    "title": "Views",
    "content": "lakectl copies all the models materialized as tables and incremental directly on your metastore. However, copying views should be done manually or with lakectl. Using lakectl . The generate_schema_name macro could be used by lakectl to create models using dbt on a dynamic schema. The following command will add a macro to your project, allowing lakectl to run dbt on the destination schema using an environment variable. lakectl dbt generate-schema-macro . Manual configuration . If you don’t want to add the generate_schema_name macro to your project, you can create the views on the destination schema manually. For every run: . | use the --skip-views flag, | change the default schema to be the branch schema in your dbt configuration file, | run dbt on all views. | . dbt run --select config.materialized:view . ",
    "url": "/v0.98/integrations/dbt.html#views",
    
    "relUrl": "/integrations/dbt.html#views"
  },"120": {
    "doc": "dbt",
    "title": "Create Schema",
    "content": "Creating the schema From your dbt project run: . lakectl dbt create-branch-schema --branch my-branch --to-schema my_branch . You can find more advanced options here. ",
    "url": "/v0.98/integrations/dbt.html#create-schema",
    
    "relUrl": "/integrations/dbt.html#create-schema"
  },"121": {
    "doc": "dbt",
    "title": "dbt",
    "content": " ",
    "url": "/v0.98/integrations/dbt.html",
    
    "relUrl": "/integrations/dbt.html"
  },"122": {
    "doc": "Delta Lake",
    "title": "Using lakeFS with Delta Lake",
    "content": "Delta Lake Delta Lake is an open-source storage framework designed to improve performance and provide transactional guarantees to data lake tables. Because lakeFS is format-agnostic, you can save data in Delta format within a lakeFS repository and benefit from the advantages of both technologies. Specifically: . | ACID operations can span across many Delta tables. | CI/CD hooks can validate Delta table contents, schema, or even referential integrity. | lakeFS supports zero-copy branching for quick experimentation with full isolation. | . ",
    "url": "/v0.98/integrations/delta.html#using-lakefs-with-delta-lake",
    
    "relUrl": "/integrations/delta.html#using-lakefs-with-delta-lake"
  },"123": {
    "doc": "Delta Lake",
    "title": "Table of contents",
    "content": ". | Viewing Delta Lake table changes in lakeFS BETA . | Installing the Delta Lake diff plugin | . | Spark Configuration | Best Practices . | When running lakeFS inside your VPC (on AWS) | Using multi cluster writes (on AWS) | . | Further Reading | . ",
    "url": "/v0.98/integrations/delta.html#table-of-contents",
    
    "relUrl": "/integrations/delta.html#table-of-contents"
  },"124": {
    "doc": "Delta Lake",
    "title": "Viewing Delta Lake table changes in lakeFS BETA",
    "content": "Using lakeFS you can . | Compare different versions of Delta Lake tables | Get a detailed view of all Delta Lake table operations performed since the tables diverged. | . For example, comparing branches dev and main, we can see that the movies table has changed on dev since the branches diverged. Expanding the delete operation, we learn that all movies with a rating &lt; 4 were deleted from the table on the dev branch. Note: The diff is available as long as the table history in Delta is retained (30 days by default). A delta lake table history is derived from the delta log JSON files. Installing the Delta Lake diff plugin . To enable the Delta Lake diff feature, you need to install a plugin on the lakeFS server. You will find the plugin binary in the release tarball (versions &gt;= 0.97.3). Put the delta_diff binary under ~/.lakefs/plugins/diff on the machine where lakeFS is running. You can customize the location of the Delta Lake diff plugin by changing the diff.delta.plugin and plugin.properties.&lt;plugin name&gt;.path configurations in the .lakefs.yaml file. Notice: If you’re using the lakeFS docker image, the plugin is installed by default. ",
    "url": "/v0.98/integrations/delta.html#viewing-delta-lake-table-changes-in-lakefs-beta",
    
    "relUrl": "/integrations/delta.html#viewing-delta-lake-table-changes-in-lakefs-beta"
  },"125": {
    "doc": "Delta Lake",
    "title": "Spark Configuration",
    "content": "Given the native integration between Delta Lake and Spark, it’s most common that you’ll interact with Delta tables in a Spark environment. To configure a Spark environment to read from and write to a Delta table within a lakeFS repository, you need to set the proper credentials and endpoint in the S3 Hadoop configuration, like you’d do with any Spark environment. Once set, you can interact with Delta tables using regular Spark path URIs. Make sure that you include the lakeFS repository and branch name: . df.write.format(\"delta\").save(\"s3a://&lt;repo-name&gt;/&lt;branch-name&gt;/path/to/delta-table\") . Note: If using the Databricks Analytics Platform, see the integration guide for configuring a Databricks cluster to use lakeFS. ",
    "url": "/v0.98/integrations/delta.html#spark-configuration",
    
    "relUrl": "/integrations/delta.html#spark-configuration"
  },"126": {
    "doc": "Delta Lake",
    "title": "Best Practices",
    "content": "Production workflows should ideally write to a single lakeFS branch that could then be safely merged into main. This is because the Delta log is an auto-generated sequence of text files used to keep track of transactions on a Delta table sequentially. Writing to one Delta table from multiple lakeFS branches is possible, but note that it would result in conflicts if later attempting to merge one branch into the other. When running lakeFS inside your VPC (on AWS) . When lakeFS runs inside your private network, your Databricks cluster needs to be able to access it. This can be done by setting up a VPC peering between the two VPCs (the one where lakeFS runs and the one where Databricks runs). For this to work on Delta Lake tables, you would also have to disable multi-cluster writes with: . spark.databricks.delta.multiClusterWrites.enabled false . Using multi cluster writes (on AWS) . When using multi-cluster writes, Databricks overrides Delta’s S3-commit action. The new action tries to contact lakeFS from servers on Databricks’ own AWS account, which of course won’t be able to access your private network. So, if you must use multi-cluster writes, you’ll have to allow access from Databricks’ AWS account to lakeFS. If you are trying to achieve that, please reach out on Slack and the community will try to assist. ",
    "url": "/v0.98/integrations/delta.html#best-practices",
    
    "relUrl": "/integrations/delta.html#best-practices"
  },"127": {
    "doc": "Delta Lake",
    "title": "Further Reading",
    "content": "See Guaranteeing Consistency in Your Delta Lake Tables With lakeFS post on the lakeFS blog to learn how to guarantee data quality in a Delta table by utilizing lakeFS branches. ",
    "url": "/v0.98/integrations/delta.html#further-reading",
    
    "relUrl": "/integrations/delta.html#further-reading"
  },"128": {
    "doc": "Delta Lake",
    "title": "Delta Lake",
    "content": " ",
    "url": "/v0.98/integrations/delta.html",
    
    "relUrl": "/integrations/delta.html"
  },"129": {
    "doc": "Dremio",
    "title": "Using lakeFS with Dremio",
    "content": "Dremio is a next-generation data lake engine that liberates your data with live, interactive queries directly on cloud data lake storage, including S3 and lakeFS. ",
    "url": "/v0.98/integrations/dremio.html#using-lakefs-with-dremio",
    
    "relUrl": "/integrations/dremio.html#using-lakefs-with-dremio"
  },"130": {
    "doc": "Dremio",
    "title": "Configuration",
    "content": "Starting from version 3.2.3, Dremio supports Minio as an experimental S3-compatible plugin. Similarly, you can connect lakeFS with Dremio. Suppose you already have both lakeFS and Dremio deployed, and want to use Dremio to query your data in the lakeFS repositories. You can follow the steps listed below to configure on Dremio UI: . | click Add Data Lake. | Under File Stores, choose Amazon S3. | Under Advanced Options, check Enable compatibility mode (experimental). | Under Advanced Options &gt; Connection Properties, add fs.s3a.path.style.access and set the value to true. | Under Advanced Options &gt; Connection Properties, add fs.s3a.endpoint and set lakeFS S3 endpoint to the value. | Under the General tab, specify the access_key_id and secret_access_key provided by lakeFS server. | Click Save, and now you should be able to browse lakeFS repositories on Dremio. | . ",
    "url": "/v0.98/integrations/dremio.html#configuration",
    
    "relUrl": "/integrations/dremio.html#configuration"
  },"131": {
    "doc": "Dremio",
    "title": "Dremio",
    "content": " ",
    "url": "/v0.98/integrations/dremio.html",
    
    "relUrl": "/integrations/dremio.html"
  },"132": {
    "doc": "DuckDB",
    "title": "Using lakeFS with DuckDB",
    "content": "DuckDB is an in-process SQL OLAP database management system. ",
    "url": "/v0.98/integrations/duckdb.html#using-lakefs-with-duckdb",
    
    "relUrl": "/integrations/duckdb.html#using-lakefs-with-duckdb"
  },"133": {
    "doc": "DuckDB",
    "title": "Table of contents",
    "content": ". | Using lakeFS with DuckDB . | Configuration . | Configure the lakeFS S3 Gateway endpoint | Querying Data | Writing Data | . | . | . ",
    "url": "/v0.98/integrations/duckdb.html#table-of-contents",
    
    "relUrl": "/integrations/duckdb.html#table-of-contents"
  },"134": {
    "doc": "DuckDB",
    "title": "Configuration",
    "content": "Querying data in lakeFS from DuckDB is similar to querying data in S3 from DuckDB. It is done using the httpfs extension. Configure the lakeFS S3 Gateway endpoint . LOAD httpfs; SET s3_region='us-east-1'; SET s3_endpoint='lakefs.example.com'; SET s3_access_key_id='AKIAIOSFODNN7EXAMPLE'; SET s3_secret_access_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'; SET s3_url_style='path'; . Querying Data . Once configured, you can query data using the lakeFS S3 Gateway using the following URI pattern: . s3://&lt;REPOSITORY NAME&gt;/&lt;REFERENCE ID&gt;/&lt;PATH TO DATA&gt; . Since the S3 Gateway implemenets all S3 functionality required by DuckDB, you can query using globs and patterns, including support for Hive-partitioned data. Example: . SELECT * FROM parquet_scan('s3://example-repo/main/data/population/by-region/*.parquet', HIVE_PARTITIONING=1) ORDER BY name; . Writing Data . No special configuration required for writing to a branch. Assuming the configuration above and write permissions to a dev branch, a write operation would look like any DuckDB write: . CREATE TABLE sampled_population AS SELECT * FROM parquet_scan('s3://example-repo/main/data/population/by-region/*.parquet', HIVE_PARTITIONING=1) USING SAMPLE reservoir(50000 ROWS) REPEATABLE (100); COPY sampled_population TO 's3://example-repo/main/data/population/sample.parquet'; -- actual write happens here . ",
    "url": "/v0.98/integrations/duckdb.html#configuration",
    
    "relUrl": "/integrations/duckdb.html#configuration"
  },"135": {
    "doc": "DuckDB",
    "title": "DuckDB",
    "content": " ",
    "url": "/v0.98/integrations/duckdb.html",
    
    "relUrl": "/integrations/duckdb.html"
  },"136": {
    "doc": "ETL Testing Environment",
    "title": "ETL Testing with Isolated Dev/Test Environment",
    "content": " ",
    "url": "/v0.98/use_cases/etl_testing.html#etl-testing-with-isolated-devtest-environment",
    
    "relUrl": "/use_cases/etl_testing.html#etl-testing-with-isolated-devtest-environment"
  },"137": {
    "doc": "ETL Testing Environment",
    "title": "Why do I need multiple environments?",
    "content": "When working with a data lake, it’s useful to have replicas of your production environment. These replicas allow you to test these ETLs and understand changes to your data without impacting the consumers of the production data. Running ETL and transformation jobs directly in production without proper ETL Testing is a guaranteed way to have data issues flow into dashboards, ML models, and other consumers sooner or later. The most common approach to avoid making changes directly in production is to create and maintain multiple data environments and perform ETL testing on them. Dev environment to develop the data pipelines and test environment where pipeline changes are tested before pushing it to production. The issue with this approach is that it’s time-consuming and costly to maintain these separate dev/test environments to enable thoruogh effective ETL testing. And for larger teams it forces multiple people to share these environments, requiring significant co-ordination. ",
    "url": "/v0.98/use_cases/etl_testing.html#why-do-i-need-multiple-environments",
    
    "relUrl": "/use_cases/etl_testing.html#why-do-i-need-multiple-environments"
  },"138": {
    "doc": "ETL Testing Environment",
    "title": "How do I create dev/test environments with lakeFS?",
    "content": "lakeFS makes creating isolated dev/test environments for ETL testing instantaneous. This frees you from spending time on environment maintenance and makes it possible to create as many environments as needed. In a lakeFS repository, data is always located on a branch. You can think of each branch in lakeFS as its own environment. This is because branches are isolated, meaning changes on one branch have no effect other branches. Objects that remain unchanged between two branches are not copied, but rather shared to both branches via metadata pointers that lakeFS manages. If you make a change on one branch and want it reflected on another, you can perform a merge operation to update one branch with the changes from another. Let’s see an example of using multiple lakeFS branches for isolation. ",
    "url": "/v0.98/use_cases/etl_testing.html#how-do-i-create-devtest-environments-with-lakefs",
    
    "relUrl": "/use_cases/etl_testing.html#how-do-i-create-devtest-environments-with-lakefs"
  },"139": {
    "doc": "ETL Testing Environment",
    "title": "Using branches as development and testing environments",
    "content": "The key difference when using lakeFS for isolated data environments is that you can create them immediately before testing a change. And once new data is merged into production, you can delete the branch - effectively deleting the old environment. This is different from creating a long-living test environment used as a staging area to test all the updates. With lakeFS, we create a new branch for each change to production that we want to make. One benefit of this is the ability to test multiple changes at one time. ",
    "url": "/v0.98/use_cases/etl_testing.html#using-branches-as-development-and-testing-environments",
    
    "relUrl": "/use_cases/etl_testing.html#using-branches-as-development-and-testing-environments"
  },"140": {
    "doc": "ETL Testing Environment",
    "title": "Creating Dev/Test Environments with lakeFS for ETL testing",
    "content": "lakeFS supports UI, CLI (lakectl commandline utility) and several client API integrations to run the Git-like operations. Let us explore how to create dev/test environments using each of these options below. ",
    "url": "/v0.98/use_cases/etl_testing.html#creating-devtest-environments-with-lakefs-for-etl-testing",
    
    "relUrl": "/use_cases/etl_testing.html#creating-devtest-environments-with-lakefs-for-etl-testing"
  },"141": {
    "doc": "ETL Testing Environment",
    "title": "Using lakeFS Playground UI",
    "content": "In this tutorial, we will use lakeFS playground to create dev/test data environments for ETL testing. Playground allows you to spin up a lakeFS instance in a click, create different data environments by simply branching out of your data repository and develop &amp; test data pipelines in these isolated branches. First, let us spin up a playground instance. Once you have a live environment, login to your instance with access and secret keys. Then, you can work with the sample data repository my-repo that is created for you. Click on my-repo and notice that by default, the repo has a main branch created and sample_data preloaded to work with. You can create a new branch (say, test-env) by going to the Branches tab and clicking Create Branch. Once it is successful, you will see two branches under the repo: main and test-env. Now you can add, modify or delete objects under the test-env branch without affecting the data in the main branch. ",
    "url": "/v0.98/use_cases/etl_testing.html#using-lakefs-playground-ui",
    
    "relUrl": "/use_cases/etl_testing.html#using-lakefs-playground-ui"
  },"142": {
    "doc": "ETL Testing Environment",
    "title": "Using lakeFS Python Client API and Jupyter notebook",
    "content": "This use case shows how to create dev/test data environments for ETL testing using lakeFS branches. The following tutorial will use an existing lakeFS environment (i.e., playground), a jupyter notebook, and python lakefs_client API to demonstrate integration of lakeFS with Spark. You can run this tutorial on your local machine. Follow the tutorial video below to get started with the playground and jupyter notebook, or follow the instructions on this page. Prerequisites . To instantly spin up a lakeFS environment and to work with sample data, use lakeFS playground. To use lakeFS with your own production data or to run it in your own machine, check out Quickstart guide. Before getting started, you will need docker installed on your machine. Once you have docker installed and a lakeFS instance running, you can access the lakeFS demo notebooks by cloning the lakeFS-samples git repo. Follow along the steps below to create dev/test environment with lakeFS. | Start by cloning the lakeFS samples Git repository: git clone https://github.com/treeverse/lakeFS-samples.git &amp;&amp; cd 03-apache-spark-python-demo . | Run following commands to download and run Docker container which includes Python, Spark, Jupyter Notebook, JDK, Hadoop binaries, lakeFS Python client and Airflow (Docker image size is around 4.5GB): docker build -t lakefs-spark-python-demo . docker run -d -p 8888:8888 -p 4040:4040 -p 8080:8080 --user root -e GRANT_SUDO=yes -v $PWD:/home/jovyan -v $PWD/jupyter_notebook_config.py:/home/jovyan/.jupyter/jupyter_notebook_config.py --name lakefs-spark-python-demo lakefs-spark-python-demo . | Open JupyterLab UI http://127.0.0.1:8888/ in your web browser. | . Once you have successfully completed setup then open “Spark Demo” notebook from JupyterLab UI and follow the instructions in the notebook. Configuring lakeFS python client . Setup lakeFS access credentials for the lakeFS instance running. If you are using lakeFS playground, use the access key, secret key and endpoint details you received in your email. If you are running lakeFS using everything bagel docker, these details are found in docker-compose.yaml file under lakefs_setup section. lakefsAccessKey = '&lt;lakeFS Access Key&gt;' lakefsSecretKey = '&lt;lakeFS Secret Key&gt;' lakefsEndPoint = '&lt;lakeFS Endpoint URL&gt;' # e.g. 'https://expert-robin.lakefs-demo.io/' . Next, setup the storage namespace to a location in the bucket you have configured. The storage namespace is a location in the underlying storage where data for this repository will be stored. If you are using the lakeFS playground, you can find the storage namespace under the repository name. storageNamespace = 's3://&lt;S3 Bucket Name&gt;/' # e.g. \"s3://treeverse-demo-lakefs-storage-production/user_expert-robin/my-repo\" . You can use lakeFS through the UI, API or lakectl commandline. For this use-case, we use python lakefs_client to run lakeFS core operations. import lakefs_client from lakefs_client import models from lakefs_client.client import LakeFSClient # lakeFS credentials and endpoint configuration = lakefs_client.Configuration() configuration.username = lakefsAccessKey configuration.password = lakefsSecretKey configuration.host = lakefsEndPoint client = LakeFSClient(configuration) . lakeFS can be configured to work with Spark in two ways: . | Access lakeFS using the S3-compatible API | Access lakeFS using the lakeFS-specific Hadoop FileSystem | . Upload the sample data to main branch . To upload an object to the my-repo, use the following command. import os contentToUpload = open(os.path.expanduser('~')+'/lakefs_test.csv', 'rb') client.objects.upload_object( repository=\"my-repo\", branch=\"main\", path=fileName, content=contentToUpload) . Once uploaded, commit the changes to the main branch and attach some metadata to the commit as well. client.commits.commit( repository=\"my-repo\", branch=\"main\", commit_creation=models.CommitCreation( message='Added my first object!', metadata={'using': 'python_api'})) . In this example, we use lakeFS S3A gateway to read data from the storage bucket. dataPath = f\"s3a://my-repo/main/lakefs_test.csv\" df = spark.read.csv(dataPath) df.show() . Create a test branch . Let us start by creating a new branch test-env on the example repo my-repo. client.branches.create_branch( repository=\"my-repo\", branch_creation=models.BranchCreation( name=\"test-env\", source=\"main\")) . Now we can use Spark to write the csv file from main branch as a Parquet file to the test-env of our lakeFS repo. Suppose we accidentally write the dataframe back to “test-env” branch again, this time in append mode. df.write.mode('overwrite').parquet('s3a://my-repo/test-env/') df.write.mode('append').parquet('s3a://my-repo/test-env/') . What happens if we re-read in the data on both branches and perform a count on the resulting DataFrames? There will be twice as many rows in test-env branch. That is, we accidentally duplicated our data! Oh no! . Data duplication introduce errors into our data analytics, BI and machine learning efforts, hence we would like to avoid duplicating our data. On the main branch however, there is still just the original data - untouched by our spark code. This shows the utility of branch-based isolated environments with lakeFS. You can safely continue working with the data from main which is unharmed due to lakeFS isolation capabilities. ",
    "url": "/v0.98/use_cases/etl_testing.html#using-lakefs-python-client-api-and-jupyter-notebook",
    
    "relUrl": "/use_cases/etl_testing.html#using-lakefs-python-client-api-and-jupyter-notebook"
  },"143": {
    "doc": "ETL Testing Environment",
    "title": "Case Study: Enigma",
    "content": "Learn how Enigma increased Data Engineers efficiency using lakeFS’ branching to achieve islated development and staging environments. ",
    "url": "/v0.98/use_cases/etl_testing.html#case-study-enigma",
    
    "relUrl": "/use_cases/etl_testing.html#case-study-enigma"
  },"144": {
    "doc": "ETL Testing Environment",
    "title": "Further Reading",
    "content": "For further details on the best practices of ETL testing on data lakes with lakeFS go to the lakeFS Blog . ",
    "url": "/v0.98/use_cases/etl_testing.html#further-reading",
    
    "relUrl": "/use_cases/etl_testing.html#further-reading"
  },"145": {
    "doc": "ETL Testing Environment",
    "title": "ETL Testing Environment",
    "content": " ",
    "url": "/v0.98/use_cases/etl_testing.html",
    
    "relUrl": "/use_cases/etl_testing.html"
  },"146": {
    "doc": "Exporting Data",
    "title": "Exporting Data",
    "content": "The export operation copies all data from a given lakeFS commit to a designated object store location. For instance, the contents lakefs://example/main might be exported on s3://company-bucket/example/latest. Clients entirely unaware of lakeFS could use that base URL to access latest files on main. Clients aware of lakeFS can continue to use the lakeFS S3 endpoint to access repository files on s3://example/main, as well as other versions and uncommitted versions. Possible use-cases: . | External consumers of data don’t have access to your lakeFS installation. | Some data pipelines in the organization are not fully migrated to lakeFS. | You want to experiment with lakeFS as a side-by-side installation first. | Create copies of your data lake in other regions (taking into account read pricing). | . ",
    "url": "/v0.98/howto/export.html",
    
    "relUrl": "/howto/export.html"
  },"147": {
    "doc": "Exporting Data",
    "title": "Table of contents",
    "content": ". | Exporting Data With Spark . | Using spark-submit . | Networking | . | Using custom code (Notebook/Spark) | . | Success/Failure Indications | Export Rounds (Spark success files) | Example | Exporting Data with Docker | . ",
    "url": "/v0.98/howto/export.html#table-of-contents",
    
    "relUrl": "/howto/export.html#table-of-contents"
  },"148": {
    "doc": "Exporting Data",
    "title": "Exporting Data With Spark",
    "content": "Using spark-submit . You can use the export main in three different modes: . | Export all the objects from branch example-branch on example-repo repository to S3 location s3://example-bucket/prefix/: .... example-repo s3://example-bucket/prefix/ --branch=example-branch . | Export all the objects from a commit c805e49bafb841a0875f49cd555b397340bbd9b8 on example-repo repository to S3 location s3://example-bucket/prefix/: .... example-repo s3://example-bucket/prefix/ --commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8 . | Export only the diff between branch example-branch and commit c805e49bafb841a0875f49cd555b397340bbd9b8 on example-repo repository to S3 location s3://example-bucket/prefix/: .... example-repo s3://example-bucket/prefix/ --branch=example-branch --prev_commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8 . | . The complete spark-submit command would look as follows: . spark-submit --conf spark.hadoop.lakefs.api.url=https://&lt;LAKEFS_ENDPOINT&gt;/api/v1 \\ --conf spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY_ID&gt; \\ --conf spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_ACCESS_KEY&gt; \\ --packages io.lakefs:lakefs-spark-client-301_2.12:0.6.5 \\ --class io.treeverse.clients.Main export-app example-repo s3://example-bucket/prefix \\ --branch=example-branch . The command assumes that the Spark cluster has permissions to write to s3://example-bucket/prefix. Otherwise, add spark.hadoop.fs.s3a.access.key and spark.hadoop.fs.s3a.secret.key with the proper credentials. Networking . Spark export communicates with the lakeFS server. Very large repositories may require increasing a read timeout. If you run into timeout errors during communication from the Spark job to lakefs consider increasing these timeouts: . | Add -c spark.hadoop.lakefs.api.read.timeout_seconds=TIMEOUT_IN_SECONDS (default 10) to allow lakeFS more time to respond to requests. | Add -c spark.hadoop.lakefs.api.connection.timeout_seconds=TIMEOUT_IN_SECONDS (default 10) to wait longer for lakeFS to accept connections. | . Using custom code (Notebook/Spark) . Set up lakeFS Spark metadata client with the endpoint and credentials as instructed in the previous page. The client exposes the Exporter object with three export options: . | Export all the objects at the HEAD of a given branch. Does not include files that were added to that branch but were not committed. | . exportAllFromBranch(branch: String) . | Export ALL objects from a commit: | . exportAllFromCommit(commitID: String) . | Export just the diff between a commit and the HEAD of a branch. This is an ideal option for continuous exports of a branch since it will change only the files that have been changed since the previous commit. exportFrom(branch: String, prevCommitID: String) . | . ",
    "url": "/v0.98/howto/export.html#exporting-data-with-spark",
    
    "relUrl": "/howto/export.html#exporting-data-with-spark"
  },"149": {
    "doc": "Exporting Data",
    "title": "Success/Failure Indications",
    "content": "When the Spark export operation ends, an additional status file will be added to the root object storage destination. If all files were exported successfully, the file path will be of the form: EXPORT_&lt;commitID&gt;_&lt;ISO-8601-time-UTC&gt;_SUCCESS. For failures: the form will beEXPORT_&lt;commitID&gt;_&lt;ISO-8601-time-UTC&gt;_FAILURE, and the file will include a log of the failed files operations. ",
    "url": "/v0.98/howto/export.html#successfailure-indications",
    
    "relUrl": "/howto/export.html#successfailure-indications"
  },"150": {
    "doc": "Exporting Data",
    "title": "Export Rounds (Spark success files)",
    "content": "Some files should be exported before others, e.g., a Spark _SUCCESS file exported before other files under the same prefix might send the wrong indication. The export operation may contain several rounds within the same export. A failing round will stop the export of all the files of the next rounds. By default, lakeFS will use the SparkFilter and have 2 rounds for each export. The first round will export any non-Spark _SUCCESS files. Second round will export all Spark’s _SUCCESS files. You may override the default behavior by passing a custom filter to the Exporter. ",
    "url": "/v0.98/howto/export.html#export-rounds-spark-success-files",
    
    "relUrl": "/howto/export.html#export-rounds-spark-success-files"
  },"151": {
    "doc": "Exporting Data",
    "title": "Example",
    "content": ". | First configure the Exporter instance: . import io.treeverse.clients.{ApiClient, Exporter} import org.apache.spark.sql.SparkSession val endpoint = \"http://&lt;LAKEFS_ENDPOINT&gt;/api/v1\" val accessKey = \"&lt;LAKEFS_ACCESS_KEY_ID&gt;\" val secretKey = \"&lt;LAKEFS_SECRET_ACCESS_KEY&gt;\" val repo = \"example-repo\" val spark = SparkSession.builder().appName(\"I can export\").master(\"local\").getOrCreate() val sc = spark.sparkContext sc.hadoopConfiguration.set(\"lakefs.api.url\", endpoint) sc.hadoopConfiguration.set(\"lakefs.api.access_key\", accessKey) sc.hadoopConfiguration.set(\"lakefs.api.secret_key\", secretKey) // Add any required spark context configuration for s3 val rootLocation = \"s3://company-bucket/example/latest\" val apiClient = new ApiClient(endpoint, accessKey, secretKey) val exporter = new Exporter(spark, apiClient, repo, rootLocation) . | Now you can export all objects from main branch to s3://company-bucket/example/latest: . val branch = \"main\" exporter.exportAllFromBranch(branch) . | Assuming a previous successful export on commit f3c450d8cd0e84ac67e7bc1c5dcde9bef82d8ba7, you can alternatively export just the difference between main branch and the commit: . val branch = \"main\" val commit = \"f3c450d8cd0e84ac67e7bc1c5dcde9bef82d8ba7\" exporter.exportFrom(branch, commit) . | . ",
    "url": "/v0.98/howto/export.html#example",
    
    "relUrl": "/howto/export.html#example"
  },"152": {
    "doc": "Exporting Data",
    "title": "Exporting Data with Docker",
    "content": "This option is recommended if you don’t have Spark at your tool-set. It doesn’t support distribution across machines, therefore may have a lower performance. Using this method, you can export data from lakeFS to S3 using the export options (in a similar way to the Spark export): . | Export all objects from a branch example-branch on example-repo repository to S3 location s3://destination-bucket/prefix/: .... example-repo s3://destination-bucket/prefix/ --branch=\"example-branch\" . | Export all objects from a commit c805e49bafb841a0875f49cd555b397340bbd9b8 on example-repo repository to S3 location s3://destination-bucket/prefix/: .... example-repo s3://destination-bucket/prefix/ --commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8 . | Export only the diff between branch example-branch and commit c805e49bafb841a0875f49cd555b397340bbd9b8 on example-repo repository to S3 location s3://destination-bucket/prefix/: .... example-repo s3://destination-bucket/prefix/ --branch=\"example-branch\" --prev_commit_id=c805e49bafb841a0875f49cd555b397340bbd9b8 . | . You will need to add the relevant environment variables. The complete docker run command would look like: . docker run \\ -e LAKEFS_ACCESS_KEY_ID=XXX -e LAKEFS_SECRET_ACCESS_KEY=YYY \\ -e LAKEFS_ENDPOINT=https://&lt;LAKEFS_ENDPOINT&gt;/ \\ -e AWS_ACCESS_KEY_ID=XXX -e AWS_SECRET_ACCESS_KEY=YYY \\ treeverse/lakefs-rclone-export:latest \\ example-repo \\ s3://destination-bucket/prefix/ \\ --branch=\"example-branch\" . Note: This feature uses rclone, and specifically rclone sync. This can change the destination path, therefore the s3 destination location must be designated to lakeFS export. ",
    "url": "/v0.98/howto/export.html#exporting-data-with-docker",
    
    "relUrl": "/howto/export.html#exporting-data-with-docker"
  },"153": {
    "doc": "FAQ",
    "title": "FAQ",
    "content": "1. Is lakeFS open-source? . lakeFS is completely free, open-source, and licensed under the Apache 2.0 License. We maintain a public product roadmap and a Slack channel for open discussions. 2. How does lakeFS data versioning work? . lakeFS uses a copy-on-write mechanism to avoid data duplication. For example, creating a new branch is a metadata-only operation: no objects are actually copied. Only when an object changes does lakeFS create another version of the data in the storage. For more information, see Versioning internals. 3. How do I get support for my lakeFS installation? . We are extremely responsive on our Slack channel, and we make sure to prioritize the most pressing issues for the community. For SLA-based support, please contact us at support@treeverse.io. 4. Do you collect data from your active installations? . We collect anonymous usage statistics to understand the patterns of use and to detect product gaps we may have so we can fix them. This is optional and may be turned off by setting stats.enabled to false. See the configuration reference for more details. The data we gather is limited to the following: . | A UUID which is generated when setting up lakeFS for the first time and contains no personal or otherwise identifiable information, | The lakeFS version currently running, | The OS and architecture lakeFS is running on, | Metadata regarding the database used (version, installed extensions and parameters such as DB Timezone and work memory), | Periodic aggregated action counters (e.g. how many “get_object” operations occurred). | . 5. How is lakeFS different from Delta Lake / Hudi / Iceberg? . Delta Lake, Hudi, and Iceberg all define dedicated, structured data formats that allow deletes and upserts. lakeFS is format-agnostic and enables consistent cross-collection versioning of your data using Git-like operations. Read our comparison for a more detailed comparison. 6. What inspired the lakeFS logo? . The Axolotl – a species of salamander, also known as the Mexican Lake Monster or the Peter Pan of the animal kingdom. It’s a magical creature, living in a lake - just like us! :) . copyright . ",
    "url": "/v0.98/faq.html",
    
    "relUrl": "/faq.html"
  },"154": {
    "doc": "Committed Objects",
    "title": "Garbage Collection: committed objects",
    "content": "By default, lakeFS keeps all your objects forever. This allows you to travel back in time to previous versions of your data. However, sometimes you may want to hard-delete your objects - namely, delete them from the underlying storage. Reasons for this include cost-reduction and privacy policies. Garbage collection rules in lakeFS define for how long to retain objects after they have been deleted. lakeFS provides a Spark program to hard-delete objects whose retention period has ended according to the GC rules. This program does not remove any commits: you will still be able to use commits containing hard-deleted objects, but trying to read these objects from lakeFS will result in a 410 Gone HTTP status. At this point, lakeFS supports Garbage Collection only on S3 and Azure. We have concrete plans to extend the support to GCP. lakeFS Cloud users enjoy a managed Garbage Collection service, and do not need to run this Spark program. ",
    "url": "/v0.98/howto/garbage-collection-committed.html#garbage-collection-committed-objects",
    
    "relUrl": "/howto/garbage-collection-committed.html#garbage-collection-committed-objects"
  },"155": {
    "doc": "Committed Objects",
    "title": "Table of contents",
    "content": ". | Understanding Garbage Collection | Configuring GC rules | Running the GC job | Considerations | Backup and restore | . Understanding Garbage Collection . For every branch, the GC job retains deleted objects for the number of days defined for the branch. In the absence of a branch-specific rule, the default rule for the repository is used. If an object is present in more than one branch ancestry, it’s retained according to the rule with the largest number of days between those branches. That is, it’s hard-deleted only after the retention period has ended for all relevant branches. Example GC rules for a repository: . { \"default_retention_days\": 14, \"branches\": [ {\"branch_id\": \"main\", \"retention_days\": 21}, {\"branch_id\": \"dev\", \"retention_days\": 7} ] } . In the above example, objects are retained for 14 days after deletion by default. However, if they are present in the branch main, they are retained for 21 days. Objects present in the dev branch (but not in any other branch) are retained for 7 days after they are deleted. Configuring GC rules . Using lakectl . Use the lakectl CLI to define the GC rules: . cat &lt;&lt;EOT &gt;&gt; example_repo_gc_rules.json { \"default_retention_days\": 14, \"branches\": [ {\"branch_id\": \"main\", \"retention_days\": 21}, {\"branch_id\": \"dev\", \"retention_days\": 7} ] } EOT lakectl gc set-config lakefs://example-repo -f example_repo_gc_rules.json . From the lakeFS UI . | Navigate to the main page of your repository. | Go to Settings -&gt; Retention. | Click Edit policy and paste your GC rule into the text box as a JSON. | Save your changes. | . Running the GC job . To run the job, use the following spark-submit command (or using your preferred method of running Spark programs). The job will hard-delete objects that were deleted and whose retention period has ended according to the GC rules. | On AWS (Spark 3.1.2 and higher) | On AWS (Spark 3.0.1) | On AWS (Spark 2.x) | On Azure | . spark-submit --class io.treeverse.clients.GarbageCollector \\ --packages org.apache.hadoop:hadoop-aws:2.7.7 \\ -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1 \\ -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\ -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\ -c spark.hadoop.fs.s3a.access.key=&lt;S3_ACCESS_KEY&gt; \\ -c spark.hadoop.fs.s3a.secret.key=&lt;S3_SECRET_KEY&gt; \\ http://treeverse-clients-us-east.s3-website-us-east-1.amazonaws.com/lakefs-spark-client-312-hadoop3/0.6.5/lakefs-spark-client-312-hadoop3-assembly-0.6.5.jar \\ example-repo us-east-1 . spark-submit --class io.treeverse.clients.GarbageCollector \\ --packages org.apache.hadoop:hadoop-aws:2.7.7 \\ -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1 \\ -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\ -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\ -c spark.hadoop.fs.s3a.access.key=&lt;S3_ACCESS_KEY&gt; \\ -c spark.hadoop.fs.s3a.secret.key=&lt;S3_SECRET_KEY&gt; \\ http://treeverse-clients-us-east.s3-website-us-east-1.amazonaws.com/lakefs-spark-client-301/0.6.5/lakefs-spark-client-301-assembly-0.6.5.jar \\ example-repo us-east-1 . spark-submit --class io.treeverse.clients.GarbageCollector \\ --packages org.apache.hadoop:hadoop-aws:2.7.7 \\ -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1 \\ -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\ -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\ -c spark.hadoop.fs.s3a.access.key=&lt;S3_ACCESS_KEY&gt; \\ -c spark.hadoop.fs.s3a.secret.key=&lt;S3_SECRET_KEY&gt; \\ http://treeverse-clients-us-east.s3-website-us-east-1.amazonaws.com/lakefs-spark-client-247/0.6.5/lakefs-spark-client-247-assembly-0.6.5.jar \\ example-repo us-east-1 . spark-submit --class io.treeverse.clients.GarbageCollector \\ --packages org.apache.hadoop:hadoop-aws:3.2.1 \\ -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1 \\ -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\ -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\ -c spark.hadoop.fs.azure.account.key.&lt;AZURE_STORAGE_ACCOUNT&gt;.dfs.core.windows.net=&lt;AZURE_STORAGE_ACCESS_KEY&gt; \\ http://treeverse-clients-us-east.s3-website-us-east-1.amazonaws.com/lakefs-spark-client-312-hadoop3/0.6.5/lakefs-spark-client-312-hadoop3-assembly-0.6.5.jar \\ example-repo . Notes: . | On Azure, GC was tested only on Spark 3.3.0, but may work with other Spark and Hadoop versions. | In case you don’t have hadoop-azure package as part of your environment, you should add the package to your spark-submit with --packages org.apache.hadoop:hadoop-azure:3.2.1 | For GC to work on Azure blob, soft delete should be disabled. | . You will find the list of objects hard-deleted by the job in the storage namespace of the repository. It is saved in Parquet format under _lakefs/logs/gc/deleted_objects. GC job options . By default, GC first creates a list of expired objects according to your retention rules and then hard-deletes those objects. However, you can use GC options to break the GC job down into two stages: . | Mark stage: GC will mark the expired objects to hard-delete, without deleting them. | Sweep stage: GC will hard-delete objects marked by a previous mark-only GC run. | . By breaking GC into these stages, you can pause and create a backup of the objects that GC is about to sweep and later restore them. You can use the GC backup and restore utility to do that. Mark only mode . To make GC run the mark stage only, add the following properties to your spark-submit command: . spark.hadoop.lakefs.gc.do_sweep=false spark.hadoop.lakefs.gc.mark_id=&lt;MARK_ID&gt; # Replace &lt;MARK_ID&gt; with your own identification string. This MARK_ID will enable you to start a sweep (actual deletion) run later . Running in mark only mode, GC will write the addresses of the expired objects to delete to the following location: STORAGE_NAMESPACE/_lakefs/retention/gc/addresses/mark_id=&lt;MARK_ID&gt;/ as a parquet. Notes: . | Mark only mode is only available from v0.4.0 of lakeFS Spark client. | The spark.hadoop.lakefs.debug.gc.no_delete property has been deprecated with v0.4.0. | . Sweep only mode . To make GC run the sweep stage only, add the following properties to your spark-submit command: . spark.hadoop.lakefs.gc.do_mark=false spark.hadoop.lakefs.gc.mark_id=&lt;MARK_ID&gt; # Replace &lt;MARK_ID&gt; with the identifier you used on a previous mark-only run . Running in sweep only mode, GC will hard-delete the expired objects marked by a mark-only run and listed in: STORAGE_NAMESPACE/_lakefs/retention/gc/addresses/mark_id=&lt;MARK_ID&gt;/. Note: Mark only mode is only available from v0.4.0 of lakeFS Spark client. Considerations . | In order for an object to be hard-deleted, it must be deleted from all branches. You should remove stale branches to prevent them from retaining old objects. For example, consider a branch that has been merged to main and has become stale. An object which is later deleted from main will always be present in the stale branch, preventing it from being hard-deleted. | lakeFS will never delete objects outside your repository’s storage namespace. In particular, objects that were imported using lakectl ingest or UI Import Wizard will not be affected by GC jobs. | In cases where deleted objects are brought back to life while a GC job is running, said objects may or may not be deleted. Such actions include: . | Reverting a commit in which a file was deleted. | Branching out from an old commit. | Expanding the retention period of a branch. | Creating a branch from an existing branch, where the new branch has a longer retention period. | . | . Backup and restore . GC was created to hard-delete objects from your underlying objects store according to your retention rules. However, when you start using the feature you may want to first gain confidence in the decisions GC makes. The GC backup and restore utility helps you do that. Use-cases: . | Backup: copy expired objects from your repository’s storage namespace to an external location before running GC in sweep only mode. | Restore: copy objects that were hard-deleted by GC from an external location you used for saving your backup into your repository’s storage namespace. | . Follow rclone documentation to configure remote access to the underlying storage used by lakeFS. Replace LAKEFS_STORAGE_NAMESPACE with remote:bucket/path which points to the lakeFS repository storage namespace. The BACKUP_STORAGE_LOCATION attribute points to a storage location outside your lakeFS storage namespace into which you want to save the backup. Backup command . rclone --include \"*.txt\" cat \"&lt;LAKEFS_STORAGE_NAMESPACE&gt;/_lakefs/retention/gc/addresses.text/mark_id=&lt;MARK_ID&gt;/\" | \\ rclone -P --no-traverse --files-from - copy &lt;LAKEFS_STORAGE_NAMESPACE&gt; &lt;BACKUP_STORAGE_LOCATION&gt; . Restore command . rclone --include \"*.txt\" cat \"&lt;LAKEFS_STORAGE_NAMESPACE&gt;/_lakefs/retention/gc/addresses.text/mark_id=&lt;MARK_ID&gt;/\" | \\ rclone -P --no-traverse --files-from - copy &lt;BACKUP_STORAGE_LOCATION&gt; &lt;LAKEFS_STORAGE_NAMESPACE&gt; . Example . The following of commands used to backup/resource a configured remote ‘azure’ (Azure blob storage) to access example repository storange namespace https://lakefs.blob.core.windows.net/repo/example/: . # Backup rclone --include \"*.txt\" cat \"azure://repo/example/_lakefs/retention/gc/addresses.text/mark_id=a64d1885-6202-431f-a0a3-8832e4a5865a/\" | \\ rclone -P --no-traverse --files-from - copy azure://repo/example/ azure://backup/repo-example/ # Restore rclone --include \"*.txt\" cat \"azure://tal/azure-br/_lakefs/retention/gc/addresses.text/mark_id=a64d1885-6202-431f-a0a3-8832e4a5865a/\" | \\ rclone -P --no-traverse --files-from - copy azure://backup/repo-example/ azure://repo/example/ . ",
    "url": "/v0.98/howto/garbage-collection-committed.html#table-of-contents",
    
    "relUrl": "/howto/garbage-collection-committed.html#table-of-contents"
  },"156": {
    "doc": "Committed Objects",
    "title": "Committed Objects",
    "content": " ",
    "url": "/v0.98/howto/garbage-collection-committed.html",
    
    "relUrl": "/howto/garbage-collection-committed.html"
  },"157": {
    "doc": "Garbage Collection",
    "title": "Garbage Collection",
    "content": " ",
    "url": "/v0.98/howto/garbage-collection-index.html",
    
    "relUrl": "/howto/garbage-collection-index.html"
  },"158": {
    "doc": "Uncommitted Objects",
    "title": "Garbage collection: uncommitted objects BETA",
    "content": "Note: Uncommitted GC is in Beta mode. Users should read this manual carefully and take precautions before applying the actual delete (“sweep”), like copying the marked objects. Deletion of objects that were never committed was always a difficulty for lakeFS, see #1933 for more details. Examples for objects that will be collected as part of the uncommitted GC job: . | Objects that were uploaded to lakeFS and deleted. | Objects that were uploaded to lakeFS and were overridden. | . ",
    "url": "/v0.98/howto/garbage-collection-uncommitted.html#garbage-collection-uncommitted-objects-beta",
    
    "relUrl": "/howto/garbage-collection-uncommitted.html#garbage-collection-uncommitted-objects-beta"
  },"159": {
    "doc": "Uncommitted Objects",
    "title": "Table of contents",
    "content": ". | Prerequisites | Running the uncommitted GC | Limitations | Next steps | . See discussion on the original design PR to understand why we didn’t go with a server-only solution. The uncommitted GC will not clean: . | Committed objects. See Committed Garbage Collection | Everything mentioned in what does not get collected | . Prerequisites . | lakeFS server version must be at least v0.87.0. If your version is lower, you should first upgrade. | Read the limitations section. | Setup rclone to access underlying bucket for backup and restore. | . Running the uncommitted GC . | Mark the files to delete - summary and report will be generated under &lt;REPOSITORY_STORAGE_NAMESPACE&gt;/_lakefs/retention/gc/uncommitted/&lt;MARK_ID&gt;/. By listing the bucket under ‘uncommitted’ the last entry represents the last mark ID of the uncommitted GC. The GC job prints out “Report for mark_id=…” which includes the mark ID with the run summary. spark-submit \\ --conf spark.hadoop.lakefs.gc.do_sweep=false \\ --conf spark.hadoop.lakefs.api.url=&lt;LAKEFS_ENDPOINT&gt; \\ --conf spark.hadoop.fs.s3a.access.key=&lt;AWS_ACCESS_KEY_ID&gt; \\ --conf spark.hadoop.fs.s3a.secret.key=&lt;AWS_SECRET_ACCESS_KEY&gt; \\ --conf spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY_ID&gt; \\ --conf spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_ACCESS_KEY&gt; \\ --class io.treeverse.gc.UncommittedGarbageCollector \\ --packages org.apache.hadoop:hadoop-aws:2.7.7 \\ &lt;APPLICATION-JAR-PATH&gt; &lt;REPOSITORY_NAME&gt; &lt;REGION&gt; . | Backup (optional but recommended) - when you start using the feature you may want to first gain confidence in the decisions uncommitted GC makes. Backup will copy the objects marked to be deleted for run ID to a specified location. Follow rclone documentation to configure remote access to lakeFS storage. Note that the lakeFS and backup locations are specified as remote:path based on how rclone was configured. rclone --include \"*.txt\" cat \"&lt;LAKEFS_STORAGE_NAMESPACE&gt;/_lakefs/retention/gc/uncommitted/&lt;MARK_ID&gt;/deleted.text/\" | \\ rclone -P --no-traverse --files-from - copy &lt;LAKEFS_STORAGE_NAMESPACE&gt; &lt;BACKUP_STORAGE_LOCATION&gt; . | Sweep - delete reported objects to delete based on mark ID . spark-submit \\ --conf spark.hadoop.lakefs.gc.mark_id=&lt;MARK_ID&gt; \\ --conf spark.hadoop.lakefs.gc.do_mark=false \\ --conf spark.hadoop.lakefs.api.url=&lt;LAKEFS_ENDPOINT&gt; \\ --conf spark.hadoop.fs.s3a.access.key=&lt;AWS_ACCESS_KEY_ID&gt; \\ --conf spark.hadoop.fs.s3a.secret.key=&lt;AWS_SECRET_ACCESS_KEY&gt; \\ --conf spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY_ID&gt; \\ --conf spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_ACCESS_KEY&gt; \\ --class io.treeverse.gc.UncommittedGarbageCollector \\ --packages org.apache.hadoop:hadoop-aws:2.7.7 \\ &lt;APPLICATION-JAR-PATH&gt; &lt;REPOSITORY_NAME&gt; &lt;REGION&gt; . | Restore - in any case we would like to undo and restore the data from from our backup. The following command will copy the objects back from the backup location using the information stored under the specific mark ID. Note that the lakeFS and backup locations are specified as remote:path based on how rclone was configured. rclone --include \"*.txt\" cat \"remote:&lt;LAKEFS_STORAGE_NAMESPACE&gt;/_lakefs/retention/gc/uncommitted/&lt;MARK_ID&gt;/deleted.text/\" | \\ rclone -P --no-traverse --files-from - copy &lt;BACKUP_STORAGE_LOCATION&gt; &lt;LAKEFS_STORAGE_NAMESPACE&gt; . | . Uncommitted GC job options . Similar to the committed GC option. Limitations . The uncommitted GC job has several limitations in its Beta version: . | Support is limited to S3 repositories, it was not tested on ABS, GS or MinIO. | Scale may be limited, see performance results below. | Issue associated to commit during copy object. | . Next steps . The uncommitted GC is under development, next releases will include: . | Incorporation of committed &amp; uncommitted GC into a single job. We understand the friction of having 2 garbage collection jobs for a lakeFS installation and working to creating a single job for it. | Removing the limitation of a read-only lakeFS during the job run. | Performance improvements: . | Better parallelization of the storage namespace traversal. | Optimized Run: GC will only iterate over objects that were written to the repository since the last GC run. For more information see the proposal. | . | Backup &amp; Restore, similar to committed GC. | Support for non-S3 repositories. | . Performance . The uncommitted GC job was tested on a repository with 1K branches, 25K uncommitted objects and 2K commits. The storage namespace number of objects prior to the cleanup was 103K objects. The job ran on a Spark cluster with a single master and 2 workers of type i3.2xlarge The job finished after 5 minutes deleting 15K objects. ",
    "url": "/v0.98/howto/garbage-collection-uncommitted.html#table-of-contents",
    
    "relUrl": "/howto/garbage-collection-uncommitted.html#table-of-contents"
  },"160": {
    "doc": "Uncommitted Objects",
    "title": "Uncommitted Objects",
    "content": " ",
    "url": "/v0.98/howto/garbage-collection-uncommitted.html",
    
    "relUrl": "/howto/garbage-collection-uncommitted.html"
  },"161": {
    "doc": "Committed GC Internals",
    "title": "Committed GC Internals",
    "content": "What gets collected . Because each object in lakeFS may be accessible from multiple branches, it might not be obvious which objects will be considered garbage and collected. Garbage collection is configured by specifying the number of days to retain objects on each branch. If a branch is configured to retain objects for a given number of days, any object that was accessible from the HEAD of a branch in that past number of days will be retained. The garbage collection process proceeds in three main phases: . | Discover which commits will retain their objects. For every branch, the garbage collection job looks at the HEAD of the branch that many days ago; every commit at or since that HEAD must be retained. Continuing the example, branch main retains for 21 days and branch dev for 7. When running GC on 2022-03-31: . | 7 days ago, on 2022-03-24 the head of branch dev was d: 2022-03-23. So, that commit is retained (along with all more recent commits on dev) but all older commits d: * will be collected. | 21 days ago, on 2022-03-10, the head of branch main was 2022-03-09. So that commit is retained (along with all more recent commits on main) but commits 2022-02-27 and 2022-03-01 will be collected. | . | Discover which objects need to be garbage collected. Hold (only) objects accessible on some retained commits. In the example, all objects of commit 2022-03-12, for instance, are retained. This includes objects added in previous commits. However, objects added in commit d: 2022-03-14 which were overwritten or deleted in commit d: 2022-03-20 are not visible in any retained commit and will be garbage collected. | Garbage collect those objects by deleting them. The data of any deleted object will no longer be accessible. lakeFS retains all metadata about the object, but attempting to read it via the lakeFS API or the S3 gateway will return HTTP status 410 (“Gone”). | . What does not get collected . Some objects will not be collected regardless of configured GC rules: . | Any object that is accessible from any branch’s HEAD. | Objects stored outside the repository’s storage namespace. For example, objects imported using the lakeFS import UI are not collected. | Uncommitted objects, see Uncommitted Garbage Collection, | . Performance . Garbage collection reads many commits. It uses Spark to spread the load of reading the contents of all of these commits. For very large jobs running on very large clusters, you may want to tweak this load. To do this: . | Add -c spark.hadoop.lakefs.gc.range.num_partitions=RANGE_PARTITIONS (default 50) to spread the initial load of reading commits across more Spark executors. | Add -c spark.hadoop.lakefs.gc.address.num_partitions=RANGE_PARTITIONS (default 200) to spread the load of reading all objects included in a commit across more Spark executors. | . Normally this should not be needed. Networking . Garbage collection communicates with the lakeFS server. Very large repositories may require increasing a read timeout. If you run into timeout errors during communication from the Spark job to lakefs consider increasing these timeouts: . | Add -c spark.hadoop.lakefs.api.read.timeout_seconds=TIMEOUT_IN_SECONDS (default 10) to allow lakeFS more time to respond to requests. | Add -c spark.hadoop.lakefs.api.connection.timeout_seconds=TIMEOUT_IN_SECONDS (default 10) to wait longer for lakeFS to accept connections. | . ",
    "url": "/v0.98/howto/gc-internals.html",
    
    "relUrl": "/howto/gc-internals.html"
  },"162": {
    "doc": "GCP",
    "title": "Deploy lakeFS on GCP",
    "content": "⏰ Expected deployment time: 25 min . ",
    "url": "/v0.98/deploy/gcp.html#deploy-lakefs-on-gcp",
    
    "relUrl": "/deploy/gcp.html#deploy-lakefs-on-gcp"
  },"163": {
    "doc": "GCP",
    "title": "Table of contents",
    "content": ". | Create a Database | Run the lakeFS Server | Create the admin user | Create your first repository | . ",
    "url": "/v0.98/deploy/gcp.html#table-of-contents",
    
    "relUrl": "/deploy/gcp.html#table-of-contents"
  },"164": {
    "doc": "GCP",
    "title": "Create a Database",
    "content": "lakeFS requires a PostgreSQL database to synchronize actions on your repositories. We will show you how to create a database on Google Cloud SQL, but you can use any PostgreSQL database as long as it’s accessible by your lakeFS installation. If you already have a database, take note of the connection string and skip to the next step . | Follow the official Google documentation on how to create a PostgreSQL instance. Make sure you’re using PostgreSQL version &gt;= 11. | On the Users tab in the console, create a user. The lakeFS installation will use it to connect to your database. | Choose the method by which lakeFS will connect to your database. Google recommends using the SQL Auth Proxy. | . ",
    "url": "/v0.98/deploy/gcp.html#create-a-database",
    
    "relUrl": "/deploy/gcp.html#create-a-database"
  },"165": {
    "doc": "GCP",
    "title": "Run the lakeFS Server",
    "content": ". | GCE Instance | Docker | GKE | . | Save the following configuration file as config.yaml: . --- database: type: \"postgres\" postgres: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: # replace this with a randomly-generated string: secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: gs # Uncomment the following lines to give lakeFS access to your buckets using a service account: # gs: # credentials_json: [YOUR SERVICE ACCOUNT JSON STRING] . | Download the binary to the GCE instance. | Run the lakefs binary on the GCE machine: lakefs --config config.yaml run . Note: it is preferable to run the binary as a service using systemd or your operating system’s facilities. | . To support container-based environments like Google Cloud Run, lakeFS can be configured using environment variables. Here is a docker run command to demonstrate starting lakeFS using Docker: . docker run \\ --name lakefs \\ -p 8000:8000 \\ -e LAKEFS_DATABASE_TYPE=\"postgres\" \\ -e LAKEFS_DATABASE_POSTGRES_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\ -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\ -e LAKEFS_BLOCKSTORE_TYPE=\"gs\" \\ treeverse/lakefs:latest run . See the reference for a complete list of environment variables. You can install lakeFS on Kubernetes using a Helm chart. To install lakeFS with Helm: . | Copy the Helm values file relevant for Google Storage: . secrets: # replace DATABASE_CONNECTION_STRING with the connection string of the database you created in a previous step. # e.g.: postgres://postgres:myPassword@localhost/postgres:5432 databaseConnectionString: [DATABASE_CONNECTION_STRING] # replace this with a randomly-generated string authEncryptSecretKey: [ENCRYPTION_SECRET_KEY] lakefsConfig: | blockstore: type: gs # Uncomment the following lines to give lakeFS access to your buckets using a service account: # gs: # credentials_json: [YOUR SERVICE ACCOUNT JSON STRING] . | Fill in the missing values and save the file as conf-values.yaml. For more configuration options, see our Helm chart README. The lakefsConfig parameter is the lakeFS configuration documented here but without sensitive information. Sensitive information like databaseConnectionString is given through separate parameters, and the chart will inject it into Kubernetes secrets. | In the directory where you created conf-values.yaml, run the following commands: . # Add the lakeFS repository helm repo add lakefs https://charts.lakefs.io # Deploy lakeFS helm install my-lakefs lakefs/lakefs -f conf-values.yaml . my-lakefs is the Helm Release name. | . ",
    "url": "/v0.98/deploy/gcp.html#run-the-lakefs-server",
    
    "relUrl": "/deploy/gcp.html#run-the-lakefs-server"
  },"166": {
    "doc": "GCP",
    "title": "Load balancing",
    "content": "To configure a load balancer to direct requests to the lakeFS servers you can use the LoadBalancer Service type or a Kubernetes Ingress. By default, lakeFS operates on port 8000 and exposes a /_health endpoint that you can use for health checks. 💡 The NGINX Ingress Controller by default limits the client body size to 1 MiB. Some clients use bigger chunks to upload objects - for example, multipart upload to lakeFS using the S3-compatible Gateway or a simple PUT request using the OpenAPI Server. Checkout Nginx documentation for increasing the limit, or an example of Nginx configuration with MinIO. ",
    "url": "/v0.98/deploy/gcp.html#load-balancing",
    
    "relUrl": "/deploy/gcp.html#load-balancing"
  },"167": {
    "doc": "GCP",
    "title": "Create the admin user",
    "content": "When you first open the lakeFS UI, you will be asked to create an initial admin user. | open http://&lt;lakefs-host&gt;/ in your browser. If you haven’t set up a load balancer, this will likely be http://&lt;instance ip address&gt;:8000/ | On first use, you’ll be redirected to the setup page: . | Follow the steps to create an initial administrator user. Save the credentials you’ve received somewhere safe, you won’t be able to see them again! . | Follow the link and go to the login screen. Use the credentials from the previous step to log in. | . ",
    "url": "/v0.98/deploy/gcp.html#create-the-admin-user",
    
    "relUrl": "/deploy/gcp.html#create-the-admin-user"
  },"168": {
    "doc": "GCP",
    "title": "Create your first repository",
    "content": ". | Use the credentials from the previous step to log in | Click Create Repository and choose Blank Repository. | Under Storage Namespace, enter a path to your desired location on the object store. This is where data written to this repository will be stored. | Click Create Repository | You should now have a configured repository, ready to use! . | . Congratulations! Your environment is now ready 🤩 . ",
    "url": "/v0.98/deploy/gcp.html#create-your-first-repository",
    
    "relUrl": "/deploy/gcp.html#create-your-first-repository"
  },"169": {
    "doc": "GCP",
    "title": "GCP",
    "content": " ",
    "url": "/v0.98/deploy/gcp.html",
    
    "relUrl": "/deploy/gcp.html"
  },"170": {
    "doc": "Glossary",
    "title": "Glossary",
    "content": "This page has definition and explanations of all terms related to lakeFS technical internals and the architecture. ",
    "url": "/v0.98/understand/glossary.html",
    
    "relUrl": "/understand/glossary.html"
  },"171": {
    "doc": "Glossary",
    "title": "Table of contents",
    "content": ". | Glossary . | Auditing | Branch | Collection | Commit | Cross-Collection Consistency | Data Lake Governance | Data Lifecycle Management | Data Pipeline Reproducibility | Data Quality Testing | Data Versioning | Git-like Operations | Graveler | Hooks | Isolated Data Snapshot | Main Branch | Metadata Management | Merge | Repository | Rollback | Storage Namespace | Underlying Storage | Tag | Fluffy | . | . Auditing . Data auditing is data assessment to ensure its accuracy, security, and efficacy for specific usage. It also involves assessing data quality through its lifecycle and understanding the impact of poor quality data on the organization’s performance and revenue. Ensuring data reproducibility, auditability, and governance is one of the key concerns of data engineers today. lakeFS commit history helps the data teams to keep track of all changes to the data, supporting data auditing. Branch . Branches in lakeFS allow users to create their own “isolated” view of the repository. Read more. Collection . A collection, roughly speaking, is a set of data. Collections may be structured or unstructured; a structured collection is often referred to as a table. Commit . Using commits, you can view a repository at a certain point in its history and you’re guaranteed that the data you see is exactly as it was at the point of committing it. Read More . Cross-Collection Consistency . It is unfortunate that the word ‘consistency’ has multiple meanings, at least four of them according to Martin Kleppmann. Consistency in the context of lakeFS and data versioning is, the guarantee that operations in a transaction are performed accurately, correctly and most important, atomically. A repository (and thus a branch) in lakeFS, can span multiple tables or collections. By providing branch, commit, merge and revert operations atomically on a branch, lakeFS achieves consistency guarantees across different logical collections. That is, data versioning is consistent across multiple collections within a repository. It is sometimes referred as multi-table transactions. That is, lakeFS offers transactional guarantees across multiple tables. Data Lake Governance . The goal of data lake governance is to apply policies, standards and processes on the data. This allows creating high-quality data and ensuring that it’s used appropriately across the organization. Data lake governance improves the data quality and increases data usage for business decision-making, leading to operational improvements, better-informed business strategies, and stronger financial performance. lakeFS Cloud offers advanced data lake management features such as: Role-Based Access Control, Branch Aware Managed Garbage Collection, Data Lineage and Audit log. Data Lifecycle Management . In data-intensive applications, data should be managed through its entire lifecycle similar to how teams manage code. By doing so, we could leverage the best practices and tools from application lifecycle management (like CI/CD operations) and apply them to data. lakeFS offers data lifecycle management via isolated data development environments instead of shared buckets. Data Pipeline Reproducibility . Reproducibility in data pipelines is the ability to repeat a process. An example of this is recreating an issue that occurred in the production pipeline. Reproducibility allows for the controlled manufacture of an error to debug and troubleshoot it at a later point in time. Reproducing a data pipeline issue is a challenge that most data engineers face on a daily basis. Learn more about how lakeFS supports data pipeline reproducibility. Other use cases include running ad-hoc queries (useful for data science), review, and backfill. Data Quality Testing . This term describes ways to test data for its accuracy, completeness, consistency, timeliness, validity, and integrity. lakeFS hooks can be used to implement and run data quality tests before promoting staging data into production. Data Versioning . To version data means creating a unique point-in-time reference for data that can be accessed later. This reference can take the form of a query, an ID, or also commonly, a DateTime identifier. Data versioning may also include saving an entire copy of the data under a new name or file path every time you want to create a version of it. More advanced versioning solutions like lakeFS perform versioning through zero-copy data operations. lakeFS also optimizes storage usage between versions and exposes special operations to manage them. Git-like Operations . lakeFS allows teams to treat their data lake as a Git repository. Git is used for code versioning, whereas lakeFS is used for data versioning. lakeFS provides Git-like operations such as branch, commit, merge and revert. Graveler . Graveler is the core versioning engine of lakeFS. It handles versioning by translating lakeFS addresses to the actual stored objects. See the versioning internals section to learn how lakeFS stores metadata. Hooks . lakeFS hooks allow you to automate and ensure that a given set of checks and validations happens before important lifecycle events. They are similar conceptually to Git Hooks, but in contrast, they run remotely on a server. Currently, lakeFS allows executing hooks when two types of events occur: pre-commit events that run before a commit is acknowledged and pre-merge events that trigger right before a merge operation. Isolated Data Snapshot . Creating a branch in lakeFS provides an isolated environment containing a snapshot of your repository. While working on your branch in isolation, all other data users will be looking at the repository’s main branch. So they won’t see your changes, and you also won’t see the changes applied to the main branch. All of this happens without any data duplication but metadata management. Main Branch . Every Git repository has the main branch (unless you take explicit steps to remove it) and it plays a key role in the software development process. In most projects, it represents the source of truth - all the code that works has been tested and is ready to be pushed to production. Similarly, main branch in lakeFS could be used as the single source of truth. For example, the live production data can be on the main branch. Metadata Management . Where there is data, there is also metadata. lakeFS uses metadata to define schema, data types, data versions, relations to other datasets, etc. This helps to improve discoverability and manageability. lakeFS performs data versioning through metadata operations. Merge . lakeFS merge command, similar to git merge functionality, allows you to merge data branches. Once you commit data, you can review it and then merge the committed data into the target branch. A merge generates a commit on the target branch with all your changes. lakeFS guarantees atomic merges that are fast, given they don’t involve copying data. Read More. Repository . In lakeFS, a repository is a set of related objects (or collections of objects). Read More . Rollback . A rollback is an atomic operation reversing the effects of a previous commit. If a developer introduces a new code version to production and discovers that it has a critical bug, they can simply roll back to the previous version. In lakeFS, a rollback is an atomic action that prevents the data consumers from receiving low-quality data until the issue is resolved. Learn more about how lakeFS supports the rollback operation. Storage Namespace . The storage namespace is a location in the underlying storage dedicated to a specific repository. lakeFS uses it to store the repository’s objects and some of its metadata. Underlying Storage . The underlying storage is a location in some object store where lakeFS keeps your objects and some metadata. Tag . Tags are a way to give a meaningful name to a specific commit. Read More . Fluffy . lakeFS Enterprise Single-Sign-On service, it’s delegated with lakeFS’ authentication requests and replies back to lakeFS with the authentication response. ",
    "url": "/v0.98/understand/glossary.html#table-of-contents",
    
    "relUrl": "/understand/glossary.html#table-of-contents"
  },"172": {
    "doc": "Glue / Hive metastore",
    "title": "Table of contents",
    "content": ". | About Glue / Hive Metastore | Managing Tables With lakeFS Branches . | Motivation | Configurations | Suggested Model | Commands . | Copy | Diff | . | . | . ",
    "url": "/v0.98/integrations/glue_hive_metastore.html#table-of-contents",
    
    "relUrl": "/integrations/glue_hive_metastore.html#table-of-contents"
  },"173": {
    "doc": "Glue / Hive metastore",
    "title": "About Glue / Hive Metastore",
    "content": "This part explains about how Glue/Hive Metastore work with lakeFS. Glue and Hive Metastore store metadata related to Hive and other services (such as Spark and Trino). They contain metadata such as the location of the table, information about columns, partitions and many more. ",
    "url": "/v0.98/integrations/glue_hive_metastore.html#about-glue--hive-metastore",
    
    "relUrl": "/integrations/glue_hive_metastore.html#about-glue--hive-metastore"
  },"174": {
    "doc": "Glue / Hive metastore",
    "title": "Without lakeFS",
    "content": "To query the table my_table, Spark will: . | Request the metadata from Hive metastore (steps 1,2), | Use the location from the metadata to access the data in S3 (steps 3,4). | . ",
    "url": "/v0.98/integrations/glue_hive_metastore.html#without-lakefs",
    
    "relUrl": "/integrations/glue_hive_metastore.html#without-lakefs"
  },"175": {
    "doc": "Glue / Hive metastore",
    "title": "With lakeFS",
    "content": "When using lakeFS, the flow stays exactly the same. Note that the location of the table my_table now contains the branch s3://example/main/path/to/table . ",
    "url": "/v0.98/integrations/glue_hive_metastore.html#with-lakefs",
    
    "relUrl": "/integrations/glue_hive_metastore.html#with-lakefs"
  },"176": {
    "doc": "Glue / Hive metastore",
    "title": "Managing Tables With lakeFS Branches",
    "content": " ",
    "url": "/v0.98/integrations/glue_hive_metastore.html#managing-tables-with-lakefs-branches",
    
    "relUrl": "/integrations/glue_hive_metastore.html#managing-tables-with-lakefs-branches"
  },"177": {
    "doc": "Glue / Hive metastore",
    "title": "Motivation",
    "content": "When creating a table in Glue/Hive metastore (using a client such as Spark, Hive, Presto), we specify the table location. Consider the table my_table that was created with the location s3://example/main/path/to/table. Suppose you created a new branch called DEV with main as the source branch. The data from s3://example/main/path/to/table is now accessible in s3://example/DEV/path/to/table. The metadata is not managed in lakeFS, meaning you don’t have any table pointing to s3://example/DEV/path/to/table. To address this, lakeFS introduces lakectl metastore commands. The case above can be handled using the copy command: it creates a copy of my_table with data located in s3://example/DEV/path/to/table. Note that this is a fast, metadata-only operation. ",
    "url": "/v0.98/integrations/glue_hive_metastore.html#motivation",
    
    "relUrl": "/integrations/glue_hive_metastore.html#motivation"
  },"178": {
    "doc": "Glue / Hive metastore",
    "title": "Configurations",
    "content": "The lakectl metastore commands can run on Glue or Hive metastore. Add the following to the lakectl configuration file (by default ~/.lakectl.yaml): . Hive . metastore: type: hive hive: uri: hive-metastore:9083 . Glue . metastore: type: glue glue: catalog-id: 123456789012 region: us-east-1 profile: default # optional, implies using a credentials file credentials: access_key_id: AKIAIOSFODNN7EXAMPLE secret_access_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY . Note: It’s recommended to set type and catalog-id/metastore-uri in the lakectl configuration file. ",
    "url": "/v0.98/integrations/glue_hive_metastore.html#configurations",
    
    "relUrl": "/integrations/glue_hive_metastore.html#configurations"
  },"179": {
    "doc": "Glue / Hive metastore",
    "title": "Suggested Model",
    "content": "For simplicity, we recommend creating a schema for each branch. That way, you can use the same table name across different schemas. For example: after creating branch example_branch, also create a schema named example_branch. For a table named my_table under the schema main, create a new table under the same name and under the schema example_branch. You now have two my_table, one in the main schema and one in the branch schema. ",
    "url": "/v0.98/integrations/glue_hive_metastore.html#suggested-model",
    
    "relUrl": "/integrations/glue_hive_metastore.html#suggested-model"
  },"180": {
    "doc": "Glue / Hive metastore",
    "title": "Commands",
    "content": "Metastore tools support three commands: copy, diff, and create-symlink. copy and diff can work both on Glue and on Hive. create-symlink works only on Glue. Note: If to-schema or to-table are not specified, the destination branch and source table names will be used as per the suggested model. Note: Metastore commands can only run on tables located in lakeFS. You should not use tables that aren’t located in lakeFS. Copy . The copy command creates a copy of a table pointing to the defined branch. In case the destination table already exists, the command will only merge the changes. Example: . Suppose we created the table inventory on branch main on schema default. CREATE EXTERNAL TABLE `inventory`( `inv_item_sk` int, `inv_warehouse_sk` int, `inv_quantity_on_hand` int) PARTITIONED BY ( `inv_date_sk` int) STORED AS ORC LOCATION 's3a://my_repo/main/path/to/table'; . We create a new lakeFS branch example_branch: . lakectl branch create lakefs://my_repo/example_branch --source lakefs://my_repo/main . The data from s3://my_repo/main/path/to/table is now accessible in s3://my_repo/DEV/path/to/table. To query the data in s3://my_repo/DEV/path/to/table, you would like to create a copy of the table inventory in schema example_branch pointing to the new branch. lakectl metastore copy --from-schema default --from-table inventory --to-schema example_branch --to-table inventory --to-branch example_branch . After running this command, query the table example_branch.inventory to get the data from s3://my_repo/DEV/path/to/table . Copy Partition . After adding a partition to the branch table, you may want to copy the partition to the main table. For example, for the new partition 2020-08-01, run the following to copy the partition to the main table: . lakectl metastore copy --type hive --from-schema example_branch --from-table inventory --to-schema default --to-table inventory --to-branch main -p 2020-08-01 . For a table partitioned by more than one column, specify the partition flag for every column. For example, for the partition (year='2020',month='08',day='01'): . lakectl metastore copy --from-schema example_branch --from-table branch_inventory --to-schema default --to-branch main -p 2020 -p 08 -p 01 . Diff . Provides a two-way diff between two tables. Shows added+ , removed- and changed~ partitions and columns. Example: . Suppose you made some changes on the copied table inventory on schema example_branch and now want to view the changes before merging back to inventory on schema default. Hive: . lakectl metastore diff --type hive --address thrift://hive-metastore:9083 --from-schema example_branch --from-table branch --to-schema default --to-table inventory . The output will look like this: . Columns are identical Partitions - 2020-07-04 + 2020-07-05 + 2020-07-06 ~ 2020-07-08 . ",
    "url": "/v0.98/integrations/glue_hive_metastore.html#commands",
    
    "relUrl": "/integrations/glue_hive_metastore.html#commands"
  },"181": {
    "doc": "Glue / Hive metastore",
    "title": "Glue / Hive metastore",
    "content": " ",
    "url": "/v0.98/integrations/glue_hive_metastore.html",
    
    "relUrl": "/integrations/glue_hive_metastore.html"
  },"182": {
    "doc": "Hive",
    "title": "Using lakeFS with Hive",
    "content": "The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive. ",
    "url": "/v0.98/integrations/hive.html#using-lakefs-with-hive",
    
    "relUrl": "/integrations/hive.html#using-lakefs-with-hive"
  },"183": {
    "doc": "Hive",
    "title": "Table of contents",
    "content": ". | Configuration | Examples . | Example with schema | Example with an external table | . | . ",
    "url": "/v0.98/integrations/hive.html#table-of-contents",
    
    "relUrl": "/integrations/hive.html#table-of-contents"
  },"184": {
    "doc": "Hive",
    "title": "Configuration",
    "content": "To configure Hive to work with lakeFS, you need to set the lakeFS credentials in the corresponding S3 credential fields. lakeFS endpoint: fs.s3a.endpoint . lakeFS access key: fs.s3a.access.key . lakeFS secret key: fs.s3a.secret.key . Note In the following examples, we set AWS credentials at runtime for clarity. In production, these properties should be set using one of Hadoop’s standard ways of Authenticating with S3. For example, you can add the configurations to the file hdfs-site.xml: . &lt;configuration&gt; ... &lt;property&gt; &lt;name&gt;fs.s3a.secret.key&lt;/name&gt; &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.access.key&lt;/name&gt; &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;https://lakefs.example.com&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . Note In this example, we set fs.s3a.path.style.access to true to remove the need for additional DNS records for virtual hosting fs.s3a.path.style.access that was introduced in Hadoop 2.8.0 . ",
    "url": "/v0.98/integrations/hive.html#configuration",
    
    "relUrl": "/integrations/hive.html#configuration"
  },"185": {
    "doc": "Hive",
    "title": "Examples",
    "content": "Example with schema . CREATE SCHEMA example LOCATION 's3a://example/main/' ; CREATE TABLE example.request_logs ( request_time timestamp, url string, ip string, user_agent string ); . Example with an external table . CREATE EXTERNAL TABLE request_logs ( request_time timestamp, url string, ip string, user_agent string ) LOCATION 's3a://example/main/request_logs' ; . ",
    "url": "/v0.98/integrations/hive.html#examples",
    
    "relUrl": "/integrations/hive.html#examples"
  },"186": {
    "doc": "Hive",
    "title": "Hive",
    "content": " ",
    "url": "/v0.98/integrations/hive.html",
    
    "relUrl": "/integrations/hive.html"
  },"187": {
    "doc": "Iceberg",
    "title": "Using lakeFS with Iceberg",
    "content": "Currently, lakeFS does not support the Iceberg table format, but we have concrete plans to add it in the near future. ",
    "url": "/v0.98/integrations/iceberg.html#using-lakefs-with-iceberg",
    
    "relUrl": "/integrations/iceberg.html#using-lakefs-with-iceberg"
  },"188": {
    "doc": "Iceberg",
    "title": "Iceberg",
    "content": " ",
    "url": "/v0.98/integrations/iceberg.html",
    
    "relUrl": "/integrations/iceberg.html"
  },"189": {
    "doc": "Import data into lakeFS",
    "title": "Import data into lakeFS",
    "content": "The simplest way to bring data into lakeFS is by copying it, but this approach may not be suitable when a lot of data is involved. To avoid copying the data, lakeFS offers Zero-copy import. With this approach, lakeFS only creates pointers to your existing objects in your new repository. ",
    "url": "/v0.98/howto/import.html",
    
    "relUrl": "/howto/import.html"
  },"190": {
    "doc": "Import data into lakeFS",
    "title": "Table of contents",
    "content": ". | Zero-copy import . | Prerequisites | AWS S3: Importing from public buckets | Azure Data Lake Gen2 | Using the lakeFS UI | lakectl import | lakectl ingest | Limitations | Working with imported data | . | Copying data into a lakeFS repository | . ",
    "url": "/v0.98/howto/import.html#table-of-contents",
    
    "relUrl": "/howto/import.html#table-of-contents"
  },"191": {
    "doc": "Import data into lakeFS",
    "title": "Zero-copy import",
    "content": "Prerequisites . lakeFS must have permissions to list the objects in the source object store, and the source bucket must be in the same region of your destination bucket. In addition, see the following storage provider specific instructions: . | AWS S3 | Azure Storage | Google Cloud Storage | . AWS S3: Importing from public buckets . lakeFS needs access to the imported location to first list the files to import and later read the files upon users request. There are some use cases where the user would like to import from a destination which isn’t owned by the account running lakeFS. For example, importing public datasets to experiment with lakeFS and Spark. lakeFS will require additional permissions to read from public buckets. For example, for S3 public buckets, the following policy needs to be attached to the lakeFS S3 service-account to allow access to public buckets, while blocking access to other owned buckets: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"PubliclyAccessibleBuckets\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetBucketVersioning\", \"s3:ListBucket\", \"s3:GetBucketLocation\", \"s3:ListBucketMultipartUploads\", \"s3:ListBucketVersions\", \"s3:GetObject\", \"s3:GetObjectVersion\", \"s3:AbortMultipartUpload\", \"s3:ListMultipartUploadParts\" ], \"Resource\": [\"*\"], \"Condition\": { \"StringNotEquals\": { \"s3:ResourceAccount\": \"&lt;YourAccountID&gt;\" } } } ] } . See Azure deployment on limitations when using account credentials. Azure Data Lake Gen2 . lakeFS requires a hint in the import source URL to understand that the provided storage account is ADLS Gen2 . For source account URL: https://&lt;my-account&gt;.core.windows.net/path/to/import/ Please add the *adls* subdomain to the URL as follows: https://&lt;my-account&gt;.adls.core.windows.net/path/to/import/ . No specific prerequisites . Using the lakeFS UI . To import using the UI, lakeFS must have permissions to list the objects in the source object store. | In your repository’s main page, click the Import button to open the import dialog: . | Under Import from, fill in the location on your object store you would like to import from. | Fill in the import destination in lakeFS and a commit message. | . Once the import is complete, you can merge the changes from the import branch to the source branch. Notes . | On the first import to a branch, a dedicated branch named _&lt;branch_name&gt;_imported will be created. lakeFS will import all objects to this branch under the given prefix. | The import duration depends on the amount of imported objects, but will roughly be a few thousand objects per second. | . lakectl import . Prerequisite: have lakectl installed. The lakectl import command acts the same as the UI import wizard. It commits the changes to a dedicated branch, with an optional flag to merge the changes to &lt;branch_name&gt;. | AWS S3 or S3 API Compatible storage | Azure Blob | Google Cloud Storage | . lakectl import \\ --from s3://bucket/optional/prefix/ \\ --to lakefs://my-repo/my-branch/optional/path/ . lakectl import \\ --from https://storageAccountName.blob.core.windows.net/container/optional/prefix/ \\ --to lakefs://my-repo/my-branch/optional/path/ . lakectl import \\ --from gs://bucket/optional/prefix/ \\ --to lakefs://my-repo/my-branch/optional/path/ . The imported objects will be committed to the _my-branch_imported, creating it if it doesn’t exists. Using the --merge flag will merge _my-branch_imported to my-branch after a successful import. lakectl ingest . Prerequisite: have lakectl installed. The ingest command adds the objects to lakeFS by listing them on the client side. The added objects will appear as uncommitted changes. The user calling lakectl ingest needs to have permissions to list the objects in the source object store. | AWS S3 or S3 API Compatible storage | Azure Blob | Google Cloud Storage | . lakectl ingest \\ --from s3://bucket/optional/prefix/ \\ --to lakefs://my-repo/ingest-branch/optional/path/ . The lakectl ingest command will attempt to use the current user’s existing credentials and respect instance profiles, environment variables, and credential files (similarly to AWS CLI) Specify an endpoint to ingest from other S3 compatible storage solutions, e.g., add --s3-endpoint-url https://play.min.io. export AZURE_STORAGE_ACCOUNT=\"storageAccountName\" export AZURE_STORAGE_ACCESS_KEY=\"EXAMPLEroozoo2gaec9fooTieWah6Oshai5Sheofievohthapob0aidee5Shaekahw7loo1aishoonuuquahr3==\" lakectl ingest \\ --from https://storageAccountName.blob.core.windows.net/container/optional/prefix/ \\ --to lakefs://my-repo/ingest-branch/optional/path/ . The lakectl ingest command currently supports storage accounts configured through environment variables as shown above. Note: Currently, lakectl import supports the http:// and https:// schemes for Azure storage URIs. wasb, abfs or adls are currently not supported. export GOOGLE_APPLICATION_CREDENTIALS=\"$HOME/.gcs_credentials.json\" # Optional, will fallback to the default configured credentials lakectl ingest \\ --from gs://bucket/optional/prefix/ \\ --to lakefs://my-repo/ingest-branch/optional/path/ . The lakectl ingest command currently supports the standard GOOGLE_APPLICATION_CREDENTIALS environment variable as described in Google Cloud’s documentation. Limitations . | Importing is only possible from the object storage service in which your installation stores its data. For example, if lakeFS is configured to use S3, you cannot import data from Azure. | Import is available for S3, GCP and Azure. | For security reasons, if you are lakeFS on top of your local disk, you need to enable the import feature explicitly. To do so, set the blockstore.local.import_enabled to true and specify the allowed import paths in blockstore.local.allowed_external_prefixes (see configuration reference). Since there are some differences between object-stores and file-systems in the way directories/prefixes are treated, local import is allowed only for directories. | . Working with imported data . Note that lakeFS cannot manage your metadata if you make changes to data in the original bucket. The following table describes the results of making changes in the original bucket, without importing it to lakeFS: . | Object action in the original bucket | ListObjects result in lakeFS | GetObject result in lakeFS | . | Create | Object not visible | Object not accessible | . | Overwrite | Object visible with outdated metadata | Updated object accessible | . | Delete | Object visible | Object not accessible | . ",
    "url": "/v0.98/howto/import.html#zero-copy-import",
    
    "relUrl": "/howto/import.html#zero-copy-import"
  },"192": {
    "doc": "Import data into lakeFS",
    "title": "Copying data into a lakeFS repository",
    "content": "Another way of getting existing data into a lakeFS repository is by copying it. This has the advantage of having the objects along with their metadata managed by the lakeFS installation, along with lifecycle rules, immutability guarantees and consistent listing. However, do make sure to account for storage cost and time. To copy data into lakeFS you can use the following tools: . | The lakectl command line tool - see the reference to learn more about using it to copy local data into lakeFS. Using lakectl fs upload --recursive you can upload multiple objects together from a given directory. | Using rclone | Using Hadoop’s DistCp | . ",
    "url": "/v0.98/howto/import.html#copying-data-into-a-lakefs-repository",
    
    "relUrl": "/howto/import.html#copying-data-into-a-lakefs-repository"
  },"193": {
    "doc": "lakeFS Enterprise",
    "title": "lakeFS Enterprise",
    "content": "lakeFS Enterprise is an enterprise-ready lakeFS solution that provides a support SLA and additional features to the open-source version of lakeFS. The additional features are: . | RBAC | SSO | Support SLA | . ",
    "url": "/v0.98/enterprise/",
    
    "relUrl": "/enterprise/"
  },"194": {
    "doc": "Deploy and Setup lakeFS",
    "title": "Deploy and setup lakeFS",
    "content": "Note: Optionally, for a hosted lakeFS service with guaranteed SLAs, try lakeFS cloud . ",
    "url": "/v0.98/deploy/#deploy-and-setup-lakefs",
    
    "relUrl": "/deploy/#deploy-and-setup-lakefs"
  },"195": {
    "doc": "Deploy and Setup lakeFS",
    "title": "Deploy and Setup lakeFS",
    "content": " ",
    "url": "/v0.98/deploy/",
    
    "relUrl": "/deploy/"
  },"196": {
    "doc": "Integrations",
    "title": "Integrations",
    "content": " ",
    "url": "/v0.98/integrations/",
    
    "relUrl": "/integrations/"
  },"197": {
    "doc": "Hooks",
    "title": "Hooks",
    "content": " ",
    "url": "/v0.98/hooks/",
    
    "relUrl": "/hooks/"
  },"198": {
    "doc": "Use Cases",
    "title": "Use Cases",
    "content": " ",
    "url": "/v0.98/use_cases/",
    
    "relUrl": "/use_cases/"
  },"199": {
    "doc": "⭐ Quickstart ⭐",
    "title": "lakeFS Quickstart",
    "content": "Welcome to lakeFS 👋🏻 . lakeFS provides a “git for data” platform enabling you to implement best practices from software engineering on your data lake, including branching and merging, CI/CD, and production-like dev/test environments. This quickstart will introduce you to some of the core ideas in lakeFS and show what you can do by illustrating the concept of branching, merging, and rolling back changes to data. It’s laid out in five short sections: . Launch . Spin up the quickstart environment locally under Docker . Query . Query the pre-populated data on the main branch . Branch . Make changes to the data on a new branch . Merge . Merge the changed data back to the main branch . Rollback . Change our mind and revert the changes . ",
    "url": "/v0.98/quickstart/#lakefs-quickstart",
    
    "relUrl": "/quickstart/#lakefs-quickstart"
  },"200": {
    "doc": "⭐ Quickstart ⭐",
    "title": "⭐ Quickstart ⭐",
    "content": " ",
    "url": "/v0.98/quickstart/",
    
    "relUrl": "/quickstart/"
  },"201": {
    "doc": "Data Lifecycle Management",
    "title": "Data Lifecycle Management",
    "content": " ",
    "url": "/v0.98/understand/data_lifecycle_management/",
    
    "relUrl": "/understand/data_lifecycle_management/"
  },"202": {
    "doc": "How it Works",
    "title": "How it Works",
    "content": "This section includes all the details about lakeFS internals and implementation details . ",
    "url": "/v0.98/understand/how/",
    
    "relUrl": "/understand/how/"
  },"203": {
    "doc": "Understanding lakeFS",
    "title": "Understanding lakeFS",
    "content": "This section includes all the details about the lakeFS open source project. ",
    "url": "/v0.98/understand/",
    
    "relUrl": "/understand/"
  },"204": {
    "doc": "Reference",
    "title": "Reference",
    "content": " ",
    "url": "/v0.98/reference/",
    
    "relUrl": "/reference/"
  },"205": {
    "doc": "Slack",
    "title": "Slack",
    "content": " ",
    "url": "/v0.98/slack/",
    
    "relUrl": "/slack/"
  },"206": {
    "doc": "lakeFS Cloud",
    "title": "lakeFS Cloud",
    "content": "lakeFS Cloud is a fully-managed lakeFS solution provided by Treeverse, implemented using our best practices, providing high availability, auto-scaling, support and enterprise-ready features. ",
    "url": "/v0.98/cloud/",
    
    "relUrl": "/cloud/"
  },"207": {
    "doc": "lakeFS Cloud",
    "title": "lakeFS Cloud Features",
    "content": ". | Role-Based Access Control | Auditing | Single-Sign-On (including support for SAML, OIDC, AD FS, Okta, and Azure AD) | Managed Garbage Collection | Private-Link | SOC 2 Type II Compliance | . ",
    "url": "/v0.98/cloud/#lakefs-cloud-features",
    
    "relUrl": "/cloud/#lakefs-cloud-features"
  },"208": {
    "doc": "lakeFS Cloud",
    "title": "How lakeFS Cloud interacts with your infrastructure",
    "content": "Treeverse hosts and manages a dedicated lakeFS instance that interfaces with data held in your object store, such as S3. flowchart TD U[Users] --&gt; LFC subgraph Your Infrastructure IAMM[lakeFS Managed GC IAM Role] --&gt; ObjectStore[Client's Object Store] IAMA[lakeFS Application IAM Role] --&gt; ObjectStore end subgraph Treeverse's Infrastructure MGC[Managed Garbage Collection] --&gt; EMR[Elastic Map Reduce] EMR --&gt; IAMM MGC --&gt; CP CP[Control Plane] LFC --&gt; CP subgraph Client's Tenant LFC[lakeFS Cloud] --&gt; DB[Refstore Database] end LFC --&gt; IAMC[lakeFS Connector IAM Role] IAMC --&gt;|ExternalID| IAMA end . ",
    "url": "/v0.98/cloud/#how-lakefs-cloud-interacts-with-your-infrastructure",
    
    "relUrl": "/cloud/#how-lakefs-cloud-interacts-with-your-infrastructure"
  },"209": {
    "doc": "lakeFS Cloud",
    "title": "Setting up lakeFS Cloud",
    "content": "AWS . Setting up lakeFS on AWS is fully automated through a self-service setup wizard. Azure . Settuping up lakeFS Cloud on Azure is currently a manual process which will be automated in the future. For now, please follow these instructions. GCP . Coming soon! Click here to register your interest. ",
    "url": "/v0.98/cloud/#setting-up-lakefs-cloud",
    
    "relUrl": "/cloud/#setting-up-lakefs-cloud"
  },"210": {
    "doc": "How-To",
    "title": "How-To",
    "content": " ",
    "url": "/v0.98/howto/",
    
    "relUrl": "/howto/"
  },"211": {
    "doc": "What is lakeFS",
    "title": "What is lakeFS",
    "content": "lakeFS brings software engineering best practices and applies them to data engineering. Concepts such as Dev/Test environments and CI/CD are harder to implement in data engineering, since the data, and not just the code, should be managed. lakeFS provides version control over the data lake, and uses git-like semantics to create and access those versions, so every engineer feels at home with lakeFS in a few minutes. In this reference diagram, lakeFS enables Python applications and Spark jobs with Git-like operations such as branching, committing and rolling back . With lakeFS, you can use concepts such as ״branch״ to create an isolated version of the data, ״commit״, to create a reproducible point it time, and “merge” in order to incorporate your changes in one atomic action. lakeFS is an open source project that supports managing data in AWS S3, Azure Blob Storage, Google Cloud Storage (GCS) and any other object storage with an S3 interface. It integrates seamlessly with popular data frameworks such as Spark, Hive Metastore, dbt, Trino, Presto, and many others and even features an S3 compatibility layer. The vision of lakeFS is bringing this functionality across all the data sources in your data pipelines, from analytics databases to key value stores - and to allow one system from which you can easily manage the underlying data in all data stores, with atomic git-like actions. ",
    "url": "/v0.98/",
    
    "relUrl": "/"
  },"212": {
    "doc": "What is lakeFS",
    "title": "How do I use lakeFS?",
    "content": "lakeFS maintains compatibility with the S3 API to minimize adoption friction. You can use it as a drop-in replacement for S3 from the perspective of any tool interacting with a data lake. For example, take the common operation of reading a collection of data from an object storage into a Spark DataFrame. For data outside a lakeFS repo, the code will look like this: . df = spark.read.parquet(\"s3a://my-bucket/collections/foo/\") . After adding the data collections into my-bucket to a repository, the same operation becomes: . df = spark.read.parquet(\"s3a://my-repo/main-branch/collections/foo/\") . You can use the same methods and syntax you are already using to read and write data when using a lakeFS repository. This simplifies the adoption of lakeFS - minimal changes are needed to get started, making further changes an incremental process. ",
    "url": "/v0.98/#how-do-i-use-lakefs",
    
    "relUrl": "/#how-do-i-use-lakefs"
  },"213": {
    "doc": "What is lakeFS",
    "title": "Why is lakeFS the data solution you’ve been missing?",
    "content": "Working with data in a lakeFS repository — as opposed to a bucket — enables simplified workflows when developing data lake pipelines. lakeFS performs all the following operations safely and efficiently: . | Copying objects between prefixes to promote new data. | Deleting specific objects to recover from data errors. | Maintaining auxilliary jobs that populate a development environment with data. | . If you spend time performing any of these actions today, adopting lakeFS will speed up your development and deployment cycles, reduce the chance of incorrect data making it into production, and make recovery less painful if it does. Through its versioning engine, lakeFS enables the following built-in operations familiar from Git: . | branch: a consistent copy of a repository, isolated from other branches and their changes. Initial creation of a branch is a metadata operation that does not duplicate objects. | commit: an immutable checkpoint containing a complete snapshot of a repository. | merge: performed between two branches — merges atomically update one branch with the changes from another. | revert: returns a repo to the exact state of a previous commit. | tag: a pointer to a single immutable commit with a readable, meaningful name. | . See the object model for an in-depth definition of these, and the CLI reference for the full list of commands. Incorporating these operations into your data lake pipelines provides the same collaboration and organizational benefits you get when managing application code with source control. The lakeFS promotion workflow . Here’s how lakeFS branches and merges improve the universal process of updating collections with the latest data. | First, create a new branch from main to instantly generate a complete “copy” of your production data. | Apply changes or make updates on the isolated branch to understand their impact prior to exposure. | And finally, perform a merge from the feature branch back to main to atomically promote the updates into production. | . Following this pattern, lakeFS facilitates a streamlined data deployment workflow that consistently produces data assets you can have total confidence in. ",
    "url": "/v0.98/#why-is-lakefs-the-data-solution-youve-been-missing",
    
    "relUrl": "/#why-is-lakefs-the-data-solution-youve-been-missing"
  },"214": {
    "doc": "What is lakeFS",
    "title": "What else does lakeFS do?",
    "content": "lakeFS helps you maintain a tidy data lake in several other ways, including: . Recovery from data errors . Human error, misconfiguration, or wide-ranging systematic effects are unavoidable. When they do happen, erroneous data may make it into production or critical data assets might accidentally by deleted. By their nature, backups are a wrong tool for recovering from such events. Backups are periodic events that are usually not tied to performing erroneous operations. So, they may be out of date, and will require sifting through data at the object level. This process is inefficient and can take hours, days, or in some cases, weeks to complete. By quickly committing entire snapshots of data at well-defined times, recovering data in deletion or corruption events becomes an instant one-line operation with lakeFS: just identify a good historical commit, and then restore to it or copy from it. Reverting your data lake back to previous version using our command-line tool is explained here. Data reprocessing and backfills . Occasionally, we might need to reprocess historical data. This can be due to several reasons: . | Implementing new logic. | Late arriving data that wasn’t included in previous analysis, and need to be backfilled after the fact. | . This is tricky as it often involves huge volumes of historical data. In addition, auditing requirements may necessitate keeping the old version of the data. lakeFS allows you to manage the reprocess on an isolated branch before merging to ensure the reprocessed data is exposed atomically. It also allows you to easily access the different versions of reprocessed data using any tag or a historical commit ID. Cross-collection consistency guarantees . Data engineers typically need to implement custom logic in scripts to guarantee two or more data assets are updated synchronously. This logic often requires extensive rewrites or periods during which data is unavailable. The lakeFS merge operation from one branch into another removes the need to implement this logic yourself. Instead, make updates to the desired data assets on a branch and then utilize a lakeFS merge to atomically expose the data to downstream consumers. To learn more about atomic cross-collection updates, check out this video which describes the concept in more detail. Troubleshooting production problems . Data engineers are often asked to validate the data. A user might report inconsistencies, question the accuracy, or simply report it to be incorrect. Since the data continuously changes, it is challenging to understand its state at the time of the error. The best way to investigate is by having a snapshot of the data as close as possible to the time of the error. Once you implement a regular commit cadence in lakeFS, each commit represents an accessible historical snapshot of the data. When needed, you may create a branch from a commit ID to debug an issue in isolation. To learn more on how to access a specific historical commit in a repository, see our seminal post on data reproducibility. Establishing data quality guarantees . The best way to deal with mistakes is to avoid them. A data source that is ingested into the lake introducing low-quality data should be blocked before exposure if possible. With lakeFS, you can achieve this by tying data quality tests to commit and merge operations via lakeFS hooks. Additional things you should know about lakeFS: . | It is format-agnostic. | Your data stays in place. | It minimizes data duplication via a copy-on-write mechanism. | It maintains high performance over data lakes of any size. | It includes configurable garbage collection capabilities. | It is highly available and production-ready. | . Is lakeFS Git for Data? . Git had conquered the world of code because it had best supported engineering best practices required by developers, mainly: . | Collaborate during development. | Develop and Test in isolation | Revert code repository to a sable version in case of an error | Reproduce and troubleshoot issues with a given version of the code | Continuously integrate and deploy new code (CI/CD) | . lakeFS provides these exact benefits, that are data practitioners are missing today, and enables them a clear intuitive git-like inetrface to easily manage their data like they manage code. Therefore, lakeFS can definitely be regarded as git for data. Downloads . Binary Releases . Binary packages are available for Linux/macOS/Windows on GitHub Releases . Docker Images . The official Docker images are available at https://hub.docker.com/r/treeverse/lakefs . Next steps . Get started and set up lakeFS on your preferred cloud environemnt . ",
    "url": "/v0.98/#what-else-does-lakefs-do",
    
    "relUrl": "/#what-else-does-lakefs-do"
  },"215": {
    "doc": "Kafka",
    "title": "Using lakeFS with Kafka",
    "content": "Apache Kafka provides a unified, high-throughput, low-latency platform for handling real-time data feeds. Different distributions of Kafka offer different methods for exporting data to S3 called Kafka Sink Connectors. The most commonly used Connector for S3 is Confluent’s S3 Sink Connector. Add the following to connector.properties file for lakeFS support: . # Your lakeFS repository s3.bucket.name=example-repo # Your lakeFS S3 endpoint and credentials store.url=https://lakefs.example.com aws.access.key.id=AKIAIOSFODNN7EXAMPLE aws.secret.access.key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # main being the branch we want to write to topics.dir=main/topics . ",
    "url": "/v0.98/integrations/kafka.html#using-lakefs-with-kafka",
    
    "relUrl": "/integrations/kafka.html#using-lakefs-with-kafka"
  },"216": {
    "doc": "Kafka",
    "title": "Kafka",
    "content": " ",
    "url": "/v0.98/integrations/kafka.html",
    
    "relUrl": "/integrations/kafka.html"
  },"217": {
    "doc": "Kubeflow",
    "title": "Using lakeFS with Kubeflow pipelines",
    "content": "Kubeflow is a project dedicated to making deployments of ML workflows on Kubernetes simple, portable, and scalable. A Kubeflow pipeline is a portable and scalable definition of an ML workflow composed of steps. Each step in the pipeline is an instance of a component represented as an instance of ContainerOp. ",
    "url": "/v0.98/integrations/kubeflow.html#using-lakefs-with-kubeflow-pipelines",
    
    "relUrl": "/integrations/kubeflow.html#using-lakefs-with-kubeflow-pipelines"
  },"218": {
    "doc": "Kubeflow",
    "title": "Table of contents",
    "content": ". | Add pipeline steps for lakeFS operations . | Function-based ContainerOps | Non-function-based ContainerOps | . | Add the lakeFS steps to your pipeline | . ",
    "url": "/v0.98/integrations/kubeflow.html#table-of-contents",
    
    "relUrl": "/integrations/kubeflow.html#table-of-contents"
  },"219": {
    "doc": "Kubeflow",
    "title": "Add pipeline steps for lakeFS operations",
    "content": "To integrate lakeFS into your Kubeflow pipeline, you need to create Kubeflow components that perform lakeFS operations. Currently, there are two methods to create lakeFS ContainerOps: . | Implement a function-based ContainerOp that uses lakeFS’s Python API to invoke lakeFS operations. | Implement a ContainerOp that uses the lakectl CLI docker image to invoke lakeFS operations. | . Function-based ContainerOps . To implement a function-based component that invokes lakeFS operations, you should use the Python OpenAPI client lakeFS provides. See the example below that demonstrates how to make the client’s package available to your ContainerOp. Example operations . Create a new branch: A function-based ContainerOp that creates a branch called example-branch based on the main branch of example-repo. from kfp import components def create_branch(repo_name, branch_name, source_branch): import lakefs_client from lakefs_client import models from lakefs_client.client import LakeFSClient # lakeFS credentials and endpoint configuration = lakefs_client.Configuration() configuration.username = 'AKIAIOSFODNN7EXAMPLE' configuration.password = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' configuration.host = 'https://lakefs.example.com' client = LakeFSClient(configuration) client.branches.create_branch(repository=repo_name, branch_creation=models.BranchCreation(name=branch_name, source=source_branch)) # Convert the function to a lakeFS pipeline step. create_branch_op = components.func_to_container_op( func=create_branch, packages_to_install=['lakefs_client==&lt;lakeFS version&gt;']) # Type in the lakeFS version you are using . You can invoke any lakeFS operation supported by lakeFS OpenAPI. For example, you could implement a commit and merge function-based ContainerOps. Check out the full API reference. Non-function-based ContainerOps . To implement a non-function based ContainerOp, you should use the treeverse/lakectl docker image. With this image, you can run lakectl commands to execute the desired lakeFS operation. For lakectl to work with Kubeflow, you will need to pass your lakeFS configurations as environment variables named: . | LAKECTL_CREDENTIALS_ACCESS_KEY_ID: AKIAIOSFODNN7EXAMPLE | LAKECTL_SECRET_ACCESS_KEY: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY | LAKECTL_SERVER_ENDPOINT_URL: https://lakefs.example.com | . Example operations . | Commit changes to a branch: A ContainerOp that commits uncommitted changes to example-branch on example-repo. from kubernetes.client.models import V1EnvVar def commit_op(): return dsl.ContainerOp( name='commit', image='treeverse/lakectl', arguments=['commit', 'lakefs://example-repo/example-branch', '-m', 'commit message']).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_ACCESS_KEY_ID',value='AKIAIOSFODNN7EXAMPLE')).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY',value='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')).add_env_variable(V1EnvVar(name='LAKECTL_SERVER_ENDPOINT_URL',value='https://lakefs.example.com')) . | Merge two lakeFS branches: A ContainerOp that merges example-branch into the main branch of example-repo. def merge_op(): return dsl.ContainerOp( name='merge', image='treeverse/lakectl', arguments=['merge', 'lakefs://example-repo/example-branch', 'lakefs://example-repo/main']).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_ACCESS_KEY_ID',value='AKIAIOSFODNN7EXAMPLE')).add_env_variable(V1EnvVar(name='LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY',value='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')).add_env_variable(V1EnvVar(name='LAKECTL_SERVER_ENDPOINT_URL',value='https://lakefs.example.com')) . | . You can invoke any lakeFS operation supported by lakectl by implementing it as a ContainerOp. Check out the complete CLI reference for the list of supported operations. Note The lakeFS Kubeflow integration that uses lakectl is supported on lakeFS version &gt;= v0.43.0. ",
    "url": "/v0.98/integrations/kubeflow.html#add-pipeline-steps-for-lakefs-operations",
    
    "relUrl": "/integrations/kubeflow.html#add-pipeline-steps-for-lakefs-operations"
  },"220": {
    "doc": "Kubeflow",
    "title": "Add the lakeFS steps to your pipeline",
    "content": "Add the steps created in the previous step to your pipeline before compiling it. Example pipeline . A pipeline that implements a simple ETL that has steps for branch creation and commits. def lakectl_pipeline(): create_branch_task = create_branch_op('example-repo', 'example-branch', 'main') # A function-based component extract_task = example_extract_op() commit_task = commit_op() transform_task = example_transform_op() commit_task = commit_op() load_task = example_load_op() . Note It’s recommended to store credentials as Kubernetes secrets and pass them as environment variables to Kubeflow operations using V1EnvVarSource. ",
    "url": "/v0.98/integrations/kubeflow.html#add-the-lakefs-steps-to-your-pipeline",
    
    "relUrl": "/integrations/kubeflow.html#add-the-lakefs-steps-to-your-pipeline"
  },"221": {
    "doc": "Kubeflow",
    "title": "Kubeflow",
    "content": " ",
    "url": "/v0.98/integrations/kubeflow.html",
    
    "relUrl": "/integrations/kubeflow.html"
  },"222": {
    "doc": "Internal database structure",
    "title": "Internal database structure",
    "content": " ",
    "url": "/v0.98/understand/how/kv.html",
    
    "relUrl": "/understand/how/kv.html"
  },"223": {
    "doc": "Internal database structure",
    "title": "Table of contents",
    "content": ". | Optimistic Locking with KV | DB Transactions and Atomic Updates | So, Which Approach is Better? | . Starting at version 0.80.2, lakeFS abandoned the tight coupling to PostgreSQL and moved all database operations to work over Key-Value Store . While SQL databases, and Postgres among them, have their obvious advantages, we felt that the tight coupling to Postgres is limiting our users and so, lakeFS with Key Value Store is introduced. Our KV Store implements a generic interface, with methods for Get, Set, Compare-and-Set, Delete and Scan. Each entry is represented by a [partition, key, value] triplet. All these fields are generic byte-array, and the using module has maximal flexibility on the format to use for each field . Under the hood, our KV implementation relies on a backing DB, which persists the data. Theoretically, it could be any type of database and out of the box, we already implemented drivers for DynamoDB, for AWS users, and PostgreSQL, using its relational nature to store a KV Store. More databases will be supported in the future, and lakeFS users and contributors can develop their own driver to use their own favorite database. For experimenting purposes, an in-memory KV store can be used, though it obviously lack the persistency aspect . In order to store its metadata objects (that is Repositories, Branches, Commits, Tags, and Uncommitted Objects), lakeFS implements another layer over the generic KV Store, which supports serialization and deserialization of these objects as protobuf. As this layer relies on the generic interface of the KV Store layer, it is totally agnostic to whichever store implementation is in use, gaining our users the maximal flexibility . For further reading, please refer to our KV Design . ",
    "url": "/v0.98/understand/how/kv.html#table-of-contents",
    
    "relUrl": "/understand/how/kv.html#table-of-contents"
  },"224": {
    "doc": "Internal database structure",
    "title": "Optimistic Locking with KV",
    "content": "One important key difference between SQL databases and Key Value Store is the ability to lock resources. While this is a common practice with relational databases, Key Value stores not always support this ability. When designing our KV Store, we tried to support the most simplistic straight-forward interface, with flexibility in backing DB selection, and so, we decided not to support locking. This decision brought some concurrency challenges we had to overcome. Let us take a look at a common lakeFS flow, Commit, during which several database operations are performed: . | All relevant (Branch correlated) uncommitted objects are collected and marked as committed | A new Commit object is created | The relevant Branch is updated to point to the new commit | . The Commit flow includes multiple database accesses and modifications, and is very sensitive to concurrent executions: If 2 Commit flows run in parallel, we must guarantee correctness of the data. lakeFS with PostgreSQL simply locks the Branch for the entire Commit operation, preventing concurrent execution of such flows. Now, with KV Store replacing the SQL database, this easy solution is no longer available. Instead, we implemented an Optimistic Locking algorithm, which leverages the KV Store Compare-And-Set (CAS) functionality to remember the Branch state at the beginning of the Commit flow, and updating the branch at the end, only if it remains unchanged, using CAS, with the former Branch state, used as a comparison criteria. If the sampled Branch state and the current state differ, it could only mean that another, later, Commit is in progress, causing the first Commit to fail, and give the later Commit a chance to complete. Here’s a running example: . | Commit A sets the StagingToken to tokenA and samples the Branch, | Commit B sets the StagingToken to tokenB and samples the Branch, | Commit A finishes, tries to update the Branch and fails due to the recent modification by Commit B - the StagingToken is set to tokenB and not tokenA as expected by Commit A, | Commit B finishes and updates the branch, as tokenB is set as StagingToken and it matches the flow expectation | . An important detail to note, is that as a Commit starts, and the StagingToken is set a new value, the former value is added to a list of ‘still valid’ StagingTokens - SealedToken - on the Branch, which makes sure no StagingToken and no object are lost due to a failed Commit . You can read more on the Commit Flow in the dedicated section in the KV Design . ",
    "url": "/v0.98/understand/how/kv.html#optimistic-locking-with-kv",
    
    "relUrl": "/understand/how/kv.html#optimistic-locking-with-kv"
  },"225": {
    "doc": "Internal database structure",
    "title": "DB Transactions and Atomic Updates",
    "content": "Another notable difference is the existence of DB transactions with PostgreSQL, ability that our KV Store lacks. This ability was leveraged by lakeFS to construct several DB updates, into one “atomic” operation - each failure, in each step, rolled back the entire operation, keeping the DB consistent and clean. With KV Store, this ability is gone, and we had to come up with various solutions. As a starting point, the DB consistency is, obviously, not anything we can risk. On the other hand, maintaining the DB clean, and as a result smaller, is something that can be sacrificed, at least as a first step. Let us take a look at a relatively simple flow of a new Repository creation: A brand new Repository has 3 objects: The Repository object itself, an initial Branch object and an initial Commit, which the Branch points to. With SQL DB, it was as simple as creating all 3 objects in the DB under one transaction (at this order). Any failure resulted in a rollback and no redundant leftovers in our DB. With no transaction in KV Store, if for example the Branch creation fails, it will leave the Repository without an initial Branch (or a Branch at all), yet the Repository will be accessible. Trying to delete the Repository as a response to Branch creation failure is ony a partial solution as this operation can fail as well. To mitigate this we introduced a per-Repository-partition, which holds all repository related objects (the Branch and Commit in this scenario). The partition key can only be derived from the specificRepository instance itself. In addition we first create the Repository objects, the Commit and the Branch, under the Repository’s partition key, and then the Repository is created. The Repository and its objects will be accessible only after a successful creation of all 3 entities. A failure in this flow might leave some dangling objects, but consistency is maintained. The number of such dangling objects is not expected to be significant, and we plan to implement a cleaning algorithm to keep our KV Store neat and clean . ",
    "url": "/v0.98/understand/how/kv.html#db-transactions-and-atomic-updates",
    
    "relUrl": "/understand/how/kv.html#db-transactions-and-atomic-updates"
  },"226": {
    "doc": "Internal database structure",
    "title": "So, Which Approach is Better?",
    "content": "This documents provides a peek into lakeFS’ new database approach - Key Value Store instead of a Relational SQL. It discusses the challenges we faced, and the solutions we provided to overcome these challenges. Considering the fact that lakeFS over with relational database did work, you might ask yourself why did we bother to develop another solution. The simple answer, is that while PostgreSQL was not a bad option, it was the only option, and any drawback of PostgreSQL, reflected on our users: . | PostgreSQL can only scale vertically and that is a limitation. At some point this might not hold. | PostgreSQL is not a managed solution, meaning that users had to take care of all maintenance tasks, including the above mentioned scale (when needed) | As an unmanaged database, scaling means downtime - is that acceptable? | It might even get to the point that your organization is not willing to work with PostgreSQL due to various business considerations | . If none of the above apply, and you have no seemingly reason to switch from PostgreSQL, it can definitely still be used as an excellent option for the backing database for lakeFS’s KV Store. If you do need another solution, you have DynamoDB support, out of the box. DynamoDB, as a fully managed solution, with horizontal scalability support and optimized partitions support, answers all the pain-points specified above. It is definitely an option to consider, if you need to overcome these And, of course, you can always decide to implement your own KV Store driver to use your database of choice - we would love to add your contribution to lakeFS . ",
    "url": "/v0.98/understand/how/kv.html#so-which-approach-is-better",
    
    "relUrl": "/understand/how/kv.html#so-which-approach-is-better"
  },"227": {
    "doc": "1️⃣ Run lakeFS",
    "title": "👩🏻‍💻 Spin up the environment 👨🏻‍💻",
    "content": "The quickstart uses Docker Compose to bring up the lakeFS container, pre-populate it with some data, and also provide a DuckDB container from where we can interact with the data. You’ll need Docker and git installed to run this. The first step is to clone the lakeFS git repository: . git clone git@github.com:treeverse/lakeFS.git cd lakeFS/quickstart . and then launch the environment with Docker Compose: . docker-compose up . After a few moments you should see the lakeFS container ready to use: . […] lakefs | -------- Let's go and have axolotl fun! -------- lakefs | lakefs | &gt;(.＿.)&lt; http://127.0.0.1:8000/ lakefs | ( )_ lakefs | Access Key ID : AKIA-EXAMPLE-KEY lakefs | Secret Access Key: EXAMPLE-SECRET lakefs | lakefs | ------------------------------------------------ . You’re now ready to dive into lakeFS! . Login to lakeFS’s web interface at http://127.0.0.1:8000 using these credentials: . | Access Key ID: AKIA-EXAMPLE-KEY | Secret Access Key: EXAMPLE-SECRET | . You’ll see that there’s a repository that’s been created automagically for you, imaginatively called quickstart. Click on the repository name to open it. Now we’re ready to explore the data that’s been loaded into the quickstart environment. ",
    "url": "/v0.98/quickstart/launch.html#-spin-up-the-environment-",
    
    "relUrl": "/quickstart/launch.html#-spin-up-the-environment-"
  },"228": {
    "doc": "1️⃣ Run lakeFS",
    "title": "1️⃣ Run lakeFS",
    "content": " ",
    "url": "/v0.98/quickstart/launch.html",
    
    "relUrl": "/quickstart/launch.html"
  },"229": {
    "doc": "🧑🏻‍🎓 Learn more about lakeFS",
    "title": "Learn more about lakeFS",
    "content": "The lakeFS quickstart is just the beginning of your lakeFS journey 🛣️ . Here are some more resources to help you find out more about lakeFS. ",
    "url": "/v0.98/quickstart/learning-more-lakefs.html#learn-more-about-lakefs",
    
    "relUrl": "/quickstart/learning-more-lakefs.html#learn-more-about-lakefs"
  },"230": {
    "doc": "🧑🏻‍🎓 Learn more about lakeFS",
    "title": "Connecting lakeFS to your own object storage",
    "content": "Enjoyed the quickstart and want to try out lakeFS against your own data? Here’s how to run lakeFS locally as a Docker container locally connecting to an object store. | AWS S3 | Azure Blob Storage | Google Cloud Storage | MinIO | . Note: Make sure the Quickstart Docker Compose from the previous steps isn’t also running as you’ll get a port conflict. docker run --pull always -p 8000:8000 \\ -e LAKEFS_BLOCKSTORE_TYPE='s3' \\ -e AWS_ACCESS_KEY_ID='YourAccessKeyValue' \\ -e AWS_SECRET_ACCESS_KEY='YourSecretKeyValue' \\ treeverse/lakefs run --local-settings . Note: Make sure the Quickstart Docker Compose from the previous steps isn’t also running as you’ll get a port conflict. docker run --pull always -p 8000:8000 \\ -e LAKEFS_BLOCKSTORE_TYPE='azure' \\ -e LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCOUNT='YourAzureStorageAccountName' \\ -e LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCESS_KEY='YourAzureStorageAccessKey' \\ treeverse/lakefs run --local-settings . Note: Make sure the Quickstart Docker Compose from the previous steps isn’t also running as you’ll get a port conflict. docker run --pull always -p 8000:8000 \\ -e LAKEFS_BLOCKSTORE_TYPE='gs' \\ -e LAKEFS_BLOCKSTORE_GS_CREDENTIALS_JSON='YourGoogleServiceAccountKeyJSON' \\ treeverse/lakefs run --local-settings . where you will replace YourGoogleServiceAccountKeyJSON with JSON string that contains your Google service account key. If you want to use the JSON file that contains your Google service account key instead of JSON string (as in the previous command) then go to the directory where JSON file is stored and run the command with local parameters: . docker run --pull always -p 8000:8000 \\ -v $PWD:/myfiles \\ -e LAKEFS_BLOCKSTORE_TYPE='gs' \\ -e LAKEFS_BLOCKSTORE_GS_CREDENTIALS_FILE='/myfiles/YourGoogleServiceAccountKey.json' \\ treeverse/lakefs run --local-settings . This command will mount your present working directory (PWD) within the container and will read the JSON file from your PWD. To use lakeFS with MinIO (or other S3-compatible object storage), use the following example: . Note: Make sure the Quickstart Docker Compose from the previous steps isn’t also running as you’ll get a port conflict. docker run --pull always -p 8000:8000 \\ -e LAKEFS_BLOCKSTORE_TYPE='s3' \\ -e LAKEFS_BLOCKSTORE_S3_FORCE_PATH_STYLE='true' \\ -e LAKEFS_BLOCKSTORE_S3_ENDPOINT='http://&lt;minio_endpoint&gt;' \\ -e LAKEFS_BLOCKSTORE_S3_DISCOVER_BUCKET_REGION='false' \\ -e LAKEFS_BLOCKSTORE_S3_CREDENTIALS_ACCESS_KEY_ID='&lt;minio_access_key&gt;' \\ -e LAKEFS_BLOCKSTORE_S3_CREDENTIALS_SECRET_ACCESS_KEY='&lt;minio_secret_key&gt;' \\ treeverse/lakefs run --local-settings . ",
    "url": "/v0.98/quickstart/learning-more-lakefs.html#connecting-lakefs-to-your-own-object-storage",
    
    "relUrl": "/quickstart/learning-more-lakefs.html#connecting-lakefs-to-your-own-object-storage"
  },"231": {
    "doc": "🧑🏻‍🎓 Learn more about lakeFS",
    "title": "Deploying lakeFS",
    "content": "Ready to do this thing for real? The deployment guides show you how to deploy lakeFS locally (including on Kubernetes) or on AWS, Azure, or GCP. Alternatively you might want to have a look at lakeFS Cloud which provides a fully-managed, SOC-2 compliant, lakeFS service. ",
    "url": "/v0.98/quickstart/learning-more-lakefs.html#deploying-lakefs",
    
    "relUrl": "/quickstart/learning-more-lakefs.html#deploying-lakefs"
  },"232": {
    "doc": "🧑🏻‍🎓 Learn more about lakeFS",
    "title": "lakeFS Samples",
    "content": "The lakeFS Samples GitHub repository includes some excellent examples including: . | How to implement multi-table transaction on multiple Delta Tables | Notebooks to show integration of lakeFS with Spark, Python, Delta Lake, Airflow and Hooks. | Examples of using lakeFS webhooks to run automated data quality checks on different branches. | Using lakeFS branching features to create dev/test data environments for ETL testing and experimentation. | Reproducing ML experiments with certainty using lakeFS tags. | . ",
    "url": "/v0.98/quickstart/learning-more-lakefs.html#lakefs-samples",
    
    "relUrl": "/quickstart/learning-more-lakefs.html#lakefs-samples"
  },"233": {
    "doc": "🧑🏻‍🎓 Learn more about lakeFS",
    "title": "lakeFS Community",
    "content": "lakeFS’ community is important to us. Our guiding principles are: . | Fully open, in code and conversation | We learn and grow together | Compassion and respect in every interaction | . We’d love for you to join our Slack group and come and introduce yourself on #say-hello. Or just lurk and soak up the vibes 😎 . If you’re interested in getting involved in lakeFS’ development head over our the GitHub repo to look at the code and peruse the issues. The comprehensive contributing document should have you covered on next steps but if you’ve any questions the #dev channel on Slack will be delighted to help. We love speaking at meetups and chatting to community members at them - you can find a list of these here. Finally, make sure to drop by to say hi on Twitter, Mastodon, and LinkedIn 👋🏻 . ",
    "url": "/v0.98/quickstart/learning-more-lakefs.html#lakefs-community",
    
    "relUrl": "/quickstart/learning-more-lakefs.html#lakefs-community"
  },"234": {
    "doc": "🧑🏻‍🎓 Learn more about lakeFS",
    "title": "lakeFS Concepts and Internals",
    "content": "We describe lakeFS as “git for data” but what does that actually mean? Have a look at the concepts and architecture guides, as well as the explanation of how merges are handled. To go deeper you might be interested in the internals of versioning and our internal database structure. ",
    "url": "/v0.98/quickstart/learning-more-lakefs.html#lakefs-concepts-and-internals",
    
    "relUrl": "/quickstart/learning-more-lakefs.html#lakefs-concepts-and-internals"
  },"235": {
    "doc": "🧑🏻‍🎓 Learn more about lakeFS",
    "title": "🧑🏻‍🎓 Learn more about lakeFS",
    "content": " ",
    "url": "/v0.98/quickstart/learning-more-lakefs.html",
    
    "relUrl": "/quickstart/learning-more-lakefs.html"
  },"236": {
    "doc": "Lua",
    "title": "Lua Hooks Experimental",
    "content": "As of version 0.86.0, lakeFS supports running hooks without relying on external components using an embedded Lua VM . Using Lua hooks, it is possible to pass a Lua script to be executed directly by the lakeFS server when an action occurs. The Lua runtime embedded in lakeFS is limited for security reasons. It provides a narrow set of APIs and functions that by default do not allow: . | Accessing any of the running lakeFS server’s environment | Accessing the local filesystem available the lakeFS process | . ",
    "url": "/v0.98/hooks/lua.html#lua-hooks-experimental",
    
    "relUrl": "/hooks/lua.html#lua-hooks-experimental"
  },"237": {
    "doc": "Lua",
    "title": "Table of contents",
    "content": ". | Lua Library reference . | aws/s3.get_object(bucket, key) | aws/s3.put_object(bucket, key, value) | aws/s3.delete_object(bucket [, key]) | aws/s3.list_objects(bucket [, prefix, continuation_token, delimiter]) | aws/s3.delete_recursive(bucket, prefix) | crypto/aes/encryptCBC(key, plaintext) | crypto/aes/decryptCBC(key, ciphertext) | crypto/hmac/sign_sha256(message, key) | crypto/hmac/sign_sha1(message, key) | crypto/md5/digest(data) | crypto/sha256/digest(data) | encoding/base64/encode(data) | encoding/base64/decode(data) | encoding/base64/url_encode(data) | encoding/base64/url_decode(data) | encoding/hex/encode(value) | encoding/hex/decode(value) | encoding/json/marshal(table) | encoding/json/unmarshal(string) | encoding/parquet/get_schema(payload) | lakefs | lakefs/create_tag(repository_id, reference_id, tag_id) | lakefs/diff_refs(repository_id, lef_reference_id, right_reference_id [, after, prefix, delimiter, amount]) | lakefs/list_objects(repository_id, reference_id [, after, prefix, delimiter, amount]) | lakefs/get_object(repository_id, reference_id, path) | path/parse(path_string) | path/join(*path_parts) | path/is_hidden(path_string [, seperator, prefix]) | path/default_separator() | regexp/match(pattern, s) | regexp/quote_meta(s) | regexp/compile(pattern) | regexp/compiled_pattern.find_all(s, n) | regexp/compiled_pattern.find_all_submatch(s, n) | regexp/compiled_pattern.find(s) | regexp/compiled_pattern.find_submatch(s) | strings/split(s, sep) | strings/trim(s) | strings/replace(s, old, new, n) | strings/has_prefix(s, prefix) | strings/has_suffix(s, suffix) | strings/contains(s, substr) | time/now() | time/format(epoch_nano, layout, zone) | time/format_iso(epoch_nano, zone) | time/sleep(duration_ns) | time/since(epoch_nano) | time/add(epoch_time, duration_table) | time/parse(layout, value) | time/parse_iso(value) | uuid/new() | . | Example Hooks | . ",
    "url": "/v0.98/hooks/lua.html#table-of-contents",
    
    "relUrl": "/hooks/lua.html#table-of-contents"
  },"238": {
    "doc": "Lua",
    "title": "Lua Library reference",
    "content": "aws/s3.get_object(bucket, key) . Returns the body (as a Lua string) of the requested object and a boolean value that is true if the requested object exists . aws/s3.put_object(bucket, key, value) . Sets the object at the given bucket and key to the value of the supplied value string . aws/s3.delete_object(bucket [, key]) . Deletes the object at the given key . aws/s3.list_objects(bucket [, prefix, continuation_token, delimiter]) . Returns a table of results containing the following structure: . | is_truncated: (boolean) whether there are more results to paginate through using the continuation token | next_continuation_token: (string) to pass in the next request to get the next page of results | results (table of tables) information about the objects (and prefixes if a delimiter is used) | . a result could in one of the following structures . { [\"key\"] = \"a/common/prefix/\", [\"type\"] = \"prefix\" } . or: . { [\"key\"] = \"path/to/object\", [\"type\"] = \"object\", [\"etag\"] = \"etagString\", [\"size\"] = 1024, [\"last_modified\"] = \"2023-12-31T23:10:00Z\" } . aws/s3.delete_recursive(bucket, prefix) . Deletes all objects under the given prefix . crypto/aes/encryptCBC(key, plaintext) . Returns a ciphertext for the aes encrypted text . crypto/aes/decryptCBC(key, ciphertext) . Returns the decrypted (plaintext) string for the encrypted ciphertext . crypto/hmac/sign_sha256(message, key) . Returns a SHA256 hmac signature for the given message with the supplied key (using the SHA256 hashing algorithm) . crypto/hmac/sign_sha1(message, key) . Returns a SHA1 hmac signature for the given message with the supplied key (using the SHA1 hashing algorithm) . crypto/md5/digest(data) . Returns the MD5 digest (string) of the given data . crypto/sha256/digest(data) . Returns the SHA256 digest (string) of the given data . encoding/base64/encode(data) . Encodes the given data to a base64 string . encoding/base64/decode(data) . Decodes the given base64 encoded data and return it as a string . encoding/base64/url_encode(data) . Encodes the given data to an unpadded alternate base64 encoding defined in RFC 4648. encoding/base64/url_decode(data) . Decodes the given unpadded alternate base64 encoding defined in RFC 4648 and return it as a string . encoding/hex/encode(value) . Encode the given value string to hexadecimal values (string) . encoding/hex/decode(value) . Decode the given hexadecimal string back to the string it represents (UTF-8) . encoding/json/marshal(table) . Encodes the given table into a JSON string . encoding/json/unmarshal(string) . Decodes the given string into the equivalent Lua structure . encoding/parquet/get_schema(payload) . Read the payload (string) as the contents of a Parquet file and return its schema in the following table structure: . { { [\"name\"] = \"column_a\", [\"type\"] = \"INT32\" }, { [\"name\"] = \"column_b\", [\"type\"] = \"BYTE_ARRAY\" } } . lakefs . The Lua Hook library allows calling back to the lakeFS API using the identity of the user that triggered the action. For example, if user A tries to commit and triggers a pre-commit hook - any call made inside that hook to the lakefs API, will automatically use user A’s identity for authorization and auditing purposes. lakefs/create_tag(repository_id, reference_id, tag_id) . Create a new tag for the given reference . lakefs/diff_refs(repository_id, lef_reference_id, right_reference_id [, after, prefix, delimiter, amount]) . Returns an object-wise diff between left_reference_id and right_reference_id. lakefs/list_objects(repository_id, reference_id [, after, prefix, delimiter, amount]) . List objects in the specified repository and reference (branch, tag, commit ID, etc.). If delimiter is empty, will default to a recursive listing. Otherwise, common prefixes up to delimiter will be shown as a single entry. lakefs/get_object(repository_id, reference_id, path) . Returns 2 values: . | The HTTP status code returned by the lakeFS API | The content of the specified object as a lua string | . path/parse(path_string) . Returns a table for the given path string with the following structure: . &gt; require(\"path\") &gt; path.parse(\"a/b/c.csv\") { [\"parent\"] = \"a/b/\" [\"base_name\"] = \"c.csv\" } . path/join(*path_parts) . Receives a variable number of strings and returns a joined string that represents a path: . &gt; require(\"path\") &gt; path.join(\"path/\", \"to\", \"a\", \"file.data\") path/o/a/file.data . path/is_hidden(path_string [, seperator, prefix]) . returns a boolean - true if the given path string is hidden (meaning it starts with prefix) - or if any of its parents start with prefix. &gt; require(\"path\") &gt; path.is_hidden(\"a/b/c\") -- false &gt; path.is_hidden(\"a/b/_c\") -- true &gt; path.is_hidden(\"a/_b/c\") -- true &gt; path.is_hidden(\"a/b/_c/\") -- true . path/default_separator() . Returns a constant string (/) . regexp/match(pattern, s) . Returns true if the string s matches pattern. This is a thin wrapper over Go’s regexp.MatchString. regexp/quote_meta(s) . Escapes any meta-characters in string s and returns a new string . regexp/compile(pattern) . Returns a regexp match object for the given pattern . regexp/compiled_pattern.find_all(s, n) . Returns a table list of all matches for the pattern, (up to n matches, unless n == -1 in which case all possible matches will be returned) . regexp/compiled_pattern.find_all_submatch(s, n) . Returns a table list of all sub-matches for the pattern, (up to n matches, unless n == -1 in which case all possible matches will be returned). Submatches are matches of parenthesized subexpressions (also known as capturing groups) within the regular expression, numbered from left to right in order of opening parenthesis. Submatch 0 is the match of the entire expression, submatch 1 is the match of the first parenthesized subexpression, and so on . regexp/compiled_pattern.find(s) . Returns a string representing the left-most match for the given pattern in string s . regexp/compiled_pattern.find_submatch(s) . find_submatch returns a table of strings holding the text of the leftmost match of the regular expression in s and the matches, if any, of its submatches . strings/split(s, sep) . returns a table of strings, the result of splitting s with sep. strings/trim(s) . Returns a string with all leading and trailing white space removed, as defined by Unicode . strings/replace(s, old, new, n) . Returns a copy of the string s with the first n non-overlapping instances of old replaced by new. If old is empty, it matches at the beginning of the string and after each UTF-8 sequence, yielding up to k+1 replacements for a k-rune string. If n &lt; 0, there is no limit on the number of replacements . strings/has_prefix(s, prefix) . Returns true if s begins with prefix . strings/has_suffix(s, suffix) . Returns true if s ends with suffix . strings/contains(s, substr) . Returns true if substr is contained anywhere in s . time/now() . Returns a float64 representing the amount of nanoseconds since the unix epoch (01/01/1970 00:00:00). time/format(epoch_nano, layout, zone) . Returns a string representation of the given epoch_nano timestamp for the given Timezone (e.g. \"UTC\", \"America/Los_Angeles\", …) The layout parameter should follow Go’s time layout format. time/format_iso(epoch_nano, zone) . Returns a string representation of the given epoch_nano timestamp for the given Timezone (e.g. \"UTC\", \"America/Los_Angeles\", …) The returned string will be in ISO8601 format. time/sleep(duration_ns) . Sleep for duration_ns nanoseconds . time/since(epoch_nano) . Returns the amount of nanoseconds elapsed since epoch_nano . time/add(epoch_time, duration_table) . Returns a new timestamp (in nanoseconds passed since 01/01/1970 00:00:00) for the given duration. The duration should be a table with the following structure: . &gt; require(\"time\") &gt; time.add(time.now(), { [\"hour\"] = 1, [\"minute\"] = 20, [\"second\"] = 50 }) . You may omit any of the fields from the table, resulting in a default value of 0 for omitted fields . time/parse(layout, value) . Returns a float64 representing the amount of nanoseconds since the unix epoch (01/01/1970 00:00:00). This timestamp will represent date value parsed using the layout format. The layout parameter should follow Go’s time layout format . time/parse_iso(value) . Returns a float64 representing the amount of nanoseconds since the unix epoch (01/01/1970 00:00:00 for value. The value string should be in ISO8601 format . uuid/new() . Returns a new 128-bit RFC 4122 UUID in string representation. ",
    "url": "/v0.98/hooks/lua.html#lua-library-reference",
    
    "relUrl": "/hooks/lua.html#lua-library-reference"
  },"239": {
    "doc": "Lua",
    "title": "Example Hooks",
    "content": "This example will simply print out a JSON representation of the event that occurred: . name: dump_all on: post-commit: post-merge: post-create-tag: post-create-branch: hooks: - id: dump_event type: lua properties: script: | json = require(\"encoding/json\") print(json.marshal(action)) . A more useful example: ensure every commit contains a required metadata field: . name: pre commit metadata field check on: pre-commit: branches: - main - dev hooks: - id: ensure_commit_metadata type: lua properties: args: notebook_url: {\"pattern\": \"my-jupyter.example.com/.*\"} spark_version: {} script: | regexp = require(\"regexp\") for k, props in pairs(args) do current_value = action.commit.metadata[k] if current_value == nil then error(\"missing mandatory metadata field: \" .. k) end if props.pattern and not regexp.match(props.pattern, current_value) then error(\"current value for commit metadata field \" .. k .. \" does not match pattern: \" .. props.pattern .. \" - got: \" .. current_value) end end . For more examples and configuration samples, check out the examples/hooks/ directory in the lakeFS repository. ",
    "url": "/v0.98/hooks/lua.html#example-hooks",
    
    "relUrl": "/hooks/lua.html#example-hooks"
  },"240": {
    "doc": "Lua",
    "title": "Lua",
    "content": " ",
    "url": "/v0.98/hooks/lua.html",
    
    "relUrl": "/hooks/lua.html"
  },"241": {
    "doc": "Managed Garbage Collection",
    "title": "Managed Garbage Collection",
    "content": "lakeFS Cloud . Managed Garbage Collection is only available for lakeFS Cloud. If you are using self-managed lakeFS, Garbage collection is available to run manually. ",
    "url": "/v0.98/cloud/managed-gc.html",
    
    "relUrl": "/cloud/managed-gc.html"
  },"242": {
    "doc": "Managed Garbage Collection",
    "title": "The benefits of using managed GC on lakeFS cloud are:",
    "content": ". | Advanced Engine to detect and delete objects quickly and safely | Managed Solution | Garbage Collection SLA | Support | . ",
    "url": "/v0.98/cloud/managed-gc.html#the-benefits-of-using-managed-gc-on-lakefs-cloud-are",
    
    "relUrl": "/cloud/managed-gc.html#the-benefits-of-using-managed-gc-on-lakefs-cloud-are"
  },"243": {
    "doc": "Managed Garbage Collection",
    "title": "How does it work?",
    "content": "lakeFS Cloud Managed Garbage Collection is using the same configuration of garbage collection rules that are used in the self-managed version of lakeFS, but, we run it on our infrastructure, with a super-fast and efficient engine to detect stale objects and branches (depends on your configuration) and prioritize them for deletion. ",
    "url": "/v0.98/cloud/managed-gc.html#how-does-it-work",
    
    "relUrl": "/cloud/managed-gc.html#how-does-it-work"
  },"244": {
    "doc": "Managed Garbage Collection",
    "title": "Setting up Managed Garbage Collection",
    "content": "lakeFS Cloud Onboarding Setup Wizard contains a toggle to enable Managed GC. This will create additional cloud resources for us to use and have access to delete those objects. ",
    "url": "/v0.98/cloud/managed-gc.html#setting-up-managed-garbage-collection",
    
    "relUrl": "/cloud/managed-gc.html#setting-up-managed-garbage-collection"
  },"245": {
    "doc": "Merge",
    "title": "Merge",
    "content": "The merge operation in lakeFS is similar to Git. It incorporates changes from a merge source (a commit/reference) into a merge destination (a branch). ",
    "url": "/v0.98/understand/how/merge.html",
    
    "relUrl": "/understand/how/merge.html"
  },"246": {
    "doc": "Merge",
    "title": "How does it work?",
    "content": "lakeFS first finds the merge base: the nearest common ancestor of the two commits. It can now perform a three-way merge, by examining the presence and identity of files in each commit. In the table below, “A”, “B” and “C” are possible file contents, “X” is a missing file, and “conflict” (which only appears as a result) is a merge failure. | In base | In source | In destination | Result | Comment | . | A | A | A | A | Unchanged file | . | A | B | B | B | Files changed on both sides in same way | . | A | B | C | conflict | Files changed on both sides differently | . | A | A | B | B | File changed only on one branch | . | A | B | A | B | File changed only on one branch | . | A | X | X | X | Files deleted on both sides | . | A | B | X | conflict | File changed on one side, deleted on the other | . | A | X | B | conflict | File changed on one side, deleted on the other | . | A | A | X | X | File deleted on one side | . | A | X | A | X | File deleted on one side | . ",
    "url": "/v0.98/understand/how/merge.html#how-does-it-work",
    
    "relUrl": "/understand/how/merge.html#how-does-it-work"
  },"247": {
    "doc": "Merge",
    "title": "Merge Strategies",
    "content": "The API and lakectl allow passing an optional strategy flag with the following values: . source-wins . In case of a conflict, merge will pick the source objects. Example . lakectl merge lakefs://example-repo/validated-data lakefs://example-repo/production --strategy source-wins . When a merge conflict arises, the conflicting objects in the validated-data branch will be chosen to end up in production. dest-wins . In case of a conflict, merge will pick the destination objects. Example . lakectl merge lakefs://example-repo/validated-data lakefs://example-repo/production --strategy dest-wins . When a merge conflict arises, the conflicting objects in the production branch will be chosen to end up in validated-data. The production branch will not be affected by object changes from validated-data conflicting objects. The strategy will affect all conflicting objects in the merge if it is set. Currently it is not possible to treat conflicts individually. As a format-agnostic system, lakeFS currently merges by complete files. Format-specific and other user-defined merge strategies for handling conflicts are on the roadmap. ",
    "url": "/v0.98/understand/how/merge.html#merge-strategies",
    
    "relUrl": "/understand/how/merge.html#merge-strategies"
  },"248": {
    "doc": "Migrating away from lakeFS",
    "title": "Migrating away from lakeFS",
    "content": " ",
    "url": "/v0.98/howto/migrate-away.html",
    
    "relUrl": "/howto/migrate-away.html"
  },"249": {
    "doc": "Migrating away from lakeFS",
    "title": "Copying data from a lakeFS repository to an S3 bucket",
    "content": "The simplest way to migrate away from lakeFS is by copying data from a lakeFS repository to an S3 bucket (or any other object store). For smaller repositories, you can do this by using the AWS CLI or Rclone. For larger repositories, running distcp with lakeFS as the source is also an option. ",
    "url": "/v0.98/howto/migrate-away.html#copying-data-from-a-lakefs-repository-to-an-s3-bucket",
    
    "relUrl": "/howto/migrate-away.html#copying-data-from-a-lakefs-repository-to-an-s3-bucket"
  },"250": {
    "doc": "Model",
    "title": "Model",
    "content": " ",
    "url": "/v0.98/understand/model.html",
    
    "relUrl": "/understand/model.html"
  },"251": {
    "doc": "Model",
    "title": "Table of contents",
    "content": ". | Objects | Version Control . | Repository | Commits | Branches | Tags | History | Merge | Ref expressions | . | Concepts unique to lakeFS . | lakefs protocol URIs | . | . lakeFS blends concepts from object stores such as S3 with concepts from Git. This reference defines the common concepts of lakeFS. ",
    "url": "/v0.98/understand/model.html#table-of-contents",
    
    "relUrl": "/understand/model.html#table-of-contents"
  },"252": {
    "doc": "Model",
    "title": "Objects",
    "content": "lakeFS is an interface to manage objects in an object store. The actual data itself is not stored inside lakeFS directly but in an underlying object store. lakeFS manages pointers and additional metadata about these objects. ",
    "url": "/v0.98/understand/model.html#objects",
    
    "relUrl": "/understand/model.html#objects"
  },"253": {
    "doc": "Model",
    "title": "Version Control",
    "content": "lakeFS is spearheading version control semantics for data. Most of these concepts will be familiar to Git users: . Repository . In lakeFS, a repository is a set of related objects (or collections of objects). In many cases, these represent tables of various formats for tabular data, semi-structured data such as JSON or log files - or a set of unstructured objects such as images, videos, sensor data, etc. lakeFS represents repositories as a logical namespace used to group together objects, branches, and commits - analogous to a repository in Git. Commits . Using commits, you can view a repository at a certain point in its history and you’re guaranteed that the data you see is exactly as it was at the point of committing it. These commits are immutable “checkpoints” containing all contents of a repository at a given point in the repository’s history. Each commit contains metadata - the committer, timestamp, a commit message, as well as arbitrary key/value pairs you can choose to add. Identifying Commits A commit is identified by its commit ID, a digest of all contents of the commit. Commit IDs are by nature long, so you may use a unique prefix to abbreviate them. A commit may also be identified by using a textual definition, called a ref. Examples of refs include tags, branch names, and expressions. Branches . Branches in lakeFS allow users to create their own “isolated” view of the repository. Changes on one branch do not appear on other branches. Users can take changes from one branch and apply it to another by merging them. Under the hood, branches are simply a pointer to a commit along with a set of uncommitted changes. Tags . Tags are a way to give a meaningful name to a specific commit. Using tags allow users to reference specific releases, experiments or versions by using a human friendly name. Example tags: . | v2.3 to mark a release. | dev:jane-before-v2.3-merge to mark Jane’s private temporary point. | . History . The history of the branch is the list of commits from the branch tip through the first parent of each commit. Histories go back in time. Merge . Merging is the way to integrate changes from a branch into another branch. The result of a merge is a new commit, with the destination as the first parent and the source as the second. To learn more about how merging works in lakeFS, see the merge reference . Ref expressions . lakeFS also supports expressions for creating a ref. These are similar to revisions in Git; indeed all ~ and ^ examples at the end of that section will work unchanged in lakeFS. | A branch or a tag are ref expressions. | If &lt;ref&gt; is a ref expression, then: . | &lt;ref&gt;^ is a ref expression referring to its first parent. | &lt;ref&gt;^N is a ref expression referring to its N’th parent; in particular &lt;ref&gt;^1 is the same as &lt;ref&gt;^. | &lt;ref&gt;~ is a ref expression referring to its first parent; in particular &lt;ref&gt;~ is the same as &lt;ref&gt;^ and &lt;ref&gt;~. | &lt;ref&gt;~N is a ref expression referring to its N’th parent, always traversing to the first parent. So &lt;ref&gt;~N is the same as &lt;ref&gt;^^...^ with N consecutive carets ^. | . | . ",
    "url": "/v0.98/understand/model.html#version-control",
    
    "relUrl": "/understand/model.html#version-control"
  },"254": {
    "doc": "Model",
    "title": "Concepts unique to lakeFS",
    "content": "The underlying storage is a location in an object store where lakeFS keeps your objects and some immutable metadata. When creating a lakeFS repository, you assign it with a storage namespace. The repository’s storage namespace is a location in the underlying storage where data for this repository will be stored. We sometimes refer to underlying storage as physical. The path used to store the contents of an object is then termed a physical path. Once lakeFS saves an object in the underlying storage it is never modified, except to remove it entirely during some cleanups. A lot of what lakeFS does is to manage how lakeFS paths translate to physical paths on the object store. This mapping is generally not straightforward. Importantly (and contrary to many object stores), lakeFS may map multiple paths to the same object on backing storage, and always does this for objects that are unchanged across versions. lakefs protocol URIs . lakeFS uses a specific format for path URIs. The URI lakefs://&lt;REPO&gt;/&lt;REF&gt;/&lt;KEY&gt; is a path to objects in the given repo and ref expression under key. This is used both for path prefixes and for full paths. In similar fashion, lakefs://&lt;REPO&gt;/&lt;REF&gt; identifies the repository at a ref expression, and lakefs://&lt;REPO&gt; identifes a repo. ",
    "url": "/v0.98/understand/model.html#concepts-unique-to-lakefs",
    
    "relUrl": "/understand/model.html#concepts-unique-to-lakefs"
  },"255": {
    "doc": "Monitoring using Prometheus",
    "title": "Monitoring using Prometheus",
    "content": " ",
    "url": "/v0.98/reference/monitor.html",
    
    "relUrl": "/reference/monitor.html"
  },"256": {
    "doc": "Monitoring using Prometheus",
    "title": "Table of contents",
    "content": ". | Example prometheus.yml | Metrics exposed by lakeFS | Example queries . | 99th percentile of API request latencies | 50th percentile of S3-compatible API latencies | Number of errors in outgoing S3 requests | Number of open connections to the database | Example Grafana dashboard | . | . ",
    "url": "/v0.98/reference/monitor.html#table-of-contents",
    
    "relUrl": "/reference/monitor.html#table-of-contents"
  },"257": {
    "doc": "Monitoring using Prometheus",
    "title": "Example prometheus.yml",
    "content": "lakeFS exposes metrics through the same port used by the lakeFS service, using the standard /metrics path. An example prometheus.yml could look like this: . scrape_configs: - job_name: lakeFS scrape_interval: 10s metrics_path: /metrics static_configs: - targets: - lakefs.example.com:8000 . ",
    "url": "/v0.98/reference/monitor.html#example-prometheusyml",
    
    "relUrl": "/reference/monitor.html#example-prometheusyml"
  },"258": {
    "doc": "Monitoring using Prometheus",
    "title": "Metrics exposed by lakeFS",
    "content": "By default, Prometheus exports metrics with OS process information like memory and CPU. It also includes Go-specific metrics such as details about GC and a number of goroutines. You can learn about these default metrics in this post. In addition, lakeFS exposes the following metrics to help monitor your deployment: . | Name in Prometheus | Description | Labels | . | api_requests_total | lakeFS API requests (counter) | code: http statusmethod: http method | . | api_request_duration_seconds | Durations of lakeFS API requests (histogram) | operation: name of API operationcode: http status | . | gateway_request_duration_seconds | lakeFS S3-compatible endpoint request (histogram) | operation: name of gateway operationcode: http status | . | s3_operation_duration_seconds | Outgoing S3 operations (histogram) | operation: operation nameerror: “true” if error, “false” otherwise | . | gs_operation_duration_seconds | Outgoing Google Storage operations (histogram) | operation: operation nameerror: “true” if error, “false” otherwise | . | azure_operation_duration_seconds | Outgoing Azure storage operations (histogram) | operation: operation nameerror: “true” if error, “false” otherwise | . | dynamo_request_duration_seconds | Time spent doing DynamoDB requests | operation: DynamoDB operation name | . | dynamo_consumed_capacity_total | The capacity units consumed by operation | operation: DynamoDB operation name | . | dynamo_failures_total | The total number of errors while working for kv store | operation: DynamoDB operation name | . | pgxpool_acquire_count | PostgreSQL cumulative count of successful acquires from the pool | db_name default to the kv table name (kv) | . | pgxpool_acquire_duration_ns | PostgreSQL total duration of all successful acquires from the pool in nanoseconds | db_name default to the kv table name (kv) | . | pgxpool_acquired_conns | PostgreSQL number of currently acquired connections in the pool | db_name default to the kv table name (kv) | . | pgxpool_canceled_acquire_count | PostgreSQL cumulative count of acquires from the pool that were canceled by a context | db_name default to the kv table name (kv) | . | pgxpool_constructing_conns | PostgreSQL number of conns with construction in progress in the pool | db_name default to the kv table name (kv) | . | pgxpool_empty_acquire | PostgreSQL cumulative count of successful acquires from the pool that waited for a resource to be released or constructed because the pool was empty | db_name default to the kv table name (kv) | . | pgxpool_idle_conns | PostgreSQL number of currently idle conns in the pool | db_name default to the kv table name (kv) | . | pgxpool_max_conns | PostgreSQL maximum size of the pool | db_name default to the kv table name (kv) | . | pgxpool_total_conns | PostgreSQL total number of resources currently in the pool | db_name default to the kv table name (kv) | . ",
    "url": "/v0.98/reference/monitor.html#metrics-exposed-by-lakefs",
    
    "relUrl": "/reference/monitor.html#metrics-exposed-by-lakefs"
  },"259": {
    "doc": "Monitoring using Prometheus",
    "title": "Example queries",
    "content": "Note: when using Prometheus functions like rate or increase, results are extrapolated and may not be exact. 99th percentile of API request latencies . sum by (operation)(histogram_quantile(0.99, rate(api_request_duration_seconds_bucket[1m]))) . 50th percentile of S3-compatible API latencies . sum by (operation)(histogram_quantile(0.5, rate(gateway_request_duration_seconds_bucket[1m]))) . Number of errors in outgoing S3 requests . sum by (operation) (increase(s3_operation_duration_seconds_count{error=\"true\"}[1m])) . Number of open connections to the database . go_sql_stats_connections_open . Example Grafana dashboard . ",
    "url": "/v0.98/reference/monitor.html#example-queries",
    
    "relUrl": "/reference/monitor.html#example-queries"
  },"260": {
    "doc": "On-Premises Deployment of lakeFS",
    "title": "On-Premises deployment",
    "content": "⏰ Expected deployment time: 25 min . ",
    "url": "/v0.98/deploy/onprem.html#on-premises-deployment",
    
    "relUrl": "/deploy/onprem.html#on-premises-deployment"
  },"261": {
    "doc": "On-Premises Deployment of lakeFS",
    "title": "Table of contents",
    "content": ". | Prerequisites | Setting up a database | Setting up a lakeFS Server | Local Blockstore . | Sample configuration using local blockstore | Limitations | . | Create the admin user | Create your first repository | . ",
    "url": "/v0.98/deploy/onprem.html#table-of-contents",
    
    "relUrl": "/deploy/onprem.html#table-of-contents"
  },"262": {
    "doc": "On-Premises Deployment of lakeFS",
    "title": "Prerequisites",
    "content": "To use lakeFS, you’ll need to have access to an S3-compatible object store such as MinIO . For more information on how to set up MinIO, see the official deployment guide . ",
    "url": "/v0.98/deploy/onprem.html#prerequisites",
    
    "relUrl": "/deploy/onprem.html#prerequisites"
  },"263": {
    "doc": "On-Premises Deployment of lakeFS",
    "title": "Setting up a database",
    "content": "lakeFS requires a PostgreSQL database to synchronize actions on your repositories. This section assumes that you already have a PostgreSQL &gt;= 11.0 database accessible. ",
    "url": "/v0.98/deploy/onprem.html#setting-up-a-database",
    
    "relUrl": "/deploy/onprem.html#setting-up-a-database"
  },"264": {
    "doc": "On-Premises Deployment of lakeFS",
    "title": "Setting up a lakeFS Server",
    "content": ". | Linux | Docker | Kubernetes | . Connect to your host using SSH: . | Create a config.yaml on your VM, with the following parameters: . --- database: type: \"postgres\" postgres: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: # replace this with a randomly-generated string. Make sure to keep it safe! secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: s3 s3: force_path_style: true endpoint: http://&lt;minio_endpoint&gt; discover_bucket_region: false credentials: access_key_id: &lt;minio_access_key&gt; secret_access_key: &lt;minio_secret_key&gt; . ⚠️ Notice that the lakeFS Blockstore type is set to s3 - This configuration works with S3-compatible storage engines such as MinIO. | Download the binary to the server. | Run the lakefs binary: . lakefs --config config.yaml run . | . Note: It’s preferable to run the binary as a service using systemd or your operating system’s facilities. To support container-based environments, you can configure lakeFS using environment variables. Here is a docker run command to demonstrate starting lakeFS using Docker: . docker run \\ --name lakefs \\ -p 8000:8000 \\ -e LAKEFS_DATABASE_TYPE=\"postgres\" \\ -e LAKEFS_DATABASE_POSTGRES_CONNECTION_STRING=\"[DATABASE_CONNECTION_STRING]\" \\ -e LAKEFS_AUTH_ENCRYPT_SECRET_KEY=\"[ENCRYPTION_SECRET_KEY]\" \\ -e LAKEFS_BLOCKSTORE_TYPE=\"s3\" \\ -e LAKEFS_BLOCKSTORE_S3_FORCE_PATH_STYLE=\"true\" \\ -e LAKEFS_BLOCKSTORE_S3_ENDPOINT=\"http://&lt;minio_endpoint&gt;\" \\ -e LAKEFS_BLOCKSTORE_S3_DISCOVER_BUCKET_REGION=\"false\" \\ -e LAKEFS_BLOCKSTORE_S3_CREDENTIALS_ACCESS_KEY_ID=\"&lt;minio_access_key&gt;\" \\ -e LAKEFS_BLOCKSTORE_S3_CREDENTIALS_SECRET_ACCESS_KEY=\"&lt;minio_secret_key&gt;\" \\ treeverse/lakefs:latest run . ⚠️ Notice that the lakeFS Blockstore type is set to s3 - This configuration works with S3-compatible storage engines such as MinIO. See the reference for a complete list of environment variables. You can install lakeFS on Kubernetes using a Helm chart. To install lakeFS with Helm: . | Copy the Helm values file relevant for S3-Compatible storage (MinIO in this example): . secrets: # replace this with the connection string of the database you created in a previous step: databaseConnectionString: [DATABASE_CONNECTION_STRING] # replace this with a randomly-generated string authEncryptSecretKey: [ENCRYPTION_SECRET_KEY] lakefsConfig: | blockstore: type: s3 s3: force_path_style: true endpoint: http://&lt;minio_endpoint&gt; discover_bucket_region: false credentials: access_key_id: &lt;minio_access_key&gt; secret_access_key: &lt;minio_secret_key&gt; . ⚠️ Notice that the lakeFS Blockstore type is set to s3 - This configuration works with S3-compatible storage engines such as MinIO. | Fill in the missing values and save the file as conf-values.yaml. For more configuration options, see our Helm chart README. The lakefsConfig parameter is the lakeFS configuration documented here but without sensitive information. Sensitive information like databaseConnectionString is given through separate parameters, and the chart will inject it into Kubernetes secrets. | In the directory where you created conf-values.yaml, run the following commands: . # Add the lakeFS repository helm repo add lakefs https://charts.lakefs.io # Deploy lakeFS helm install my-lakefs lakefs/lakefs -f conf-values.yaml . my-lakefs is the Helm Release name. ",
    "url": "/v0.98/deploy/onprem.html#setting-up-a-lakefs-server",
    
    "relUrl": "/deploy/onprem.html#setting-up-a-lakefs-server"
  },"265": {
    "doc": "On-Premises Deployment of lakeFS",
    "title": "Load balancing",
    "content": "To configure a load balancer to direct requests to the lakeFS servers you can use the LoadBalancer Service type or a Kubernetes Ingress. By default, lakeFS operates on port 8000 and exposes a /_health endpoint that you can use for health checks. 💡 The NGINX Ingress Controller by default limits the client body size to 1 MiB. Some clients use bigger chunks to upload objects - for example, multipart upload to lakeFS using the S3-compatible Gateway or a simple PUT request using the OpenAPI Server. Checkout Nginx documentation for increasing the limit, or an example of Nginx configuration with MinIO. | . ",
    "url": "/v0.98/deploy/onprem.html#load-balancing",
    
    "relUrl": "/deploy/onprem.html#load-balancing"
  },"266": {
    "doc": "On-Premises Deployment of lakeFS",
    "title": "Local Blockstore",
    "content": "You can configure a block adapter to a POSIX compatible storage location shared by all lakeFS instances. Using the shared storage location, both data and metadata will be stored there. Using the local blockstore import and allowing lakeFS access to a specific prefix, it is possible to import files from a shared location. Import is not enabled by default, as it doesn’t assume the local path is shared and there is a security concern about accessing a path outside the specified in the blockstore configuration. Enabling is done by blockstore.local.import_enabled and blockstore.local.allowed_external_prefixes as described in the configuration reference. Sample configuration using local blockstore . database: type: \"postgres\" postgres: connection_string: \"[DATABASE_CONNECTION_STRING]\" auth: encrypt: # replace this with a randomly-generated string. Make sure to keep it safe! secret_key: \"[ENCRYPTION_SECRET_KEY]\" blockstore: type: local local: path: /shared/location/lakefs_data # location where data and metadata kept by lakeFS import_enabled: true # required to be true to enable import files # from `allowed_external_prefixes` locations allowed_external_prefixes: - /shared/location/files_to_import # location with files we can import into lakeFS, require access from lakeFS . Limitations . | Using a local adapter on a shared location is relativly new and not battle-tested yet | lakeFS doesn’t control the way a shared location is managed across machines | Import works only for folders | Garbage collector (for committed and uncommitted) and lakeFS Hadoop FileSystem currently unsupported | . ",
    "url": "/v0.98/deploy/onprem.html#local-blockstore",
    
    "relUrl": "/deploy/onprem.html#local-blockstore"
  },"267": {
    "doc": "On-Premises Deployment of lakeFS",
    "title": "Create the admin user",
    "content": "When you first open the lakeFS UI, you will be asked to create an initial admin user. | open http://&lt;lakefs-host&gt;/ in your browser. If you haven’t set up a load balancer, this will likely be http://&lt;instance ip address&gt;:8000/ | On first use, you’ll be redirected to the setup page: . | Follow the steps to create an initial administrator user. Save the credentials you’ve received somewhere safe, you won’t be able to see them again! . | Follow the link and go to the login screen. Use the credentials from the previous step to log in. | . ",
    "url": "/v0.98/deploy/onprem.html#create-the-admin-user",
    
    "relUrl": "/deploy/onprem.html#create-the-admin-user"
  },"268": {
    "doc": "On-Premises Deployment of lakeFS",
    "title": "Create your first repository",
    "content": ". | Use the credentials from the previous step to log in | Click Create Repository and choose Blank Repository. | Under Storage Namespace, enter a path to your desired location on the object store. This is where data written to this repository will be stored. | Click Create Repository | You should now have a configured repository, ready to use! . | . Congratulations! Your environment is now ready 🤩 . ",
    "url": "/v0.98/deploy/onprem.html#create-your-first-repository",
    
    "relUrl": "/deploy/onprem.html#create-your-first-repository"
  },"269": {
    "doc": "On-Premises Deployment of lakeFS",
    "title": "On-Premises Deployment of lakeFS",
    "content": " ",
    "url": "/v0.98/deploy/onprem.html",
    
    "relUrl": "/deploy/onprem.html"
  },"270": {
    "doc": "Overview",
    "title": "lakeFS Hooks",
    "content": " ",
    "url": "/v0.98/hooks/overview.html#lakefs-hooks",
    
    "relUrl": "/hooks/overview.html#lakefs-hooks"
  },"271": {
    "doc": "Overview",
    "title": "Table of contents",
    "content": ". | Example use cases | Hook types | Supported Events | Terminology | Uploading Action files | Runs API &amp; CLI | . Like other version control systems, lakeFS allows you to configure Actions to trigger when predefined events occur. ",
    "url": "/v0.98/hooks/overview.html#table-of-contents",
    
    "relUrl": "/hooks/overview.html#table-of-contents"
  },"272": {
    "doc": "Overview",
    "title": "Example use cases",
    "content": ". | Format Validator: A webhook that checks new files to ensure they are of a set of allowed data formats. | Schema Validator: A webhook that reads new Parquet and ORC files to ensure they don’t contain a block list of column names (or name prefixes). This is useful for avoiding accidental PII exposure. | Integration with external systems: Post-merge and post-commit hooks could be used to export metadata about the change to another system. A common example is exporting symlink.txt files that allow e.g. AWS Athena to read data from lakeFS. | Notifying downstream consumers: Running a post-merge hook to trigger an Airflow DAG or to send a Webhook to an API, notifying it of the change that happened | . For more examples and configuration samples, check out the examples on the lakeFS repository. ",
    "url": "/v0.98/hooks/overview.html#example-use-cases",
    
    "relUrl": "/hooks/overview.html#example-use-cases"
  },"273": {
    "doc": "Overview",
    "title": "Hook types",
    "content": "Currently, there are two types of Hooks that are supported by lakeFS: Webhook and Airflow. Experimental support for Lua was introduced in lakeFS 0.86.0. ",
    "url": "/v0.98/hooks/overview.html#hook-types",
    
    "relUrl": "/hooks/overview.html#hook-types"
  },"274": {
    "doc": "Overview",
    "title": "Supported Events",
    "content": "| Event | Description | . | pre-commit | Runs when the commit occurs, before the commit is finalized | . | post-commit | Runs after the commit is finalized | . | pre-merge | Runs on the source branch when the merge occurs, before the merge is finalized | . | post-merge | Runs on the merge result, after the merge is finalized | . | pre-create-branch | Runs on the source branch prior to creating a new branch | . | post-create-branch | Runs on the new branch after the branch was created | . | pre-delete-branch | Runs prior to deleting a branch | . | post-delete-branch | Runs after the branch was deleted | . | pre-create-tag | Runs prior to creating a new tag | . | post-create-tag | Runs after the tag was created | . | pre-delete-tag | Runs prior to deleting a tag | . | post-delete-tag | Runs after the tag was deleted | . lakeFS Actions are handled per repository and cannot be shared between repositories. A failure of any Hook under any Action of a pre-* event will result in aborting the lakeFS operation that is taking place. Hook failures under any Action of a post-* event will not revert the operation. Hooks are managed by Action files that are written to a prefix in the lakeFS repository. This allows configuration-as-code inside lakeFS, where Action files are declarative and written in YAML. ",
    "url": "/v0.98/hooks/overview.html#supported-events",
    
    "relUrl": "/hooks/overview.html#supported-events"
  },"275": {
    "doc": "Overview",
    "title": "Terminology",
    "content": "Action . An Action is a list of Hooks with the same trigger configuration, i.e. an event will trigger all Hooks under an Action or none at all. The Hooks under an Action are ordered and so is their execution. A Hook will only be executed if all the previous Hooks that were triggered with it had passed. Hook . A Hook is the basic building block of an Action. The failure of a single Hook will stop the execution of the containing Action and fail the Run. Action file . Schema of the Action file: . | Property | Description | Data Type | Required | Default Value | . | name | Identify the Action file | String | false | If missing, filename is used instead | . | on | List of events that will trigger the hooks | List | true |   | . | on.branches | Glob pattern list of branches that triggers the hooks | List | false | Not applicable to Tag events. If empty, Action runs on all branches | . | hooks | List of hooks to be executed | List | true |   | . | hook.id | ID of the hook, must be unique within the Action | String | true |   | . | hook.type | Type of the hook (types) | String | true |   | . | hook.properties | Hook’s specific configuration | Dictionary | true |   | . Example: . name: Good files check description: set of checks to verify that branch is good on: pre-commit: pre-merge: branches: - main hooks: - id: no_temp type: webhook description: checking no temporary files found properties: url: \"https://your.domain.io/webhook?notmp=true?t=1za2PbkZK1bd4prMuTDr6BeEQwWYcX2R\" - id: no_freeze type: webhook description: check production is not in dev freeze properties: url: \"https://your.domain.io/webhook?nofreeze=true?t=1za2PbkZK1bd4prMuTDr6BeEQwWYcX2R\" . Note: lakeFS will validate action files only when an Event has occurred. Use lakectl actions validate &lt;path&gt; to validate your action files locally. Run . A Run is an instantiation of the repository’s Action files when the triggering event occurs. For example, if your repository contains a pre-commit hook, every commit would generate a Run for that specific commit. lakeFS will fetch, parse and filter the repository Action files and start to execute the Hooks under each Action. All executed Hooks (each with hook_run_id) exist in the context of that Run (run_id). ",
    "url": "/v0.98/hooks/overview.html#terminology",
    
    "relUrl": "/hooks/overview.html#terminology"
  },"276": {
    "doc": "Overview",
    "title": "Uploading Action files",
    "content": "Action files should be uploaded with the prefix _lakefs_actions/ to the lakeFS repository. When an actionable event (see Supported Events above) takes place, lakeFS will read all files with prefix _lakefs_actions/ in the repository branch where the action occurred. A failure to parse an Action file will result with a failing Run. For example, lakeFS will search and execute all the matching Action files with the prefix lakefs://example-repo/feature-1/_lakefs_actions/ on: . | Commit to feature-1 branch on example-repo repository. | Merge to main branch from feature-1 branch on repo1 repository. | . ",
    "url": "/v0.98/hooks/overview.html#uploading-action-files",
    
    "relUrl": "/hooks/overview.html#uploading-action-files"
  },"277": {
    "doc": "Overview",
    "title": "Runs API &amp; CLI",
    "content": "The lakeFS API and lakectl expose the results of executions per repository, branch, commit, and specific Action. The endpoint also allows to download the execution log of any executed Hook under each Run for observability. Result Files . The metadata section of lakeFS repository with each Run contains two types of files: . | _lakefs/actions/log/&lt;runID&gt;/&lt;hookRunID&gt;.log - Execution log of the specific Hook run. | _lakefs/actions/log/&lt;runID&gt;/run.manifest - Manifest with all Hooks execution for the run with their results and additional metadata. | . Note: Metadata section of a lakeFS repository is where lakeFS keeps its metadata, like commits and metaranges. Metadata files stored in the metadata section aren’t accessible like user stored files. ",
    "url": "/v0.98/hooks/overview.html#runs-api--cli",
    
    "relUrl": "/hooks/overview.html#runs-api--cli"
  },"278": {
    "doc": "Overview",
    "title": "Overview",
    "content": " ",
    "url": "/v0.98/hooks/overview.html",
    
    "relUrl": "/hooks/overview.html"
  },"279": {
    "doc": "Performance Best Practices",
    "title": "Performance Best Practices",
    "content": " ",
    "url": "/v0.98/understand/performance-best-practices.html",
    
    "relUrl": "/understand/performance-best-practices.html"
  },"280": {
    "doc": "Performance Best Practices",
    "title": "Table of contents",
    "content": ". | Overview | Avoid concurrent commits/merges | Perform meaningful commits | Use zero-copy import | Read data using the commit ID | Operate directly on the storage | Zero-copy | . ",
    "url": "/v0.98/understand/performance-best-practices.html#table-of-contents",
    
    "relUrl": "/understand/performance-best-practices.html#table-of-contents"
  },"281": {
    "doc": "Performance Best Practices",
    "title": "Overview",
    "content": "Use this guide to achieve the best performance with lakeFS. ",
    "url": "/v0.98/understand/performance-best-practices.html#overview",
    
    "relUrl": "/understand/performance-best-practices.html#overview"
  },"282": {
    "doc": "Performance Best Practices",
    "title": "Avoid concurrent commits/merges",
    "content": "Just like in Git, branch history is composed by commits and is linear by nature. Concurrent commits/merges on the same branch result in a race. The first operation will finish successfully while the rest will retry. ",
    "url": "/v0.98/understand/performance-best-practices.html#avoid-concurrent-commitsmerges",
    
    "relUrl": "/understand/performance-best-practices.html#avoid-concurrent-commitsmerges"
  },"283": {
    "doc": "Performance Best Practices",
    "title": "Perform meaningful commits",
    "content": "It’s a good idea to perform commits that are meaningful in the senese that they represent a logical point in your data’s lifecycle. While lakeFS supports arbirartily large commits, avoiding commits with a huge number of objects will result in a more comprehensible commit history. ",
    "url": "/v0.98/understand/performance-best-practices.html#perform-meaningful-commits",
    
    "relUrl": "/understand/performance-best-practices.html#perform-meaningful-commits"
  },"284": {
    "doc": "Performance Best Practices",
    "title": "Use zero-copy import",
    "content": "To import object into lakeFS, either a single time or regularly, lakeFS offers a zero-copy import feature. Use this feature to import a large number of objects to lakeFS, instead of simply copying them into your repository. This feature will create a reference to the existing objects on your bucket and avoids the copy. ",
    "url": "/v0.98/understand/performance-best-practices.html#use-zero-copy-import",
    
    "relUrl": "/understand/performance-best-practices.html#use-zero-copy-import"
  },"285": {
    "doc": "Performance Best Practices",
    "title": "Read data using the commit ID",
    "content": "In cases where you are only interested in reading committed data: . | Use a commit ID (or a tag ID) in your path (e.g: lakefs://repo/a1b2c3). | Add @ before the path lakefs://repo/main@/path. | . When accessing data using the branch name (e.g. lakefs://repo/main/path) lakeFS will also try to fetch uncommitted data, which may result in reduced performance. For more information, see how uncommitted data is managed in lakeFS . ",
    "url": "/v0.98/understand/performance-best-practices.html#read-data-using-the-commit-id",
    
    "relUrl": "/understand/performance-best-practices.html#read-data-using-the-commit-id"
  },"286": {
    "doc": "Performance Best Practices",
    "title": "Operate directly on the storage",
    "content": "Sometimes, storage operations can become a bottleneck. For example, when your data pipelines upload many big objects. In such cases, it can be beneficial to perform only versioning operations on lakeFS, while performing storage reads/writes directly on the object store. lakeFS offers multiple ways to do that: . | The lakectl upload --direct command (or download). | The lakeFS Hadoop Filesystem. | The staging API which can be used to add lakeFS references to objects after having written them to the storage. | . Accessing the object store directly is a faster way to interact with your data. ",
    "url": "/v0.98/understand/performance-best-practices.html#operate-directly-on-the-storage",
    
    "relUrl": "/understand/performance-best-practices.html#operate-directly-on-the-storage"
  },"287": {
    "doc": "Performance Best Practices",
    "title": "Zero-copy",
    "content": "lakeFS provides a zero-copy mechanism to data. Instead of copying the data, we can check out to a new branch. Creating a new branch will take constant time as the new branch points to the same data as its parent. It will also lower the storage cost. ",
    "url": "/v0.98/understand/performance-best-practices.html#zero-copy",
    
    "relUrl": "/understand/performance-best-practices.html#zero-copy"
  },"288": {
    "doc": "Presigned URL",
    "title": "Presigned URL",
    "content": " ",
    "url": "/v0.98/reference/presigned-url.html",
    
    "relUrl": "/reference/presigned-url.html"
  },"289": {
    "doc": "Presigned URL",
    "title": "Table of contents",
    "content": ". | Using presigned URLs in the UI . | Example: AWS S3 | Example: Google Storage | Example: Azure blob storage | . | . With lakeFS, you can access data directly from the storage and not through lakeFS using a presined URL. Based on the user’s access to an object in the object store, the presigned URL will get read or write access. The presign support is enabled for block adapter that supports it (S3, GCP, Azure), and can be disabled by the configuration (blockstore.blockstore-name.disable_pre_signed). Note that the UI support is disabled by default. ",
    "url": "/v0.98/reference/presigned-url.html#table-of-contents",
    
    "relUrl": "/reference/presigned-url.html#table-of-contents"
  },"290": {
    "doc": "Presigned URL",
    "title": "Using presigned URLs in the UI",
    "content": "For using presigned URLs in the UI: . | Enable the presigned URL support UI in the lakeFS configuration (blockstore.blockstore-name.disable_pre_signed_ui). | Add CORS (Cross-Origin Resource Sharing) permissions to the bucket for the UI to fetch objects using a presigned URL (instead of through lakeFS). | The disable_pre_signed needs to be enabled to enable it in the UI. | . ⚠️ Note Currently DuckDB fetching data from lakeFS does not support fetching data using presigned URL. Example: AWS S3 . [ { \"AllowedHeaders\": [ \"*\" ], \"AllowedMethods\": [ \"GET\", \"PUT\" ], \"AllowedOrigins\": [ \"lakefs.endpoint\" ], \"ExposeHeaders\": [ \"ETag\" ] } ] . Example: Google Storage . [ { \"origin\": [\"lakefs.endpoint\"], \"responseHeader\": [\"ETag\"], \"method\": [\"PUT\", \"GET\"], \"maxAgeSeconds\": 3600 } ] . Example: Azure blob storage . &lt;Cors&gt; &lt;CorsRule&gt; &lt;AllowedOrigins&gt;lakefs.endpoint&lt;/AllowedOrigins&gt; &lt;AllowedMethods&gt;PUT,GET&lt;/AllowedMethods&gt; &lt;AllowedHeaders&gt;*&lt;/AllowedHeaders&gt; &lt;ExposedHeaders&gt;ETag&lt;/ExposedHeaders&gt; &lt;MaxAgeInSeconds&gt;3600&lt;/MaxAgeInSeconds&gt; &lt;/CorsRule&gt; &lt;/Cors&gt; . ",
    "url": "/v0.98/reference/presigned-url.html#using-presigned-urls-in-the-ui",
    
    "relUrl": "/reference/presigned-url.html#using-presigned-urls-in-the-ui"
  },"291": {
    "doc": "Presto/Trino",
    "title": "Using lakeFS with Presto/Trino",
    "content": "Presto and Trino are a distributed SQL query engines designed to query large data sets distributed over one or more heterogeneous data sources. ",
    "url": "/v0.98/integrations/presto_trino.html#using-lakefs-with-prestotrino",
    
    "relUrl": "/integrations/presto_trino.html#using-lakefs-with-prestotrino"
  },"292": {
    "doc": "Presto/Trino",
    "title": "Table of contents",
    "content": ". | Using lakeFS with Presto/Trino . | | Configuration . | Configure the Hive connector | Configure Hive | . | Examples . | Example with schema | Example with External table | Example of copying a table with metastore tools: | . | . | . Querying data in lakeFS from Presto/Trino is similar to querying data in S3 from Presto/Trino. It is done using the Presto Hive connector or Trino Hive connector. Note In the following examples, we set AWS credentials at runtime for clarity. In production, these properties should be set using one of Hadoop’s standard ways of Authenticating with S3. ",
    "url": "/v0.98/integrations/presto_trino.html#table-of-contents",
    
    "relUrl": "/integrations/presto_trino.html#table-of-contents"
  },"293": {
    "doc": "Presto/Trino",
    "title": "Configuration",
    "content": "Configure the Hive connector . Create /etc/catalog/hive.properties with the following contents to mount the hive-hadoop2 connector as the hive catalog, replacing example.net:9083 with the correct host and port for your Hive Metastore Thrift service: . connector.name=hive-hadoop2 hive.metastore.uri=thrift://example.net:9083 . Add the lakeFS configurations to /etc/catalog/hive.properties in the corresponding S3 configuration properties: . hive.s3.aws-access-key=AKIAIOSFODNN7EXAMPLE hive.s3.aws-secret-key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY hive.s3.endpoint=https://lakefs.example.com hive.s3.path-style-access=true . Configure Hive . Presto/Trino uses Hive Metastore Service (HMS) or a compatible implementation of the Hive Metastore such as AWS Glue Data Catalog to write data to S3. In case you are using Hive Metastore, you will need to configure Hive as well. In file hive-site.xml add to the configuration: . &lt;configuration&gt; ... &lt;property&gt; &lt;name&gt;fs.s3a.access.key&lt;/name&gt; &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.secret.key&lt;/name&gt; &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;https://lakefs.example.com&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . ",
    "url": "/v0.98/integrations/presto_trino.html#configuration",
    
    "relUrl": "/integrations/presto_trino.html#configuration"
  },"294": {
    "doc": "Presto/Trino",
    "title": "Examples",
    "content": "Here are some examples based on examples from the Presto Hive connector examples and Trino Hive connector examples . Example with schema . Create a new schema named main that will store tables in a lakeFS repository named example branch: master: . CREATE SCHEMA main WITH (location = 's3a://example/main') . Create a new Hive table named page_views in the web schema stored using the ORC file format, partitioned by date and country, and bucketed by user into 50 buckets (note that Hive requires the partition columns to be the last columns in the table): . CREATE TABLE main.page_views ( view_time timestamp, user_id bigint, page_url varchar, ds date, country varchar ) WITH ( format = 'ORC', partitioned_by = ARRAY['ds', 'country'], bucketed_by = ARRAY['user_id'], bucket_count = 50 ) . Example with External table . Create an external Hive table named request_logs that points at existing data in lakeFS: . CREATE TABLE main.request_logs ( request_time timestamp, url varchar, ip varchar, user_agent varchar ) WITH ( format = 'TEXTFILE', external_location = 's3a://example/main/data/logs/' ) . Example of copying a table with metastore tools: . Copy the created table page_views on schema main to schema example_branch with location s3a://example/example_branch/page_views/ . lakectl metastore copy --from-schema main --from-table page_views --to-branch example_branch . ",
    "url": "/v0.98/integrations/presto_trino.html#examples",
    
    "relUrl": "/integrations/presto_trino.html#examples"
  },"295": {
    "doc": "Presto/Trino",
    "title": "Presto/Trino",
    "content": " ",
    "url": "/v0.98/integrations/presto_trino.html",
    
    "relUrl": "/integrations/presto_trino.html"
  },"296": {
    "doc": "Private Link",
    "title": "Private Link",
    "content": "lakeFS Cloud . Private Link enables lakeFS Cloud to interact with your infrastructure using private networking. ",
    "url": "/v0.98/cloud/private-link.html",
    
    "relUrl": "/cloud/private-link.html"
  },"297": {
    "doc": "Private Link",
    "title": "Table of contents",
    "content": ". | Private Link . | Supported Vendors | Access Methods | Setting up Private Link . | Front-End Access | Back-End Access | . | . | . ",
    "url": "/v0.98/cloud/private-link.html#table-of-contents",
    
    "relUrl": "/cloud/private-link.html#table-of-contents"
  },"298": {
    "doc": "Private Link",
    "title": "Supported Vendors",
    "content": "At the moment, we support Private-Link with AWS. If you are looking for Private Link for Azure or GCP please contact us. ",
    "url": "/v0.98/cloud/private-link.html#supported-vendors",
    
    "relUrl": "/cloud/private-link.html#supported-vendors"
  },"299": {
    "doc": "Private Link",
    "title": "Access Methods",
    "content": "There are two types of Private Link implementation: . | Front-End Access refers to API and UI access. Use this option if you’d like your lakeFS application to be exposed only to your infrastructure and not to the whole internet. | Back-End Access refers to the network communication between the lakeFS clusters we host, and your infrastructure. Use this option if you’d like lakeFS to communicate with your servers privately and not over the internet. | . The two types of access are not mutually exclusive nor are they dependent on each other. ",
    "url": "/v0.98/cloud/private-link.html#access-methods",
    
    "relUrl": "/cloud/private-link.html#access-methods"
  },"300": {
    "doc": "Private Link",
    "title": "Setting up Private Link",
    "content": "Front-End Access . Prerequisites: . | Administrator access to your AWS account | In order for us to communicate with your account privately, we’ll need to create a service endpoint on our end first. | . Steps: . | Login to your AWS account | Go to AWS VPC Service | Filter the relevant VPC &amp; Navigate to Endpoints | Click Create endpoint | Fill in the following: . | Name: lakefs-cloud | Service category: Other endpoint services | Service name: input from Treeverse team (see prerequisites) | Click Verify service | Pick the VPC you’d like to expose this service to. | Click Create endpoint | . | . Now you can access your infrastructure privately using the endpoint DNS name. If you would like to change the DNS name to a friendly one please contact support@treeverse.io. Back-End Access . Prerequisites: . | Administrator access to your AWS account | . Steps: . | Login to your AWS account | Go to AWS VPC Service | Filter the relevant VPC &amp; Navigate to Endpoints | Click endpoint service | Fill in the following: . | Name: lakefs-cloud | Load Balancer Type: Network | Available load balancers: pick the load balancer you’d like lakefs-cloud to send events to. | Click Create | . | Pick the newly created Endpoint Service from within the Endpoint Services page. | Navigate to the Allow principals tab. | Click Allow principals | Fill in the following ARN: arn:aws:iam::924819537486:root | Click Allow principals | . That’s it on your end! Now, we’ll need the service name you’ve just created in order to assosicate it with our infrastructure, once we do, we’ll be ready to use the back-end access privately. ",
    "url": "/v0.98/cloud/private-link.html#setting-up-private-link",
    
    "relUrl": "/cloud/private-link.html#setting-up-private-link"
  },"301": {
    "doc": "In Production",
    "title": "In Production",
    "content": "Errors with data in production inevitably occur. When they do, they best thing we can do is remove the erroneous data, understand why the issue happened, and deploy changes that prevent it from occurring again. Example 1: RollBack! - Data ingested from a Kafka stream . If you introduce a new code version to production and discover it has a critical bug, you can simply roll back to the previous version. But you also need to roll back the results of running it. Similar to Git, lakeFS allows you to revert your commits in case they introduced low-quality data. Revert in lakeFS is an atomic action that prevents the data consumers from receiving low quality data until the issue is resolved. As previously mentioned, with lakeFS the recommended branching schema is to ingest data to a dedicated branch. When streaming data, we can decide to merge the incoming data to main at a given time interval or checkpoint, depending on how we chose to write it from Kafka. You can run quality tests for each merge (as discussed in the During Deployment section). Alas, tests are not perfect and we might still introduce low quality data to our main branch at some point. In such a case, we can revert the bad commits from main to the last known high quality commit. This will record new commits reversing the effect of the bad commits. Reverting commits using the CLI . lakectl branch revert lakefs://example-repo/main 20c30c96 ababea32 . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. Example 2: Troubleshoot - Reproduce a bug in production . You upgraded spark and deployed changes in production. A few days or weeks later, you identify a data quality issue, a performance degradation, or an increase to your infra costs. Something that requires investigation and fixing (aka, a bug). lakeFS allows you to open a branch of your lake from the specific merge/commit that introduced the changes to production. Using the metadata saved on the merge/commit you can reproduce all aspects of the environment, then reproduce the issue on the branch and debug it. Meanwhile, you can revert the main to a previous point in time, or keep it as is, depending on the use case . Reading from a historic version (a previous commit) using Spark . // represents the data as existed at commit \"11eef40b\": spark.read.parquet(\"s3://example-repo/11eef40b/events/by-date\") . Example 3: Cross collection consistency . We often need consistency between different data collections. A few examples may be: . | To join different collections in order to create a unified view of an account, a user or another entity we measure. | To introduce the same data in different formats | To introduce the same data with a different leading index or sorting due to performance considerations | . lakeFS will help ensure you introduce only consistent data to your consumers by exposing the new collections and their join in one atomic action to main. Once you consumed the collections on a different branch, and only when both are synchronized, we calculated the join and merged to main. In this example you can see two data sets (Sales data and Marketing data) consumed each to its own independent branch, and after the write of both data sets is completed, they are merged to a different branch (leads branch) where the join ETL runs and creates a joined collection by account. The joined table is then merged to main. The same logic can apply if the data is ingested in streaming, using standard formats, or formats that allow upsert/delete such as Apache Hudi, Delta Lake or Iceberg. ",
    "url": "/v0.98/understand/data_lifecycle_management/production.html",
    
    "relUrl": "/understand/data_lifecycle_management/production.html"
  },"302": {
    "doc": "In Production",
    "title": "Case Study: Windward",
    "content": "See how Windward is using lakeFS’ isolation and atomic commits to achieve consistency on top of S3. ",
    "url": "/v0.98/understand/data_lifecycle_management/production.html#case-study-windward",
    
    "relUrl": "/understand/data_lifecycle_management/production.html#case-study-windward"
  },"303": {
    "doc": "Protect Branches",
    "title": "Branch Protection Rules",
    "content": "Define branch protection rules to prevent direct changes and commits to specific branches. Only merges are allowed into protected branches. Together with the power of pre-merge hooks, you can run validations on your data before it reaches your important branches and is exposed to consumers. You can create rules for a specific branch or any branch that matches a name pattern you specify with glob syntax (supporting ? and * wildcards). ",
    "url": "/v0.98/howto/protect-branches.html#branch-protection-rules",
    
    "relUrl": "/howto/protect-branches.html#branch-protection-rules"
  },"304": {
    "doc": "Protect Branches",
    "title": "How it works",
    "content": "When at least one protection rule applies to a branch, the branch is protected. The following operations will fail on protected branches: . | Object write operations: upload and delete objects. | Branch operations: commit and reset uncommitted changes. | . To operate on a protected branch, merge commits from other branches into it. Use pre-merge hooks to validate the changes before they are merged. Reverting a previous commit using lakectl branch revert is allowed on a protected branch. ",
    "url": "/v0.98/howto/protect-branches.html#how-it-works",
    
    "relUrl": "/howto/protect-branches.html#how-it-works"
  },"305": {
    "doc": "Protect Branches",
    "title": "Managing branch protection rules",
    "content": "This section explains how to use the lakeFS UI to manage rules. You can also use the command line. Reaching the branch protection rules page . | On lakeFS, navigate to the main page of the repository. | Click on the Settings tab. | In the left menu, click Branches. | . Adding a rule . To add a new rule, click the Add button. In the dialog, enter the branch name pattern and then click Create. Deleting a rule . To delete a rule, click the Delete button next to it. ",
    "url": "/v0.98/howto/protect-branches.html#managing-branch-protection-rules",
    
    "relUrl": "/howto/protect-branches.html#managing-branch-protection-rules"
  },"306": {
    "doc": "Protect Branches",
    "title": "Protect Branches",
    "content": " ",
    "url": "/v0.98/howto/protect-branches.html",
    
    "relUrl": "/howto/protect-branches.html"
  },"307": {
    "doc": "Python",
    "title": "Table of contents",
    "content": ". | Boto vs. lakeFS SDK | Using the lakeFS SDK . | Installing | Initializing | Usage Examples | . | Using Boto . | Initializing | Usage Examples | . | . ",
    "url": "/v0.98/integrations/python.html#table-of-contents",
    
    "relUrl": "/integrations/python.html#table-of-contents"
  },"308": {
    "doc": "Python",
    "title": "Boto vs. lakeFS SDK",
    "content": "To interact with lakeFS from Python: . | Use Boto to perform object operations through the lakeFS S3 gateway. | Use the lakeFS SDK to perform versioning and other lakeFS-specific operations. | . ",
    "url": "/v0.98/integrations/python.html#boto-vs-lakefs-sdk",
    
    "relUrl": "/integrations/python.html#boto-vs-lakefs-sdk"
  },"309": {
    "doc": "Python",
    "title": "Using the lakeFS SDK",
    "content": "Installing . Install the Python client using pip: . pip install 'lakefs_client==&lt;lakeFS version&gt;' . Initializing . Here’s how to instantiate a client: . import lakefs_client from lakefs_client import models from lakefs_client.client import LakeFSClient # lakeFS credentials and endpoint configuration = lakefs_client.Configuration() configuration.username = 'AKIAIOSFODNN7EXAMPLE' configuration.password = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' configuration.host = 'http://localhost:8000' client = LakeFSClient(configuration) . Usage Examples . Now that you have a client object, you can use it to interact with the API. Creating a repository . repo = models.RepositoryCreation(name='example-repo', storage_namespace='s3://storage-bucket/repos/example-repo', default_branch='main') client.repositories.create_repository(repo) # output: # {'creation_date': 1617532175, # 'default_branch': 'main', # 'id': 'example-repo', # 'storage_namespace': 's3://storage-bucket/repos/example-repo'} . Creating a branch, uploading files, committing changes . List the repository branches: . client.branches.list_branches('example-repo') # output: # [{'commit_id': 'cdd673a4c5f42d33acdf3505ecce08e4d839775485990d231507f586ebe97656', 'id': 'main'}] . Create a new branch: . client.branches.create_branch(repository='example-repo', branch_creation=models.BranchCreation(name='experiment-aggregations1', source='main')) # output: # 'cdd673a4c5f42d33acdf3505ecce08e4d839775485990d231507f586ebe97656' . List again to see your newly created branch: . client.branches.list_branches('example-repo').results # output: # [{'commit_id': 'cdd673a4c5f42d33acdf3505ecce08e4d839775485990d231507f586ebe97656', 'id': 'experiment-aggregations1'}, {'commit_id': 'cdd673a4c5f42d33acdf3505ecce08e4d839775485990d231507f586ebe97656', 'id': 'main'}] . Great. Now, let’s upload a file into your new branch: . with open('file.csv', 'rb') as f: client.objects.upload_object(repository='example-repo', branch='experiment-aggregations1', path='path/to/file.csv', content=f) # output: # {'checksum': '0d3b39380e2500a0f60fb3c09796fdba', # 'mtime': 1617534834, # 'path': 'path/to/file.csv', # 'path_type': 'object', # 'physical_address': 'local://example-repo/1865650a296c42e28183ad08e9b068a3', # 'size_bytes': 18} . Diffing a single branch will show all the uncommitted changes on that branch: . client.branches.diff_branch(repository='example-repo', branch='experiment-aggregations1').results # output: # [{'path': 'path/to/file.csv', 'path_type': 'object', 'type': 'added'}] . As expected, our change appears here. Let’s commit it and attach some arbitrary metadata: . client.commits.commit( repository='example-repo', branch='experiment-aggregations1', commit_creation=models.CommitCreation(message='Added a CSV file!', metadata={'using': 'python_api'})) # output: # {'committer': 'barak', # 'creation_date': 1617535120, # 'id': 'e80899a5709509c2daf797c69a6118be14733099f5928c14d6b65c9ac2ac841b', # 'message': 'Added a CSV file!', # 'meta_range_id': '', # 'metadata': {'using': 'python_api'}, # 'parents': ['cdd673a4c5f42d33acdf3505ecce08e4d839775485990d231507f586ebe97656']} . Diffing again, this time there should be no uncommitted files: . client.branches.diff_branch(repository='example-repo', branch='experiment-aggregations1').results # output: # [] . Merging changes from a branch into main . Let’s diff between your branch and the main branch: . client.refs.diff_refs(repository='example-repo', left_ref='main', right_ref='experiment-aggregations1').results # output: # [{'path': 'path/to/file.csv', 'path_type': 'object', 'type': 'added'}] . Looks like you have a change. Let’s merge it: . client.refs.merge_into_branch(repository='example-repo', source_ref='experiment-aggregations1', destination_branch='main') # output: # {'reference': 'd0414a3311a8c1cef1ef355d6aca40db72abe545e216648fe853e25db788fa2e', # 'summary': {'added': 1, 'changed': 0, 'conflict': 0, 'removed': 0}} . Let’s diff again - there should be no changes as all changes are on our main branch already: . client.refs.diff_refs(repository='example-repo', left_ref='main', right_ref='experiment-aggregations1').results # output: # [] . Python Client documentation . For the documentation of lakeFS’s Python package, see https://pydocs.lakefs.io . Full API reference . For a full reference of the lakeFS API, see lakeFS API . ",
    "url": "/v0.98/integrations/python.html#using-the-lakefs-sdk",
    
    "relUrl": "/integrations/python.html#using-the-lakefs-sdk"
  },"310": {
    "doc": "Python",
    "title": "Using Boto",
    "content": "💡 To use Boto with lakeFS alongside S3, check out Boto S3 Router. It will route requests to either S3 or lakeFS according to the provided bucket name. lakeFS exposes an S3-compatible API, so you can use Boto to interact with your objects on lakeFS. Initializing . Create a Boto3 S3 client with your lakeFS endpoint and key-pair: . import boto3 s3 = boto3.client('s3', endpoint_url='https://lakefs.example.com', aws_access_key_id='AKIAIOSFODNN7EXAMPLE', aws_secret_access_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY') . The client is now configured to operate on your lakeFS installation. Usage Examples . Put an object into lakeFS . Use a branch name and a path to put an object in lakeFS: . with open('/local/path/to/file_0', 'rb') as f: s3.put_object(Body=f, Bucket='example-repo', Key='main/example-file.parquet') . You can now commit this change using the lakeFS UI or CLI. List objects . List the branch objects starting with a prefix: . list_resp = s3.list_objects_v2(Bucket='example-repo', Prefix='main/example-prefix') for obj in list_resp['Contents']: print(obj['Key']) . Or, use a lakeFS commit ID to list objects for a specific commit: . list_resp = s3.list_objects_v2(Bucket='example-repo', Prefix='c7a632d74f/example-prefix') for obj in list_resp['Contents']: print(obj['Key']) . Get object metadata . Get object metadata using branch and path: . s3.head_object(Bucket='example-repo', Key='main/example-file.parquet') # output: # {'ResponseMetadata': {'RequestId': '72A9EBD1210E90FA', # 'HostId': '', # 'HTTPStatusCode': 200, # 'HTTPHeaders': {'accept-ranges': 'bytes', # 'content-length': '1024', # 'etag': '\"2398bc5880e535c61f7624ad6f138d62\"', # 'last-modified': 'Sun, 24 May 2020 10:42:24 GMT', # 'x-amz-request-id': '72A9EBD1210E90FA', # 'date': 'Sun, 24 May 2020 10:45:42 GMT'}, # 'RetryAttempts': 0}, # 'AcceptRanges': 'bytes', # 'LastModified': datetime.datetime(2020, 5, 24, 10, 42, 24, tzinfo=tzutc()), # 'ContentLength': 1024, # 'ETag': '\"2398bc5880e535c61f7624ad6f138d62\"', # 'Metadata': {}} . ",
    "url": "/v0.98/integrations/python.html#using-boto",
    
    "relUrl": "/integrations/python.html#using-boto"
  },"311": {
    "doc": "Python",
    "title": "Python",
    "content": " ",
    "url": "/v0.98/integrations/python.html",
    
    "relUrl": "/integrations/python.html"
  },"312": {
    "doc": "2️⃣ Query the data",
    "title": "Let’s Query Something 👀",
    "content": "The lakeFS server has been loaded with a sample parquet datafile. Fittingly enough for a piece of software to help users of data lakes, the lakes.parquet file holds data about lakes around the world. You’ll notice that the branch is set to main. This is conceptually the same as your main branch in git against which you develop software code. Let’s have a look at the data, ahead of making some changes to it on a branch in the following steps. Click on lakes.parquet and notice that the built-it DuckDB runs a query to show a preview of the file’s contents. Now we’ll run our own query on it to look at the top five countries represented in the data. Copy and paste the following SQL statement into the DuckDB query panel and click on Execute. SELECT country, COUNT(*) FROM READ_PARQUET(LAKEFS_OBJECT('quickstart', 'main', 'lakes.parquet')) GROUP BY country ORDER BY COUNT(*) DESC LIMIT 5; . Next we’re going to make some changes to the data—but on a development branch so that the data in the main branch remains untouched. ",
    "url": "/v0.98/quickstart/query.html#lets-query-something-",
    
    "relUrl": "/quickstart/query.html#lets-query-something-"
  },"313": {
    "doc": "2️⃣ Query the data",
    "title": "2️⃣ Query the data",
    "content": " ",
    "url": "/v0.98/quickstart/query.html",
    
    "relUrl": "/quickstart/query.html"
  },"314": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Role-Based Access Control (RBAC)",
    "content": "lakeFS Cloud . lakeFS Enterprise . RBAC is available on lakeFS Cloud and lakeFS Enterprise. If you’re using the open source version of lakeFS then the ACL-based authorization mechanism is an alternative to RBAC. ",
    "url": "/v0.98/reference/rbac.html",
    
    "relUrl": "/reference/rbac.html"
  },"315": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Table of contents",
    "content": ". | Role-Based Access Control (RBAC) . | RBAC Model | Authorization process | Policy Precedence | Resource naming - ARNs | Actions and Permissions | Preconfigured Policies . | FSFullAccess | FSReadAll | FSReadWriteAll | AuthFullAccess | AuthManageOwnCredentials | RepoManagementFullAccess | RepoManagementReadAll | . | Additional Policies | Preconfigured Groups | Pluggable Authentication and Authorization | . | . ",
    "url": "/v0.98/reference/rbac.html#table-of-contents",
    
    "relUrl": "/reference/rbac.html#table-of-contents"
  },"316": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "RBAC Model",
    "content": "Access to resources is managed very much like AWS IAM. There are five basic components to the system: . | Users - Representing entities that access and use the system. A user is given one or more Access Credentials for authentication. | Actions - Representing a logical action within the system - reading a file, creating a repository, etc. | Resources - A unique identifier representing a specific resource in the system - a repository, an object, a user, etc. | Policies - Representing a set of Actions, a Resource and an effect: whether or not these actions are allowed or denied for the given resource(s). | Groups - A named collection of users. Users can belong to multiple groups. | . Controlling access is done by attaching Policies, either directly to Users, or to Groups they belong to. ",
    "url": "/v0.98/reference/rbac.html#rbac-model",
    
    "relUrl": "/reference/rbac.html#rbac-model"
  },"317": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Authorization process",
    "content": "Every action in the system - be it an API request, UI interaction, S3 Gateway call, or CLI command - requires a set of actions to be allowed for one or more resources. When a user makes a request to perform that action, the following process takes place: . | Authentication - the credentials passed in the request are evaluated and the user’s identity is extracted. | Action permission resolution - lakeFS then calculates the set of allowed actions and resources that this request requires. | Effective policy resolution - the user’s policies (either attached directly or through group memberships) are calculated. | Policy/Permission evaluation - lakeFS will compare the given user policies with the request actions and determine whether or not the request is allowed to continue. | . ",
    "url": "/v0.98/reference/rbac.html#authorization-process",
    
    "relUrl": "/reference/rbac.html#authorization-process"
  },"318": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Policy Precedence",
    "content": "Each policy attached to a user or a group has an Effect - either allow or deny. During evaluation of a request, deny would take precedence over any other allow policy. This helps us compose policies together. For example, we could attach a very permissive policy to a user and use deny rules to then selectively restrict what that user can do. ",
    "url": "/v0.98/reference/rbac.html#policy-precedence",
    
    "relUrl": "/reference/rbac.html#policy-precedence"
  },"319": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Resource naming - ARNs",
    "content": "lakeFS uses ARN identifier - very similar in structure to those used by AWS. The resource segment of the ARN supports wildcards: use * to match 0 or more characters, or ? to match exactly one character. Additionally, the current user’s ID is interpolated in runtime into the ARN using the ${user} placeholder. Here are a few examples of valid ARNs within lakeFS: . arn:lakefs:auth:::user/jane.doe arn:lakefs:auth:::user/* arn:lakefs:fs:::repository/myrepo/* arn:lakefs:fs:::repository/myrepo/object/foo/bar/baz arn:lakefs:fs:::repository/myrepo/object/* arn:lakefs:fs:::repository/* arn:lakefs:fs:::* . this allows us to create fine-grained policies affecting only a specific subset of resources. See below for a full reference of ARNs and actions. ",
    "url": "/v0.98/reference/rbac.html#resource-naming---arns",
    
    "relUrl": "/reference/rbac.html#resource-naming---arns"
  },"320": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Actions and Permissions",
    "content": "For the full list of actions and their required permissions see the following table: . | Action name | required action | Resource | API endpoint | S3 gateway operation | . | List Repositories | fs:ListRepositories | * | GET /repositories | ListBuckets | . | Get Repository | fs:ReadRepository | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId} | HeadBucket | . | Get Commit | fs:ReadCommit | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/commits/{commitId} | - | . | Create Commit | fs:CreateCommit | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | POST /repositories/{repositoryId}/branches/{branchId}/commits | - | . | Get Commit log | fs:ReadBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | GET /repositories/{repositoryId}/branches/{branchId}/commits | - | . | Create Repository | fs:CreateRepository | arn:lakefs:fs:::repository/{repositoryId} | POST /repositories | - | . | Namespace Attach to Repository | fs:AttachStorageNamespace | arn:lakefs:fs:::namespace/{storageNamespace} | POST /repositories | - | . | Delete Repository | fs:DeleteRepository | arn:lakefs:fs:::repository/{repositoryId} | DELETE /repositories/{repositoryId} | - | . | List Branches | fs:ListBranches | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/branches | ListObjects/ListObjectsV2 (with delimiter = / and empty prefix) | . | Get Branch | fs:ReadBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | GET /repositories/{repositoryId}/branches/{branchId} | - | . | Create Branch | fs:CreateBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | POST /repositories/{repositoryId}/branches | - | . | Delete Branch | fs:DeleteBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | DELETE /repositories/{repositoryId}/branches/{branchId} | - | . | Merge branches | fs:CreateCommit | arn:lakefs:fs:::repository/{repositoryId}/branch/{destinationBranchId} | POST /repositories/{repositoryId}/refs/{sourceBranchId}/merge/{destinationBranchId} | - | . | Diff branch uncommitted changes | fs:ListObjects | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/branches/{branchId}/diff | - | . | Diff refs | fs:ListObjects | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/refs/{leftRef}/diff/{rightRef} | - | . | Stat object | fs:ReadObject | arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey} | GET /repositories/{repositoryId}/refs/{ref}/objects/stat | HeadObject | . | Get Object | fs:ReadObject | arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey} | GET /repositories/{repositoryId}/refs/{ref}/objects | GetObject | . | List Objects | fs:ListObjects | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/refs/{ref}/objects/ls | ListObjects, ListObjectsV2 (no delimiter, or “/” + non-empty prefix) | . | Upload Object | fs:WriteObject | arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey} | POST /repositories/{repositoryId}/branches/{branchId}/objects | PutObject, CreateMultipartUpload, UploadPart, CompleteMultipartUpload | . | Delete Object | fs:DeleteObject | arn:lakefs:fs:::repository/{repositoryId}/object/{objectKey} | DELETE /repositories/{repositoryId}/branches/{branchId}/objects | DeleteObject, DeleteObjects, AbortMultipartUpload | . | Revert Branch | fs:RevertBranch | arn:lakefs:fs:::repository/{repositoryId}/branch/{branchId} | PUT /repositories/{repositoryId}/branches/{branchId} | - | . | Get Branch Protection Rules | branches:GetBranchProtectionRules | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repository}/branch_protection | - | . | Set Branch Protection Rules | branches:SetBranchProtectionRules | arn:lakefs:fs:::repository/{repositoryId} | POST /repositories/{repository}/branch_protection | - | . | Delete Branch Protection Rules | branches:SetBranchProtectionRules | arn:lakefs:fs:::repository/{repositoryId} | DELETE /repositories/{repository}/branch_protection | - | . | Create User | auth:CreateUser | arn:lakefs:auth:::user/{userId} | POST /auth/users | - | . | List Users | auth:ListUsers | * | GET /auth/users | - | . | Get User | auth:ReadUser | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId} | - | . | Delete User | auth:DeleteUser | arn:lakefs:auth:::user/{userId} | DELETE /auth/users/{userId} | - | . | Get Group | auth:ReadGroup | arn:lakefs:auth:::group/{groupId} | GET /auth/groups/{groupId} | - | . | List Groups | auth:ListGroups | * | GET /auth/groups | - | . | Create Group | auth:CreateGroup | arn:lakefs:auth:::group/{groupId} | POST /auth/groups | - | . | Delete Group | auth:DeleteGroup | arn:lakefs:auth:::group/{groupId} | DELETE /auth/groups/{groupId} | - | . | List Policies | auth:ListPolicies | * | GET /auth/policies | - | . | Create Policy | auth:CreatePolicy | arn:lakefs:auth:::policy/{policyId} | POST /auth/policies | - | . | Update Policy | auth:UpdatePolicy | arn:lakefs:auth:::policy/{policyId} | POST /auth/policies | - | . | Delete Policy | auth:DeletePolicy | arn:lakefs:auth:::policy/{policyId} | DELETE /auth/policies/{policyId} | - | . | Get Policy | auth:ReadPolicy | arn:lakefs:auth:::policy/{policyId} | GET /auth/policies/{policyId} | - | . | List Group Members | auth:ReadGroup | arn:lakefs:auth:::group/{groupId} | GET /auth/groups/{groupId}/members | - | . | Add Group Member | auth:AddGroupMember | arn:lakefs:auth:::group/{groupId} | PUT /auth/groups/{groupId}/members/{userId} | - | . | Remove Group Member | auth:RemoveGroupMember | arn:lakefs:auth:::group/{groupId} | DELETE /auth/groups/{groupId}/members/{userId} | - | . | List User Credentials | auth:ListCredentials | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId}/credentials | - | . | Create User Credentials | auth:CreateCredentials | arn:lakefs:auth:::user/{userId} | POST /auth/users/{userId}/credentials | - | . | Delete User Credentials | auth:DeleteCredentials | arn:lakefs:auth:::user/{userId} | DELETE /auth/users/{userId}/credentials/{accessKeyId} | - | . | Get User Credentials | auth:ReadCredentials | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId}/credentials/{accessKeyId} | - | . | List User Groups | auth:ReadUser | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId}/groups | - | . | List User Policies | auth:ReadUser | arn:lakefs:auth:::user/{userId} | GET /auth/users/{userId}/policies | - | . | Attach Policy To User | auth:AttachPolicy | arn:lakefs:auth:::user/{userId} | PUT /auth/users/{userId}/policies/{policyId} | - | . | Detach Policy From User | auth:DetachPolicy | arn:lakefs:auth:::user/{userId} | DELETE /auth/users/{userId}/policies/{policyId} | - | . | List Group Policies | auth:ReadGroup | arn:lakefs:auth:::group/{groupId} | GET /auth/groups/{groupId}/policies | - | . | Attach Policy To Group | auth:AttachPolicy | arn:lakefs:auth:::group/{groupId} | PUT /auth/groups/{groupId}/policies/{policyId} | - | . | Detach Policy From Group | auth:DetachPolicy | arn:lakefs:auth:::group/{groupId} | DELETE /auth/groups/{groupId}/policies/{policyId} | - | . | Read Storage Config | fs:ReadConfig | * | GET /config/storage | - | . | Get Garbage Collection Rules | retention:GetGarbageCollectionRules | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repositoryId}/gc/rules | - | . | Set Garbage Collection Rules | retention:SetGarbageCollectionRules | arn:lakefs:fs:::repository/{repositoryId} | POST /repositories/{repositoryId}/gc/rules | - | . | Prepare Garbage Collection Commits | retention:PrepareGarbageCollectionCommits | arn:lakefs:fs:::repository/{repositoryId} | POST /repositories/{repositoryId}/gc/prepare_commits | - | . | List Repository Action Runs | ci:ReadAction | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repository}/actions/runs | - | . | Get Action Run | ci:ReadAction | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repository}/actions/runs/{run_id} | - | . | List Action Run Hooks | ci:ReadAction | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repository}/actions/runs/{run_id}/hooks | - | . | Get Action Run Hook Output | ci:ReadAction | arn:lakefs:fs:::repository/{repositoryId} | GET /repositories/{repository}/actions/runs/{run_id}/hooks/{hook_run_id}/output | - | . Some APIs may require more than one action.For instance, in order to create a repository (POST /repositories), you need permission to fs:CreateRepository for the name of the repository and also fs:AttachStorageNamespace for the storage namespace used. ",
    "url": "/v0.98/reference/rbac.html#actions-and-permissions",
    
    "relUrl": "/reference/rbac.html#actions-and-permissions"
  },"321": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Preconfigured Policies",
    "content": "The following Policies are created during initial setup: . FSFullAccess . { \"statement\": [ { \"action\": [ \"fs:*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . FSReadAll . { \"statement\": [ { \"action\": [ \"fs:List*\", \"fs:Read*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . FSReadWriteAll . { \"statement\": [ { \"action\": [ \"fs:ListRepositories\", \"fs:ReadRepository\", \"fs:ReadCommit\", \"fs:ListBranches\", \"fs:ListObjects\", \"fs:ReadObject\", \"fs:WriteObject\", \"fs:DeleteObject\", \"fs:RevertBranch\", \"fs:ReadBranch\", \"fs:CreateBranch\", \"fs:DeleteBranch\", \"fs:CreateCommit\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . AuthFullAccess . { \"statement\": [ { \"action\": [ \"auth:*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . AuthManageOwnCredentials . { \"statement\": [ { \"action\": [ \"auth:CreateCredentials\", \"auth:DeleteCredentials\", \"auth:ListCredentials\", \"auth:ReadCredentials\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:auth:::user/${user}\" } ] } . RepoManagementFullAccess . { \"statement\": [ { \"action\": [ \"ci:*\" ], \"effect\": \"allow\", \"resource\": \"*\" }, { \"action\": [ \"retention:*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . RepoManagementReadAll . { \"statement\": [ { \"action\": [ \"ci:Read*\" ], \"effect\": \"allow\", \"resource\": \"*\" }, { \"action\": [ \"retention:Get*\" ], \"effect\": \"allow\", \"resource\": \"*\" } ] } . ",
    "url": "/v0.98/reference/rbac.html#preconfigured-policies",
    
    "relUrl": "/reference/rbac.html#preconfigured-policies"
  },"322": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Additional Policies",
    "content": "You can create additional policies to further limit user access. Use the web UI or the lakectl auth command to create policies. Here is an example to define read/write access for a specific repository: . { \"statement\": [ { \"action\": [ \"fs:ReadRepository\", \"fs:ReadCommit\", \"fs:ListBranches\", \"fs:ListTags\", \"fs:ListObjects\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;\" }, { \"action\": [ \"fs:RevertBranch\", \"fs:ReadBranch\", \"fs:CreateBranch\", \"fs:DeleteBranch\", \"fs:CreateCommit\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;/branch/*\" }, { \"action\": [ \"fs:ListObjects\", \"fs:ReadObject\", \"fs:WriteObject\", \"fs:DeleteObject\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;/object/*\" }, { \"action\": [ \"fs:ReadTag\", \"fs:CreateTag\", \"fs:DeleteTag\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:fs:::repository/&lt;repository-name&gt;/tag/*\" }, { \"action\": [\"fs:ReadConfig\"], \"effect\": \"allow\", \"resource\": \"*\" } ] } . ",
    "url": "/v0.98/reference/rbac.html#additional-policies",
    
    "relUrl": "/reference/rbac.html#additional-policies"
  },"323": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Preconfigured Groups",
    "content": "lakeFS has four preconfigured groups: . | Admins | SuperUsers | Developers | Viewers | . They have the following policies granted to them: . | Policy | Admins | SuperUsers | Developers | Viewers | . | FSFullAccess | ✅ | ✅ |   |   | . | AuthFullAccess | ✅ |   |   |   | . | RepoManagementFullAccess | ✅ |   |   |   | . | AuthManageOwnCredentials |   | ✅ | ✅ | ✅ | . | RepoManagementReadAll |   | ✅ | ✅ |   | . | FSReadWriteAll |   |   | ✅ |   | . | FSReadAll |   |   |   | ✅ | . ",
    "url": "/v0.98/reference/rbac.html#preconfigured-groups",
    
    "relUrl": "/reference/rbac.html#preconfigured-groups"
  },"324": {
    "doc": "Role-Based Access Control (RBAC)",
    "title": "Pluggable Authentication and Authorization",
    "content": "Authorization and authentication is pluggable in lakeFS. If lakeFS is attached to a remote authentication server (or you are using lakeFS Cloud) then the role-based access control user interface can be used. If you are using RBAC with your self-managed lakeFS then the lakeFS configuration element auth.ui_config.RBAC should be set to external. If you are using self-managed lakeFS and do not have a remote authentication server then you should set auth.ui_config.RBAC to simplified and refer to the access control list documentation instead. ",
    "url": "/v0.98/reference/rbac.html#pluggable-authentication-and-authorization",
    
    "relUrl": "/reference/rbac.html#pluggable-authentication-and-authorization"
  },"325": {
    "doc": "Remote Authenticator",
    "title": "Remote Authenticator",
    "content": "Remote Authenticator is a pluggable architecture for lakeFS which allows you to use existing organizational identity policies and infrastructure with the authentication mechanism of lakeFS. The Remote Authenticator’s job is to abstract away the complexities of existing infrastructure and implement a standard interface, which lakeFS can use to resolve user identity and manage access to lakeFS. This loose coupling allows you to implement federated identity without providing lakeFS with direct access to your identity infrastructure. ",
    "url": "/v0.98/reference/remote-authenticator.html",
    
    "relUrl": "/reference/remote-authenticator.html"
  },"326": {
    "doc": "Remote Authenticator",
    "title": "Architecture",
    "content": "Here’s the authentication flow that lakeFS uses when configured with a remote authenticator: . sequenceDiagram participant A as lakeFS Client participant B as lakeFS Server participant C as Remote Authenticator participant D as IdP A-&gt;&gt;B: Submit login form B-&gt;&gt;C: POST user credentials C-&gt;&gt;D: IdP request D-&gt;&gt;C: IdP response C-&gt;&gt;B: Auth response B-&gt;&gt;A: auth JWT . ",
    "url": "/v0.98/reference/remote-authenticator.html#architecture",
    
    "relUrl": "/reference/remote-authenticator.html#architecture"
  },"327": {
    "doc": "Remote Authenticator",
    "title": "The Interface",
    "content": "To configure lakeFS to work with a Remote Authenticator add the following YAML to your lakeFS configuration: . auth: remote_authenticator: enabled: true endpoint: &lt;url-to-remote-authenticator-endpoint&gt; default_user_group: \"Developers\" ui_config: logout_url: /logout login_cookie_names: - internal_auth_session . | auth.remote_authenticator.enabled - set lakeFS to use the remote authenticator | auth.remote_authenticator.endpoint - an endpoint where the remote authenticator is able to receive a POST request from lakeFS | auth.remote_authenticator.default_user_group - the group assigned by default to new users | auth.ui_config.logout_url - the URL to redirect the browser when clicking the logout link in the user menu | auth.ui_config.login_cookie_names - the name of the cookie(s) lakeFS will set following a successful authentication. The value is the authenticated user’s JWT | . A Remote Authenticator implementation should expose a single endpoint, which expects the following JSON request: . { \"username\": \"testy.mctestface@example.com\", \"password\": \"Password1\" } . and returns a JSON response like this: . { \"external_user_identifier\": \"TestyMcTestface\" } . Example Request &amp; Responses . Request . POST https://remote-authenticator.example.com/auth Content-Type: application/json { \"username\": \"testy.mctestface@example.com\", \"password\": \"Password1\" } . Successful Response . HTTP/1.1 200 OK Content-Type: application/json { \"external_user_identifier\": \"TestyMcTestface\" } . Unauthorized Response . HTTP/1.1 401 Unauthorized Content-Type: application/json { \"external_user_identifier\": \"\" } . If the Remote Authenticator returns any HTTP status in the 2xx range, lakeFS considers this a successful authentication. Any HTTP status &lt; 200 or &gt; 300 is considered a failed authentication. If the Remote Authenticator returns a non-empty value for the external_user_identifier property along with a success HTTP status, lakeFS will show this identifier instead of an internal lakeFS user identifier in the UI. ",
    "url": "/v0.98/reference/remote-authenticator.html#the-interface",
    
    "relUrl": "/reference/remote-authenticator.html#the-interface"
  },"328": {
    "doc": "Remote Authenticator",
    "title": "Sample Implementation",
    "content": "Here is a sample Remote Authenticator implemented using node and express and written in TypeScript. This example implementation doesn’t integrate with any real IdP but illustrates the expected request/response patterns that you need to implement. import dotenv from \"dotenv\"; import express, { Express, Request, Response } from \"express\"; import { StatusCodes } from \"http-status-codes\"; type AuthRequestBody = { username: string; password: string; }; type AuthResponseBody = { external_user_identifier: string; }; const DEFAULT_PORT = 80; dotenv.config(); const port = process.env.PORT || DEFAULT_PORT; const app: Express = express(); app.post( \"/auth\", (req: Request&lt;AuthResponseBody, {}, AuthRequestBody&gt;, res: Response) =&gt; { const { username, password } = req.body; if (!username?.length || !password?.length) { return res.status(StatusCodes.BAD_REQUEST).json({ external_user_identifier: \"\", }); } // 👇🏻 This is where you would implement your own authentication logic if ( username === \"testy.mctestface@example.com\" &amp;&amp; password === \"Password1\" ) { return res.status(StatusCodes.OK).json({ external_user_identifier: \"TestyMcTestface\", }); } else { return res.status(StatusCodes.UNAUTHORIZED).json({ external_user_identifier: \"\", }); } } ); app.listen(port, () =&gt; { console.log(`Remote Authenticator listening on port ${port}`); }); . To run this service on the sub-domain idp.example.com, use a lakeFS configuration that looks like this: . auth: remote_authenticator: enabled: true endpoint: https://idp.example.com/auth default_user_group: \"Developers\" ui_config: logout_url: /logout login_cookie_names: - internal_auth_session . ",
    "url": "/v0.98/reference/remote-authenticator.html#sample-implementation",
    
    "relUrl": "/reference/remote-authenticator.html#sample-implementation"
  },"329": {
    "doc": "Reproducibility",
    "title": "The Benefits of Reproducible Data",
    "content": "Data changes frequently. This makes the task of keeping track of its exact state over time difficult. Oftentimes, people maintain only one state of their data––its current state. This has a negative impact on the work, as it becomes hard to: . | Debug a data issue. | Validate machine learning training accuracy (re-running a model over different data gives different results). | Comply with data audits. | . In comparison, lakeFS exposes a Git-like interface to data that allows keeping track of more than just the current state of data. This makes reproducing its state at any point in time straightforward. ",
    "url": "/v0.98/use_cases/reproducibility.html#the-benefits-of-reproducible-data",
    
    "relUrl": "/use_cases/reproducibility.html#the-benefits-of-reproducible-data"
  },"330": {
    "doc": "Reproducibility",
    "title": "Achieving Reproducibility with lakeFS",
    "content": "To make data reproducible, we recommend taking a new commit of your lakeFS repository every time the data in it changes. As long as there’s a commit taken, the process to reproduce a given state is as simple as reading the data from a path that includes the unique commit_id generated for each commit. To read data at it’s current state, we can use a static path containing the repository and branch names. To give an example, if you have a repository named example with a branch named main, reading the latest state of this data into a Spark Dataframe is always: . df = spark.read.parquet(‘s3://example/main/”) . Note: The code above assumes that all objects in the repository under this path are stored in parquet format. If a different format is used, the applicable Spark read method should be used. In a lakeFS repository, we are capable of taking many commits over the data, making many points in time reproducible. In the repository above, a new commit is taken each time a model training script is run, and the commit message includes the specific run number. If we wanted to re-run the model training script and reproduce the exact same results for a historical run, say run #435, we could copy the commit ID associated with the run and read the data into a dataframe like so: . df = spark.read.parquet(\"s3://example/296e54fbee5e176f3f4f4aeb7e087f9d57515750e8c3d033b8b841778613cb23/training_dataset/”) . The ability to reference a specific commit_id in code simplifies reproducing the specific state a data collection or even multiple collections. This has many applications that are common in data development, such as historical debugging, identifying deltas in a data collection, audit compliance, and more. ",
    "url": "/v0.98/use_cases/reproducibility.html#achieving-reproducibility-with-lakefs",
    
    "relUrl": "/use_cases/reproducibility.html#achieving-reproducibility-with-lakefs"
  },"331": {
    "doc": "Reproducibility",
    "title": "Reproducibility",
    "content": " ",
    "url": "/v0.98/use_cases/reproducibility.html",
    
    "relUrl": "/use_cases/reproducibility.html"
  },"332": {
    "doc": "Roadmap",
    "title": "Roadmap",
    "content": " ",
    "url": "/v0.98/roadmap.html",
    
    "relUrl": "/roadmap.html"
  },"333": {
    "doc": "Roadmap",
    "title": "Table of contents",
    "content": ". | Ecosystem . | Presigned URL support in LakeFSHadoopFS High Priority | Table format support . | Iceberg support High Priority | Metastore integration hooks | Delta Lake merges | . | Improved Azure Support . | HadoopFS: Support Azure Blob Storage | Support Azure CosmosDB as backend KV store | . | Improved streaming support for Apache Kafka | Native connector: Trino | . | Versioning Capabilities . | Support long-running quality checks | Git Integration | . | . ",
    "url": "/v0.98/roadmap.html#table-of-contents",
    
    "relUrl": "/roadmap.html#table-of-contents"
  },"334": {
    "doc": "Roadmap",
    "title": "Ecosystem",
    "content": "Presigned URL support in LakeFSHadoopFS High Priority . We plan on implementing support for the lakeFS pre-signed URL API into the lakeFS Hadoop Filesystem. Currently, users have to choose between: . | managing authorization in lakeFS as well as the object store to allow direct access to lakeFS-managed data | Use a gateway or proxy component such as the S3-compatible API that requires additional sizing | . Using pre-signed URLs, users will be able to enjoy both worlds: direct access without a proxy, with access control managed in one place . Here’s a simplified example of what this would look like: . sequenceDiagram autonumber Spark Job-&gt;&gt;lakeFS HadoopFS: spark.read.parquet(\"lakefs://repo/branch/path/\") lakeFS HadoopFS-&gt;&gt;lakeFS Server: ListObjects(\"lakefs://repo/branch/path/\", presign=True) lakeFS Server--&gt;lakeFS Server: Authorize user is allowed to access underlying data lakeFS Server--&gt;&gt;lakeFS HadoopFS: [\"https://&lt;s3 url&gt;/?X-AMZ-Signature=&lt;sig&gt;\", \"https://...\"] par lakeFS HadoopFS to S3 lakeFS HadoopFS-&gt;&gt;S3: GET http://&lt;s3 url&gt;/?X-AMZ-Signature=&lt;sig&gt; S3--&gt;&gt;lakeFS HadoopFS: object content end lakeFS HadoopFS-&gt;&gt;Spark Job: Parquet Data . Track and discuss it on GitHub . Table format support . Iceberg support High Priority . A table in Iceberg points to a single metadata file, containing a “location” property. Iceberg uses this location to store: . | Manifests describing where the data is stored. | The actual data. | . Once a table is created, the location property doesn’t change. Therefore, a branch creation in lakeFS in meaningless, since new data added to this branch will be added to the main branch. There are some workarounds for this, but it is our priority to create an excellent integration with Iceberg. Track and discuss it on GitHub . Metastore integration hooks . Allow lakeFS users that query and manipulate data using Hive Metastore to automate the integration between both components. | Creating a branch/tag with a predefined naming convention will result in automatic creation of metastore tables (for a predefined set of table names) | Deleting a branch/tag with a predefined naming convention will result in automatic deletion of metastore tables (for a predefined set of table names) | Merging a branch into another branch will result in metastore changes between branches being applied to the metastore for modified tables defined on the destination branch | . Track and discuss it on GitHub . Delta Lake merges . Delta lake stores metadata files that represent a logical transaction log that relies on numerical ordering. Currently, when trying to modify a Delta table from two different branches, lakeFS would correctly recognize a conflict: this log diverged into two different copies, representing different changes. Users would then have to forgo one of the change sets by either retaining the destination’s branch set of changes or the source’s branch. A much better user experience would be to allow merging this log into a new unified set of changes, representing changes made in both branches as a new set of log files (and potentially, data files too!). Track and discuss it on GitHub . Improved Azure Support . HadoopFS: Support Azure Blob Storage . Extend the lakeFS HadoopFilesystem to support working directly with Azure Blob Storage (see also: support for pre-signed URLs in the lakeFS Hadoop Filesystem above) . Track and discuss it on GitHub . Support Azure CosmosDB as backend KV store . Allow Azure users to use a serverless database such as CosmosDB as their backing KV for lakeFS metadata. This will also allow auto-provisioning of a CosmosDB database and table as part of the Helm installation process, similar to how its done for DynamoDB on AWS. Track and discuss it on GitHub . Improved streaming support for Apache Kafka . Committing (along with attaching useful information to the commit) makes a lot of sense for batch workloads: . | run a job or a pipeline on a separate branch and commit, | record information such as the git hash of the code executed, the versions of frameworks used, and information about the data artifacts, | once the pipeline has completed successfully, commit, and attach the recorded information as metadata. | . For streaming, however, this is currently less clear: There’s no obvious point in time to commit as things never actually “finish successfully”. The recommended pattern would be to ingest from a stream on a separate branch, periodically committing - storing not only the data added since last commit but also capturing the offset read from the stream, for reproducibility. These commits can then be merged into a main branch given they pass all relevant quality checks and other validations using hooks, exposing consumers to validated, clean data. In practice, implementing such a workflow is a little challenging. Users need to: . | Orchestrate the commits and merge operations. | Figure out how to attach the correct offset read from the stream broker. | Handle writes coming in while the commit is taking place. | . Ideally, lakeFS should provide tools to automate this, with native support for Apache Kafka. Track and discuss it on GitHub . Native connector: Trino . Currently, the Trino integration works well using the lakeFS S3 Gateway. While easy to integrate and useful out-of-the-box, due to the S3 protocol, it means that the data itself must pass through the lakeFS server. For larger installations, a native integration where lakeFS handles metadata and returns locations in the underlying object store that Trino can then access directly would allow reducing the operational overhead and increasing the scalability of lakeFS. This would be done in a similar way to the Native Spark integration using the Hadoop Filesystem implementation. Track and discuss it on GitHub . ",
    "url": "/v0.98/roadmap.html#ecosystem",
    
    "relUrl": "/roadmap.html#ecosystem"
  },"335": {
    "doc": "Roadmap",
    "title": "Versioning Capabilities",
    "content": "Support long-running quality checks . Support running quality checks on a branch that might take many minutes to complete. Currently, pre-commit and pre-merge hooks in lakeFS are tied to the lifecycle of the API request that triggers the said commit or merge operation. In order to support long-running checks, there are enhancements to make to lakeFS APIs in order to support an asynchronous commit and merge operations that are no longer tied to the HTTP request that triggered them. Track and discuss it on GitHub . Git Integration . Support an integration between a lakeFS repository and a Git repository. Allow versioning data assets along with the code that was used to modify/generate them. Track and discuss it on GitHub . ",
    "url": "/v0.98/roadmap.html#versioning-capabilities",
    
    "relUrl": "/roadmap.html#versioning-capabilities"
  },"336": {
    "doc": "Rollback",
    "title": "Rollbacks",
    "content": " ",
    "url": "/v0.98/use_cases/rollback.html#rollbacks",
    
    "relUrl": "/use_cases/rollback.html#rollbacks"
  },"337": {
    "doc": "Rollback",
    "title": "What Is a Rollback?",
    "content": "A rollback operation is used to to fix critical data errors immediately. What is a critical data error? Think of a situation where erroneous or misformatted data causes a signficant issue with an important service or function. In such situations, the first thing to do is stop the bleeding. Rolling back returns data to a state in the past, before the error was present. You might not be showing all the latest data after a rollback, but at least you aren’t showing incorrect data or raising errors. ",
    "url": "/v0.98/use_cases/rollback.html#what-is-a-rollback",
    
    "relUrl": "/use_cases/rollback.html#what-is-a-rollback"
  },"338": {
    "doc": "Rollback",
    "title": "Why Rollbacks Are Useful",
    "content": "A Rollback is used as a stopgap measure to “put out the fire” as quickly as possible while RCA (root cause analysis) is performed to understand 1) exactly how the error happened, and 2) what can be done to prevent it from happening again. It can be a pressured, stressful situation to deal with a critical data error. Having the ability to employ a rollback relieves some of the pressure and makes it more likely you can figure out what happened without creating additional issues. As a real world example, the 14-day outage some Atlassian users experienced in May 2022 could have been an uninteresting minor incident had rolling back the deleted customer data been an option. ",
    "url": "/v0.98/use_cases/rollback.html#why-rollbacks-are-useful",
    
    "relUrl": "/use_cases/rollback.html#why-rollbacks-are-useful"
  },"339": {
    "doc": "Rollback",
    "title": "Performing Rollbacks with lakeFS",
    "content": "lakeFS lets you develop in your data lake in such a way that rollbacks are simple to perform. This starts by taking a commit over your lakeFS repository whenever a change to its state occurs. Using the lakeFS UI or CLI, you can set the current state, or HEAD, of a branch to any historical commit in seconds, effectively performing a rollback. To demonstrate how this works, let’s take the example of a lakeFS repo with the following commit history: . As can be inferred from the history, this repo is updated every minute with a data sync from some data source. An example data sync is a typical ETL job that replicates data from an internal database or any other data source. After each sync, a commit is taken in lakeFS to save a snapshot of data at that point in time. How to Rollback From a Bad Data Sync? . Say a situation occurs where one of the syncs had bad data and is causing downstream dashboards to fail to load. Since we took a commit of the repo right after the sync ran, we can use a revert operation to undo the changes introduced in that sync. Step 1: Copy the commit_id associated with the commit we want to revert. As the screenshot above shows, you can use the Copy ID to Clipboard button to do this. Step 2: Run the revert command using lakectl, the lakeFS CLI. In this example, the command will be as follows: . lakectl branch revert lakefs://example/main 9666d7c9daf37b3ba6964e733d08596ace2ec2c7bc3a4023ad8e80737a6c3e9d . This will undo the changes introduced by this commit, completing the rollback! . The rollback operation is that simple, even if many changes were introduced in a commit, spanning acrossmultiple data collections. In lakeFS, rolling back data is always a one-liner. ",
    "url": "/v0.98/use_cases/rollback.html#performing-rollbacks-with-lakefs",
    
    "relUrl": "/use_cases/rollback.html#performing-rollbacks-with-lakefs"
  },"340": {
    "doc": "Rollback",
    "title": "Rollback",
    "content": " ",
    "url": "/v0.98/use_cases/rollback.html",
    
    "relUrl": "/use_cases/rollback.html"
  },"341": {
    "doc": "5️⃣ Roll back Changes",
    "title": "Rolling back Changes in lakeFS ↩️",
    "content": "Our intrepid user (you) merged a change back into the main branch and realised that they had made a mistake 🤦🏻. The good news for them (you) is that lakeFS can revert changes made, similar to how you would in git 😅. From your terminal window run lakectl with the revert command: . docker exec -it lakefs \\ lakectl branch revert \\ lakefs://quickstart/main \\ main --parent-number 1 --yes . You should see a confirmation of a successful rollback: . Branch: lakefs://quickstart/main commit main successfully reverted . Back in the object page and the DuckDB query we can see that the original file is now back to how it was: . ",
    "url": "/v0.98/quickstart/rollback.html#rolling-back-changes-in-lakefs-%EF%B8%8F",
    
    "relUrl": "/quickstart/rollback.html#rolling-back-changes-in-lakefs-️"
  },"342": {
    "doc": "5️⃣ Roll back Changes",
    "title": "Bonus Challenge",
    "content": "And so with that, this quickstart for lakeFS draws to a close. If you’re simply having too much fun to stop then here’s an exercise for you. Implement the requirement from above correctly, such that you write denmark-lakes.parquet in the respective branch and successfully merge it back into main. Look up how to list the contents of the main branch and verify that it looks like this: . object 2023-03-21 17:33:51 +0000 UTC 20.9 kB denmark-lakes.parquet object 2023-03-21 14:45:38 +0000 UTC 916.4 kB lakes.parquet . ",
    "url": "/v0.98/quickstart/rollback.html#bonus-challenge",
    
    "relUrl": "/quickstart/rollback.html#bonus-challenge"
  },"343": {
    "doc": "5️⃣ Roll back Changes",
    "title": "Finishing Up",
    "content": "Once you’ve finished the quickstart, shut down your local environment with the following command: . docker-compose down . ",
    "url": "/v0.98/quickstart/rollback.html#finishing-up",
    
    "relUrl": "/quickstart/rollback.html#finishing-up"
  },"344": {
    "doc": "5️⃣ Roll back Changes",
    "title": "5️⃣ Roll back Changes",
    "content": " ",
    "url": "/v0.98/quickstart/rollback.html",
    
    "relUrl": "/quickstart/rollback.html"
  },"345": {
    "doc": "S3 Supported API",
    "title": "S3-Supported API",
    "content": "The S3 Gateway emulates a subset of the API exposed by S3. This subset includes all API endpoints relevant to data systems. For more information, see architecture . lakeFS supports the following API operations: . | Identity and authorization . | SIGv2 | SIGv4 | . | Bucket operations: . | HEAD bucket | . | Object operations: . | DeleteObject | DeleteObjects | GetObject . | Support for caching headers, ETag | Support for range requests | No support for SSE | No support for SelectObject operations | . | HeadObject | PutObject . | Support multi-part uploads | No support for storage classes | No object level tagging | . | CopyObject | . | Object Listing: . | ListObjects | ListObjectsV2 | Delimiter support (for \"/\" only) | . | Multipart Uploads: . | AbortMultipartUpload | CompleteMultipartUpload | CreateMultipartUpload | ListParts | Upload Part | UploadPartCopy | . | . ",
    "url": "/v0.98/reference/s3.html#s3-supported-api",
    
    "relUrl": "/reference/s3.html#s3-supported-api"
  },"346": {
    "doc": "S3 Supported API",
    "title": "S3 Supported API",
    "content": " ",
    "url": "/v0.98/reference/s3.html",
    
    "relUrl": "/reference/s3.html"
  },"347": {
    "doc": "SageMaker",
    "title": "Using lakeFS with SageMaker",
    "content": "Amazon SageMaker helps to prepare, build, train and deploy ML models quickly by bringing together a broad set of capabilities purpose-built for ML. ",
    "url": "/v0.98/integrations/sagemaker.html#using-lakefs-with-sagemaker",
    
    "relUrl": "/integrations/sagemaker.html#using-lakefs-with-sagemaker"
  },"348": {
    "doc": "SageMaker",
    "title": "Table of contents",
    "content": ". | Initializing session and client | Usage Examples . | Upload train and test data | Download objects | . | . ",
    "url": "/v0.98/integrations/sagemaker.html#table-of-contents",
    
    "relUrl": "/integrations/sagemaker.html#table-of-contents"
  },"349": {
    "doc": "SageMaker",
    "title": "Initializing session and client",
    "content": "Initialize a Sagemaker session and an S3 client with lakeFS as the endpoint: . import sagemaker import boto3 endpoint_url = '&lt;LAKEFS_ENDPOINT&gt;' aws_access_key_id = '&lt;LAKEFS_ACCESS_KEY_ID&gt;' aws_secret_access_key = '&lt;LAKEFS_SECRET_ACCESS_KEY&gt;' repo = 'example-repo' sm = boto3.client('sagemaker', endpoint_url=endpoint_url, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key) s3_resource = boto3.resource('s3', endpoint_url=endpoint_url, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key) session = sagemaker.Session(boto3.Session(), sagemaker_client=sm, default_bucket=repo) session.s3_resource = s3_resource . ",
    "url": "/v0.98/integrations/sagemaker.html#initializing-session-and-client",
    
    "relUrl": "/integrations/sagemaker.html#initializing-session-and-client"
  },"350": {
    "doc": "SageMaker",
    "title": "Usage Examples",
    "content": "Upload train and test data . Let’s use the created session for uploading data to the ‘main’ branch: . prefix = \"/prefix-within-branch\" branch = 'main' train_file = 'train_data.csv'; train_data.to_csv(train_file, index=False, header=True) train_data_s3_path = session.upload_data(path=train_file, key_prefix=branch + prefix + \"/train\") test_file = 'test_data.csv'; test_data_no_target.to_csv(test_file, index=False, header=False) test_data_s3_path = session.upload_data(path=test_file, key_prefix=branch + prefix + \"/test\") . Download objects . You can use the integration with lakeFS to download a portion of the data you see fit: . repo = 'example-repo' prefix = \"/prefix-to-download\" branch = 'main' localpath = './' + branch session.download_data(path=localpath, bucket=repo, key_prefix = branch + prefix) . Note: Advanced AWS SageMaker features, like Autopilot jobs, are encapsulated and don’t have the option to override the S3 endpoint. However, it is possible to export the required inputs from lakeFS to S3. If you’re using SageMaker features that aren’t supported by lakeFS, we’d love to hear from you. ",
    "url": "/v0.98/integrations/sagemaker.html#usage-examples",
    
    "relUrl": "/integrations/sagemaker.html#usage-examples"
  },"351": {
    "doc": "SageMaker",
    "title": "SageMaker",
    "content": " ",
    "url": "/v0.98/integrations/sagemaker.html",
    
    "relUrl": "/integrations/sagemaker.html"
  },"352": {
    "doc": "SAML Authentication Integration",
    "title": "SAML Authentication Integration",
    "content": "SAML authentication integration is available at lakeFS cloud, a fully managed hosted lakeFS service with guaranteed SLAs. ",
    "url": "/v0.98/reference/saml-authentication-integration.html",
    
    "relUrl": "/reference/saml-authentication-integration.html"
  },"353": {
    "doc": "Setting up lakeFS Cloud on Azure",
    "title": "Setting up lakeFS Cloud on Azure",
    "content": "lakeFS saves all the data and the metadata on your storage account. In this manual you will create a dedicated storage account for lakeFS and provide lakefs-cloud access to your storage account. ",
    "url": "/v0.98/cloud/setup-azure.html",
    
    "relUrl": "/cloud/setup-azure.html"
  },"354": {
    "doc": "Setting up lakeFS Cloud on Azure",
    "title": "Setting up a Storage container and App registration:",
    "content": ". | Open your Azure Portal. | Select Azure Active Directory &gt; App registrations from the left-menu. | If you have an existing App registration you can use, select it. Otherwise create a new one. | Click + New registration to create an App registration. Name it, leave the default settings and click Register. | . | . Record the values Application (client) ID and Directory (tenant) ID for later. | On the same App registration page, select Certificates &amp; secrets from the left-menu. | Click + New client secret to create a new one. Select a time frame for its expiration, add a description, and click Add. | . Record the application secret for later. (Note that the password value will not be available once you leave the page.) . ",
    "url": "/v0.98/cloud/setup-azure.html#setting-up-a-storage-container-and-app-registration",
    
    "relUrl": "/cloud/setup-azure.html#setting-up-a-storage-container-and-app-registration"
  },"355": {
    "doc": "Setting up lakeFS Cloud on Azure",
    "title": "Create a Storage account",
    "content": ". | Click the main menu in the top-left corner, and select Storage account. | If you have an existing Storage account you can use, select it. Otherwise create a new one. | Click + Create to create a new account. | . | . Record the storage account name for later. ",
    "url": "/v0.98/cloud/setup-azure.html#create-a-storage-account",
    
    "relUrl": "/cloud/setup-azure.html#create-a-storage-account"
  },"356": {
    "doc": "Setting up lakeFS Cloud on Azure",
    "title": "Create a Storage container",
    "content": "In the Storage account, create a storage container (or select an existing one). ",
    "url": "/v0.98/cloud/setup-azure.html#create-a-storage-container",
    
    "relUrl": "/cloud/setup-azure.html#create-a-storage-container"
  },"357": {
    "doc": "Setting up lakeFS Cloud on Azure",
    "title": "Assign App &amp; role to your Storage container",
    "content": "Still on the Storage container page . | Select Access Control (IAM) from the left-menu. | Select Add role assignments | Fill in the form: . | Role: Select Storage Blob Data Contributor. | Assign access to: Leave the defaults unchanged (should be User, group, or service principal). | Start typing in the name of the app and select it from the dropdown list. | . | Click save. | . ",
    "url": "/v0.98/cloud/setup-azure.html#assign-app--role-to-your-storage-container",
    
    "relUrl": "/cloud/setup-azure.html#assign-app--role-to-your-storage-container"
  },"358": {
    "doc": "Setting up lakeFS Cloud on Azure",
    "title": "Limiting access to trusted networks",
    "content": "To make sure that your Azure Blob storage cannot be accessed by malicious actors who get hold of the access credentials, you can configure the Azure Blob account to only allow logins from trusted networks. Every storage account in Azure has Security rules, which define the access permissions. By default, the storage account is accessible by any network as long as the user has the access credentials. Before you begin, you’ll need: Azure CLI installed, and replace the following values: . | Resource Group Name (Provided by Treeverse) | Virtual Network Name (Provided by Treeverse) | Subnet Name (Provided by Treeverse) | Customer Storage Account Name | . az storage account network-rule add --subnet /subscriptions/947382ea-681a-4541-99ab-b718960c6289/resourceGroups/&lt;Resource Group Name&gt;/providers/Microsoft.Network/virtualNetworks/&lt;Virtual Network Name&gt;/subnets/&lt;Subnet Name&gt; --account-name &lt;Customer Storage Account Name&gt; . ",
    "url": "/v0.98/cloud/setup-azure.html#limiting-access-to-trusted-networks",
    
    "relUrl": "/cloud/setup-azure.html#limiting-access-to-trusted-networks"
  },"359": {
    "doc": "Setting up lakeFS Cloud on Azure",
    "title": "Recorded Values",
    "content": "In order to Treeverse to finish setting up your environment on Azure, we’ll need the following values recorded in the steps above: . | Storage Account Name | Client (Application) ID | Client Secret Password | Directory (Tenant) ID | . ",
    "url": "/v0.98/cloud/setup-azure.html#recorded-values",
    
    "relUrl": "/cloud/setup-azure.html#recorded-values"
  },"360": {
    "doc": "Sizing Guide",
    "title": "Sizing guide",
    "content": " ",
    "url": "/v0.98/howto/sizing-guide.html#sizing-guide",
    
    "relUrl": "/howto/sizing-guide.html#sizing-guide"
  },"361": {
    "doc": "Sizing Guide",
    "title": "Table of contents",
    "content": ". | System Requirements . | Operating Systems and ISA | Memory and CPU requirements | Network | Disk | lakeFS KV Store . | Storage | . | . | Scaling factors . | Understanding latency and throughput considerations | . | Benchmarks . | PostgresSQL | Random reads | Random Writes | Branch creation | DynamoDB | Random reads | Random Writes | Branch creation | . | Important metrics | Reference architectures . | Reference Architecture: Data Science/Research environment | Reference Architecture: Automated Production Pipelines | . | . Note: For a scalable managed lakeFS service with guaranteed SLAs, try lakeFS cloud . ",
    "url": "/v0.98/howto/sizing-guide.html#table-of-contents",
    
    "relUrl": "/howto/sizing-guide.html#table-of-contents"
  },"362": {
    "doc": "Sizing Guide",
    "title": "System Requirements",
    "content": "Operating Systems and ISA . lakeFS can run on MacOS and Linux. Windows binaries are available but not rigorously tested - we don’t recommend deploying lakeFS to production on Windows. x86_64 and arm64 architectures are supported for both MacOS and Linux. Memory and CPU requirements . lakeFS servers require a minimum of 512mb of RAM and 1 CPU core. For high throughput, additional CPUs help scale requests across different cores. “Expensive” operations such as large diff or commit operations can take advantage of multiple cores. Network . If using the data APIs such as the S3 Gateway, lakeFS will require enough network bandwidth to support the planned concurrent network upload/download operations. For most cloud providers, more powerful machines (i.e., more expensive and usually containing more CPU cores) also provide increased network bandwidth. If using only the metadata APIs (for example, only using the Hadoop/Spark clients), network bandwidth is minimal, at roughly 1Kb per request. Disk . lakeFS greatly benefits from fast local disks. A lakeFS instance doesn’t require any strong durability guarantees from the underlying storage, as the disk is only ever used as a local caching layer for lakeFS metadata and not for long-term storage. lakeFS is designed to work with ephemeral disks - these are usually based on NVMe and are tied to the machine’s lifecycle. Using ephemeral disks lakeFS can provide a very high throughput/cost ratio, probably the best that could be achieved on a public cloud, so we recommend those. A local cache of at least 512 MiB should be provided. For large installations (managing &gt;100 concurrently active branches, with &gt;100M objects per commit), we recommend allocating at least 10 GiB - since it’s a caching layer over a relatively slow storage (the object store), see Important metrics below to understand how to size this: it should be big enough to hold all commit metadata for actively referenced commits. lakeFS KV Store . lakeFS uses a key-value database to manage branch references, authentication and authorization information and to keep track of currently uncommitted data across branches. Please refer to the relevant driver tab for best practices, requirements and benchmarks. Storage . The dataset stored in the metadata store is relatively modest as most metadata is pushed down into the object store. Required storage is mostly a factor of the amount of uncommitted writes across all branches at any given point in time: in the range of 150 MiB per every 100,000 uncommitted writes. We recommend starting at 10 GiB for a production deployment, as it will likely be more than enough. | PostgreSQL | DynamoDB | . RAM Since the data size is small, it’s recommended to provide enough memory to hold the vast majority of that data in RAM. Cloud providers will save you the need to tune this parameter - it will be set to a fixed percentage the chosen instance’s available RAM (25% on AWS RDS, 30% on Google Cloud SQL). It is recommended that you check with your selected cloud provider for configuration and provisioning information for you database. For self-managed database instances follow these best practices . Ideally, configure the shared_buffers of your PostgreSQL instances to be large enough to contain the currently active dataset. Pick a database instance with enough RAM to accommodate this buffer size at roughly x4 the size given for shared_buffers. For example, if an installation has ~500,000 uncommitted writes at any given time, it would require about 750 MiB of shared_buffers that would require about 3 GiB of RAM. CPU PostgreSQL CPU cores help scale concurrent requests. 1 CPU core for every 5,000 requests/second is ideal. lakeFS will create a DynamoDB table for you, defaults to on-demand capacity setting. No need to specify how much read and write throughput you expect your application to perform, as DynamoDB instantly accommodates your workloads as they ramp up or down. You can customize the table settings to provisioned capacity which allows you to manage and optimize your costs by allocating read/write capacity in advance (see Benchmarks) . Notes: . | Using DynamoDB on-demand capacity might generate unwanted costs if the table is abused, if you’d like to cap your costs, make sure to change the table to use provisioned capacity instead. | lakeFS doesn’t manage the DynamoDB’s table lifecycle, we’ve included the table creation in order to help evaluating the system with minimal effort, any change to the table beyond the table creation - will need to be handled manually or by 3rd party tools. | . RAM Managed by AWS. CPU Managed by AWS. ",
    "url": "/v0.98/howto/sizing-guide.html#system-requirements",
    
    "relUrl": "/howto/sizing-guide.html#system-requirements"
  },"363": {
    "doc": "Sizing Guide",
    "title": "Scaling factors",
    "content": "Scaling lakeFS, like most data systems, moves across two axes: throughput of requests (amount per given timeframe) and latency (time to complete a single request). Understanding latency and throughput considerations . Most lakeFS operations are designed to be very low in latency. Assuming a well-tuned local disk cache (see Storage above), most critical path operations (writing objects, requesting objects, deleting objects) are designed to complete in &lt;25ms at p90. Listing objects obviously requires accessing more data, but should always be on-par with what the underlying object store can provide, and in most cases, it’s actually faster. At the worst case, for directory listing with 1,000 common prefixes returned, expect a latency of 75ms at p90. Managing branches (creating them, listing them and deleting them) are all constant-time operations, generally taking &lt;30ms at p90. Committing and merging can take longer, as they are proportional to the amount of changes introduced. This is what makes lakeFS optimal for large Data Lakes - the amount of changes introduced per commit usually stays relatively stable while the entire data set usually grows over time. This means lakeFS will provide predictable performance: committing 100 changes will take roughly the same amount of time whether the resulting commit contains 500 or 500 million objects. See Data Model for more information. Scaling throughput depends very much on the amount of CPU cores available to lakeFS. In many cases, it’s easier to scale lakeFS across a fleet of smaller cloud instances (or containers) than scale up with machines that have many cores. In fact, lakeFS works well in both cases. Most critical path operations scale very well across machines. ",
    "url": "/v0.98/howto/sizing-guide.html#scaling-factors",
    
    "relUrl": "/howto/sizing-guide.html#scaling-factors"
  },"364": {
    "doc": "Sizing Guide",
    "title": "Benchmarks",
    "content": ". | PostgreSQL | DynamoDB | . PostgresSQL . All benchmarks below were measured using 2 x c5ad.4xlarge instances on AWS us-east-1. Similar results can be achieved on Google Cloud using a c2-standard-16 machine type, with an attached local SSD. On Azure, you can use a Standard_F16s_v2 virtual machine. The PostgreSQL instance that was used is a db.m6g.2xlarge (8 vCPUs, 32 GB RAM). Equivalent machines on Google Cloud or Azure should yield similar results. The example repository we tested against contains the metadata of a large lakeFS installation, where each commit contains ~180,000,000 objects (representing ~7.5 Petabytes of data). All tests are reproducible using the lakectl abuse command, so use it to properly size and tune your setup. All tests are accompanied by the relevant lakectl abuse command that generated them. Random reads . This test generates random read requests to lakeFS, in a given commit. Paths are requested randomly from a file containing a set of preconfigured (and existing) paths. command executed: . lakectl abuse random-read \\ --from-file randomly_selected_paths.txt \\ --amount 500000 \\ --parallelism 128 \\ lakefs://example-repo/&lt;commit hash&gt; . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and commit hash. Result Histogram (raw): . Histogram (ms): 1 0 2 0 5 37945 7 179727 10 296964 15 399682 25 477502 50 499625 75 499998 100 499998 250 500000 350 500000 500 500000 750 500000 1000 500000 5000 500000 min 3 max 222 total 500000 . So 50% of all requests took &lt;10ms, while 99.9% of them took &lt;50ms . throughput: . Average throughput during the experiment was 10851.69 requests/second . Random Writes . This test generates random write requests to a given lakeFS branch. All the paths are pre-generated and don’t overwrite each other (as overwrites are relatively rare in a Data Lake setup). command executed: . lakectl abuse random-write \\ --amount 500000 \\ --parallelism 64 \\ lakefs://example-repo/main . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and branch. Result Histogram (raw): . Histogram (ms): 1 0 2 0 5 30715 7 219647 10 455807 15 498144 25 499535 50 499742 75 499784 100 499802 250 500000 350 500000 500 500000 750 500000 1000 500000 5000 500000 min 3 max 233 total 500000 . So, 50% of all requests took &lt;10ms, while 99.9% of them took &lt;25ms. throughput: . The average throughput during the experiment was 7595.46 requests/second. Branch creation . This test creates branches from a given reference. command executed: . lakectl abuse create-branches \\ --amount 500000 \\ --branch-prefix \"benchmark-\" \\ --parallelism 256 \\ lakefs://example-repo/&lt;commit hash&gt; . Note lakeFS version &lt;= v0.33.1 uses ‘@’ (instead of ‘/’) as separator between repository and commit hash. Result Histogram (raw): . Histogram (ms): 1 0 2 1 5 5901 7 39835 10 135863 15 270201 25 399895 50 484932 75 497180 100 499303 250 499996 350 500000 500 500000 750 500000 1000 500000 5000 500000 min 2 max 304 total 500000 . So, 50% of all requests took &lt;15ms, while 99.9% of them took &lt;100ms. throughput: . The average throughput during the experiment was 7069.03 requests/second. DynamoDB . All benchmarks below were measured using m5.xlarge instance on AWS us-east-1. The DynamoDB table that was used was provisioned with 500/1000 read/write capacity. The example repository we tested against contains the metadata of a large lakeFS installation, where each commit contains ~100,000,000 objects (representing ~3.5 Petabytes of data). All tests are reproducible using the lakectl abuse command, so use it to properly size and tune your setup. All tests are accompanied by the relevant lakectl abuse command that generated them. Random reads . This test generates random read requests to lakeFS, in a given commit. Paths are requested randomly from a file containing a set of preconfigured (and existing) paths. command executed: . lakectl abuse random-read \\ --from-file randomly_selected_paths.txt \\ --amount 500000 \\ --parallelism 128 \\ lakefs://example-repo/&lt;commit hash&gt; . Result Histogram (raw): Provisioned read capacity units = 1000 Provisioned write capacity units = 1000 . Histogram (ms): 1 0 2 0 5 0 7 0 10 0 15 0 25 122 50 47364 75 344489 100 460404 250 497912 350 498016 500 498045 750 498111 1000 498176 5000 499478 min 18 max 52272 total 500000 . Result Histogram (raw): Provisioned read capacity units = 500 Provisioned write capacity units = 500 . Histogram (ms): 1 0 2 0 5 0 7 0 10 0 15 1 25 2672 50 239661 75 420171 100 470146 250 486603 350 486715 500 486789 750 487443 1000 488113 5000 493201 min 14 max 648085 total 499998 . Random Writes . This test generates random write requests to a given lakeFS branch. All the paths are pre-generated and don’t overwrite each other (as overwrites are relatively rare in a Data Lake setup). command executed: . lakectl abuse random-write \\ --amount 500000 \\ --parallelism 64 \\ lakefs://example-repo/main . Result Histogram (raw): Provisioned read capacity units = 1000 Provisioned write capacity units = 1000 . Histogram (ms): 1 0 2 0 5 0 7 0 10 0 15 0 25 24 50 239852 75 458504 100 485225 250 493687 350 493872 500 493960 750 496239 1000 499194 5000 500000 min 23 max 4437 total 500000 . Result Histogram (raw): Provisioned read capacity units = 500 Provisioned write capacity units = 500 . Histogram (ms): 1 0 2 0 5 0 7 0 10 0 15 0 25 174 50 266460 75 462641 100 484486 250 490633 350 490856 500 490984 750 492973 1000 495605 5000 498920 min 21 max 50157 total 500000 . Branch creation . This test creates branches from a given reference. command executed: . lakectl abuse create-branches \\ --amount 500000 \\ --branch-prefix \"benchmark-\" \\ --parallelism 256 \\ lakefs://example-repo/&lt;commit hash&gt; . Result Histogram (raw): Provisioned read capacity units = 1000 Provisioned write capacity units = 1000 . Histogram (ms): 1 0 2 0 5 0 7 0 10 0 15 0 25 0 50 628 75 26153 100 58099 250 216160 350 307078 500 406165 750 422898 1000 431332 5000 475848 min 41 max 430725 total 490054 . Result Histogram (raw): Provisioned read capacity units = 500 Provisioned write capacity units = 500 . Histogram (ms): 1 0 2 0 5 0 7 0 10 0 15 0 25 0 50 3132 75 155570 100 292745 250 384224 350 397258 500 431141 750 441360 1000 445597 5000 469538 min 39 max 760626 total 497520 . ",
    "url": "/v0.98/howto/sizing-guide.html#benchmarks",
    
    "relUrl": "/howto/sizing-guide.html#benchmarks"
  },"365": {
    "doc": "Sizing Guide",
    "title": "Important metrics",
    "content": "lakeFS exposes metrics using the Prometheus protocol. Every lakeFS instance exposes a /metrics endpoint that could be used to extract them. Here are a few notable metrics to keep track of when sizing lakeFS: . api_requests_total - Tracks throughput of API requests over time. api_request_duration_seconds - Histogram of latency per operation type. gateway_request_duration_seconds - Histogram of latency per S3 Gateway operation. | PostgreSQL | DynamoDB | . dynamo_request_duration_seconds - Time spent doing DynamoDB requests. dynamo_consumed_capacity_total - The capacity units consumed by operation. dynamo_failures_total - The total number of errors while working for kv store. ",
    "url": "/v0.98/howto/sizing-guide.html#important-metrics",
    
    "relUrl": "/howto/sizing-guide.html#important-metrics"
  },"366": {
    "doc": "Sizing Guide",
    "title": "Reference architectures",
    "content": "Below are a few example architectures for lakeFS deployment. Reference Architecture: Data Science/Research environment . Use case: Manage Machine learning or algorithms development. Use lakeFS branches to achieve both isolation and reproducibility of experiments. Data being managed by lakeFS is both structured tabular data, as well as unstructured sensor and image data used for training. Assuming a team of 20-50 researchers, with a dataset size of 500 TiB across 20M objects. Environment: lakeFS will be deployed on Kubernetes. managed by AWS EKS with PostgreSQL on AWS RDS Aurora . Sizing: Since most of the work is done by humans (vs. automated pipelines), most experiments tend to be small in scale, reading and writing 10s to 1000s of objects. The expected amount of branches active in parallel is relatively low, around 1-2 per user, each representing a small amount of uncommitted changes at any given point in time. Let’s assume 5,000 uncommitted writes per branch = ~500k. To support the expected throughput, a single moderate lakeFS instance should be more than enough, since requests per second would be on the order of 10s to 100s. For high availability, we’ll deploy 2 pods with 1 CPU core and 1 GiB of RAM each. Since the PostgreSQL instance is expected to hold a very small dataset (at 500k, expected dataset size is 150MiB (for 100k records) * 5 = 750MiB). To ensure we have enough RAM to hold this, we’ll need 3 GiB of RAM, so, a very moderate Aurora instance db.t3.large (2 vCPUs, 8 GB RAM) will be more than enough. An equivalent database instance on GCP or Azure should give similar results. Reference Architecture: Automated Production Pipelines . Use case: Manage multiple concurrent data pipelines using Apache Spark and Airflow. Airflow DAGs start by creating a branch for isolation and for CI/CD. Data being managed by lakeFS is structured, tabular data. The total dataset size is 10 PiB, spanning across 500M objects. The expected throughput is 10k reads/second + 2k writes per second across 100 concurrent branches. Environment: lakeFS will be deployed on Kubernetes. managed by AWS EKS with PostgreSQL on AWS RDS . Sizing: Data pipelines tend to be bursty in nature: reading in a lot of objects concurrently, doing some calculation or aggregation, and then writing many objects concurrently. The expected amount of branches active in parallel is high, with many Airflow DAGs running per day, each representing a moderate amount of uncommitted changes at any given point in time. Let’s assume 1,000 uncommitted writes/branch * 2500 branches = ~2.5M records. To support the expected throughput, looking the benchmarking numbers above, we’re doing roughly 625 requests/core, so 24 cores should cover our peak traffic. We can deploy 6 * 4 CPU core pods. On to the PostgreSQL instance - at 500k, the expected dataset size is 150MiB (for 100k records) * 25 = 3750 MiB. To ensure we have enough RAM to hold this, we’ll need at least 15 GiB of RAM, so we’ll go with a db.r5.xlarge (4 vCPUs, 32GB RAM) Aurora instance. An equivalent database instance on GCP or Azure should give similar results. ",
    "url": "/v0.98/howto/sizing-guide.html#reference-architectures",
    
    "relUrl": "/howto/sizing-guide.html#reference-architectures"
  },"367": {
    "doc": "Sizing Guide",
    "title": "Sizing Guide",
    "content": " ",
    "url": "/v0.98/howto/sizing-guide.html",
    
    "relUrl": "/howto/sizing-guide.html"
  },"368": {
    "doc": "Spark Client",
    "title": "lakeFS Spark Client Reference",
    "content": "Utilize the power of Spark to interact with the metadata on lakeFS. Possible use cases include: . | Creating a DataFrame for listing the objects in a specific commit or branch. | Computing changes between two commits. | Exporting your data for consumption outside lakeFS. | Bulk operations on the underlying storage. | . ",
    "url": "/v0.98/reference/spark-client.html#lakefs-spark-client-reference",
    
    "relUrl": "/reference/spark-client.html#lakefs-spark-client-reference"
  },"369": {
    "doc": "Spark Client",
    "title": "Getting Started",
    "content": "Start Spark Shell / PySpark with the --packages flag: . | Spark 2.x | Spark 3.x | Spark 3.x on Hadoop 3.x | . This client is compiled for Spark 2.4.7 and tested with it, but can work for higher versions. spark-shell --packages io.lakefs:lakefs-spark-client-247_2.11:0.6.5 . Alternatively an assembled jar is available on S3, at s3://treeverse-clients-us-east/lakefs-spark-client-247/0.6.5/lakefs-spark-client-247-assembly-0.6.5.jar . This client is compiled for Spark 3.0.1 with Hadoop 2 and tested with it, but can work for higher versions. spark-shell --packages io.lakefs:lakefs-spark-client-301_2.12:0.6.5 . Alternatively an assembled jar is available on S3, at s3://treeverse-clients-us-east/lakefs-spark-client-301/0.6.5/lakefs-spark-client-301-assembly-0.6.5.jar . This client is compiled for Spark 3.1.2 with Hadoop 3.2.1, but can work for other Spark versions and higher Hadoop versions. spark-shell --packages io.lakefs:lakefs-spark-client-312-hadoop3-assembly_2.12:0.6.5 . Alternatively an assembled jar is available on S3, at s3://treeverse-clients-us-east/lakefs-spark-client-312-hadoop3/0.6.5/lakefs-spark-client-312-hadoop3-assembly-0.6.5.jar . ",
    "url": "/v0.98/reference/spark-client.html#getting-started",
    
    "relUrl": "/reference/spark-client.html#getting-started"
  },"370": {
    "doc": "Spark Client",
    "title": "Configuration",
    "content": ". | To read metadata from lakeFS, the client should be configured with your lakeFS endpoint and credentials, using the following Hadoop configurations: . | Configuration | Description | . | spark.hadoop.lakefs.api.url | lakeFS API endpoint, e.g: http://lakefs.example.com/api/v1 | . | spark.hadoop.lakefs.api.access_key | The access key to use for fetching metadata from lakeFS | . | spark.hadoop.lakefs.api.secret_key | Corresponding lakeFS secret key | . | The client will also directly interact with your storage using Hadoop FileSystem. Therefore, your Spark session must be able to access the underlying storage of your lakeFS repository. There are various ways to do this, but for a non-production environment you can use the following Hadoop configurations: . | Configuration | Description | . | spark.hadoop.fs.s3a.access.key | Access key to use for accessing underlying storage on S3 | . | spark.hadoop.fs.s3a.secret.key | Corresponding secret key to use with S3 access key | . Assuming role on S3 (Hadoop 3 only) . The client includes support for assuming a separate role on S3 when running on Hadoop 3. It uses the same configuration used by S3AFileSystem to assume the role on S3A. Apache Hadoop AWS documentation has details under “Working with IAM Assumed Roles”. You will need to use the following Hadoop configurations: . | Configuration | Description | . | fs.s3a.aws.credentials.provider | Set to org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider | . | fs.s3a.assumed.role.arn | Set to the ARN of the role to assume | . | . ",
    "url": "/v0.98/reference/spark-client.html#configuration",
    
    "relUrl": "/reference/spark-client.html#configuration"
  },"371": {
    "doc": "Spark Client",
    "title": "Examples",
    "content": ". | Get a DataFrame for listing all objects in a commit: . import io.treeverse.clients.LakeFSContext val commitID = \"a1b2c3d4\" val df = LakeFSContext.newDF(spark, \"example-repo\", commitID) df.show /* output example: +------------+--------------------+--------------------+-------------------+----+ | key | address| etag| last_modified|size| +------------+--------------------+--------------------+-------------------+----+ | file_1 |791457df80a0465a8...|7b90878a7c9be5a27...|2021-03-05 11:23:30| 36| file_2 |e15be8f6e2a74c329...|95bee987e9504e2c3...|2021-03-05 11:45:25| 36| file_3 |f6089c25029240578...|32e2f296cb3867d57...|2021-03-07 13:43:19| 36| file_4 |bef38ef97883445c8...|e920efe2bc220ffbb...|2021-03-07 13:43:11| 13| +------------+--------------------+--------------------+-------------------+----+ */ . | Run SQL queries on your metadata: . df.createOrReplaceTempView(\"files\") spark.sql(\"SELECT DATE(last_modified), COUNT(*) FROM files GROUP BY 1 ORDER BY 1\") /* output example: +----------+--------+ | dt|count(1)| +----------+--------+ |2021-03-05| 2|2021-03-07| 2| +----------+--------+ */ . | . ",
    "url": "/v0.98/reference/spark-client.html#examples",
    
    "relUrl": "/reference/spark-client.html#examples"
  },"372": {
    "doc": "Spark Client",
    "title": "Spark Client",
    "content": " ",
    "url": "/v0.98/reference/spark-client.html",
    
    "relUrl": "/reference/spark-client.html"
  },"373": {
    "doc": "Spark",
    "title": "Using lakeFS with Spark",
    "content": " ",
    "url": "/v0.98/integrations/spark.html#using-lakefs-with-spark",
    
    "relUrl": "/integrations/spark.html#using-lakefs-with-spark"
  },"374": {
    "doc": "Spark",
    "title": "Ways to use lakeFS with Spark",
    "content": ". | The S3-compatible API: Scalable and best to get started. All Storage Vendors | The lakeFS FileSystem: Direct data flow from client to storage, highly scalable. AWS S3 . | lakeFS FileSystem in Presigned mode: Best of both worlds, but still in beta. AWS S3Azure Blob | . | . ",
    "url": "/v0.98/integrations/spark.html#ways-to-use-lakefs-with-spark",
    
    "relUrl": "/integrations/spark.html#ways-to-use-lakefs-with-spark"
  },"375": {
    "doc": "Spark",
    "title": "Use the S3-compatible API",
    "content": "lakeFS has an S3-compatible endpoint. Simply point Spark to this endpoint to get started quickly. You will access your data using S3-style URIs, e.g. s3a://example-repo/example-branch/example-table. You can use the S3-compatible API regardless of where your data is hosted. Configuration . To configure Spark to work with lakeFS, we set S3A Hadoop configuration to the lakeFS endpoint and credentials: . | fs.s3a.access.key: lakeFS access key | fs.s3a.secret.key: lakeFS secret key | fs.s3a.endpoint: lakeFS S3-compatible API endpoint (e.g. https://example-org.us-east-1.lakefscloud.io) | fs.s3a.path.style.access: true | . Here is how to do it: . | CLI | Scala | XML Configuration | EMR | . spark-shell --conf spark.hadoop.fs.s3a.access.key='AKIAlakefs12345EXAMPLE' \\ --conf spark.hadoop.fs.s3a.secret.key='abc/lakefs/1234567bPxRfiCYEXAMPLEKEY' \\ --conf spark.hadoop.fs.s3a.path.style.access=true \\ --conf spark.hadoop.fs.s3a.endpoint='https://example-org.us-east-1.lakefscloud.io' ... spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", \"AKIAlakefs12345EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.endpoint\", \"https://example-org.us-east-1.lakefscloud.io\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.path.style.access\", \"true\") . Add these into a configuration file, e.g. $SPARK_HOME/conf/hdfs-site.xml: . &lt;?xml version=\"1.0\"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.s3a.access.key&lt;/name&gt; &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.secret.key&lt;/name&gt; &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;https://example-org.us-east-1.lakefscloud.io&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . Use the below configuration when creating the cluster. You may delete any app configuration that is not suitable for your use case: . [ { \"Classification\": \"spark-defaults\", \"Properties\": { \"spark.sql.catalogImplementation\": \"hive\" } }, { \"Classification\": \"core-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3.path.style.access\": \"true\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3a.path.style.access\": \"true\" } }, { \"Classification\": \"emrfs-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3.path.style.access\": \"true\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3a.path.style.access\": \"true\" } }, { \"Classification\": \"presto-connector-hive\", \"Properties\": { \"hive.s3.aws-access-key\": \"AKIAIOSFODNN7EXAMPLE\", \"hive.s3.aws-secret-key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"hive.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"hive.s3.path-style-access\": \"true\", \"hive.s3-file-system-type\": \"PRESTO\" } }, { \"Classification\": \"hive-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3.path.style.access\": \"true\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3a.path.style.access\": \"true\" } }, { \"Classification\": \"hdfs-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3.path.style.access\": \"true\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3a.path.style.access\": \"true\" } }, { \"Classification\": \"mapred-site\", \"Properties\": { \"fs.s3.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3.path.style.access\": \"true\", \"fs.s3a.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3a.path.style.access\": \"true\" } } ] . Alternatively, you can pass these configuration values when adding a step. For example: . aws emr add-steps --cluster-id j-197B3AEGQ9XE4 \\ --steps=\"Type=Spark,Name=SparkApplication,ActionOnFailure=CONTINUE, \\ Args=[--conf,spark.hadoop.fs.s3a.access.key=AKIAIOSFODNN7EXAMPLE, \\ --conf,spark.hadoop.fs.s3a.secret.key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY, \\ --conf,spark.hadoop.fs.s3a.endpoint=https://example-org.us-east-1.lakefscloud.io, \\ --conf,spark.hadoop.fs.s3a.path.style.access=true, \\ s3a://&lt;lakefs-repo&gt;/&lt;lakefs-branch&gt;/path/to/jar]\" . Per-bucket configuration . The above configuration will use lakeFS as the sole S3 endpoint. To use lakeFS in parallel with S3, you can configure Spark to use lakeFS only for specific bucket names. For example, to configure only example-repo to use lakeFS, set the following configurations: . | CLI | Scala | XML Configuration | EMR | . spark-shell --conf spark.hadoop.fs.s3a.bucket.example-repo.access.key='AKIAlakefs12345EXAMPLE' \\ --conf spark.hadoop.fs.s3a.bucket.example-repo.secret.key='abc/lakefs/1234567bPxRfiCYEXAMPLEKEY' \\ --conf spark.hadoop.fs.s3a.bucket.example-repo.endpoint='https://example-org.us-east-1.lakefscloud.io' \\ --conf spark.hadoop.fs.s3a.path.style.access=true . spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.access.key\", \"AKIAlakefs12345EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.bucket.example-repo.endpoint\", \"https://example-org.us-east-1.lakefscloud.io\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.path.style.access\", \"true\") . Add these into a configuration file, e.g. $SPARK_HOME/conf/hdfs-site.xml: . &lt;?xml version=\"1.0\"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.s3a.bucket.example-repo.access.key&lt;/name&gt; &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.bucket.example-repo.secret.key&lt;/name&gt; &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.bucket.example-repo.endpoint&lt;/name&gt; &lt;value&gt;https://example-org.us-east-1.lakefscloud.io&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . Use the below configuration when creating the cluster. You may delete any app configuration that is not suitable for your use case: . [ { \"Classification\": \"spark-defaults\", \"Properties\": { \"spark.sql.catalogImplementation\": \"hive\" } }, { \"Classification\": \"core-site\", \"Properties\": { \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3.bucket.example-repo.path.style.access\": \"true\", \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3a.bucket.example-repo.path.style.access\": \"true\" } }, { \"Classification\": \"emrfs-site\", \"Properties\": { \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3.bucket.example-repo.path.style.access\": \"true\", \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3a.bucket.example-repo.path.style.access\": \"true\" } }, { \"Classification\": \"presto-connector-hive\", \"Properties\": { \"hive.s3.aws-access-key\": \"AKIAIOSFODNN7EXAMPLE\", \"hive.s3.aws-secret-key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"hive.s3.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"hive.s3.path-style-access\": \"true\", \"hive.s3-file-system-type\": \"PRESTO\" } }, { \"Classification\": \"hive-site\", \"Properties\": { \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3.bucket.example-repo.path.style.access\": \"true\", \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3a.bucket.example-repo.path.style.access\": \"true\" } }, { \"Classification\": \"hdfs-site\", \"Properties\": { \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3.bucket.example-repo.path.style.access\": \"true\", \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3a.bucket.example-repo.path.style.access\": \"true\" } }, { \"Classification\": \"mapred-site\", \"Properties\": { \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3.bucket.example-repo.path.style.access\": \"true\", \"fs.s3a.bucket.example-repo.access.key\": \"AKIAIOSFODNN7EXAMPLE\", \"fs.s3a.bucket.example-repo.secret.key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\", \"fs.s3a.bucket.example-repo.endpoint\": \"https://example-org.us-east-1.lakefscloud.io\", \"fs.s3a.bucket.example-repo.path.style.access\": \"true\" } } ] . Alternatively, you can pass these configuration values when adding a step. For example: . aws emr add-steps --cluster-id j-197B3AEGQ9XE4 \\ --steps=\"Type=Spark,Name=SparkApplication,ActionOnFailure=CONTINUE, \\ Args=[--conf,spark.hadoop.fs.s3a.bucket.example-repo.access.key=AKIAIOSFODNN7EXAMPLE, \\ --conf,spark.hadoop.fs.s3a.bucket.example-repo.secret.key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY, \\ --conf,spark.hadoop.fs.s3a.bucket.example-repo.endpoint=https://example-org.us-east-1.lakefscloud.io, \\ --conf,spark.hadoop.fs.s3a.path.style.access=true, \\ s3a://&lt;lakefs-repo&gt;/&lt;lakefs-branch&gt;/path/to/jar]\" . With this configuration set, you read S3A paths with example-repo as the bucket will use lakeFS, while all other buckets will use AWS S3. Usage . Here’s an example for reading a Parquet file from lakeFS to a Spark DataFrame: . val repo = \"example-repo\" val branch = \"main\" val df = spark.read.parquet(s\"s3a://${repo}/${branch}/example-path/example-file.parquet\") . Here’s how to write some results back to a lakeFS path: . df.write.partitionBy(\"example-column\").parquet(s\"s3a://${repo}/${branch}/output-path/\") . The data is now created in lakeFS as new changes in your branch. You can now commit these changes or revert them. Configuring Azure Databricks with the S3-compatible API . If you use Azure Databricks, you can take advantage of the lakeFS S3-compatible API with your Azure account and the S3A FileSystem. This will require installing the hadoop-aws package (with the same version as your hadoop-azure package) to your Databricks cluster. Define your FileSystem configurations in the following way: . spark.hadoop.fs.lakefs.impl=org.apache.hadoop.fs.s3a.S3AFileSystem spark.hadoop.fs.lakefs.access.key=‘AKIAlakefs12345EXAMPLE’ // The access key to your lakeFS server spark.hadoop.fs.lakefs.secret.key=‘abc/lakefs/1234567bPxRfiCYEXAMPLEKEY’ // The secret key to your lakeFS server spark.hadoop.fs.lakefs.path.style.access=true spark.hadoop.fs.lakefs.endpoint=‘https://example-org.us-east-1.lakefscloud.io’ // The endpoint of your lakeFS server . For more details about Mounting cloud object storage on Databricks. ",
    "url": "/v0.98/integrations/spark.html#use-the-s3-compatible-api",
    
    "relUrl": "/integrations/spark.html#use-the-s3-compatible-api"
  },"376": {
    "doc": "Spark",
    "title": "Use the lakeFS Hadoop FileSystem",
    "content": "If you’re using lakeFS on top of S3, this mode will enhance your application’s performance. In this mode, Spark will read and write objects directly from S3, reducing the load on the lakeFS server. It will still access the lakeFS server for metadata operations. After configuring the lakeFS Hadoop FileSystem below, use URIs of the form lakefs://example-repo/ref/path/to/data to interact with your data on lakeFS. Installation . | Spark Standalone | Databricks | . Add the package to your spark-submit command: . --packages io.lakefs:hadoop-lakefs-assembly:0.1.13 . In your cluster settings, under the Libraries tab, add the following Maven package: . io.lakefs:hadoop-lakefs-assembly:0.1.13 . Once installed, it should look something like this: . Configuration . Set the fs.lakefs.* Hadoop configurations to point to your lakeFS installation: . | fs.lakefs.impl: io.lakefs.LakeFSFileSystem | fs.lakefs.access.key: lakeFS access key | fs.lakefs.secret.key: lakeFS secret key | fs.lakefs.endpoint: lakeFS API URL (e.g. https://example-org.us-east-1.lakefscloud.io/api/v1) | . Configure the S3A FileSystem to access your S3 storage, for example using the fs.s3a.* configurations (these are not your lakeFS credentials): . | fs.s3a.access.key: AWS S3 access key | fs.s3a.secret.key: AWS S3 secret key | . Here are some configuration examples: . | CLI | Scala | PySpark | XML Configuration | Databricks | . spark-shell --conf spark.hadoop.fs.s3a.access.key='AKIAIOSFODNN7EXAMPLE' \\ --conf spark.hadoop.fs.s3a.secret.key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' \\ --conf spark.hadoop.fs.s3a.endpoint='https://s3.eu-central-1.amazonaws.com' \\ --conf spark.hadoop.fs.lakefs.impl=io.lakefs.LakeFSFileSystem \\ --conf spark.hadoop.fs.lakefs.access.key=AKIAlakefs12345EXAMPLE \\ --conf spark.hadoop.fs.lakefs.secret.key=abc/lakefs/1234567bPxRfiCYEXAMPLEKEY \\ --conf spark.hadoop.fs.lakefs.endpoint=https://example-org.us-east-1.lakefscloud.io/api/v1 \\ --packages io.lakefs:hadoop-lakefs-assembly:0.1.13 \\ io.example.ExampleClass . spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", \"AKIAIOSFODNN7EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.endpoint\", \"https://s3.eu-central-1.amazonaws.com\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.access.key\", \"AKIAlakefs12345EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.endpoint\", \"https://example-org.us-east-1.lakefscloud.io/api/v1\") . sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIAIOSFODNN7EXAMPLE\") sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\") sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"https://s3.eu-central-1.amazonaws.com\") sc._jsc.hadoopConfiguration().set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\") sc._jsc.hadoopConfiguration().set(\"fs.lakefs.access.key\", \"AKIAlakefs12345EXAMPLE\") sc._jsc.hadoopConfiguration().set(\"fs.lakefs.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") sc._jsc.hadoopConfiguration().set(\"fs.lakefs.endpoint\", \"https://example-org.us-east-1.lakefscloud.io/api/v1\") . Make sure that you load the lakeFS FileSystem into Spark by running it with --packages or --jars, and then add these into a configuration file, e.g., $SPARK_HOME/conf/hdfs-site.xml: . &lt;?xml version=\"1.0\"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.s3a.access.key&lt;/name&gt; &lt;value&gt;AKIAIOSFODNN7EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.secret.key&lt;/name&gt; &lt;value&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;https://s3.eu-central-1.amazonaws.com&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.impl&lt;/name&gt; &lt;value&gt;io.lakefs.LakeFSFileSystem&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.access.key&lt;/name&gt; &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.secret.key&lt;/name&gt; &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.endpoint&lt;/name&gt; &lt;value&gt;https://example-org.us-east-1.lakefscloud.io/api/v1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . Add the following the cluster’s configuration under Configuration ➡️ Advanced options: . spark.hadoop.fs.lakefs.impl io.lakefs.LakeFSFileSystem spark.hadoop.fs.lakefs.access.key AKIAlakefs12345EXAMPLE spark.hadoop.fs.lakefs.secret.key abc/lakefs/1234567bPxRfiCYEXAMPLEKEY spark.hadoop.fs.s3a.access.key AKIAIOSFODNN7EXAMPLE spark.hadoop.fs.s3a.secret.key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY spark.hadoop.fs.s3a.impl shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem spark.hadoop.fs.lakefs.endpoint https://example-org.us-east-1.lakefscloud.io/api/v1 . Alternatively, follow this step by step Databricks integration tutorial, including lakeFS Hadoop File System, Python client and lakeFS SPARK client. ⚠️ If your bucket is on a region other than us-east-1, you may also need to configure fs.s3a.endpoint with the correct region. Amazon provides S3 endpoints you can use. Usage . Hadoop FileSystem paths use the lakefs:// protocol, with paths taking the form lakefs://&lt;repository&gt;/&lt;ref&gt;/path/to/object. &lt;ref&gt; can be a branch, tag, or commit ID in lakeFS. Here’s an example for reading a Parquet file from lakeFS to a Spark DataFrame: . val repo = \"example-repo\" val branch = \"main\" val df = spark.read.parquet(s\"lakefs://${repo}/${branch}/example-path/example-file.parquet\") . Here’s how to write some results back to a lakeFS path: . df.write.partitionBy(\"example-column\").parquet(s\"lakefs://${repo}/${branch}/output-path/\") . The data is now created in lakeFS as new changes in your branch. You can now commit these changes or revert them. Hadoop FileSystem in Presigned mode BETA . Available starting version 0.1.13 of the FileSystem . In this mode, the lakeFS server is responsible for authenticating with your storage. The client will still perform data operations directly on the storage. To do so, it will use pre-signed storage URLs provided by the lakeFS server. When using this mode, you don’t need to configure the client with access to your storage: . | CLI | Scala | PySpark | XML Configuration | Databricks | . spark-shell --conf spark.hadoop.fs.access.mode=presigned \\ --conf spark.hadoop.fs.lakefs.impl=io.lakefs.LakeFSFileSystem \\ --conf spark.hadoop.fs.lakefs.access.key=AKIAlakefs12345EXAMPLE \\ --conf spark.hadoop.fs.lakefs.secret.key=abc/lakefs/1234567bPxRfiCYEXAMPLEKEY \\ --conf spark.hadoop.fs.lakefs.endpoint=https://example-org.us-east-1.lakefscloud.io/api/v1 \\ --packages io.lakefs:hadoop-lakefs-assembly:0.1.13 \\ io.example.ExampleClass . spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.access.mode\", \"presigned\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.access.key\", \"AKIAlakefs12345EXAMPLE\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") spark.sparkContext.hadoopConfiguration.set(\"fs.lakefs.endpoint\", \"https://example-org.us-east-1.lakefscloud.io/api/v1\") . sc._jsc.hadoopConfiguration().set(\"fs.lakefs.access.mode\", \"presigned\") sc._jsc.hadoopConfiguration().set(\"fs.lakefs.impl\", \"io.lakefs.LakeFSFileSystem\") sc._jsc.hadoopConfiguration().set(\"fs.lakefs.access.key\", \"AKIAlakefs12345EXAMPLE\") sc._jsc.hadoopConfiguration().set(\"fs.lakefs.secret.key\", \"abc/lakefs/1234567bPxRfiCYEXAMPLEKEY\") sc._jsc.hadoopConfiguration().set(\"fs.lakefs.endpoint\", \"https://example-org.us-east-1.lakefscloud.io/api/v1\") . Make sure that you load the lakeFS FileSystem into Spark by running it with --packages or --jars, and then add these into a configuration file, e.g., $SPARK_HOME/conf/hdfs-site.xml: . &lt;?xml version=\"1.0\"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.access.mode&lt;/name&gt; &lt;value&gt;presigned&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.impl&lt;/name&gt; &lt;value&gt;io.lakefs.LakeFSFileSystem&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.access.key&lt;/name&gt; &lt;value&gt;AKIAlakefs12345EXAMPLE&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.secret.key&lt;/name&gt; &lt;value&gt;abc/lakefs/1234567bPxRfiCYEXAMPLEKEY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.lakefs.endpoint&lt;/name&gt; &lt;value&gt;https://example-org.us-east-1.lakefscloud.io/api/v1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . Add the following the cluster’s configuration under Configuration ➡️ Advanced options: . spark.hadoop.fs.access.mode presigned spark.hadoop.fs.lakefs.impl io.lakefs.LakeFSFileSystem spark.hadoop.fs.lakefs.access.key AKIAlakefs12345EXAMPLE spark.hadoop.fs.lakefs.secret.key abc/lakefs/1234567bPxRfiCYEXAMPLEKEY spark.hadoop.fs.lakefs.endpoint https://example-org.us-east-1.lakefscloud.io/api/v1 . ",
    "url": "/v0.98/integrations/spark.html#use-the-lakefs-hadoop-filesystem",
    
    "relUrl": "/integrations/spark.html#use-the-lakefs-hadoop-filesystem"
  },"377": {
    "doc": "Spark",
    "title": "Case Study: SimilarWeb",
    "content": "See how SimilarWeb is using lakeFS with Spark to manage algorithm changes in data pipelines. ",
    "url": "/v0.98/integrations/spark.html#case-study-similarweb",
    
    "relUrl": "/integrations/spark.html#case-study-similarweb"
  },"378": {
    "doc": "Spark",
    "title": "Spark",
    "content": " ",
    "url": "/v0.98/integrations/spark.html",
    
    "relUrl": "/integrations/spark.html"
  },"379": {
    "doc": "Single Sign On (SSO) in lakeFS Enterprise",
    "title": "Single Sign On (SSO) in lakeFS Enterprise",
    "content": "lakeFS Enterprise . SSO is also available on lakeFS Cloud. Using the open-source version of lakeFS? Read more on authentication. Authentication in lakeFS Enterprise is handled by a secondary service which runs side-by-side with lakeFS. With a nod to Hogwarts and their security system, we’ve named this service Fluffy. Details for configuring the supported identity providers with Fluffy are shown below. In addition, please review the necessary Helm configuration to configure Fluffy. | Active Directory Federation Services (AD FS) (using SAML) | OpenID Connect | LDAP | . If you’re using an authentication provider that is not listed please contact us for further assistance. | AD FS | OpenID Connect | LDAP | . ",
    "url": "/v0.98/enterprise/sso.html",
    
    "relUrl": "/enterprise/sso.html"
  },"380": {
    "doc": "Single Sign On (SSO) in lakeFS Enterprise",
    "title": "Active Directory Federation Services (AD FS) (using SAML)",
    "content": "AD FS integration uses certificates to sign &amp; encrypt requests going out from Fluffy and decrypt incoming requests from AD FS server. In order for Fluffy to work, the following values must be configured. Update (or override) the following attributes in the chart’s values.yaml file. | Replace fluffy.saml_rsa_public_cert and fluffy.saml_rsa_private_key with real certificate values | Replace fluffyConfig.auth.saml.idp_metadata_url with the metadata URL of the AD FS provider (e.g adfs-auth.company.com) | Replace fluffyConfig.auth.saml.external_user_id_claim_name with the claim name representing user id name in AD FS | Replace lakefs.company.com with your lakeFS server URL. | . If you’d like to generate the certificates using OpenSSL, you can take a look at the following example: . openssl req -x509 -newkey rsa:2048 -keyout myservice.key -out myservice.cert -days 365 -nodes -subj \"/CN=lakefs.company.com\" - . lakeFS Server Configuration (Update in helm’s values.yaml file): . lakefsConfig: | # Important: make sure to include the rest of your lakeFS Configuration here! auth: cookie_auth_verification: auth_source: saml friendly_name_claim_name: displayName external_user_id_claim_name: samName default_initial_groups: - \"Developers\" logout_redirect_url: \"https://lakefs.company.com/logout-saml\" encrypt: secret_key: shared-secrey-key ui_config: login_url: \"https://lakefs.company.com/sso/login-saml\" logout_url: \"https://lakefs.company.com/sso/logout-saml\" login_cookie_names: - internal_auth_session - saml_auth_session rbac: external . Fluffy Configuration (Update in helm’s values.yaml file): . fluffyConfig: &amp;fluffyConfig | logging: format: \"json\" level: \"INFO\" audit_log_level: \"INFO\" output: \"=\" installation: fixed_id: fluffy-authenticator auth: # better set from secret FLUFFY_AUTH_ENCRYPT_SECRET_KEY must be equal to what is used in lakeFS for auth_encrypt_secret_key encrypt: secret_key: shared-secrey-key logout_redirect_url: https://lakefs.company.com post_login_redirect_url: https://lakefs.company.com saml: enabled: true sp_root_url: https://lakefs.company.com sp_x509_key_path: '/etc/saml_certs/rsa_saml_private.cert' sp_x509_cert_path: '/etc/saml_certs/rsa_saml_public.pem' sp_sign_request: true sp_signature_method: \"http://www.w3.org/2001/04/xmldsig-more#rsa-sha256\" idp_metadata_url: \"https://adfs-auth.company.com/federationmetadata/2007-06/federationmetadata.xml\" # idp_authn_name_id_format: \"urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified\" external_user_id_claim_name: samName # idp_metadata_file_path: # idp_skip_verify_tls_cert: true . ",
    "url": "/v0.98/enterprise/sso.html#active-directory-federation-services-ad-fs-using-saml",
    
    "relUrl": "/enterprise/sso.html#active-directory-federation-services-ad-fs-using-saml"
  },"381": {
    "doc": "Single Sign On (SSO) in lakeFS Enterprise",
    "title": "OpenID Connect",
    "content": "In order for Fluffy to work, the following values must be configured. Update (or override) the following attributes in the chart’s values.yaml file. | Replace lakefsConfig.friendly_name_claim_name and fluffyConfig.friendly_name_claim_name with the right claim name | Replace fluffyConfig.auth.logout_redirect_url with your full OIDC logout URL (e.g https://oidc-provider-url.com/logout/path) | Replace fluffyConfig.auth.oidc.url with your OIDC provider URL (e.g https://oidc-provider-url.com) | Replace fluffyConfig.auth.oidc.logout_endpoint_query_parameters with parameters you’d like to pass to theOIDC provider for logout. | Replace fluffyConfig.auth.oidc.client_id and fluffyConfig.auth.oidc.client_secret with the client ID &amp; secret for OIDC. | Replace fluffyConfig.auth.oidc.logout_client_id_query_parameter with the query parameter that represent the client_id, note that it should match the the key/query param that represents the client id and required by the specific OIDC provider. | Replace lakefs.company.com with the lakeFS server URL. | . lakeFS Server Configuration (Update in helm’s values.yaml file): . lakefsConfig: | # Important: make sure to include the rest of your lakeFS Configuration here! auth: encrypt: secret_key: shared-secrey-key oidc: friendly_name_claim_name: \"name\" default_initial_groups: [\"Developers\"] ui_config: login_url: /oidc/login logout_url: /oidc/logout login_cookie_names: - internal_auth_session - oidc_auth_session rbac: external . Fluffy Configuration (Update in helm’s values.yaml file): . fluffyConfig: &amp;fluffyConfig | logging: format: \"json\" level: \"INFO\" audit_log_level: \"INFO\" output: \"=\" installation: fixed_id: fluffy-authenticator auth: post_login_redirect_url: / logout_redirect_url: https://oidc-provider-url.com/logout/url oidc: enabled: true url: https://oidc-provider-url.com/ client_id: &lt;oidc-client-id&gt; client_secret: &lt;oidc-client-secret&gt; callback_base_url: https://lakefs.company.com is_default_login: true default_initial_groups: [\"Developers\"] friendly_name_claim_name: \"name\" logout_client_id_query_parameter: client_id logout_endpoint_query_parameters: - returnTo - https://lakefs.company.com/oidc/login # better set from secret FLUFFY_AUTH_ENCRYPT_SECRET_KEY must be equal to what is used in lakeFS for auth_encrypt_secret_key # encrypt: # secret_key: shared-secrey-key . ",
    "url": "/v0.98/enterprise/sso.html#openid-connect",
    
    "relUrl": "/enterprise/sso.html#openid-connect"
  },"382": {
    "doc": "Single Sign On (SSO) in lakeFS Enterprise",
    "title": "LDAP",
    "content": "In order for Fluffy to work, the following values must be configured. Update (or override) the following attributes in the chart’s values.yaml file. | Replace lakefsConfig.auth.remote_authenticator.endpoint with the lakeFS server URL combined with the api/v1/ldap/login suffix (e.g http://lakefs.company.com/api/v1/ldap/login) | Repalce fluffyConfig.auth.ldap.remote_authenticator.server_endpoint with your LDAP server endpoint (e.g ldaps://ldap.ldap-address.com:636) | Replace fluffyConfig.auth.ldap.remote_authenticator.bind_dn with the LDAP bind user/permissions to query your LDAP server. | Replace fluffyConfig.auth.ldap.remote_authenticator.user_base_dn with the user base to search users in. | . lakeFS Server Configuration (Update in helm’s values.yaml file): . lakefsConfig: | # Important: make sure to include the rest of your lakeFS Configuration here! auth: remote_authenticator: enabled: true endpoint: https://lakefs.company.com/api/v1/ldap/login default_user_group: \"Developers\" ui_config: logout_url: /logout login_cookie_names: - internal_auth_session rbac: external . Fluffy Configuration (Update in helm’s values.yaml file): . fluffyConfig: &amp;fluffyConfig | logging: format: \"json\" level: \"INFO\" audit_log_level: \"INFO\" output: \"=\" installation: fixed_id: fluffy-authenticator auth: post_login_redirect_url: / ldap: server_endpoint: 'ldaps://ldap.company.com:636' bind_dn: uid=&lt;bind-user-name&gt;,ou=Users,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com bind_password: '&lt;ldap pwd&gt;' username_attribute: uid user_base_dn: ou=Users,o=&lt;org-id&gt;,dc=&lt;company&gt;,dc=com user_filter: (objectClass=inetOrgPerson) connection_timeout_seconds: 15 request_timeout_seconds: 7 . ",
    "url": "/v0.98/enterprise/sso.html#ldap",
    
    "relUrl": "/enterprise/sso.html#ldap"
  },"383": {
    "doc": "Single Sign On (SSO) in lakeFS Enterprise",
    "title": "Helm",
    "content": "In order to use lakeFS Enterprise and Fluffy, see lakeFS Helm chart configuration. Notes: . | secrets.authEncryptSecretKey is shared between fluffy &amp; lakeFS and must be equal | lakeFS image.tag must be &gt;= 0.95.0 | Change the fluffy.ingress.host from lakefs.company.com to a real host (usually same as lakeFS), also update additional references in the file (note: URL path after host if provided should stay unchanged). | Update the fluffy.ingress configuration with other optional fields if used | Fluffy docker image: replace the fluffy.image.secretToken with real token to dockerhub for the fluffy docker image. | . ############################################### ############################################### ############ COMMON CONFIGURATION ############# ############################################### ############################################### fluffy: serviceAccountName: default name: &amp;fluffyName lakefs-fluffy logLevel: INFO replicaCount: &amp;fluffyReplicaCount 1 resources: &amp;resources limits: memory: 256Mi requests: memory: 128Mi ingress: enabled: &amp;enableIngress true host: &amp;ingressHost lakefs.company.com ingressClassName: &amp;ingressClassName '' annotations: &amp;ingressAnnotations {} image: repository: 'treeverse/fluffy' tag: '0.1.1' secretName: 'fluffy-dockerhub' secretToken: &amp;fluffyImageSecretToken '&lt;docker token&gt;' saml_rsa_public_cert: | -----BEGIN CERTIFICATE----- -----END CERTIFICATE----- saml_rsa_private_key: | -----BEGIN PRIVATE KEY----- -----END PRIVATE KEY----- ############################################### ############################################### ###### ADVANCED DON'T EDIT VALUES BELOW ####### ############################################### ############################################### ingress: enabled: *enableIngress ingressClassName: *ingressClassName annotations: *ingressAnnotations hosts: - host: *ingressHost paths: - / pathsOverrides: # OIDC related overrides, fluffy and lakefs must be on the same host - path: /oidc/ serviceName: *fluffyName servicePort: 80 - path: /api/v1/oidc/ serviceName: *fluffyName servicePort: 80 # SAML related overrides, fluffy and lakefs must be on the same host - path: /saml/ serviceName: *fluffyName servicePort: 80 - path: /sso/ serviceName: *fluffyName servicePort: 80 # Optional if fluffy and lakefs both shariong the same ingress then this override - path: /api/v1/ldap/ serviceName: *fluffyName servicePort: 80 extraManifestsValues: fluffyContainerPort: &amp;fluffyContainerPort 8000 dockerhubConfig: auths: https://index.docker.io/v1/: username: externallakefs password: '{{ .Values.fluffy.image.secretToken }}' labels: &amp;labels helm.sh/chart: '{{ include \"lakefs.chart\" . }}' app: fluffy app.kubernetes.io/name: fluffy app.kubernetes.io/instance: '{{ .Release.Name }}' app.kubernetes.io/version: '{{ .Chart.AppVersion }}' app.kubernetes.io/managed-by: '{{ .Release.Service }}' selectorLabels: &amp;selectorLabels app: fluffy app.kubernetes.io/name: fluffy app.kubernetes.io/instance: '{{ .Release.Name }}' extraManifests: - apiVersion: v1 kind: Service metadata: name: '{{ .Values.fluffy.name }}' labels: *labels spec: type: '{{ .Values.service.type }}' ports: - port: 80 targetPort: http protocol: TCP name: http selector: *selectorLabels - apiVersion: apps/v1 kind: Deployment metadata: name: '{{ .Values.fluffy.name }}' labels: *labels spec: replicas: *fluffyReplicaCount selector: matchLabels: *selectorLabels template: metadata: annotations: checksum/config: '{{ .Values.fluffyConfig | sha256sum }}' labels: *selectorLabels spec: serviceAccountName: \"{{ .Values.fluffy.serviceAccountName }}\" imagePullSecrets: - name: \"{{ .Values.fluffy.image.secretName }}\" containers: - name: fluffy args: [\"run\"] image: \"{{ .Values.fluffy.image.repository }}:{{ .Values.fluffy.image.tag }}\" imagePullPolicy: '{{ .Values.image.pullPolicy }}' env: - name: FLUFFY_AUTH_ENCRYPT_SECRET_KEY valueFrom: secretKeyRef: name: '{{ include \"lakefs.fullname\" . }}' key: auth_encrypt_secret_key ports: - name: http containerPort: *fluffyContainerPort protocol: TCP resources: *resources volumeMounts: - name: fluffy-config mountPath: /etc/fluffy/ - name: secret-volume readOnly: true mountPath: \"/etc/saml_certs/\" volumes: - name: fluffy-config configMap: name: '{{ .Values.fluffy.name }}-config' - name: secret-volume secret: secretName: saml-certificates - apiVersion: v1 kind: ConfigMap metadata: name: '{{ .Values.fluffy.name }}-config' data: config.yaml: *fluffyConfig - apiVersion: v1 kind: Secret metadata: name: '{{ .Values.fluffy.image.secretName }}' data: .dockerconfigjson: '{{ tpl (.Values.extraManifestsValues.dockerhubConfig | toJson ) . | b64enc }}' type: kubernetes.io/dockerconfigjson - apiVersion: v1 kind: Secret metadata: name: saml-certificates data: rsa_saml_public.pem: '{{ .Values.fluffy.saml_rsa_public_cert | b64enc }}' rsa_saml_private.cert: '{{ .Values.fluffy.saml_rsa_private_key | b64enc }}' . ",
    "url": "/v0.98/enterprise/sso.html#helm",
    
    "relUrl": "/enterprise/sso.html#helm"
  },"384": {
    "doc": "Single Sign On (SSO) for lakeFS Cloud",
    "title": "Single Sign On (SSO) for lakeFS Cloud",
    "content": "lakeFS Cloud . SSO is also available for lakeFS Enterprise. Using the open-source version of lakeFS? Read more on authentication. lakeFS Cloud uses Auth0 for authentication and thus support the same identity providers as Auth0 including Active Directory/LDAP, ADFS, Azure Active Directory Native, Google Workspace, OpenID Connect, Okta, PingFederate, SAML, and Azure Active Directory. | Okta | AD FS | Azure AD | . ",
    "url": "/v0.98/cloud/sso.html",
    
    "relUrl": "/cloud/sso.html"
  },"385": {
    "doc": "Single Sign On (SSO) for lakeFS Cloud",
    "title": "Okta",
    "content": "This guide is based on Okta’s Create OIDC app integrations guide. Steps: . | Login to your Okta account | Select Applications &gt; Applications, then Create App Integration. | Select Create New App and enter the following: . | For Sign-in method, choose OIDC. | Under Application type, choose Web app. | Select Next. | . | Under General Settings: . | App integration name, enter a name for your application. (i.e lakeFS Cloud) | . | In the Sign-in redirect URIs field, enter https://lakefs-cloud.us.auth0.com/login (United States) or https://lakefs-cloud.eu.auth0.com/login (Europe). | Under Sign-in redirect URIs, click Add URI, enter https://lakefs-cloud.us.auth0.com/login/callback (United States) or https://lakefs-cloud.eu.auth0.com/login/callback (Europe). | Under Assignments, choose the wanted Controlled access. (i.e Allow everyone in your organization to access) | Uncheck Enable immediate access with Federation Broker Mode. | Select Save. | . Once you finish registering your application with Okta, save the Client ID, Client Secret and your Okta Domain, send this to Treeverse’s team to finish the integration. ",
    "url": "/v0.98/cloud/sso.html#okta",
    
    "relUrl": "/cloud/sso.html#okta"
  },"386": {
    "doc": "Single Sign On (SSO) for lakeFS Cloud",
    "title": "Active Directory Federation Services (AD FS)",
    "content": "Prerequisites: . | Client’s AD FS server should be exposed publicly or to Auth0’s IP ranges (either directly or using Web Application Proxy) | . Steps: . | Connect to the AD FS server | Open AD FS’ PowerShell CLI as Administrator through the server manager | Execute the following: . (new-object Net.WebClient -property @{Encoding = [Text.Encoding]::UTF8}).DownloadString(\"https://raw.github.com/auth0/adfs-auth0/master/adfs.ps1\") | iex AddRelyingParty \"urn:auth0:lakefs-cloud\" \"https://lakefs-cloud.us.auth0.com/login/callback\" . Note: If your organization data is located in Europe, use lakefs-cloud.eu.auth0.com instead of lakefs-cloud.us.auth0.com. | . Once you finish registering lakeFS Cloud with AD FS, save the AD FS URL and send this to Treeverse’s team to finish the integration. ",
    "url": "/v0.98/cloud/sso.html#active-directory-federation-services-ad-fs",
    
    "relUrl": "/cloud/sso.html#active-directory-federation-services-ad-fs"
  },"387": {
    "doc": "Single Sign On (SSO) for lakeFS Cloud",
    "title": "Azure Active Directory (AD)",
    "content": "Prerequisites: . | Azure account with permissions to manage applications in Azure Active Directory | . Note: If you’ve already set uplakeFS Cloud with your Azure account, you can skip the Register lakeFS Cloud with Azure and Add client secret and go directly to Add a redirect URI. Register lakeFS Cloud with Azure . Steps: . | Sign in to the Azure portal. | If you have access to multiple tenants, use the Directories + subscriptions filter in the top menu to switch to the tenant in which you want to register the application. | Search for and select Azure Active Directory. | Under Manage, select App registrations &gt; New registration. | Enter a display Name for your application. Users of your application might see the display name when they use the app, for example during sign-in. You can change the display name at any time and multiple app registrations can share the same name. The app registration’s automatically generated Application (client) ID, not its display name, uniquely identifies your app within the identity platform. | Specify who can use the application, sometimes called its sign-in audience. Note: don’t enter anything for Redirect URI (optional). You’ll configure a redirect URI in the next section. | Select Register to complete the initial app registration. | . When registration finishes, the Azure portal displays the app registration’s Overview pane. You see the Application (client) ID. Also called the client ID, this value uniquely identifies your application in the Microsoft identity platform. Important: new app registrations are hidden to users by default. When you are ready for users to see the app on their My Apps page you can enable it. To enable the app, in the Azure portal navigate to Azure Active Directory &gt; Enterprise applications and select the app. Then on the Properties page toggle Visible to users? to Yes. Add a secret . Sometimes called an application password, a client secret is a string value your app can use in place of a certificate to identity itself. Client secrets are considered less secure than certificate credentials. Application developers sometimes use client secrets during local app development because of their ease of use. However, you should use certificate credentials for any of your applications that are running in production. Steps: . | In the Azure portal, in App registrations, select your application. | Select Certificates &amp; secrets &gt; Client secrets &gt; New client secret. | Add a description for your client secret. | Select an expiration for the secret or specify a custom lifetime. | Client secret lifetime is limited to two years (24 months) or less. You can’t specify a custom lifetime longer than 24 months. | Microsoft recommends that you set an expiration value of less than 12 months. | . | Select Add. | Record the secret’s value for use in your client application code. This secret value is never displayed again after you leave this page. | . Add a redirect URI . A redirect URI is the location where the Microsoft identity platform redirects a user’s client and sends security tokens after authentication. You add and modify redirect URIs for your registered applications by configuring their platform settings. Enter https://lakefs-cloud.us.auth0.com/login/callback or https://lakefs-cloud.eu.auth0.com/login/callback (depends on your organization data location) as your redirect URI. Settings for each application type, including redirect URIs, are configured in Platform configurations in the Azure portal. Some platforms, like Web and Single-page applications, require you to manually specify a redirect URI. For other platforms, like mobile and desktop, you can select from redirect URIs generated for you when you configure their other settings. Steps: . | In the Azure portal, in App registrations, select your application. | Under Manage, select Authentication. | Under Platform configurations, select Add a platform. | Under Configure platforms, select the web option. | Select Configure to complete the platform configuration. | . Once you finish registering lakeFS Cloud with Azure AD, save the Application (Client) ID, Application Secret Value and send this to Treeverse’s team to finish the integration. ",
    "url": "/v0.98/cloud/sso.html#azure-active-directory-ad",
    
    "relUrl": "/cloud/sso.html#azure-active-directory-ad"
  },"388": {
    "doc": "Upgrade lakeFS",
    "title": "Upgrading lakeFS",
    "content": "Note: For a fully managed lakeFS service with guaranteed SLAs, try lakeFS cloud . Upgrading lakeFS from a previous version usually just requires re-deploying with the latest image (or downloading the latest version if you’re using the binary). If you’re upgrading, check whether the release requires a migration. ",
    "url": "/v0.98/howto/upgrade.html#upgrading-lakefs",
    
    "relUrl": "/howto/upgrade.html#upgrading-lakefs"
  },"389": {
    "doc": "Upgrade lakeFS",
    "title": "When DB migrations are required",
    "content": "lakeFS 0.80.0 or greater (KV Migration) . Starting with version 0.80.2, lakeFS has transitioned from using a PostgreSQL based database implementation to a Key-Value datastore interface supporting multiple database implementations. More information can be found here. Users upgrading from a previous version of lakeFS must pass through the KV migration version (0.80.2) before upgrading to newer versions of lakeFS. IMPORTANT: Pre Migrate Requirements . | Users using OS environment variables for database configuration must define the connection_string explicitly or as environment variable before proceeding with the migration. | Database storage free capacity of at least twice the amount of the currently used capacity | It is strongly recommended to perform these additional steps: . | Commit all uncommitted data on branches | Create a snapshot of your database | . | By default, old database tables are not being deleted by the migration process, and should be removed manually after a successful migration. To enable table drop as part of the migration, set the database.drop_tables configuration param to true | . Migration Steps . For each lakeFS instance currently running with the database . | Modify the database section under lakeFS configuration yaml: . | Add type field with \"postgres\" as value | Copy the current configuration parameters to a new section called postgres | . --- database: type: \"postgres\" connection_string: \"postgres://localhost:5432/postgres?sslmode=disable\" max_open_connections: 20 postgres: connection_string: \"postgres://localhost:5432/postgres?sslmode=disable\" max_open_connections: 20 . | Stop all lakeFS instances | Using the lakefs binary for the new version (0.80.2), run the following: . lakefs migrate up . | lakeFS will run the migration process, which in the end should display the following message with no errors: . time=\"2022-08-10T14:46:25Z\" level=info msg=\"KV Migration took 717.629563ms\" func=\"pkg/logging.(*logrusEntryWrapper).Infof\" file=\"build/pkg/logging/logger.go:246\" TempDir=/tmp/kv_migrate_2913402680 . | It is now possible to remove the old database configuration. The updated configuration should look as such: . --- database: type: \"postgres\" postgres: connection_string: \"postgres://localhost:5432/postgres?sslmode=disable\" max_open_connections: 20 . | Deploy (or run) the new version of lakeFS. | . lakeFS 0.30.0 or greater . In case migration is required, you first need to stop the running lakeFS service. Using the lakefs binary for the new version, run the following: . lakefs migrate up . Deploy (or run) the new version of lakeFS. Note that an older version of lakeFS cannot run on a migrated database. Prior to lakeFS 0.30.0 . Note: with lakeFS &lt; 0.30.0, you should first upgrade to 0.30.0 following this guide. Then, proceed to upgrade to the newest version. Starting version 0.30.0, lakeFS handles your committed metadata in a new way, which is more robust and has better performance. To move your existing data, you will need to run the following upgrade commands. Verify lakeFS version == 0.30.0 (can skip if using Docker) . lakefs --version . Migrate data from the previous format: . lakefs migrate db . Or migrate using Docker image: . docker run --rm -it -e LAKEFS_DATABASE_CONNECTION_STRING=&lt;database connection string&gt; treeverse/lakefs:rocks-migrate migrate db . Once migrated, it is possible to now use more recent lakeFS versions. Please refer to their release notes for more information on ugrading and usage). If you want to start over, discarding your existing data, you need to explicitly state this in your lakeFS configuration file. To do so, add the following to your configuration (relevant only for 0.30.0): . cataloger: type: rocks . ",
    "url": "/v0.98/howto/upgrade.html#when-db-migrations-are-required",
    
    "relUrl": "/howto/upgrade.html#when-db-migrations-are-required"
  },"390": {
    "doc": "Upgrade lakeFS",
    "title": "Data Migration for Version v0.50.0",
    "content": "We discovered a bug in the way lakeFS is storing objects in the underlying object store. It affects only repositories on Azure and GCP, and not all of them. Issue #2397 describes the repository storage namespaces patterns that are affected by this bug. When first upgrading to a version greater or equal to v0.50.0, you must follow these steps: . | Stop lakeFS. | Perform a data migration (details below) | Start lakeFS with the new version. | After a successful run of the new version and validation that the objects are accessible, you can delete the old data prefix. | . Note: Migrating data is a delicate procedure. The lakeFS team is here to help, reach out to us on Slack. We’ll be happy to walk you through the process. Data migration . The following patterns have been impacted by the bug: . | Type | Storage Namespace pattern | Copy From | Copy To | . | gs | gs://bucket/prefix | gs://bucket//prefix/* | gs://bucket/prefix/* | . | gs | gs://bucket/prefix/ | gs://bucket//prefix/* | gs://bucket/prefix/* | . | azure | https://account.blob.core.windows.net/containerid | https://account.blob.core.windows.net/containerid//* | https://account.blob.core.windows.net/containerid/* | . | azure | https://account.blob.core.windows.net/containerid/ | https://account.blob.core.windows.net/containerid//* | https://account.blob.core.windows.net/containerid/* | . | azure | https://account.blob.core.windows.net/containerid/prefix/ | https://account.blob.core.windows.net/containerid/prefix// | https://account.blob.core.windows.net/containerid/prefix/* | . You can find the repositories storage namespaces with: . lakectl repo list . Or the settings tab in the UI. Migrating Google Storage data with gsutil . gsutil is a Python application that lets you access Cloud Storage from the command line. We can use it for copying the data between the prefixes in the Google bucket, and later on removing it. For every affected repository, copy its data with: . gsutil -m cp -r gs://&lt;BUCKET&gt;//&lt;PREFIX&gt;/ gs://&lt;BUCKET&gt;/ . Note the double slash after the bucket name. Migrating Azure Blob Storage data with AzCopy . AzCopy is a command-line utility that you can use to copy blobs or files to or from a storage account. We can use it for copying the data between the prefixes in the Azure storage account container, and later on removing it. First, you need to acquire an Account SAS. Using the Azure CLI: . az storage container generate-sas \\ --account-name &lt;ACCOUNT&gt; \\ --name &lt;CONTAINER&gt; \\ --permissions cdrw \\ --auth-mode key \\ --expiry 2021-12-31 . With the resulted SAS, use AzCopy to copy the files. If a prefix exists after the container: . azcopy copy \\ \"https://&lt;ACCOUNT&gt;.blob.core.windows.net/&lt;CONTAINER&gt;/&lt;PREFIX&gt;//?&lt;SAS_TOKEN&gt;\" \\ \"https://&lt;ACCOUNT&gt;.blob.core.windows.net/&lt;CONTAINER&gt;?&lt;SAS_TOKEN&gt;\" \\ --recursive=true . Or when using the container without a prefix: . azcopy copy \\ \"https://&lt;ACCOUNT&gt;.blob.core.windows.net/&lt;CONTAINER&gt;//?&lt;SAS_TOKEN&gt;\" \\ \"https://&lt;ACCOUNT&gt;.blob.core.windows.net/&lt;CONTAINER&gt;/./?&lt;SAS_TOKEN&gt;\" \\ --recursive=true . ",
    "url": "/v0.98/howto/upgrade.html#data-migration-for-version-v0500",
    
    "relUrl": "/howto/upgrade.html#data-migration-for-version-v0500"
  },"391": {
    "doc": "Upgrade lakeFS",
    "title": "Upgrade lakeFS",
    "content": " ",
    "url": "/v0.98/howto/upgrade.html",
    
    "relUrl": "/howto/upgrade.html"
  },"392": {
    "doc": "Versioning Internals",
    "title": "Versioning Internals",
    "content": " ",
    "url": "/v0.98/understand/how/versioning-internals.html",
    
    "relUrl": "/understand/how/versioning-internals.html"
  },"393": {
    "doc": "Versioning Internals",
    "title": "Table of contents",
    "content": ". | Overview | SSTable File Format (“Graveler File”) | Constructing a consistent view of the keyspace (i.e., a commit) | Representing references and uncommitted metadata | . ",
    "url": "/v0.98/understand/how/versioning-internals.html#table-of-contents",
    
    "relUrl": "/understand/how/versioning-internals.html#table-of-contents"
  },"394": {
    "doc": "Versioning Internals",
    "title": "Overview",
    "content": "Since commits in lakeFS are immutable, they are easy to store on an immutable object store. Older commits are rarely accessed, while newer commits are accessed very frequently, a tiered storage approach can work very well - the object store is the source of truth, while local disk and even RAM can be used to cache the more frequently accessed ones. Since they are immutable - once cached, you only need to evict them when space is running out. There’s no complex invalidation that needs to happen. In terms of storage format, commits are be stored as SSTables, compatible with RocksDB. SSTables were chosen as a storage format for 3 major reasons: . | Extremely high read throughput on modern hardware: using commits representing a 200m object repository (modeled after the S3 inventory of one of our design partners), we were able to achieve close to 500k random GetObject calls / second. This provides a very high throughput/cost ratio, probably as high as can be achieved on public clouds. | Being a known storage format means it’s relatively easy to generate and consume. Storing it in the object store makes it accessible to data engineering tools for analysis and distributed computation, effectively reducing the silo effect of storing it in an operational database. | The SSTable format supports delta encoding for keys which makes them very space efficient for data lakes where many keys share the same common prefixes. | . Each lakeFS commit is represented as a set of contiguous, non-overlapping SSTables that make up the entire keyspace of a repository at that commit. ",
    "url": "/v0.98/understand/how/versioning-internals.html#overview",
    
    "relUrl": "/understand/how/versioning-internals.html#overview"
  },"395": {
    "doc": "Versioning Internals",
    "title": "SSTable File Format (“Graveler File”)",
    "content": "lakeFS metadata is encoded into a format called “Graveler” - a standardized way to encode content-addressable key value pairs. This is what a Graveler file looks like: . Each Key/Value pair (“ValueRecord”) is constructed of a key, identity, and value. A simple identity could be, for example, a sha256 hash of the value’s bytes. It could be any sequence of bytes that uniquely identifies the value. As far as the Graveler is concerned, two ValueRecords are considered identical if their key and identity fields are equal. A Graveler file itself is content-addressable, i.e., similarly to Git, the name of the file is its identity. File identity is calculated based on the identity of the ValueRecords the file contains: . valueRecordID = h(h(valueRecord.key) || h(valueRecord.Identity)) fileID = h(valueRecordID1 + … + valueRecordIDN) . ",
    "url": "/v0.98/understand/how/versioning-internals.html#sstable-file-format-graveler-file",
    
    "relUrl": "/understand/how/versioning-internals.html#sstable-file-format-graveler-file"
  },"396": {
    "doc": "Versioning Internals",
    "title": "Constructing a consistent view of the keyspace (i.e., a commit)",
    "content": "We have two additional requirements for the storage format: . | Be space and time efficient when creating a commit - assuming a commit changes a single object out of a billion, we don’t want to write a full snapshot of the entire repository. Ideally, we’ll be able to reuse some data files that haven’t changed to make the commit operations (in both space and time) proportional to the size of the difference as opposed to the total size of the repository. | Allow an efficient diff between commits which runs in time proportional to the size of their difference and not their absolute sizes. | . To support these requirements, we decided to essentially build a 2-layer Merkle tree composed of a set of leaf nodes (“Range”) addressed by their content address, and a “Meta Range”, which is a special range containing all ranges, thus representing an entire consistent view of the keyspace: . Assuming commit B is derived from commit A, and only changed files in range e-f, it can reuse all ranges except for SSTable #N (the one containing the modified range of keys), which will be recreated with a new hash representing the state as exists after applying commit B’s changes. This will, in turn, also create a new Metarange since its hash is now changed as well (as it is derived from the hash of all contained ranges). Assuming most commits usually change related objects (i.e., that are likely to share some common prefix), the reuse ratio could be very high. We tested this assumption using S3 inventory from 2 design partners - we partitioned the keyspace to an arbitrary number of simulated blocks and measured their change over time. We saw a daily change rate of about 5-20%. Given the size of the repositories, it’s safe to assume that a single day would translate into multiple commits. At a modest 20 commits per day, a commit is expected to reuse &gt;= 99% of the previous commit blocks, so acceptable in terms of write amplification generated on commit. On the object store, ranges are stored in the following hierarchy: . &lt;lakefs root&gt; _lakefs/ &lt;range hash1&gt; &lt;range hash2&gt; &lt;range hashN&gt; ... &lt;metarange hash1&gt; &lt;metarange hash2&gt; &lt;metarange hashN&gt; ... &lt;data object hash1&gt; &lt;data object hash2&gt; &lt;data object hashN&gt; ... Note: This relatively flat structure could be modified in the future. Looking at the diagram above, it imposes no real limitations on the depth of the tree. A tree could easily be made recursive by having Meta Ranges point to other Meta Ranges - and still provide all the same characteristics. For simplicity, we decided to start with a fixed 2-level hierarchy. ",
    "url": "/v0.98/understand/how/versioning-internals.html#constructing-a-consistent-view-of-the-keyspace-ie-a-commit",
    
    "relUrl": "/understand/how/versioning-internals.html#constructing-a-consistent-view-of-the-keyspace-ie-a-commit"
  },"397": {
    "doc": "Versioning Internals",
    "title": "Representing references and uncommitted metadata",
    "content": "lakeFS always stores the object data in the storage namespace in the user’s object store, committed and uncommitted data alike. However, the lakeFS object metadata might be stored in either the object store or a key-value store. Unlike committed metadata which is immutable, uncommitted (or “staged”) metadata experiences frequent random writes and is very mutable in nature. This is also true for “refs” - in particular, branches, which are simply pointers to an underlying commit, are modified frequently: on every commit or merge operation. Both these types of metadata are not only mutable, but also require strong consistency guarantees while also being fault tolerant. If we can’t access the current pointer of the main branch, a big portion of the system is essentially down. Luckily, this is also much smaller set of metadata compared to the committed metadata. References and uncommitted metadata are currently stored on a key-value store (See supported databases) for consistency guarantees. ",
    "url": "/v0.98/understand/how/versioning-internals.html#representing-references-and-uncommitted-metadata",
    
    "relUrl": "/understand/how/versioning-internals.html#representing-references-and-uncommitted-metadata"
  },"398": {
    "doc": "Webhooks",
    "title": "Webhooks",
    "content": " ",
    "url": "/v0.98/hooks/webhooks.html",
    
    "relUrl": "/hooks/webhooks.html"
  },"399": {
    "doc": "Webhooks",
    "title": "Table of contents",
    "content": ". | Action file Webhook properties | Request body schema | . A Webhook is a Hook type that sends an HTTP POST request to the configured URL. Any non 2XX response by the responding endpoint will fail the Hook, cancel the execution of the following Hooks under the same Action. For pre-* hooks, the triggering operation will also be aborted. Warning: You should not use pre-* webhooks for long-running tasks, since they block the performed operation. Moreover, the branch is locked during the execution of pre-* hooks, so the webhook server cannot perform any write operations on the branch (like uploading or commits). ",
    "url": "/v0.98/hooks/webhooks.html#table-of-contents",
    
    "relUrl": "/hooks/webhooks.html#table-of-contents"
  },"400": {
    "doc": "Webhooks",
    "title": "Action file Webhook properties",
    "content": "| Property | Description | Data Type | Required | Default Value | Env Vars Support | . | url | The URL address of the request | String | true |   | no | . | timeout | Time to wait for response before failing the hook | String (golang’s Duration representation) | false | 1 minute | no | . | query_params | List of query params that will be added to the request | Dictionary(String:String or String:List(String) | false |   | yes | . | headers | Headers to add to the request | Dictionary(String:String) | false |   | yes | . Secrets &amp; Environment Variables lakeFS Actions supports secrets by using environment variables. The format {{ ENV.SOME_ENV_VAR }} will be replaced with the value of $SOME_ENV_VAR during the execution of the action. If that environment variable doesn’t exist in the lakeFS server environment, the action run will fail. Example: ... hooks: - id: prevent_user_columns type: webhook description: Ensure no user_* columns under public/ properties: url: \"http://&lt;host:port&gt;/webhooks/schema\" timeout: 1m30s query_params: disallow: [\"user_\", \"private_\"] prefix: public/ headers: secret_header: \"{{ ENV.MY_SECRET }}\" ... ",
    "url": "/v0.98/hooks/webhooks.html#action-file-webhook-properties",
    
    "relUrl": "/hooks/webhooks.html#action-file-webhook-properties"
  },"401": {
    "doc": "Webhooks",
    "title": "Request body schema",
    "content": "Upon execution, a webhook will send a request containing a JSON object with the following fields: . | Field | Description | Type | . | event_type | Type of the event that triggered the Action | string | . | event_time | Time of the event that triggered the Action (RFC3339 formatted) | string | . | action_name | Containing Hook Action’s Name | string | . | hook_id | ID of the Hook | string | . | repository_id | ID of the Repository | string | . | branch_id1 | ID of the Branch | string | . | source_ref | Reference to the source on which the event was triggered | string | . | commit_message2 | The message for the commit (or merge) that is taking place | string | . | committer2 | Name of the committer | string | . | commit_metadata2 | The metadata for the commit that is taking place | string | . | tag_id3 | The ID of the created/deleted tag | string | . Example: . { \"event_type\": \"pre-merge\", \"event_time\": \"2021-02-28T14:03:31Z\", \"action_name\": \"test action\", \"hook_id\": \"prevent_user_columns\", \"repository_id\": \"repo1\", \"branch_id\": \"feature-1\", \"source_ref\": \"feature-1\", \"commit_message\": \"merge commit message\", \"committer\": \"committer\", \"commit_metadata\": { \"key\": \"value\" } } . | N\\A for Tag events &#8617; . | N\\A for Tag and Create/Delete Branch events &#8617; &#8617;2 &#8617;3 . | Applicable only for Tag events &#8617; . | . ",
    "url": "/v0.98/hooks/webhooks.html#request-body-schema",
    
    "relUrl": "/hooks/webhooks.html#request-body-schema"
  }
}
